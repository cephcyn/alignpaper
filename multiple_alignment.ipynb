{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model source: https://code.google.com/archive/p/word2vec/\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'model/GoogleNews-vectors-negative300.bin', \n",
    "    binary=True)\n",
    "cached_word2vec_phrases = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word2vec embedding of a phrase\n",
    "def get_phrase_embed_word2vec(word2vec, phrase):\n",
    "    try:\n",
    "        phraseS = phrase.split()\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    unknowns = []\n",
    "    emb = []\n",
    "    for w in phraseS:\n",
    "        try:\n",
    "            emb.append(word2vec[w])\n",
    "        except:\n",
    "            unknowns.append(w)\n",
    "    if len(emb) == 0:\n",
    "        return pd.DataFrame()\n",
    "    emb_sum = pd.DataFrame(emb).sum() / len(phraseS)\n",
    "    emb_sum['word'] = phrase\n",
    "    return pd.DataFrame([emb_sum])\n",
    "\n",
    "get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'test sentence')\n",
    "# get_phrase_embed_word2vec(\n",
    "#     word2vec, \n",
    "#     'This is a test sentence !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import sample dataset\n",
    "(The code to construct the file `temp/ebm-pio_consegments.hdf` is in analyze.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data we've already constructed out of constituency parse of specific phrases in specific sentences\n",
    "con_segments = pd.read_hdf(f'temp/ebm-pio_consegments.hdf','mydata')\n",
    "con_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform that data into the format that is more readable for alignment\n",
    "# (sorry, this is sort of an abuse of DataFrame datatypes)\n",
    "\n",
    "def transformTuples(row):\n",
    "    # turn each row into the segment tuples used for alignment\n",
    "    output = pd.DataFrame()\n",
    "    for i in range(len(row['alignsegments'])):\n",
    "        output[f'txt{i}'] = [(row['alignsegments'][i], row['aligntypes'][i], row['alignctypes'][i])]\n",
    "    return output.set_index(pd.Series([row.name]))\n",
    "\n",
    "transformTuples(con_segments.loc[7298])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_df = con_segments.groupby(con_segments.index, group_keys=False).apply(\n",
    "    lambda group: transformTuples(group.iloc[0]))\n",
    "alignment_df = alignment_df.applymap(lambda x: ('', '', []) if x is np.nan else x)\n",
    "alignment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment operations / transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = con_segments\n",
    "\n",
    "def mergeAdjacentNP(row):\n",
    "    # merge adjacent noun phrases\n",
    "    # TODO adapt this for alignment df format!!!\n",
    "    for i in reversed(range(len(row['alignsegments'])-1)):\n",
    "        if row['aligntypes'][i]=='NP' and row['aligntypes'][i+1]=='NP':\n",
    "            row['alignsegments'][i] += ' ' + row['alignsegments'][i+1]\n",
    "            row['alignsegments'][i+1] = []\n",
    "            row['aligntypes'][i+1] = []\n",
    "            row['alignctypes'][i] += row['alignctypes'][i+1]\n",
    "            row['alignctypes'][i+1] = []\n",
    "    row = row.drop('aligntup')\n",
    "    row['alignsegments'] = [e for e in row['alignsegments'] if e != []]\n",
    "    row['aligntypes'] = [e for e in row['aligntypes'] if e != []]\n",
    "    row['alignctypes'] = [e for e in row['alignctypes'] if e != []]\n",
    "    return row\n",
    "\n",
    "temp_df = temp_df.apply(\n",
    "    lambda row: mergeAdjacentNP(row), \n",
    "    axis=1, result_type='expand')\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this is still buggy, don't use it\n",
    "temp_df = con_segments\n",
    "\n",
    "def mergeParentheses(row):\n",
    "    # merge parenthetical clauses\n",
    "    numOpenParens = 0\n",
    "    lastStart = -1\n",
    "    mergeSegments = []\n",
    "    for i in range(len(row['alignsegments'])):\n",
    "        for c in [c for c in row['alignsegments'][i] if c in ['(', ')']]:\n",
    "            if c == '(':\n",
    "                numOpenParens += 1\n",
    "                if numOpenParens == 1:\n",
    "                    lastStart = i\n",
    "            else:\n",
    "                numOpenParens -= 1\n",
    "                if numOpenParens == 0 and lastStart != i:\n",
    "                    # close the parentheses\n",
    "                    mergeSegments.append((lastStart, i))\n",
    "    if numOpenParens > 0:\n",
    "        mergeSegments.append((lastStart, len(row['alignsegments'])))\n",
    "    mergeSegments = list(set(mergeSegments))\n",
    "    if mergeSegments != []:\n",
    "        for t in reversed(mergeSegments):\n",
    "            print(row['aligntypes'][t[0]:t[1]+1])\n",
    "            print(row['alignsegments'][t[0]:t[1]+1])\n",
    "        print()\n",
    "    return row\n",
    "\n",
    "temp_df.apply(\n",
    "    lambda row: mergeParentheses(row), \n",
    "    axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTup(data, tup_i=0, is_frame=True):\n",
    "    types = {\n",
    "        'segment': 0,\n",
    "        'type': 1,\n",
    "        'ctype': 2\n",
    "    }\n",
    "    if tup_i in types:\n",
    "        tup_i = types[tup_i]\n",
    "    if is_frame:\n",
    "        return data.applymap(lambda x: x[tup_i])\n",
    "    else:\n",
    "        return data.map(lambda x: x[tup_i])\n",
    "\n",
    "# extractTup(transformTuples(temp_df.loc[7298]), tup_i='segment', is_frame=True)\n",
    "extractTup(alignment_df.loc[[7298]], tup_i='segment', is_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEmptyColumns(align_df):\n",
    "    for c in align_df.columns:\n",
    "        align_df_c = extractTup(align_df.loc[:, c], tup_i='segment', is_frame=False)\n",
    "        if len([e for e in align_df_c if e != '']) == 0:\n",
    "            del align_df[c]\n",
    "    align_df.columns = [f'txt{i}' for i in range(len(align_df.columns))]\n",
    "    return align_df\n",
    "\n",
    "removeEmptyColumns(alignment_df.loc[[7298, 7321]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def alignRowMajorLocal(align_a, align_b, use_types=False, remove_empty_cols=False, debug_print=False):\n",
    "    # An implementation of Smith-Waterman alignment\n",
    "    def removeEmptyColumns(align_df):\n",
    "        for c in align_df.columns:\n",
    "            align_df_c = extractTup(align_df.loc[:, c], tup_i='segment', is_frame=False)\n",
    "            if len([e for e in align_df_c if e.strip() != '']) == 0:\n",
    "                del align_df[c]\n",
    "        align_df.columns = [f'txt{i}' for i in range(len(align_df.columns))]\n",
    "        return align_df\n",
    "    if remove_empty_cols:\n",
    "        align_a = removeEmptyColumns(align_a)\n",
    "        align_b = removeEmptyColumns(align_b)\n",
    "    align_a_segment = extractTup(align_a, tup_i='segment')\n",
    "    align_b_segment = extractTup(align_b, tup_i='segment')\n",
    "    align_a_type = extractTup(align_a, tup_i='type')\n",
    "    align_b_type = extractTup(align_b, tup_i='type')\n",
    "    align_a_ctype = extractTup(align_a, tup_i='ctype')\n",
    "    align_b_ctype = extractTup(align_b, tup_i='ctype')\n",
    "    # If we are aligning purely on NP elements... not implemented currently.\n",
    "#     align_a_elems = [i for i in range(len(align_a.columns)) \n",
    "#                      if 'NP' in set(align_a_type[align_a.columns[i]])]\n",
    "#     align_b_elems = [i for i in range(len(align_b.columns)) \n",
    "#                      if 'NP' in set(align_b_type[align_a.columns[i]])]\n",
    "    # If we are doing a general alignment\n",
    "    align_a_elems = [i for i in range(len(align_a.columns))]\n",
    "    align_b_elems = [i for i in range(len(align_b.columns))]\n",
    "    if debug_print:\n",
    "        print(align_a_elems)\n",
    "        print(align_b_elems)\n",
    "        print()\n",
    "    def getScoreAligningIndices(index_a, index_b):\n",
    "        # A higher score is better / more match!\n",
    "        # make sure all the segment texts are precomputed lol\n",
    "        text_a = list(align_a_segment[align_a.columns[index_a]])\n",
    "        text_b = list(align_b_segment[align_b.columns[index_b]])\n",
    "        for text in text_a+text_b:\n",
    "            if text not in cached_word2vec_phrases:\n",
    "                try:\n",
    "                    cached_word2vec_phrases[text] = get_phrase_embed_word2vec(word2vec, text).drop('word', 1)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        # start off with phrase embedding distance (current max is 60 for perfect match)\n",
    "        # if we have embeds for any word in each set, ignore others and just use words we have embeds for\n",
    "        if any(s in cached_word2vec_phrases for s in text_a)\\\n",
    "                and any(s in cached_word2vec_phrases for s in text_b):\n",
    "            # calculate overall embeds\n",
    "            embed_a = pd.concat([cached_word2vec_phrases[text] for text \n",
    "                                 in text_a if text in cached_word2vec_phrases]).apply(lambda x: x.mean())\n",
    "            embed_b = pd.concat([cached_word2vec_phrases[text] for text \n",
    "                                 in text_b if text in cached_word2vec_phrases]).apply(lambda x: x.mean())\n",
    "            # TODO can tweak this scoring calculation a little for performance\n",
    "            score = 10 * (6 - np.linalg.norm(embed_a-embed_b))\n",
    "        else:\n",
    "            # use levenshtein dist as fallback... if either set has NO words with embeds available\n",
    "            scaled_edits_sum = 0\n",
    "            for phrase_a in [p for p in text_a if len(p) != 0]:\n",
    "                for phrase_b in [p for p in text_b if len(p) != 0]:\n",
    "                    scaled_edits_sum += edit_distance(phrase_a,phrase_b) / max(len(phrase_a), len(phrase_b))\n",
    "            score = 60 * (1 - (scaled_edits_sum / (len(text_a) * len(text_b))))\n",
    "        # add a component based on phrase type if flag is set to true (by default it is)\n",
    "        # TODO improve this?; this currently just returns -inf if mismatch of type sets\n",
    "        # Might want to add support for aligning different types of phrase together...\n",
    "        if use_types:\n",
    "            types_a = set([t for t in align_a_type[align_a.columns[index_a]] if t != ''])\n",
    "            types_b = set([t for t in align_b_type[align_b.columns[index_b]] if t != ''])\n",
    "            if len(types_a) != 0 and len(types_b) != 0 and types_a != types_b:\n",
    "                score = -1 * math.inf\n",
    "        # TODO: add a component based on phrase ctype (phrase POS breakdown) (?)\n",
    "        if debug_print:\n",
    "            print(f'scoring between '\n",
    "                  +f'\"{list(align_a_segment[align_a.columns[index_a]])}\" and '\n",
    "                  +f'\"{list(align_b_segment[align_b.columns[index_b]])}\": {score}')\n",
    "        return score\n",
    "    def getGapPenalty(length):\n",
    "        return -1 * (1 * min(length,1) + 0.5 * max(length-1,0))\n",
    "    # Build score matrix of size (a-alignables + 1)x(b-alignables + 1)\n",
    "    scores = np.zeros((len(align_a_elems)+1, len(align_b_elems)+1))\n",
    "    # Build traceback matrix\n",
    "    # traceback = 0 for end, 4 for W, 7 for NW, 9 for N (to calculate traceback, t%2 is N-ness, t%3 is W-ness)\n",
    "    traceback = np.zeros((len(align_a_elems)+1, len(align_b_elems)+1))\n",
    "    # Iterate through all of the cells to populate both the score and traceback matrices\n",
    "    for i in range(1, scores.shape[0]):\n",
    "        for j in range(1, scores.shape[1]):\n",
    "            score_map = {}\n",
    "            # calculate score for aligning nouns a[i] and b[j]\n",
    "            score_map[\n",
    "                scores[i-1,j-1] + getScoreAligningIndices(align_a_elems[i-1], align_b_elems[j-1])\n",
    "            ] = 7\n",
    "            # calculate score for gap in i\n",
    "            for i_gap in range(1, i):\n",
    "                igap_score = scores[i-i_gap,j] + getGapPenalty(i_gap)\n",
    "                score_map[igap_score] = 9\n",
    "            # calculate score for gap in j\n",
    "            for j_gap in range(1, j):\n",
    "                jgap_score = scores[i,j-j_gap] + getGapPenalty(j_gap)\n",
    "                score_map[jgap_score] = 4\n",
    "            # add the possibility for unrelatedness\n",
    "            score_map[0] = 0\n",
    "            if debug_print:\n",
    "                print(score_map)\n",
    "            scores[i,j] = max(score_map.keys())\n",
    "            traceback[i,j] = score_map[max(score_map.keys())]\n",
    "    if debug_print:\n",
    "        print()\n",
    "        print(scores)\n",
    "        print(traceback)\n",
    "        print()\n",
    "    # Do traceback to build our final alignment\n",
    "    tracepoint = np.unravel_index(np.argmax(scores, axis=None), scores.shape)\n",
    "    points_a = []\n",
    "    points_b = []\n",
    "    while traceback[tracepoint] != 0:\n",
    "        # contribute to the align information\n",
    "        if traceback[tracepoint] == 7:\n",
    "            # this is a point where two elements were aligned\n",
    "            points_a.append(align_a_elems[tracepoint[0]-1])\n",
    "            points_b.append(align_b_elems[tracepoint[1]-1])\n",
    "        elif traceback[tracepoint] == 4:\n",
    "            # this is a point where there was a gap inserted for row_a\n",
    "            points_a.append(-1)\n",
    "            points_b.append(align_b_elems[tracepoint[1]-1])\n",
    "        elif traceback[tracepoint] == 9:\n",
    "            # this is a point where there was a gap inserted for row_b\n",
    "            points_a.append(align_a_elems[tracepoint[0]-1])\n",
    "            points_b.append(-1)\n",
    "        # step backwards\n",
    "        tracepoint = (\n",
    "            tracepoint[0] - int(traceback[tracepoint] % 2),\n",
    "            tracepoint[1] - int(traceback[tracepoint] % 3))\n",
    "    points_a = list(reversed(points_a))\n",
    "    points_b = list(reversed(points_b))\n",
    "    if len(points_a) != len(points_b):\n",
    "        # enforce that align_a and align_b are the same length (they should be)\n",
    "        raise ValueError('should not occur; bug in S-W local alignment?')\n",
    "    if debug_print:\n",
    "        print(points_a)\n",
    "        print(points_b)\n",
    "        print()\n",
    "    # Create a nice neat form of this alignment\n",
    "    # TODO add support for NP-only alignment gaps?\n",
    "    range_a = [i for i in points_a if i >= 0]\n",
    "    range_b = [i for i in points_b if i >= 0]\n",
    "    range_a = (range_a[0], range_a[-1])\n",
    "    range_b = (range_b[0], range_b[-1])\n",
    "    output = pd.DataFrame(columns=[f'txt{i}' for i in range(\n",
    "        (range_a[0] + range_b[0]) + len(points_a)\n",
    "        + max(0, (len(align_a.columns) - range_a[1]) - 1) \n",
    "        + max(0, (len(align_b.columns) - range_b[1]) - 1)\n",
    "    )])\n",
    "    # build the segment from align_a\n",
    "    realign_a = align_a.loc[:, [f'txt{i}' for i in range(range_a[0])]]\n",
    "    for i in range(range_b[0]):\n",
    "        realign_a.insert(len(realign_a.columns), f'insx{i}', np.nan, True)\n",
    "    for i in points_a:\n",
    "        if i >= 0:\n",
    "            realign_a[align_a.columns[i]] = align_a.loc[:, align_a.columns[i]]\n",
    "        else:\n",
    "            realign_a.insert(len(realign_a.columns), f'ins{len(realign_a.columns)}', np.nan, True)\n",
    "    for i in range(range_a[1]+1, len(align_a.columns)):\n",
    "        realign_a[align_a.columns[i]] = align_a.loc[:, align_a.columns[i]]\n",
    "    for i in range(range_b[1]+1, len(align_b.columns)):\n",
    "        realign_a.insert(len(realign_a.columns), f'insx{i+range_b[0]}', np.nan, True)\n",
    "    # build the segment from align_b\n",
    "    realign_b = align_b.loc[:, [f'txt{i}' for i in range(range_b[0])]]\n",
    "    for i in range(range_a[0]):\n",
    "        realign_b.insert(0, f'insx{i}', np.nan, True)\n",
    "    for i in points_b:\n",
    "        if i >= 0:\n",
    "            realign_b[align_b.columns[i]] = align_b.loc[:, align_b.columns[i]]\n",
    "        else:\n",
    "            realign_b.insert(len(realign_b.columns), f'ins{len(realign_b.columns)}', np.nan, True)\n",
    "    for i in range(range_a[1]+1, len(align_a.columns)):\n",
    "        realign_b.insert(len(realign_b.columns), f'insx{i+range_a[0]}', np.nan, True)\n",
    "    for i in range(range_b[1]+1, len(align_b.columns)):\n",
    "        realign_b[align_b.columns[i]] = align_b.loc[:, align_b.columns[i]]\n",
    "    # build final output\n",
    "    realign_a.columns = output.columns\n",
    "    realign_b.columns = output.columns\n",
    "    output = output.append(realign_a)\n",
    "    output = output.append(realign_b)\n",
    "    return output.applymap(lambda x: ('', '', []) if x is np.nan else x)\n",
    "\n",
    "toy_align = alignRowMajorLocal(transformTuples(temp_df.loc[7298]), transformTuples(temp_df.loc[7321]))\n",
    "toy_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_align = alignRowMajorLocal(alignment_df.loc[[7298]], alignment_df.loc[[7321]], remove_empty_cols=True)\n",
    "toy_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the toy data\n",
    "toy_data = pd.DataFrame(\n",
    "    ['Asperger syndrome', \n",
    "     'high - functioning ASD', \n",
    "     'unrecognized and untreated anxiety', \n",
    "     'generalized anxiety disorders', \n",
    "     'anxiety', \n",
    "     'high - functioning autism spectrum disorders and anxiety', \n",
    "     'high - functioning ASD and anxiety', \n",
    "     'high - functioning ASD', \n",
    "     'high - functioning autism spectrum disorders', \n",
    "     'previously undetected anxiety', \n",
    "     'untreated anxiety']\n",
    ").rename({0: 'txt'}, axis=1)\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column split step 1: Build word tree with node = word units running right->left\n",
    "\n",
    "# add text to the given trienode\n",
    "def wordTreeHelper(tree_node, text, id_data=None, right_align=False):\n",
    "    text = text.strip()\n",
    "    # Check for base case\n",
    "    if text == '':\n",
    "        tree_node[id_data] = id_data\n",
    "        return tree_node\n",
    "    # Select the right key (for now, just pick the key based on right-to-left ordering)\n",
    "    key = ''\n",
    "    if right_align:\n",
    "        key = text.split(' ')[-1]\n",
    "        text = ' '.join(text.split(' ')[0:-1])\n",
    "    else:\n",
    "        key = text.split(' ')[0]\n",
    "        text = ' '.join(text.split(' ')[1:])\n",
    "    # Put the key and text into the trie\n",
    "    if key not in tree_node:\n",
    "        tree_node[key] = {}\n",
    "    tree_node[key] = wordTreeHelper(tree_node[key], text, id_data=id_data, right_align=right_align)\n",
    "    return tree_node\n",
    "\n",
    "def wordTree(df, colname, right_align=False):\n",
    "    tree = {}\n",
    "    for e_id in df.index:\n",
    "        tree = wordTreeHelper(tree, df.loc[e_id][colname], id_data=e_id, right_align=right_align)\n",
    "    return tree\n",
    "\n",
    "st = wordTree(toy_data, 'txt')\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column split step 2: Collapse the suffix trie (merge nodes with only one child)\n",
    "\n",
    "# edits the input trie\n",
    "def wordTreeCollapse(tree, right_align=False):\n",
    "    # Collapse children nodes first\n",
    "    added_keys = {}\n",
    "    removed_keys = []\n",
    "    for child in tree:\n",
    "        if type(child) is str:\n",
    "            tree[child] = wordTreeCollapse(tree[child], right_align=right_align)\n",
    "            # Check if the new child node is collapsible\n",
    "            if len(tree[child]) == 1 and type(list(tree[child])[0]) is str:\n",
    "                grandchild = list(tree[child])[0]\n",
    "                grandchild_tree = tree[child][grandchild]\n",
    "                # Perform the merge (put into edit queue)\n",
    "                removed_keys.append(child)\n",
    "                if right_align:\n",
    "                    added_keys[grandchild + ' ' + child] = grandchild_tree\n",
    "                else:\n",
    "                    added_keys[child + ' ' + grandchild] = grandchild_tree\n",
    "    # Perform removals\n",
    "    for key in removed_keys:\n",
    "        tree.pop(key)\n",
    "    # Perform additions\n",
    "    for key in added_keys:\n",
    "        tree[key] = added_keys[key]\n",
    "    return tree\n",
    "\n",
    "st = wordTreeCollapse(st)\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column split step 3: Output the suffix trie to multiple columns\n",
    "\n",
    "# Calculate how many output columns we'll need\n",
    "# Get the depth of the trie (a trie with one terminal node {0:0} has depth 0)\n",
    "def wordTreeDepth(tree):\n",
    "    max_depth = 0\n",
    "    for child in tree:\n",
    "        if type(child) is str:\n",
    "            max_depth = max(max_depth, 1 + wordTreeDepth(tree[child]))\n",
    "    return max_depth\n",
    "\n",
    "def wordTreeSplitHelper(tree, max_depth, output, so_far=[], right_align=False):\n",
    "    for child in tree:\n",
    "        if type(child) is not str:\n",
    "            # we have hit a base, put in an entry\n",
    "            if right_align:\n",
    "                output[child] = ['']*(max_depth - len(so_far)) + so_far\n",
    "            else:\n",
    "                output[child] = so_far + ['']*(max_depth - len(so_far))\n",
    "        else:\n",
    "            # this node has further children!\n",
    "            if right_align:\n",
    "                output = wordTreeSplitHelper(tree[child], \n",
    "                                             max_depth, \n",
    "                                             output, \n",
    "                                             [child] + so_far, \n",
    "                                             right_align=right_align)\n",
    "            else:\n",
    "                output = wordTreeSplitHelper(tree[child], \n",
    "                                             max_depth, \n",
    "                                             output, \n",
    "                                             so_far + [child], \n",
    "                                             right_align=right_align)\n",
    "    return output\n",
    "\n",
    "def wordTreeSplit(tree, colname, right_align=False):\n",
    "    tree_depth = wordTreeDepth(tree)\n",
    "    output = pd.DataFrame(columns=[f'{colname}{i}' for i in range(tree_depth)])\n",
    "    rearranged = wordTreeSplitHelper(tree, tree_depth, {}, right_align=right_align)\n",
    "    for id in rearranged:\n",
    "        output.loc[id] = rearranged[id]\n",
    "    return output\n",
    "    \n",
    "wordTreeSplit(st, 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitCol(src_alignment, split_col, right_align):\n",
    "    # TODO make splitCol partially preserve POS information?\n",
    "    splitted = wordTreeSplit(\n",
    "        wordTreeCollapse(wordTree(\n",
    "            src_alignment,\n",
    "            split_col, \n",
    "            right_align=right_align), right_align=right_align),\n",
    "        f'{split_col}.', \n",
    "        right_align=right_align)\n",
    "    result = src_alignment.join(splitted)\n",
    "    result = result.drop(split_col, 1)\n",
    "    result = result.reindex(\n",
    "        columns=[x for _,x in sorted(zip(\n",
    "            [float(c[3:]) for c in result.columns],\n",
    "            result.columns))]\n",
    "    )\n",
    "    result.columns = [f'txt{i}' for i in range(len(result.columns))]\n",
    "    return result\n",
    "\n",
    "toy_align = splitCol(extractTup(toy_align, tup_i='segment'), 'txt0', right_align=True)\n",
    "toy_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO is it worth it to apply the old POS tags again, or can those just be discarded at this point?\n",
    "\n",
    "def applyEmptyTup(row):\n",
    "    output = pd.DataFrame()\n",
    "    for i in row.index:\n",
    "        output[i] = [(row[i], '', [])]\n",
    "    return output.set_index(pd.Series([row.name]))\n",
    "\n",
    "# applyEmptyTup(toy_align.loc[7298])\n",
    "\n",
    "toy_align = toy_align.groupby(toy_align.index, group_keys=False).apply(\n",
    "    lambda group: applyEmptyTup(group.iloc[0])\n",
    ")\n",
    "toy_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeCol(src_alignment, merge_col):\n",
    "    # TODO make mergeCol partially preserve POS information?\n",
    "    merge_col_next = src_alignment.columns[list(src_alignment.columns).index(merge_col)+1]\n",
    "    merged = src_alignment[merge_col] + ' ' + src_alignment[merge_col_next]\n",
    "    result = src_alignment.copy()\n",
    "    result[merge_col] = merged\n",
    "    del result[merge_col_next]\n",
    "    result.columns = [f'txt{i}' for i in range(len(result.columns))]\n",
    "    return result\n",
    "\n",
    "mergeCol(extractTup(toy_align, tup_i='segment'), 'txt0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [7298, 7321, 5126, 5134, 4594, 4618, 6507, 6474, 7308, 5130, 2552]:\n",
    "    print(i, temp_df[temp_df.index==i].iloc[0]['alignsegments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment in manually selected \"nice\" order with types enforced\n",
    "temp_align = []\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[5126]], alignment_df.loc[[5134]],\n",
    "                                    remove_empty_cols=True, use_types=True))\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[7298]], alignment_df.loc[[7321]],\n",
    "                                    remove_empty_cols=True, use_types=True))\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[4594]], alignment_df.loc[[4618]],\n",
    "                                    remove_empty_cols=True, use_types=True))\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[5130]], alignment_df.loc[[2552]],\n",
    "                                    remove_empty_cols=True, use_types=True))\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[6474]], alignment_df.loc[[7308]],\n",
    "                                    remove_empty_cols=True, use_types=True))\n",
    "update_temp_align = []\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[2], temp_align[3], \n",
    "                                            remove_empty_cols=True, use_types=True))\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[1], temp_align[4], \n",
    "                                            remove_empty_cols=True, use_types=True))\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[0], alignment_df.loc[[6507]], \n",
    "                                            remove_empty_cols=True, use_types=True))\n",
    "temp_align = update_temp_align\n",
    "update_temp_align = []\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[0], temp_align[2], \n",
    "                                            remove_empty_cols=True, use_types=True))\n",
    "update_temp_align[0] = alignRowMajorLocal(update_temp_align[0], temp_align[1], \n",
    "                                          remove_empty_cols=True, use_types=True)\n",
    "manually_aligned_group = update_temp_align[0]\n",
    "extractTup(manually_aligned_group, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment in manually selected \"nice\" order without types enforced\n",
    "temp_align = []\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[5126]], alignment_df.loc[[5134]], \n",
    "                                     remove_empty_cols=True))\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[7298]], alignment_df.loc[[7321]], \n",
    "                                     remove_empty_cols=True))\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[4594]], alignment_df.loc[[4618]], \n",
    "                                     remove_empty_cols=True))\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[5130]], alignment_df.loc[[2552]], \n",
    "                                     remove_empty_cols=True))\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[6474]], alignment_df.loc[[7308]], \n",
    "                                     remove_empty_cols=True))\n",
    "update_temp_align = []\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[2], temp_align[3]))\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[1], temp_align[4]))\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[0], alignment_df.loc[[6507]], \n",
    "                                            remove_empty_cols=True))\n",
    "temp_align = update_temp_align\n",
    "update_temp_align = []\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[0], temp_align[2]))\n",
    "update_temp_align[0] = alignRowMajorLocal(update_temp_align[0], temp_align[1])\n",
    "manually_aligned_group = update_temp_align[0]\n",
    "extractTup(manually_aligned_group, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment in random-ish order without types enforced\n",
    "temp_align = alignRowMajorLocal(alignment_df.loc[[7298]], alignment_df.loc[[7321]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[5126]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[5134]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[4594]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[4618]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[6507]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[6474]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[7308]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[5130]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[2552]], remove_empty_cols=True)\n",
    "manually_aligned_group2 = temp_align\n",
    "extractTup(manually_aligned_group2, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment in random-ish order without types enforced (of a slightly different dataset!!!)\n",
    "temp_align = alignRowMajorLocal(alignment_df.loc[[7494]], alignment_df.loc[[7541]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[7549]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[7585]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[7594]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[416]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[423]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[443]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[447]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[409]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[1960]], remove_empty_cols=True)\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[1989]], remove_empty_cols=True)\n",
    "manually_aligned_group2 = temp_align\n",
    "extractTup(manually_aligned_group2, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate a merge operation\n",
    "manually_aligned_group_merge = mergeCol(extractTup(manually_aligned_group, tup_i='segment'), 'txt4')\n",
    "manually_aligned_group_merge = mergeCol(manually_aligned_group_merge, 'txt4')\n",
    "manually_aligned_group_merge = mergeCol(manually_aligned_group_merge, 'txt1')\n",
    "manually_aligned_group_merge = mergeCol(manually_aligned_group_merge, 'txt4')\n",
    "manually_aligned_group_merge.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate a split operation\n",
    "manually_aligned_group_split = splitCol(manually_aligned_group_merge, 'txt0', right_align=True)\n",
    "manually_aligned_group_split = splitCol(manually_aligned_group_split, 'txt4', right_align=False)\n",
    "manually_aligned_group_split = splitCol(manually_aligned_group_split, 'txt5', right_align=True)\n",
    "manually_aligned_group_split = splitCol(manually_aligned_group_split, 'txt7', right_align=False)\n",
    "manually_aligned_group_split.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the split operation output into something more standard alignment DF format\n",
    "manually_aligned_group_split = manually_aligned_group_split.groupby(\n",
    "    manually_aligned_group_split.index, group_keys=False).apply(\n",
    "    lambda group: applyEmptyTup(group.iloc[0])\n",
    ")\n",
    "manually_aligned_group_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-align the output of the split function using the same ordering as initial alignment\n",
    "manually_aligned_group_realign = []\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[5126]],\n",
    "                       manually_aligned_group_split.loc[[5134]],\n",
    "                       remove_empty_cols=True))\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[7298]],\n",
    "                       manually_aligned_group_split.loc[[7321]],\n",
    "                       remove_empty_cols=True))\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[4594]],\n",
    "                       manually_aligned_group_split.loc[[4618]],\n",
    "                       remove_empty_cols=True))\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[5130]],\n",
    "                       manually_aligned_group_split.loc[[2552]],\n",
    "                       remove_empty_cols=True))\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[6474]],\n",
    "                       manually_aligned_group_split.loc[[7308]],\n",
    "                       remove_empty_cols=True))\n",
    "update_manually_aligned_group_realign = []\n",
    "update_manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_realign[2],\n",
    "                       manually_aligned_group_realign[3],\n",
    "                       remove_empty_cols=True))\n",
    "update_manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_realign[1],\n",
    "                       manually_aligned_group_realign[4],\n",
    "                       remove_empty_cols=True))\n",
    "update_manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_realign[0],\n",
    "                       manually_aligned_group_split.loc[[6507]],\n",
    "                       remove_empty_cols=True))\n",
    "manually_aligned_group_realign = update_manually_aligned_group_realign\n",
    "update_manually_aligned_group_realign = []\n",
    "update_manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_realign[0],\n",
    "                       manually_aligned_group_realign[2],\n",
    "                       remove_empty_cols=True))\n",
    "update_manually_aligned_group_realign[0] = alignRowMajorLocal(update_manually_aligned_group_realign[0],\n",
    "                                                              manually_aligned_group_realign[1],\n",
    "                                                              remove_empty_cols=True)\n",
    "manually_aligned_group_realign = update_manually_aligned_group_realign[0]\n",
    "extractTup(manually_aligned_group_realign, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-align the output of the split function by a manual ordering\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    manually_aligned_group_split.loc[[7298]],\n",
    "    manually_aligned_group_split.loc[[7321]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5126]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[7308]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[6474]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[4594]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[4618]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5130]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5134]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[2552]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[6507]],\n",
    "    remove_empty_cols=True)\n",
    "extractTup(update_manually_aligned_group_realign, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-align the output of the split function using a DIFFERENT manual ordering\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    manually_aligned_group_split.loc[[2552]],\n",
    "    manually_aligned_group_split.loc[[4618]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[4594]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5130]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[7298]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[7308]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[7321]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[6474]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5134]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[6507]],\n",
    "    remove_empty_cols=True)\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5126]],\n",
    "    remove_empty_cols=True)\n",
    "extractTup(update_manually_aligned_group_realign, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractTup(update_manually_aligned_group_realign, tup_i='segment').loc[\n",
    "#     [2552, 4594, 4618, 5126, 5130, 5134, 6474, 6507, 7298, 7308, 7321]\n",
    "# ]\n",
    "extractTup(update_manually_aligned_group_realign, tup_i='segment').loc[\n",
    "    [2552, 4594, 4618, 5130, 6474, 7298, 7308, 7321, 5126, 5134, 6507]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
