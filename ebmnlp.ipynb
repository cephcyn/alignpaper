{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sentence tokenization\n",
    "from nltk import tokenize\n",
    "import pandas as pd\n",
    "\n",
    "def read_paper(pid, section, source):\n",
    "    with open(f'data/ebm_nlp_2_00/documents/{pid}.txt', 'r') as f:\n",
    "        alltext = ''.join(f.readlines())\n",
    "        sentences = tokenize.sent_tokenize(alltext)\n",
    "        return pd.DataFrame({\n",
    "            'URL': [f'https://pubmed.ncbi.nlm.nih.gov/{pid}'] * (2),\n",
    "            'ID': [pid] * (2),\n",
    "            'Type': ['Title', 'Abstract'],\n",
    "            'Text': [sentences[0], ' '.join(sentences[1:])],\n",
    "            'ann_section': [section] * (2),\n",
    "            'ann_source': [source] * (2)\n",
    "        })\n",
    "\n",
    "read_paper(6989377, 'test', 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the entire list of paper abstracts we have\n",
    "import os\n",
    "\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/hierarchical_labels/interventions/test/crowd'\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/participants/test/crowd'\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/outcomes/test/crowd'\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/train/crowd'\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/test/crowd'\n",
    "directory = f'data/ebm_nlp_2_00/annotations/aggregated/hierarchical_labels/interventions/test/gold'\n",
    "df = None\n",
    "\n",
    "for filename in os.scandir(directory):\n",
    "    # assume that the filepath does not contain .s except for the final extension!\n",
    "#     print(filename.path)\n",
    "#     print(filename.name.split('.')[0])\n",
    "    df_file = read_paper(filename.name.split('.')[0], 'test', 'gold')\n",
    "    # and append it to the summary dataframe\n",
    "    if df is None:\n",
    "        df = df_file\n",
    "    else:\n",
    "        df = df.append(df_file)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "def sentence_tokenize(group):\n",
    "    row = group.reset_index(drop=True).loc[0]\n",
    "    sentences = tokenize.sent_tokenize(row['Text'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * (len(sentences)),\n",
    "        'ID': [row['ID']] * (len(sentences)),\n",
    "        'Type': [row['Type']] * (len(sentences)),\n",
    "        'Index': list(range(len(sentences))),\n",
    "        'Text': sentences,\n",
    "        'ann_section': ['test'] * (len(sentences)),\n",
    "        'ann_source': ['gold'] * (len(sentences))\n",
    "    })\n",
    "\n",
    "df_sentences = df.groupby(['ID', 'Type', 'ann_section', 'ann_source'], group_keys=False).apply(\n",
    "    lambda group: sentence_tokenize(group)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labeled_terms(tokens, tags):\n",
    "    in_tag = False\n",
    "    hits = []\n",
    "    hit_indices = []\n",
    "    sofar = []\n",
    "    for i in range(len(tokens)):\n",
    "        if int(tags[i]) != 0:\n",
    "            if not in_tag:\n",
    "                in_tag = True\n",
    "            sofar.append(tokens[i])\n",
    "        elif in_tag:\n",
    "            in_tag = False\n",
    "            hits.append(sofar)\n",
    "            hit_indices.append(i - len(sofar))\n",
    "            sofar = []\n",
    "    if in_tag:\n",
    "        hits.append(sofar)\n",
    "        hit_indices.append(len(tokens) - len(sofar))\n",
    "    return hits, hit_indices\n",
    "\n",
    "test_extract_labeled_terms = extract_labeled_terms(\n",
    "    ['This', 'is', 'an', 'example', 'sentence', '.'],\n",
    "    ['1',    '0',  '0',  '1',       '1',        '0']\n",
    ")\n",
    "test_extract_labeled_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_0 is the text literally preceding the anchor\n",
    "# split_1 is the text that the anchor consists of\n",
    "# split_2 is the text literally following the anchor\n",
    "# split_tokens is the list of tokens that split identifies\n",
    "# split_anchor_span is a tuple (anchor_first_token_index, anchor_last_token_index) or null if there is no anchor\n",
    "# split_anchor_indices is a tuple (anchor_start_char_index, anchor_end_char_index) or null if there is no anchor\n",
    "splitting_headers = ['split_0','split_1','split_2', \n",
    "                     'split_tokens', 'split_anchor_span', 'split_anchor_indices', \n",
    "                     'within_anchor_index']\n",
    "# Use columns ID, Type, Index to join with df_sentences\n",
    "join_headers = ['ID', 'Type', 'Index', 'ann_section', 'ann_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract tokenization, part-of-speech tags from EBMNLP dataset\n",
    "def ebmnlp_addtdata(group):\n",
    "    group = df_sentences\n",
    "    group = group.sort_values(by=['Index'])\n",
    "    group = group.sort_values(by=['Type'], ascending=False)\n",
    "    group = group.reset_index()\n",
    "    id_num = group.iloc[0]['ID']\n",
    "    ann_section = group.iloc[0]['ann_section']\n",
    "    ann_source = group.iloc[0]['ann_source']\n",
    "    # extract split_tokens data from .tokens file\n",
    "    with open(f'data/ebm_nlp_2_00/documents/{id_num}.tokens', 'r') as f:\n",
    "        tokens = [s.strip() for s in f.readlines()]\n",
    "    split_tokens = []\n",
    "    for i in range(len(group.index)):\n",
    "        row = group.iloc[i]\n",
    "        rowtext = row['Text'].strip()\n",
    "        i_tokens = []\n",
    "        while len(tokens) > 0 and rowtext.find(tokens[0]) == 0:\n",
    "            i_tokens.append(tokens[0])\n",
    "            rowtext = rowtext[len(tokens[0]):].strip()\n",
    "            tokens = tokens[1:]\n",
    "        split_tokens.append(i_tokens)\n",
    "    metadata = pd.DataFrame({'split_tokens': split_tokens})\n",
    "    # extract POS data from .pos file\n",
    "    with open(f'data/ebm_nlp_2_00/documents/{id_num}.pos', 'r') as f:\n",
    "        pos = [s.strip() for s in f.readlines()]\n",
    "    pos_labels = []\n",
    "    for i in range(len(group.index)):\n",
    "        sent_tok_len = len(metadata.iloc[i]['split_tokens'])\n",
    "        pos_labels.append(pos[:sent_tok_len])\n",
    "        pos = pos[sent_tok_len:]\n",
    "    metadata['split_tokens_pos'] = pos_labels\n",
    "    # Extract the direct annotations...\n",
    "    for annlevel in [('starting_spans', 'ss'), ('hierarchical_labels', 'hl')]:\n",
    "        for anntype in [('participants', 'p'), ('interventions', 'i'), ('outcomes', 'o')]:\n",
    "            # read an intersection of (ss/hl)*(p/i/o)\n",
    "            # (also get annotation clumps while we're doing that)\n",
    "            with open(f'data/ebm_nlp_2_00/annotations/aggregated/{annlevel[0]}/{anntype[0]}/{ann_section}/{ann_source}/{id_num}.AGGREGATED.ann', 'r') as f:\n",
    "                annotations = [s.strip() for s in f.readlines()]\n",
    "            ann_col = []\n",
    "            ann_col_clumps = []\n",
    "            for i in range(len(group.index)):\n",
    "                sent_tok = metadata.iloc[i]['split_tokens']\n",
    "                sent_tok_len = len(sent_tok)\n",
    "                ann_col.append(annotations[:sent_tok_len])\n",
    "                ann_col_clumps.append(extract_labeled_terms(sent_tok, annotations[:sent_tok_len]))\n",
    "                annotations = annotations[sent_tok_len:]\n",
    "            metadata[f'{annlevel[1]}_{anntype[1]}'] = ann_col\n",
    "            metadata[f'{annlevel[1]}_{anntype[1]}_clumps'] = ann_col_clumps\n",
    "    for c in join_headers:\n",
    "        metadata[c] = group[c]\n",
    "    return metadata\n",
    "\n",
    "df_metadata = df_sentences.groupby(['ID', 'ann_source'], group_keys=False).apply(\n",
    "    lambda group: ebmnlp_addtdata(group)\n",
    ").reset_index(drop=True)\n",
    "df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, I'm just going to pretend that EBM-NLP is directly \"handling\" the issue of splitting\n",
    "# for us, because we can use the P/I/O annotations as splits!\n",
    "\n",
    "def pio_split(group):\n",
    "    row = group.iloc[0]\n",
    "    output = []\n",
    "    for anntype in ['hl_i']: # ['ss_p', 'ss_i', 'ss_o', 'hl_p', 'hl_i', 'hl_o']:\n",
    "        clumps, clumps_loc = row[anntype+'_clumps']\n",
    "        for i in range(len(clumps)):\n",
    "            # set up split_n\n",
    "            output_i = [' '.join(row['split_tokens'][:clumps_loc[i]]), \n",
    "                        ' '.join(clumps[i]), \n",
    "                        ' '.join(row['split_tokens'][clumps_loc[i]+len(clumps[i]):])]\n",
    "            # split_tokens (copy what we already got as input, haha)\n",
    "            output_i.append(row['split_tokens'])\n",
    "            # split_anchor_span\n",
    "            output_i.append((clumps_loc[i], clumps_loc[i]+len(clumps[i])))\n",
    "            # split_anchor_indices\n",
    "            output_i.append((len(output_i[0]), len(output_i[0])+len(output_i[1])))\n",
    "            # within_anchor_index\n",
    "            # TODO - perhaps we could assign an actual anchorindex eventually\n",
    "            # but I'm ignoring that these are known entity names in PIO annotations for now\n",
    "            output_i.append(-1)\n",
    "            # Add join headers\n",
    "            output_i += list(row[join_headers])\n",
    "            output.append(output_i)\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    splits = pd.DataFrame(dict(zip(splitting_headers+join_headers,output_t)))\n",
    "    return splits\n",
    "\n",
    "pio_output = df_metadata.groupby(df_metadata.index, group_keys=False).apply(\n",
    "    lambda group: pio_split(group)\n",
    ").reset_index(drop=True)\n",
    "pio_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the join of df_sentences and df_metadata into something that includes all of the annotations IDed\n",
    "df_sentences.merge(\n",
    "    pio_output,\n",
    "    how='outer',\n",
    "    left_on=join_headers,\n",
    "    right_on=join_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f'temp/ebm-df.pkl')\n",
    "df_sentences.to_pickle(f'temp/ebm-df_sentences.pkl')\n",
    "df_metadata.to_pickle(f'temp/ebm-df_metadata.pkl')\n",
    "pio_output.to_pickle(f'temp/ebm-pio_output.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
