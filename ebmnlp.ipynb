{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sentence tokenization\n",
    "from nltk import tokenize\n",
    "import pandas as pd\n",
    "\n",
    "def read_paper(pid, section, source):\n",
    "    with open(f'data/ebm_nlp_2_00/documents/{pid}.txt', 'r') as f:\n",
    "        alltext = ''.join(f.readlines())\n",
    "        sentences = tokenize.sent_tokenize(alltext)\n",
    "        return pd.DataFrame({\n",
    "            'URL': [f'https://pubmed.ncbi.nlm.nih.gov/{pid}'] * (2),\n",
    "            'ID': [pid] * (2),\n",
    "            'Type': ['Title', 'Abstract'],\n",
    "            'Text': [sentences[0], ' '.join(sentences[1:])],\n",
    "            'ann_section': [section] * (2),\n",
    "            'ann_source': [source] * (2)\n",
    "        })\n",
    "\n",
    "read_paper(6989377, 'test', 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the entire list of paper abstracts we have\n",
    "import os\n",
    "\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/hierarchical_labels/interventions/test/crowd'\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/participants/test/crowd'\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/outcomes/test/crowd'\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/train/crowd'\n",
    "# directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/test/crowd'\n",
    "directory = f'data/ebm_nlp_2_00/annotations/aggregated/hierarchical_labels/interventions/test/gold'\n",
    "df = None\n",
    "\n",
    "for filename in os.scandir(directory):\n",
    "    # assume that the filepath does not contain .s except for the final extension!\n",
    "#     print(filename.path)\n",
    "#     print(filename.name.split('.')[0])\n",
    "    df_file = read_paper(filename.name.split('.')[0], 'test', 'gold')\n",
    "    # and append it to the summary dataframe\n",
    "    if df is None:\n",
    "        df = df_file\n",
    "    else:\n",
    "        df = df.append(df_file)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.to_pickle(f'temp/ebm-df.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "def sentence_tokenize(group):\n",
    "    row = group.reset_index(drop=True).loc[0]\n",
    "    sentences = tokenize.sent_tokenize(row['Text'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * (len(sentences)),\n",
    "        'ID': [row['ID']] * (len(sentences)),\n",
    "        'Type': [row['Type']] * (len(sentences)),\n",
    "        'Index': list(range(len(sentences))),\n",
    "        'Text': sentences,\n",
    "        'ann_section': ['test'] * (len(sentences)),\n",
    "        'ann_source': ['gold'] * (len(sentences))\n",
    "    })\n",
    "\n",
    "df_sentences = df.groupby(['ID', 'Type', 'ann_section', 'ann_source'], group_keys=False).apply(\n",
    "    lambda group: sentence_tokenize(group)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_sentences.to_pickle(f'temp/ebm-df_sentences.pkl')\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labeled_terms(tokens, tags):\n",
    "    in_tag = False\n",
    "    hits = []\n",
    "    hit_indices = []\n",
    "    sofar = []\n",
    "    for i in range(len(tokens)):\n",
    "        if int(tags[i]) != 0:\n",
    "            if not in_tag:\n",
    "                in_tag = True\n",
    "            sofar.append(tokens[i])\n",
    "        elif in_tag:\n",
    "            in_tag = False\n",
    "            hits.append(sofar)\n",
    "            hit_indices.append(i - len(sofar))\n",
    "            sofar = []\n",
    "    if in_tag:\n",
    "        hits.append(sofar)\n",
    "        hit_indices.append(len(tokens) - len(sofar))\n",
    "    return hits, hit_indices\n",
    "\n",
    "test_extract_labeled_terms = extract_labeled_terms(\n",
    "    ['This', 'is', 'an', 'example', 'sentence', '.'],\n",
    "    ['1',    '0',  '0',  '1',       '1',        '0']\n",
    ")\n",
    "test_extract_labeled_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_0 is the text literally preceding the anchor\n",
    "# split_1 is the text that the anchor consists of\n",
    "# split_2 is the text literally following the anchor\n",
    "# split_tokens is the list of tokens that split identifies\n",
    "# split_anchor_span is a tuple (anchor_first_token_index, anchor_last_token_index) or null if there is no anchor\n",
    "# split_anchor_indices is a tuple (anchor_start_char_index, anchor_end_char_index) or null if there is no anchor\n",
    "# split_anchor_type is unique to the EBM dataset - identifies what type of anchor we are using\n",
    "splitting_headers = ['split_0','split_1','split_2', \n",
    "                     'split_tokens', 'split_anchor_span', 'split_anchor_indices', \n",
    "                     'within_anchor_index',\n",
    "                     'split_anchor_type']\n",
    "# Use columns ID, Type, Index to join with df_sentences\n",
    "join_headers = ['ID', 'Type', 'Index', 'ann_section', 'ann_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract tokenization, part-of-speech tags from EBMNLP dataset\n",
    "def ebmnlp_addtdata(group):\n",
    "    group = group.sort_values(by=['Index'])\n",
    "    group = group.sort_values(by=['Type'], ascending=False)\n",
    "    group = group.reset_index()\n",
    "    id_num = group.iloc[0]['ID']\n",
    "    ann_section = group.iloc[0]['ann_section']\n",
    "    ann_source = group.iloc[0]['ann_source']\n",
    "    # extract split_tokens data from .tokens file\n",
    "    with open(f'data/ebm_nlp_2_00/documents/{id_num}.tokens', 'r') as f:\n",
    "        tokens = [s.strip() for s in f.readlines()]\n",
    "    split_tokens = []\n",
    "    for i in range(len(group.index)):\n",
    "        row = group.iloc[i]\n",
    "        rowtext = row['Text'].strip()\n",
    "        i_tokens = []\n",
    "        while len(tokens) > 0 and rowtext.find(tokens[0]) == 0:\n",
    "            i_tokens.append(tokens[0])\n",
    "            rowtext = rowtext[len(tokens[0]):].strip()\n",
    "            tokens = tokens[1:]\n",
    "        split_tokens.append(i_tokens)\n",
    "    metadata = pd.DataFrame({'split_tokens': split_tokens})\n",
    "    # extract POS data from .pos file\n",
    "    with open(f'data/ebm_nlp_2_00/documents/{id_num}.pos', 'r') as f:\n",
    "        pos = [s.strip() for s in f.readlines()]\n",
    "    pos_labels = []\n",
    "    for i in range(len(group.index)):\n",
    "        sent_tok_len = len(metadata.iloc[i]['split_tokens'])\n",
    "        pos_labels.append(pos[:sent_tok_len])\n",
    "        pos = pos[sent_tok_len:]\n",
    "    metadata['split_tokens_pos'] = pos_labels\n",
    "    # Extract the direct annotations...\n",
    "    for annlevel in [('starting_spans', 'ss'), ('hierarchical_labels', 'hl')]:\n",
    "        for anntype in [('participants', 'p'), ('interventions', 'i'), ('outcomes', 'o')]:\n",
    "            # read an intersection of (ss/hl)*(p/i/o)\n",
    "            # (also get annotation clumps while we're doing that)\n",
    "            try:\n",
    "                with open(f'data/ebm_nlp_2_00/annotations/aggregated/{annlevel[0]}/{anntype[0]}/{ann_section}/{ann_source}/{id_num}.AGGREGATED.ann', 'r') as f:\n",
    "                    annotations = [s.strip() for s in f.readlines()]\n",
    "                ann_col = []\n",
    "                ann_col_clumps = []\n",
    "                for i in range(len(group.index)):\n",
    "                    sent_tok = metadata.iloc[i]['split_tokens']\n",
    "                    sent_tok_len = len(sent_tok)\n",
    "                    ann_col.append(annotations[:sent_tok_len])\n",
    "                    ann_col_clumps.append(extract_labeled_terms(sent_tok, annotations[:sent_tok_len]))\n",
    "                    annotations = annotations[sent_tok_len:]\n",
    "                metadata[f'{annlevel[1]}_{anntype[1]}'] = ann_col\n",
    "                metadata[f'{annlevel[1]}_{anntype[1]}_clumps'] = ann_col_clumps\n",
    "            except:\n",
    "                # if the annotation we're looking for doesn't exist, then fill in empty annotations\n",
    "                metadata[f'{annlevel[1]}_{anntype[1]}'] = [([], [])] * (len(group.index))\n",
    "                metadata[f'{annlevel[1]}_{anntype[1]}_clumps'] = [([], [])] * (len(group.index))\n",
    "    for c in join_headers:\n",
    "        metadata[c] = group[c]\n",
    "    return metadata\n",
    "\n",
    "df_metadata = df_sentences.groupby(['ID', 'ann_source'], group_keys=False).apply(\n",
    "    lambda group: ebmnlp_addtdata(group)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_metadata.to_pickle(f'temp/ebm-df_metadata.pkl')\n",
    "df_metadata.iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For now, I'm just going to pretend that EBM-NLP is directly \"handling\" the issue of splitting\n",
    "# for us, because we can use the P/I/O annotations as splits!\n",
    "# ... And if there are no P/I/O annotations, treat it the same as not having a split anchor:\n",
    "# dump the entire sentence into split_0 and keep all of the anchor-related values to be None/empty\n",
    "\n",
    "def pio_split(group):\n",
    "    row = group.iloc[0]\n",
    "    output = []\n",
    "    for anntype in ['ss_p']: # ['ss_p', 'ss_i', 'ss_o', 'hl_p', 'hl_i', 'hl_o']:\n",
    "        clumps, clumps_loc = row[anntype+'_clumps']\n",
    "        for i in range(len(clumps)):\n",
    "            # set up split_n\n",
    "            output_i = [' '.join(row['split_tokens'][:clumps_loc[i]]), \n",
    "                        ' '.join(clumps[i]), \n",
    "                        ' '.join(row['split_tokens'][clumps_loc[i]+len(clumps[i]):])]\n",
    "            # split_tokens (copy what we already got as input, haha)\n",
    "            output_i.append(row['split_tokens'])\n",
    "            # split_anchor_span\n",
    "            output_i.append((clumps_loc[i], clumps_loc[i]+len(clumps[i])))\n",
    "            # split_anchor_indices\n",
    "            output_i.append((len(output_i[0]), len(output_i[0])+len(output_i[1])))\n",
    "            # within_anchor_index\n",
    "            # TODO - perhaps we could assign an actual anchorindex eventually\n",
    "            # but I'm ignoring that there may be known entity names in PIO annotations for now\n",
    "            output_i.append(-1)\n",
    "            # split_anchor_type\n",
    "            output_i.append(anntype)\n",
    "            # Add join headers\n",
    "            output_i += list(row[join_headers])\n",
    "            output.append(output_i)\n",
    "    # If there were no annotations, keep the sentence entirely in split_0\n",
    "    if output == []:\n",
    "        output = [[' '.join(row['split_tokens']),'','',\n",
    "                   row['split_tokens'],None,None,None,None]\n",
    "                  +list(row[join_headers])]\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    splits = pd.DataFrame(dict(zip(splitting_headers+join_headers,output_t)))\n",
    "    return splits\n",
    "\n",
    "pio_output = df_metadata.groupby(df_metadata.index, group_keys=False).apply(\n",
    "    lambda group: pio_split(group)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "pio_output.to_pickle(f'temp/ebm-pio_output.pkl')\n",
    "pio_output.to_csv(f'temp/ebm-pio_output.csv')\n",
    "pio_output.iloc[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the join of df_sentences and df_metadata into something that includes all of the annotations IDed\n",
    "merged = df_sentences.merge(\n",
    "    pio_output,\n",
    "    how='outer',\n",
    "    left_on=join_headers,\n",
    "    right_on=join_headers)\n",
    "merged.iloc[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model source: https://code.google.com/archive/p/word2vec/\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'model/GoogleNews-vectors-negative300.bin', \n",
    "    binary=True)\n",
    "\n",
    "# Get the word2vec embedding of a phrase\n",
    "def get_phrase_embed_word2vec(word2vec, phrase):\n",
    "    try:\n",
    "        phraseS = phrase.split()\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    unknowns = []\n",
    "    emb = []\n",
    "    for w in phraseS:\n",
    "        try:\n",
    "            emb.append(word2vec[w])\n",
    "        except:\n",
    "            unknowns.append(w)\n",
    "    if len(emb) == 0:\n",
    "        return pd.DataFrame()\n",
    "    emb_sum = pd.DataFrame(emb).sum()\n",
    "    emb_sum['word'] = phrase\n",
    "    return pd.DataFrame([emb_sum])\n",
    "\n",
    "v = get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'test sentence')\n",
    "sent_v = get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'This is a test sentence !')\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio_w2v = pio_output.groupby(\n",
    "    pio_output.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: get_phrase_embed_word2vec(\n",
    "        word2vec,\n",
    "        group.iloc[0]['split_1']\n",
    "    )\n",
    ").reset_index(level=1, drop=True)\n",
    "\n",
    "pio_w2v.iloc[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the vectors\n",
    "# loaned from :\n",
    "# https://github.com/cephcyn/ChatlogGrapher/blob/master/data_processing.ipynb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import altair as alt\n",
    "alt.renderers.enable('default')\n",
    "\n",
    "def visualize_embeds(data, reference, color=None, tooltip=['word']):\n",
    "    x = data.iloc[:, 0:300]\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principalDf = pd.DataFrame(\n",
    "        data=principalComponents,\n",
    "        columns=['pc1', 'pc2'])\n",
    "\n",
    "    finalDf = principalDf\n",
    "    finalDf = finalDf.set_index(data.index)\n",
    "    finalDf['word'] = data['word']\n",
    "    finalDf = finalDf.join(\n",
    "        reference, \n",
    "        how='inner',\n",
    "        lsuffix='_embed', \n",
    "        rsuffix='_ref'\n",
    "    )\n",
    "\n",
    "    chart = alt.Chart(finalDf).mark_circle(size=60)\n",
    "    # should figure out a more pythonic way to do this :/\n",
    "    if color is None:\n",
    "        chart = chart.encode(\n",
    "            x='pc1',\n",
    "            y='pc2',\n",
    "            tooltip=tooltip\n",
    "        )\n",
    "    else:\n",
    "        chart = chart.encode(\n",
    "            x='pc1',\n",
    "            y='pc2',\n",
    "            color=color,\n",
    "            tooltip=tooltip\n",
    "        )\n",
    "    return chart.interactive()\n",
    "\n",
    "# visualize_embeds(pd.concat([output_w2v, output_w2v_c]).reset_index())\n",
    "visualize_embeds(pio_w2v, pio_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from\n",
    "# https://gmarti.gitlab.io/ml/2017/09/07/how-to-sort-distance-matrix.html\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "N = len(pio_w2v)\n",
    "data = pio_w2v.iloc[:, 0:300]\n",
    "# data = pio_w2v.iloc[:, 0:1024]\n",
    "# Shuffle data for extra comparison\n",
    "# data = data.sample(frac=1)\n",
    "dist_mat = squareform(pdist(data))\n",
    "\n",
    "# plt.pcolormesh(dist_mat)\n",
    "# plt.colorbar()\n",
    "# plt.xlim([0,N])\n",
    "# plt.ylim([0,N])\n",
    "# plt.show()\n",
    "\n",
    "# Borrowed from\n",
    "# https://gmarti.gitlab.io/ml/2017/09/07/how-to-sort-distance-matrix.html\n",
    "\n",
    "from fastcluster import linkage\n",
    "\n",
    "def seriation(Z,N,cur_index):\n",
    "    '''\n",
    "        input:\n",
    "            - Z is a hierarchical tree (dendrogram)\n",
    "            - N is the number of points given to the clustering process\n",
    "            - cur_index is the position in the tree for the recursive traversal\n",
    "        output:\n",
    "            - order implied by the hierarchical tree Z\n",
    "            \n",
    "        seriation computes the order implied by a hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    if cur_index < N:\n",
    "        return [cur_index]\n",
    "    else:\n",
    "        left = int(Z[cur_index-N,0])\n",
    "        right = int(Z[cur_index-N,1])\n",
    "        return (seriation(Z,N,left) + seriation(Z,N,right))\n",
    "    \n",
    "def compute_serial_matrix(dist_mat,method=\"ward\"):\n",
    "    '''\n",
    "        input:\n",
    "            - dist_mat is a distance matrix\n",
    "            - method = [\"ward\",\"single\",\"average\",\"complete\"]\n",
    "        output:\n",
    "            - seriated_dist is the input dist_mat,\n",
    "              but with re-ordered rows and columns\n",
    "              according to the seriation, i.e. the\n",
    "              order implied by the hierarchical tree\n",
    "            - res_order is the order implied by\n",
    "              the hierarchical tree\n",
    "            - res_linkage is the hierarchical tree (dendrogram)\n",
    "        \n",
    "        compute_serial_matrix transforms a distance matrix into \n",
    "        a sorted distance matrix according to the order implied \n",
    "        by the hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    N = len(dist_mat)\n",
    "    flat_dist_mat = squareform(dist_mat)\n",
    "    res_linkage = linkage(flat_dist_mat, method=method,preserve_input=True)\n",
    "    res_order = seriation(res_linkage, N, N + N-2)\n",
    "    seriated_dist = np.zeros((N,N))\n",
    "    a,b = np.triu_indices(N,k=1)\n",
    "    seriated_dist[a,b] = dist_mat[ [res_order[i] for i in a], [res_order[j] for j in b]]\n",
    "    seriated_dist[b,a] = seriated_dist[a,b]\n",
    "    \n",
    "    return seriated_dist, res_order, res_linkage\n",
    "\n",
    "ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(dist_mat,'ward')\n",
    "\n",
    "plt.pcolormesh(ordered_dist_mat)\n",
    "plt.xlim([0,N])\n",
    "plt.ylim([0,N])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pio_w2v.join(\n",
    "    pio_output, \n",
    "    how='inner'\n",
    ")\n",
    "# data = data.loc[pio_output['c_subj_split'] != ''].loc[pio_output['c_verb_split'] != '']\n",
    "data = data.loc[pio_output['split_1'] != '']\n",
    "\n",
    "# Shuffle data?\n",
    "# data = data.sample(frac=1)\n",
    "dist_mat = squareform(pdist(data.iloc[:, 0:300]))\n",
    "# dist_mat = squareform(pdist(data.iloc[:, 0:1024]))\n",
    "\n",
    "ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(\n",
    "    dist_mat,\n",
    "    'ward')\n",
    "reordered_data = data\n",
    "reordered_data['temp_index'] = data.index\n",
    "reordered_data = reordered_data.reset_index(drop=True)\n",
    "reordered_data = reordered_data.iloc[res_order].reset_index(drop=True)\n",
    "reordered_data['order_cluster'] = reordered_data.index\n",
    "reordered_data = reordered_data.set_index('temp_index')\n",
    "\n",
    "reordered_data = pio_output.join(\n",
    "    reordered_data['order_cluster'], \n",
    "    how='outer'\n",
    ")\n",
    "reordered_data.to_csv(f'temp/pio-reordered.csv')\n",
    "reordered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
