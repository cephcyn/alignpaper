,URL,ID,Type,Index,Text,split_0,split_1,split_2,split_tokens,split_anchor_span,split_anchor_indices,within_anchor_index,group,group2,group3,group4,group5,group6,group7,group8
0,https://www.semanticscholar.org/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5,0,Abstract,0,"We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.",We present,the Stanford Question Answering Dataset ( SQuAD ),", a new reading comprehension dataset consisting of 100,000 + questions posed by crowdworkers on a set of Wikipedia articles , where the answer to each question is a segment of text from the corresponding reading passage .","['We', 'present', 'the', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', ',', 'a', 'new', 'reading', 'comprehension', 'dataset', 'consisting', 'of', '100,000', '+', 'questions', 'posed', 'by', 'crowdworkers', 'on', 'a', 'set', 'of', 'Wikipedia', 'articles', ',', 'where', 'the', 'answer', 'to', 'each', 'question', 'is', 'a', 'segment', 'of', 'text', 'from', 'the', 'corresponding', 'reading', 'passage', '.']","(2, 10)","(10, 59)",42,Dataset,"['We', 'the Stanford Question Answering']","['( SQuAD ) , a new reading comprehension dataset consisting of 100,000 + questions posed by crowdworkers on a set of Wikipedia articles , where the answer to each question is a segment of text from the corresponding reading passage .']",1,43,51,32,40
1,https://www.semanticscholar.org/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5,0,Abstract,1,"We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees.",We analyze,the dataset,"to understand the types of reasoning required to answer the questions , leaning heavily on dependency and constituency trees .","['We', 'analyze', 'the', 'dataset', 'to', 'understand', 'the', 'types', 'of', 'reasoning', 'required', 'to', 'answer', 'the', 'questions', ',', 'leaning', 'heavily', 'on', 'dependency', 'and', 'constituency', 'trees', '.']","(2, 4)","(10, 21)",-1,analyze,['We'],"['the dataset', 'understand the types of reasoning required to answer the questions , leaning heavily on dependency and constituency trees']",0,3,11,3,11
2,https://www.semanticscholar.org/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5,0,Abstract,2,"We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%).","We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%).",,,"['We', 'build', 'a', 'strong', 'logistic', 'regression', 'model,', 'which', 'achieves', 'an', 'F1', 'score', 'of', '51.0%,', 'a', 'significant', 'improvement', 'over', 'a', 'simple', 'baseline', '(20%).']",,,,achieves,"[',', 'which', 'We', 'build a strong logistic regression model']","['an F1 score of 51.0 % a significant improvement over a simple baseline ( 20 % ) .', ',']",0,52,61,52,61
3,https://www.semanticscholar.org/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5,0,Abstract,3,"However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.The dataset is freely available at this https URL","However , human performance ( 86.8 % ) is much higher , indicating that",the dataset,presents a good challenge problem for future research . The dataset is freely available at this https URL,"['However', ',', 'human', 'performance', '(', '86.8', '%', ')', 'is', 'much', 'higher', ',', 'indicating', 'that', 'the', 'dataset', 'presents', 'a', 'good', 'challenge', 'problem', 'for', 'future', 'research', '.', 'The', 'dataset', 'is', 'freely', 'available', 'at', 'this', 'https', 'URL']","(14, 16)","(71, 82)",-1,",","['However , human performance ( 86.8 % ) is much higher']",['indicating that the dataset presents a good challenge problem for future research . The dataset is freely available at this https URL'],0,54,56,54,56
4,https://www.semanticscholar.org/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5,0,Abstract,3,"However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.The dataset is freely available at this https URL","However , human performance ( 86.8 % ) is much higher , indicating that the dataset presents a good challenge problem for future research .",The dataset,is freely available at this https URL,"['However', ',', 'human', 'performance', '(', '86.8', '%', ')', 'is', 'much', 'higher', ',', 'indicating', 'that', 'the', 'dataset', 'presents', 'a', 'good', 'challenge', 'problem', 'for', 'future', 'research', '.', 'The', 'dataset', 'is', 'freely', 'available', 'at', 'this', 'https', 'URL']","(25, 27)","(139, 150)",-1,",","['However , human performance ( 86.8 % ) is much higher']",['indicating that the dataset presents a good challenge problem for future research . The dataset is freely available at this https URL'],0,54,56,54,56
5,https://www.semanticscholar.org/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5,0,Title,0,"SQuAD: 100, 000+ Questions for Machine Comprehension of Text.",,SQuAD,": 100 , 000+ Questions for Machine Comprehension of Text .","['SQuAD', ':', '100', ',', '000+', 'Questions', 'for', 'Machine', 'Comprehension', 'of', 'Text', '.']","(0, 1)","(0, 5)",0,SQuAD,[],"[':', '100 , 000 + Questions for Machine Comprehension of Text']",1,0,6,0,6
6,https://www.semanticscholar.org/paper/Comparative-Analysis-of-Neural-QA-models-on-SQuAD-Wadhwa-Chandu/5eb16bfc64cda969228a27a7f0c8bbc988e53732,1,Abstract,0,The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language.,The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language.,,,"['The', 'task', 'of', 'Question', 'Answering', 'has', 'gained', 'prominence', 'in', 'the', 'past', 'few', 'decades', 'for', 'testing', 'the', 'ability', 'of', 'machines', 'to', 'understand', 'natural', 'language.']",,,,has gained,['The task of Question Answering'],[],0,31,42,31,42
7,https://www.semanticscholar.org/paper/Comparative-Analysis-of-Neural-QA-models-on-SQuAD-Wadhwa-Chandu/5eb16bfc64cda969228a27a7f0c8bbc988e53732,1,Abstract,1,Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks.,Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks.,,,"['Large', 'datasets', 'for', 'Machine', 'Reading', 'have', 'led', 'to', 'the', 'development', 'of', 'neural', 'models', 'that', 'cater', 'to', 'deeper', 'language', 'understanding', 'compared', 'to', 'information', 'retrieval', 'tasks.']",,,,have led,['Large datasets for Machine Reading'],['.'],0,35,44,35,44
8,https://www.semanticscholar.org/paper/Comparative-Analysis-of-Neural-QA-models-on-SQuAD-Wadhwa-Chandu/5eb16bfc64cda969228a27a7f0c8bbc988e53732,1,Abstract,2,Different components in these neural architectures are intended to tackle different challenges.,Different components in these neural architectures are intended to tackle different challenges.,,,"['Different', 'components', 'in', 'these', 'neural', 'architectures', 'are', 'intended', 'to', 'tackle', 'different', 'challenges.']",,,,are intended,['Different components in these neural architectures'],[],0,51,64,51,64
9,https://www.semanticscholar.org/paper/Comparative-Analysis-of-Neural-QA-models-on-SQuAD-Wadhwa-Chandu/5eb16bfc64cda969228a27a7f0c8bbc988e53732,1,Abstract,3,"As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them.","As a first step towards achieving generalization across multiple domains , we attempt to understand and compare the peculiarities of",existing end - to - end neural models on the Stanford Question Answering Dataset ( SQuAD ),by performing quantitative as well as qualitative analysis of the results attained by each of them .,"['As', 'a', 'first', 'step', 'towards', 'achieving', 'generalization', 'across', 'multiple', 'domains', ',', 'we', 'attempt', 'to', 'understand', 'and', 'compare', 'the', 'peculiarities', 'of', 'existing', 'end', '-', 'to', '-', 'end', 'neural', 'models', 'on', 'the', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', 'by', 'performing', 'quantitative', 'as', 'well', 'as', 'qualitative', 'analysis', 'of', 'the', 'results', 'attained', 'by', 'each', 'of', 'them', '.']","(20, 37)","(132, 222)",83,attempt,"['we', 'As a first step towards achieving generalization across multiple domains']",['to understand and compare the peculiarities of existing end - to - end neural models on the Stanford Question Answering Dataset ( SQuAD ) by performing quantitative as well as qualitative analysis of the results attained by each of them .'],0,78,86,78,86
10,https://www.semanticscholar.org/paper/Comparative-Analysis-of-Neural-QA-models-on-SQuAD-Wadhwa-Chandu/5eb16bfc64cda969228a27a7f0c8bbc988e53732,1,Abstract,3,"As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them.","As a first step towards achieving generalization across multiple domains , we attempt to understand and compare the peculiarities of existing end - to - end neural models on the Stanford Question Answering Dataset ( SQuAD ) by performing quantitative as well as qualitative analysis of the results attained by each of",them,.,"['As', 'a', 'first', 'step', 'towards', 'achieving', 'generalization', 'across', 'multiple', 'domains', ',', 'we', 'attempt', 'to', 'understand', 'and', 'compare', 'the', 'peculiarities', 'of', 'existing', 'end', '-', 'to', '-', 'end', 'neural', 'models', 'on', 'the', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', 'by', 'performing', 'quantitative', 'as', 'well', 'as', 'qualitative', 'analysis', 'of', 'the', 'results', 'attained', 'by', 'each', 'of', 'them', '.']","(52, 53)","(317, 321)",-1,attempt,"['we', 'As a first step towards achieving generalization across multiple domains']",['to understand and compare the peculiarities of existing end - to - end neural models on the Stanford Question Answering Dataset ( SQuAD ) by performing quantitative as well as qualitative analysis of the results attained by each of them .'],0,78,86,78,86
11,https://www.semanticscholar.org/paper/Comparative-Analysis-of-Neural-QA-models-on-SQuAD-Wadhwa-Chandu/5eb16bfc64cda969228a27a7f0c8bbc988e53732,1,Abstract,4,"We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper","We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper",,,"['We', 'observed', 'that', 'prediction', 'errors', 'reflect', 'certain', 'model-specific', 'biases,', 'which', 'we', 'further', 'discuss', 'in', 'this', 'paper']",,,,We,[],"['observed that prediction errors reflect certain model - specific biases , which we further discuss in this paper']",0,0,3,0,3
12,https://www.semanticscholar.org/paper/Comparative-Analysis-of-Neural-QA-models-on-SQuAD-Wadhwa-Chandu/5eb16bfc64cda969228a27a7f0c8bbc988e53732,1,Title,0,Comparative Analysis of Neural QA models on SQuAD.,Comparative Analysis of Neural QA models on,SQuAD,.,"['Comparative', 'Analysis', 'of', 'Neural', 'QA', 'models', 'on', 'SQuAD', '.']","(7, 8)","(43, 48)",0,Comparative,[],['Analysis of Neural QA models on SQuAD'],0,0,12,0,12
13,https://www.semanticscholar.org/paper/Ensemble-BERT-with-Data-Augmentation-and-Linguistic-Zhou/2f995b11ab14637458e7b8c2b592b0ceedc3b875,2,Abstract,0,"In this project, we proposed a question answering (QA) system based on baseline BERT model and significantly improved the single baseline BERT model on SQuAD 2.0.","In this project , we proposed a question answering ( QA ) system based on baseline BERT model and significantly improved the single baseline BERT model on",SQuAD,2.0 .,"['In', 'this', 'project', ',', 'we', 'proposed', 'a', 'question', 'answering', '(', 'QA', ')', 'system', 'based', 'on', 'baseline', 'BERT', 'model', 'and', 'significantly', 'improved', 'the', 'single', 'baseline', 'BERT', 'model', 'on', 'SQuAD', '2.0', '.']","(27, 28)","(154, 159)",0,proposed,"['we', ',']",['a question answering ( QA ) system based on baseline BERT model significantly improved the single baseline BERT model on SQuAD 2.0 .'],0,21,30,21,30
14,https://www.semanticscholar.org/paper/Ensemble-BERT-with-Data-Augmentation-and-Linguistic-Zhou/2f995b11ab14637458e7b8c2b592b0ceedc3b875,2,Abstract,1,We adopted a novel data augmentation approach and integrated linguistic knowledge to build a robust ensemble model.,We adopted a novel data augmentation approach and integrated linguistic knowledge to build a robust ensemble model.,,,"['We', 'adopted', 'a', 'novel', 'data', 'augmentation', 'approach', 'and', 'integrated', 'linguistic', 'knowledge', 'to', 'build', 'a', 'robust', 'ensemble', 'model.']",,,,adopted,['We'],"['a novel data augmentation approach and integrated linguistic knowledge to build a robust ensemble model', '.']",0,3,11,3,11
15,https://www.semanticscholar.org/paper/Ensemble-BERT-with-Data-Augmentation-and-Linguistic-Zhou/2f995b11ab14637458e7b8c2b592b0ceedc3b875,2,Abstract,2,"Our model was ranked first place on both dev and test leaderboard, and achieved 79.91(EM)/82.43(F1) and 79.87(EM)/82.54(F1) on dev and test set respectively (March 20, 2019).","Our model was ranked first place on both dev and test leaderboard, and achieved 79.91(EM)/82.43(F1) and 79.87(EM)/82.54(F1) on dev and test set respectively (March 20, 2019).",,,"['Our', 'model', 'was', 'ranked', 'first', 'place', 'on', 'both', 'dev', 'and', 'test', 'leaderboard,', 'and', 'achieved', '79.91(EM)/82.43(F1)', 'and', '79.87(EM)/82.54(F1)', 'on', 'dev', 'and', 'test', 'set', 'respectively', '(March', '20,', '2019).']",,,,was ranked,['Our model'],"['place', 'on', ',', 'both dev and test leaderboard and achieved 79.91(EM)/82.43(F1 ) and 79.87(EM)/82.54(F1 ) on dev and test set respectively ( March 20 , 2019 ) .']",0,10,21,10,21
16,https://www.semanticscholar.org/paper/Ensemble-BERT-with-Data-Augmentation-and-Linguistic-Zhou/2f995b11ab14637458e7b8c2b592b0ceedc3b875,2,Title,0,Ensemble BERT with Data Augmentation and Linguistic Knowledge on SQuAD 2.0.,Ensemble BERT with Data Augmentation and Linguistic Knowledge on,SQuAD,2.0 .,"['Ensemble', 'BERT', 'with', 'Data', 'Augmentation', 'and', 'Linguistic', 'Knowledge', 'on', 'SQuAD', '2.0', '.']","(9, 10)","(64, 69)",0,BERT,['Ensemble'],[],0,9,14,9,14
17,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Abstract,0,"We compare three new datasets for question answering: SQuAD 2.0, QuAC, and CoQA, along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers.",We compare,"three new datasets for question answering : SQuAD 2.0 , QuAC , and CoQA",", along several of their new features : ( 1 ) unanswerable questions , ( 2 ) multi - turn interactions , and ( 3 ) abstractive answers .","['We', 'compare', 'three', 'new', 'datasets', 'for', 'question', 'answering', ':', 'SQuAD', '2.0', ',', 'QuAC', ',', 'and', 'CoQA', ',', 'along', 'several', 'of', 'their', 'new', 'features', ':', '(', '1', ')', 'unanswerable', 'questions', ',', '(', '2', ')', 'multi', '-', 'turn', 'interactions', ',', 'and', '(', '3', ')', 'abstractive', 'answers', '.']","(2, 16)","(10, 81)",44,",","['We compare three new datasets for question answering : SQuAD 2.0 , QuAC , and CoQA , along several of their new features : ( 1 ) unanswerable questions']","['( 2 ) multi - turn interactions , and ( 3 ) abstractive answers .']",2,152,154,69,71
18,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Abstract,0,"We compare three new datasets for question answering: SQuAD 2.0, QuAC, and CoQA, along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers.","We compare three new datasets for question answering : SQuAD 2.0 , QuAC , and CoQA , along several of",their,"new features : ( 1 ) unanswerable questions , ( 2 ) multi - turn interactions , and ( 3 ) abstractive answers .","['We', 'compare', 'three', 'new', 'datasets', 'for', 'question', 'answering', ':', 'SQuAD', '2.0', ',', 'QuAC', ',', 'and', 'CoQA', ',', 'along', 'several', 'of', 'their', 'new', 'features', ':', '(', '1', ')', 'unanswerable', 'questions', ',', '(', '2', ')', 'multi', '-', 'turn', 'interactions', ',', 'and', '(', '3', ')', 'abstractive', 'answers', '.']","(20, 21)","(101, 106)",-1,",","['We compare three new datasets for question answering : SQuAD 2.0 , QuAC , and CoQA , along several of their new features : ( 1 ) unanswerable questions']","['( 2 ) multi - turn interactions , and ( 3 ) abstractive answers .']",2,152,154,44,46
19,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Abstract,0,"We compare three new datasets for question answering: SQuAD 2.0, QuAC, and CoQA, along several of their new features: (1) unanswerable questions, (2) multi-turn interactions, and (3) abstractive answers.",We compare three new datasets for question answering :,SQuAD 2.0,", QuAC , and CoQA , along several of their new features : ( 1 ) unanswerable questions , ( 2 ) multi - turn interactions , and ( 3 ) abstractive answers .","['We', 'compare', 'three', 'new', 'datasets', 'for', 'question', 'answering', ':', 'SQuAD', '2.0', ',', 'QuAC', ',', 'and', 'CoQA', ',', 'along', 'several', 'of', 'their', 'new', 'features', ':', '(', '1', ')', 'unanswerable', 'questions', ',', '(', '2', ')', 'multi', '-', 'turn', 'interactions', ',', 'and', '(', '3', ')', 'abstractive', 'answers', '.']","(9, 11)","(54, 63)",0,",","['We compare three new datasets for question answering : SQuAD 2.0 , QuAC , and CoQA , along several of their new features : ( 1 ) unanswerable questions']","['( 2 ) multi - turn interactions , and ( 3 ) abstractive answers .']",2,152,154,87,89
20,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Abstract,1,"We show that the datasets provide complementary coverage of the first two aspects, but weak coverage of the third.",We show that,the datasets,"provide complementary coverage of the first two aspects , but weak coverage of the third .","['We', 'show', 'that', 'the', 'datasets', 'provide', 'complementary', 'coverage', 'of', 'the', 'first', 'two', 'aspects', ',', 'but', 'weak', 'coverage', 'of', 'the', 'third', '.']","(3, 5)","(12, 24)",-1,show,['We'],[],0,3,8,3,8
21,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Abstract,2,"Because of the datasets' structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both SQuAD 2.0 and CoQA.",Because of,the datasets ',"structural similarity , a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both SQuAD 2.0 and CoQA .","['Because', 'of', 'the', 'datasets', ""'"", 'structural', 'similarity', ',', 'a', 'single', 'extractive', 'model', 'can', 'be', 'easily', 'adapted', 'to', 'any', 'of', 'the', 'datasets', 'and', 'we', 'show', 'improved', 'baseline', 'results', 'on', 'both', 'SQuAD', '2.0', 'and', 'CoQA', '.']","(2, 5)","(10, 24)",-1,can be easily,[],"['adapted to any of the datasets', 'we show improved baseline results on both SQuAD 2.0 and CoQA .']",2,76,90,50,64
22,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Abstract,2,"Because of the datasets' structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both SQuAD 2.0 and CoQA.","Because of the datasets ' structural similarity , a single extractive model can be easily adapted to any of",the datasets,and we show improved baseline results on both SQuAD 2.0 and CoQA .,"['Because', 'of', 'the', 'datasets', ""'"", 'structural', 'similarity', ',', 'a', 'single', 'extractive', 'model', 'can', 'be', 'easily', 'adapted', 'to', 'any', 'of', 'the', 'datasets', 'and', 'we', 'show', 'improved', 'baseline', 'results', 'on', 'both', 'SQuAD', '2.0', 'and', 'CoQA', '.']","(19, 21)","(107, 119)",-1,can be easily,[],"['adapted to any of the datasets', 'we show improved baseline results on both SQuAD 2.0 and CoQA .']",0,76,90,76,90
23,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Abstract,2,"Because of the datasets' structural similarity, a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both SQuAD 2.0 and CoQA.","Because of the datasets ' structural similarity , a single extractive model can be easily adapted to any of the datasets and we show improved baseline results on both",SQuAD 2.0,and CoQA .,"['Because', 'of', 'the', 'datasets', ""'"", 'structural', 'similarity', ',', 'a', 'single', 'extractive', 'model', 'can', 'be', 'easily', 'adapted', 'to', 'any', 'of', 'the', 'datasets', 'and', 'we', 'show', 'improved', 'baseline', 'results', 'on', 'both', 'SQuAD', '2.0', 'and', 'CoQA', '.']","(29, 31)","(166, 175)",0,can be easily,[],"['adapted to any of the datasets', 'we show improved baseline results on both SQuAD 2.0 and CoQA .']",0,76,90,76,90
24,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Abstract,3,"Despite the similarity, models trained on one dataset are ineffective on another dataset, but we find moderate performance improvement through pretraining.","Despite the similarity, models trained on one dataset are ineffective on another dataset, but we find moderate performance improvement through pretraining.",,,"['Despite', 'the', 'similarity,', 'models', 'trained', 'on', 'one', 'dataset', 'are', 'ineffective', 'on', 'another', 'dataset,', 'but', 'we', 'find', 'moderate', 'performance', 'improvement', 'through', 'pretraining.']",,,,are ineffective,"['Despite', 'the similarity , models trained on one dataset']",[],0,55,71,55,71
25,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Abstract,4,"To encourage cross-evaluation, we release code for conversion between datasets at this https URL","To encourage cross-evaluation, we release code for conversion between datasets at this https URL",,,"['To', 'encourage', 'cross-evaluation,', 'we', 'release', 'code', 'for', 'conversion', 'between', 'datasets', 'at', 'this', 'https', 'URL']",,,,code,[],[],0,45,50,45,50
26,https://www.semanticscholar.org/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa,3,Title,0,"A Qualitative Comparison of CoQA, SQuAD 2.0 and QuAC.","A Qualitative Comparison of CoQA ,",SQuAD,2.0 and QuAC .,"['A', 'Qualitative', 'Comparison', 'of', 'CoQA', ',', 'SQuAD', '2.0', 'and', 'QuAC', '.']","(6, 7)","(34, 39)",0,Comparison,"['A', 'Qualitative']",[],0,14,25,14,25
27,https://www.semanticscholar.org/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c,4,Abstract,0,"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context.","Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context.",,,"['Extractive', 'reading', 'comprehension', 'systems', 'can', 'often', 'locate', 'the', 'correct', 'answer', 'to', 'a', 'question', 'in', 'a', 'context', 'document,', 'but', 'they', 'also', 'tend', 'to', 'make', 'unreliable', 'guesses', 'on', 'questions', 'for', 'which', 'the', 'correct', 'answer', 'is', 'not', 'stated', 'in', 'the', 'context.']",,,,locate,['Extractive reading comprehension systems can'],[],0,51,58,51,58
28,https://www.semanticscholar.org/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c,4,Abstract,1,"Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify.","Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify.",,,"['Existing', 'datasets', 'either', 'focus', 'exclusively', 'on', 'answerable', 'questions,', 'or', 'use', 'automatically', 'generated', 'unanswerable', 'questions', 'that', 'are', 'easy', 'to', 'identify.']",,,,questions,"['Existing datasets either focus exclusively on answerable questions , or use .', 'automatically generated']",[],0,113,123,113,123
29,https://www.semanticscholar.org/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c,4,Abstract,2,"To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD).","To address these weaknesses , we present","SQuAD 2.0 , the latest version of the Stanford Question Answering Dataset ( SQuAD )",.,"['To', 'address', 'these', 'weaknesses', ',', 'we', 'present', 'SQuAD', '2.0', ',', 'the', 'latest', 'version', 'of', 'the', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', '.']","(7, 22)","(40, 123)",0,",","['To address these weaknesses we present SQuAD 2.0 , the latest version of the Stanford Question Answering Dataset ( SQuAD ) .']",[],0,28,30,28,30
30,https://www.semanticscholar.org/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c,4,Abstract,3,"SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.",,SQuAD 2.0,"combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones .","['SQuAD', '2.0', 'combines', 'existing', 'SQuAD', 'data', 'with', 'over', '50,000', 'unanswerable', 'questions', 'written', 'adversarially', 'by', 'crowdworkers', 'to', 'look', 'similar', 'to', 'answerable', 'ones', '.']","(0, 2)","(0, 9)",0,combines,['SQuAD 2.0'],['existing SQuAD data'],2,10,19,0,9
31,https://www.semanticscholar.org/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c,4,Abstract,4,"To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.",To do well on,SQuAD 2.0,", systems must not only answer questions when possible , but also determine when no answer is supported by the paragraph and abstain from answering .","['To', 'do', 'well', 'on', 'SQuAD', '2.0', ',', 'systems', 'must', 'not', 'only', 'answer', 'questions', 'when', 'possible', ',', 'but', 'also', 'determine', 'when', 'no', 'answer', 'is', 'supported', 'by', 'the', 'paragraph', 'and', 'abstain', 'from', 'answering', '.']","(4, 6)","(13, 22)",0,must answer,['only'],"['questions', 'possible', 'but also determine when no answer is supported by the paragraph and abstain from answering .']",2,34,55,10,31
32,https://www.semanticscholar.org/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c,4,Abstract,5,SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0,,SQuAD 2.0,is a challenging natural language understanding task for existing models : a strong neural system that gets 86 % F1 on SQuAD 1.1 achieves only 66 % F1 on SQuAD 2.0,"['SQuAD', '2.0', 'is', 'a', 'challenging', 'natural', 'language', 'understanding', 'task', 'for', 'existing', 'models', ':', 'a', 'strong', 'neural', 'system', 'that', 'gets', '86', '%', 'F1', 'on', 'SQuAD', '1.1', 'achieves', 'only', '66', '%', 'F1', 'on', 'SQuAD', '2.0']","(0, 2)","(0, 9)",0,is understanding,[],[],2,10,58,0,48
33,https://www.semanticscholar.org/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c,4,Abstract,5,SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0,SQuAD 2.0 is a challenging natural language understanding task for existing models : a strong neural system that gets 86 % F1 on SQuAD 1.1 achieves only 66 % F1 on,SQuAD 2.0,,"['SQuAD', '2.0', 'is', 'a', 'challenging', 'natural', 'language', 'understanding', 'task', 'for', 'existing', 'models', ':', 'a', 'strong', 'neural', 'system', 'that', 'gets', '86', '%', 'F1', 'on', 'SQuAD', '1.1', 'achieves', 'only', '66', '%', 'F1', 'on', 'SQuAD', '2.0']","(31, 33)","(163, 172)",0,is understanding,[],[],0,10,58,10,58
34,https://www.semanticscholar.org/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c,4,Title,0,Know What You Don't Know: Unanswerable Questions for SQuAD.,Know What You Do n't Know : Unanswerable Questions for,SQuAD,.,"['Know', 'What', 'You', 'Do', ""n't"", 'Know', ':', 'Unanswerable', 'Questions', 'for', 'SQuAD', '.']","(10, 11)","(54, 59)",0,Do Know,"['Know', 'What You']","[':', 'Unanswerable Questions for SQuAD']",0,14,26,14,26
35,https://www.semanticscholar.org/paper/Stochastic-Answer-Networks-for-SQuAD-2.0-Liu-Li/d0095adcaa33bc549e273a824c3b66d92897fad8,5,Abstract,0,"This paper presents an extension of the Stochastic Answer Network (SAN), one of the state-of-the-art machine reading comprehension models, to be able to judge whether a question is unanswerable or not.","This paper presents an extension of the Stochastic Answer Network (SAN), one of the state-of-the-art machine reading comprehension models, to be able to judge whether a question is unanswerable or not.",,,"['This', 'paper', 'presents', 'an', 'extension', 'of', 'the', 'Stochastic', 'Answer', 'Network', '(SAN),', 'one', 'of', 'the', 'state-of-the-art', 'machine', 'reading', 'comprehension', 'models,', 'to', 'be', 'able', 'to', 'judge', 'whether', 'a', 'question', 'is', 'unanswerable', 'or', 'not.']",,,,presents,"['This', 'paper']","['an extension of the Stochastic Answer Network ( SAN ) , one of the state - of - the - art machine reading comprehension models , to be able to judge whether a question is unanswerable or not .']",0,11,20,11,20
36,https://www.semanticscholar.org/paper/Stochastic-Answer-Networks-for-SQuAD-2.0-Liu-Li/d0095adcaa33bc549e273a824c3b66d92897fad8,5,Abstract,1,"The extended SAN contains two components: a span detector and a binary classifier for judging whether the question is unanswerable, and both components are jointly optimized.","The extended SAN contains two components: a span detector and a binary classifier for judging whether the question is unanswerable, and both components are jointly optimized.",,,"['The', 'extended', 'SAN', 'contains', 'two', 'components:', 'a', 'span', 'detector', 'and', 'a', 'binary', 'classifier', 'for', 'judging', 'whether', 'the', 'question', 'is', 'unanswerable,', 'and', 'both', 'components', 'are', 'jointly', 'optimized.']",,,,contains,['The extended SAN'],"['two components : a span detector and a binary classifier for judging whether the question is unanswerable , and both components are jointly optimized']",0,17,26,17,26
37,https://www.semanticscholar.org/paper/Stochastic-Answer-Networks-for-SQuAD-2.0-Liu-Li/d0095adcaa33bc549e273a824c3b66d92897fad8,5,Abstract,2,Experiments show that SAN achieves the results competitive to the state-of-the-art on Stanford Question Answering Dataset (SQuAD) 2.0.,Experiments show that SAN achieves the results competitive to the state-of-the-art on Stanford Question Answering Dataset (,SQuAD,) 2.0 .,"['Experiments', 'show', 'that', 'SAN', 'achieves', 'the', 'results', 'competitive', 'to', 'the', 'state-of-the-art', 'on', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', '2.0', '.']","(17, 18)","(123, 128)",0,2.0,['Answering Dataset ( SQuAD )'],['Experiments show that SAN achieves the results competitive to the state - of - the - art on Stanford Question .'],3,138,142,0,4
38,https://www.semanticscholar.org/paper/Stochastic-Answer-Networks-for-SQuAD-2.0-Liu-Li/d0095adcaa33bc549e273a824c3b66d92897fad8,5,Abstract,3,"To facilitate the research on this field, we release our code: this https URL","To facilitate the research on this field, we release our code: this https URL",,,"['To', 'facilitate', 'the', 'research', 'on', 'this', 'field,', 'we', 'release', 'our', 'code:', 'this', 'https', 'URL']",,,,release,"['To', 'facilitate the research on this field']",['our code : this https URL'],0,46,54,46,54
39,https://www.semanticscholar.org/paper/Stochastic-Answer-Networks-for-SQuAD-2.0-Liu-Li/d0095adcaa33bc549e273a824c3b66d92897fad8,5,Title,0,Stochastic Answer Networks for SQuAD 2.0.,Stochastic Answer Networks for,SQuAD,2.0 .,"['Stochastic', 'Answer', 'Networks', 'for', 'SQuAD', '2.0', '.']","(4, 5)","(30, 35)",0,Answer,['Stochastic'],['Networks for SQuAD 2.0'],0,11,18,11,18
40,https://www.semanticscholar.org/paper/FastFusionNet%3A-New-State-of-the-Art-for-DAWNBench-Wu-Li/f4b141cb646590f5033b3412b618d6434e854015,6,Abstract,0,"In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet [12].","In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet [12].",,,"['In', 'this', 'technical', 'report,', 'we', 'introduce', 'FastFusionNet,', 'an', 'efficient', 'variant', 'of', 'FusionNet', '[12].']",,,,introduce,['we'],"['FastFusionNet , an efficient variant of FusionNet [ 12 ] .']",0,30,40,30,40
41,https://www.semanticscholar.org/paper/FastFusionNet%3A-New-State-of-the-Art-for-DAWNBench-Wu-Li/f4b141cb646590f5033b3412b618d6434e854015,6,Abstract,1,"FusionNet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements.","FusionNet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements.",,,"['FusionNet', 'is', 'a', 'high', 'performing', 'reading', 'comprehension', 'architecture,', 'which', 'was', 'designed', 'primarily', 'for', 'maximum', 'retrieval', 'accuracy', 'with', 'less', 'regard', 'towards', 'computational', 'requirements.']",,,,is,['FusionNet'],"['a high performing reading comprehension architecture , which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements', '.']",0,10,13,10,13
42,https://www.semanticscholar.org/paper/FastFusionNet%3A-New-State-of-the-Art-for-DAWNBench-Wu-Li/f4b141cb646590f5033b3412b618d6434e854015,6,Abstract,2,For FastFusionNets we remove the expensive CoVe layers [21] and substitute the BiLSTMs with far more efficient SRU layers [19].,For FastFusionNets we remove the expensive CoVe layers [21] and substitute the BiLSTMs with far more efficient SRU layers [19].,,,"['For', 'FastFusionNets', 'we', 'remove', 'the', 'expensive', 'CoVe', 'layers', '[21]', 'and', 'substitute', 'the', 'BiLSTMs', 'with', 'far', 'more', 'efficient', 'SRU', 'layers', '[19].']",,,,21,['For FastFusionNets we remove the expensive CoVe layers ['],"['] substitute the BiLSTMs with far more efficient SRU layers [ 19 ] .', 'and']",0,57,60,57,60
43,https://www.semanticscholar.org/paper/FastFusionNet%3A-New-State-of-the-Art-for-DAWNBench-Wu-Li/f4b141cb646590f5033b3412b618d6434e854015,6,Abstract,3,The resulting architecture obtains state-of-the-art results on DAWNBench [5] while achieving the lowest training and inference time on SQuAD [25] to-date.,The resulting architecture obtains state-of-the-art results on DAWNBench [ 5 ] while achieving the lowest training and inference time on,SQuAD,[ 25 ] to-date .,"['The', 'resulting', 'architecture', 'obtains', 'state-of-the-art', 'results', 'on', 'DAWNBench', '[', '5', ']', 'while', 'achieving', 'the', 'lowest', 'training', 'and', 'inference', 'time', 'on', 'SQuAD', '[', '25', ']', 'to-date', '.']","(20, 21)","(136, 141)",0,architecture,[],['obtains state - of - the - art results on DAWNBench [ 5 ] while achieving the lowest training and inference time on SQuAD [ 25 ] to - date .'],0,14,27,14,27
44,https://www.semanticscholar.org/paper/FastFusionNet%3A-New-State-of-the-Art-for-DAWNBench-Wu-Li/f4b141cb646590f5033b3412b618d6434e854015,6,Abstract,4,The code is available at this https URL,The code is available at this https URL,,,"['The', 'code', 'is', 'available', 'at', 'this', 'https', 'URL']",,,,is available,['The code'],[],0,9,22,9,22
45,https://www.semanticscholar.org/paper/FastFusionNet%3A-New-State-of-the-Art-for-DAWNBench-Wu-Li/f4b141cb646590f5033b3412b618d6434e854015,6,Title,0,FastFusionNet: New State-of-the-Art for DAWNBench SQuAD.,FastFusionNet : New State-of-the-Art for DAWNBench,SQuAD,.,"['FastFusionNet', ':', 'New', 'State-of-the-Art', 'for', 'DAWNBench', 'SQuAD', '.']","(6, 7)","(50, 55)",0,FastFusionNet,[],"[':', 'New State - of - the - Art for DAWNBench SQuAD']",0,0,14,0,14
46,https://www.semanticscholar.org/paper/Lightweight-Convolutional-Approaches-to-Reading-on-Bell-Penchas/6b5e7c6444f3334905deb9dd72a234d25ac20f62,7,Abstract,0,Current state-of-the-art reading comprehension models rely heavily on recurrent neural networks.,Current state-of-the-art reading comprehension models rely heavily on recurrent neural networks.,,,"['Current', 'state-of-the-art', 'reading', 'comprehension', 'models', 'rely', 'heavily', 'on', 'recurrent', 'neural', 'networks.']",,,,rely,['Current state - of - the - art reading comprehension models'],[],0,60,65,60,65
47,https://www.semanticscholar.org/paper/Lightweight-Convolutional-Approaches-to-Reading-on-Bell-Penchas/6b5e7c6444f3334905deb9dd72a234d25ac20f62,7,Abstract,1,We explored an entirely different approach to question answering: a convolutional model.,We explored an entirely different approach to question answering: a convolutional model.,,,"['We', 'explored', 'an', 'entirely', 'different', 'approach', 'to', 'question', 'answering:', 'a', 'convolutional', 'model.']",,,,to,[],"['We explored an entirely different approach', 'question answering : a convolutional model .']",0,43,46,43,46
48,https://www.semanticscholar.org/paper/Lightweight-Convolutional-Approaches-to-Reading-on-Bell-Penchas/6b5e7c6444f3334905deb9dd72a234d25ac20f62,7,Abstract,2,"By their nature, these convolutional models are fast to train and capture local dependencies well, though they can struggle with longer-range dependencies and thus require augmentation to achieve comparable performance to RNN-based models.","By their nature, these convolutional models are fast to train and capture local dependencies well, though they can struggle with longer-range dependencies and thus require augmentation to achieve comparable performance to RNN-based models.",,,"['By', 'their', 'nature,', 'these', 'convolutional', 'models', 'are', 'fast', 'to', 'train', 'and', 'capture', 'local', 'dependencies', 'well,', 'though', 'they', 'can', 'struggle', 'with', 'longer-range', 'dependencies', 'and', 'thus', 'require', 'augmentation', 'to', 'achieve', 'comparable', 'performance', 'to', 'RNN-based', 'models.']",,,,are fast,['these convolutional models'],"['train and capture', 'though they can struggle with longer - range dependencies', 'and thus require augmentation to achieve comparable performance to RNN - based models .']",0,45,54,45,54
49,https://www.semanticscholar.org/paper/Lightweight-Convolutional-Approaches-to-Reading-on-Bell-Penchas/6b5e7c6444f3334905deb9dd72a234d25ac20f62,7,Abstract,3,"We conducted over two dozen controlled experiments with convolutional models and various kernel/attention/regularization schemes to determine the precise performance gains of each strategy, while maintaining a focus on speed.","We conducted over two dozen controlled experiments with convolutional models and various kernel/attention/regularization schemes to determine the precise performance gains of each strategy, while maintaining a focus on speed.",,,"['We', 'conducted', 'over', 'two', 'dozen', 'controlled', 'experiments', 'with', 'convolutional', 'models', 'and', 'various', 'kernel/attention/regularization', 'schemes', 'to', 'determine', 'the', 'precise', 'performance', 'gains', 'of', 'each', 'strategy,', 'while', 'maintaining', 'a', 'focus', 'on', 'speed.']",,,,to determine,['We conducted over two dozen controlled experiments with convolutional models and various kernel / attention / regularization schemes'],"['the precise performance gains of each strategy ,', 'while maintaining a focus on speed .']",0,133,146,133,146
50,https://www.semanticscholar.org/paper/Lightweight-Convolutional-Approaches-to-Reading-on-Bell-Penchas/6b5e7c6444f3334905deb9dd72a234d25ac20f62,7,Abstract,4,"We ultimately ensembled three models: crossconv (0.5398 dev F1), attnconv (0.5665), and maybeconv (0.5285).","We ultimately ensembled three models: crossconv (0.5398 dev F1), attnconv (0.5665), and maybeconv (0.5285).",,,"['We', 'ultimately', 'ensembled', 'three', 'models:', 'crossconv', '(0.5398', 'dev', 'F1),', 'attnconv', '(0.5665),', 'and', 'maybeconv', '(0.5285).']",,,,models,"['We ultimately ensembled', 'three']","[':', 'crossconv ( 0.5398 dev F1 ) , attnconv ( 0.5665 ) , and maybeconv ( 0.5285 ) .']",0,30,37,30,37
51,https://www.semanticscholar.org/paper/Lightweight-Convolutional-Approaches-to-Reading-on-Bell-Penchas/6b5e7c6444f3334905deb9dd72a234d25ac20f62,7,Abstract,5,The ensembled model was able to achieve a 0.6238 F1 score using the official SQuAD evaluation script.,The ensembled model was able to achieve a 0.6238 F1 score using the official,SQuAD,evaluation script .,"['The', 'ensembled', 'model', 'was', 'able', 'to', 'achieve', 'a', '0.6238', 'F1', 'score', 'using', 'the', 'official', 'SQuAD', 'evaluation', 'script', '.']","(14, 15)","(76, 81)",0,was able,['The ensembled model'],[],0,20,29,20,29
52,https://www.semanticscholar.org/paper/Lightweight-Convolutional-Approaches-to-Reading-on-Bell-Penchas/6b5e7c6444f3334905deb9dd72a234d25ac20f62,7,Abstract,6,Our individual convolutional model crossconv was able to exceed the performance of the RNN-plus-attention baseline by 25% while training 6 times faster,Our individual convolutional model crossconv was able to exceed the performance of the RNN-plus-attention baseline by 25% while training 6 times faster,,,"['Our', 'individual', 'convolutional', 'model', 'crossconv', 'was', 'able', 'to', 'exceed', 'the', 'performance', 'of', 'the', 'RNN-plus-attention', 'baseline', 'by', '25%', 'while', 'training', '6', 'times', 'faster']",,,,was able,[],[],0,45,54,45,54
53,https://www.semanticscholar.org/paper/Lightweight-Convolutional-Approaches-to-Reading-on-Bell-Penchas/6b5e7c6444f3334905deb9dd72a234d25ac20f62,7,Title,0,Lightweight Convolutional Approaches to Reading Comprehension on SQuAD.,Lightweight Convolutional Approaches to Reading Comprehension on,SQuAD,.,"['Lightweight', 'Convolutional', 'Approaches', 'to', 'Reading', 'Comprehension', 'on', 'SQuAD', '.']","(7, 8)","(64, 69)",0,Convolutional,['Lightweight'],['Approaches'],0,12,26,12,26
54,https://www.semanticscholar.org/paper/New-Metric-Based-on-SQuAD-for-Evaluating-Accuracy-Kulkarni-Gupta/ee5abe5774c622f5019a4510d1743af50cb703a0,8,Abstract,0,"Enterprise Search is a continuously evolving and important field, which is seeing a resurgence driven by artificial intelligence.","Enterprise Search is a continuously evolving and important field, which is seeing a resurgence driven by artificial intelligence.",,,"['Enterprise', 'Search', 'is', 'a', 'continuously', 'evolving', 'and', 'important', 'field,', 'which', 'is', 'seeing', 'a', 'resurgence', 'driven', 'by', 'artificial', 'intelligence.']",,,,is,"['Enterprise', 'Search']","['a continuously evolving and important field , which is seeing a resurgence driven by artificial intelligence']",0,18,21,18,21
55,https://www.semanticscholar.org/paper/New-Metric-Based-on-SQuAD-for-Evaluating-Accuracy-Kulkarni-Gupta/ee5abe5774c622f5019a4510d1743af50cb703a0,8,Abstract,1,"Still, there is no objective, generally accepted way to compare various enterprise search systems.","Still, there is no objective, generally accepted way to compare various enterprise search systems.",,,"['Still,', 'there', 'is', 'no', 'objective,', 'generally', 'accepted', 'way', 'to', 'compare', 'various', 'enterprise', 'search', 'systems.']",,,,is,"[',', 'no objective', ',']","['generally accepted way to compare various enterprise search systems', '.']",0,14,17,14,17
56,https://www.semanticscholar.org/paper/New-Metric-Based-on-SQuAD-for-Evaluating-Accuracy-Kulkarni-Gupta/ee5abe5774c622f5019a4510d1743af50cb703a0,8,Abstract,2,SQuAD is becoming popular for measuring algorithmic reading comprehension (MRC) but is ineffective for quantifying effectiveness of enterprise search in business-use situations.,,SQuAD,is becoming popular for measuring algorithmic reading comprehension ( MRC ) but is ineffective for quantifying effectiveness of enterprise search in business - use situations .,"['SQuAD', 'is', 'becoming', 'popular', 'for', 'measuring', 'algorithmic', 'reading', 'comprehension', '(', 'MRC', ')', 'but', 'is', 'ineffective', 'for', 'quantifying', 'effectiveness', 'of', 'enterprise', 'search', 'in', 'business', '-', 'use', 'situations', '.']","(0, 1)","(0, 5)",0,is,['SQuAD'],['becoming popular for measuring algorithmic reading comprehension ( MRC ) but is ineffective for quantifying effectiveness of enterprise search in business - use situations'],2,6,9,0,3
57,https://www.semanticscholar.org/paper/New-Metric-Based-on-SQuAD-for-Evaluating-Accuracy-Kulkarni-Gupta/ee5abe5774c622f5019a4510d1743af50cb703a0,8,Abstract,3,In this paper we modify the SQuAD scoring methodology to propose a scoring system for enterprise search systems that aligns with the real world expectations of users.,In this paper we modify the,SQuAD,scoring methodology to propose a scoring system for enterprise search systems that aligns with the real world expectations of users .,"['In', 'this', 'paper', 'we', 'modify', 'the', 'SQuAD', 'scoring', 'methodology', 'to', 'propose', 'a', 'scoring', 'system', 'for', 'enterprise', 'search', 'systems', 'that', 'aligns', 'with', 'the', 'real', 'world', 'expectations', 'of', 'users', '.']","(6, 7)","(27, 32)",0,modify,"['this paper', 'we']",['the SQuAD scoring methodology'],0,17,24,17,24
58,https://www.semanticscholar.org/paper/New-Metric-Based-on-SQuAD-for-Evaluating-Accuracy-Kulkarni-Gupta/ee5abe5774c622f5019a4510d1743af50cb703a0,8,Abstract,4,"Further, we use a search system based on Calibrated Quantum Mesh (CQM) to underscore the relevance of this metric","Further, we use a search system based on Calibrated Quantum Mesh (CQM) to underscore the relevance of this metric",,,"['Further,', 'we', 'use', 'a', 'search', 'system', 'based', 'on', 'Calibrated', 'Quantum', 'Mesh', '(CQM)', 'to', 'underscore', 'the', 'relevance', 'of', 'this', 'metric']",,,,use,"[',', 'we']","['a search system', 'based']",0,13,17,13,17
59,https://www.semanticscholar.org/paper/New-Metric-Based-on-SQuAD-for-Evaluating-Accuracy-Kulkarni-Gupta/ee5abe5774c622f5019a4510d1743af50cb703a0,8,Title,0,New Metric Based on SQuAD for Evaluating Accuracy of Enterprise Search Algorithms.,New Metric Based on,SQuAD,for Evaluating Accuracy of Enterprise Search Algorithms .,"['New', 'Metric', 'Based', 'on', 'SQuAD', 'for', 'Evaluating', 'Accuracy', 'of', 'Enterprise', 'Search', 'Algorithms', '.']","(4, 5)","(19, 24)",0,Metric,['New'],['Based'],0,4,11,4,11
60,https://www.semanticscholar.org/paper/Comparative-Study-of-Machine-Learning-Models-and-on-Patel-Raval/12867c63f39cbed83713dd0d68a56cda66554790,9,Abstract,0,This study aims to provide a comparative analysis of performance of certain models popular in machine learning and the BERT model on the Stanford Question Answering Dataset (SQuAD).,This study aims to provide a comparative analysis of performance of certain models popular in machine learning and,the BERT model,on the Stanford Question Answering Dataset ( SQuAD ) .,"['This', 'study', 'aims', 'to', 'provide', 'a', 'comparative', 'analysis', 'of', 'performance', 'of', 'certain', 'models', 'popular', 'in', 'machine', 'learning', 'and', 'the', 'BERT', 'model', 'on', 'the', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', '.']","(18, 21)","(114, 128)",-1,aims,"['This', 'study']",[],0,11,16,11,16
61,https://www.semanticscholar.org/paper/Comparative-Study-of-Machine-Learning-Models-and-on-Patel-Raval/12867c63f39cbed83713dd0d68a56cda66554790,9,Abstract,0,This study aims to provide a comparative analysis of performance of certain models popular in machine learning and the BERT model on the Stanford Question Answering Dataset (SQuAD).,This study aims to provide a comparative analysis of performance of certain models popular in machine learning and the BERT model on,the Stanford Question Answering Dataset ( SQuAD ),.,"['This', 'study', 'aims', 'to', 'provide', 'a', 'comparative', 'analysis', 'of', 'performance', 'of', 'certain', 'models', 'popular', 'in', 'machine', 'learning', 'and', 'the', 'BERT', 'model', 'on', 'the', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', '.']","(22, 30)","(132, 181)",42,aims,"['This', 'study']",[],0,11,16,11,16
62,https://www.semanticscholar.org/paper/Comparative-Study-of-Machine-Learning-Models-and-on-Patel-Raval/12867c63f39cbed83713dd0d68a56cda66554790,9,Abstract,1,"The analysis shows that the BERT model, which was once state-of-the-art on SQuAD, gives higher accuracy in comparison to other models.",The analysis shows that,"the BERT model , which was once state - of - the - art on SQuAD",", gives higher accuracy in comparison to other models .","['The', 'analysis', 'shows', 'that', 'the', 'BERT', 'model', ',', 'which', 'was', 'once', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'SQuAD', ',', 'gives', 'higher', 'accuracy', 'in', 'comparison', 'to', 'other', 'models', '.']","(4, 20)","(23, 86)",58,shows,['The analysis'],['.'],0,13,19,13,19
63,https://www.semanticscholar.org/paper/Comparative-Study-of-Machine-Learning-Models-and-on-Patel-Raval/12867c63f39cbed83713dd0d68a56cda66554790,9,Abstract,1,"The analysis shows that the BERT model, which was once state-of-the-art on SQuAD, gives higher accuracy in comparison to other models.","The analysis shows that the BERT model , which was once state - of - the - art on",SQuAD,", gives higher accuracy in comparison to other models .","['The', 'analysis', 'shows', 'that', 'the', 'BERT', 'model', ',', 'which', 'was', 'once', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'SQuAD', ',', 'gives', 'higher', 'accuracy', 'in', 'comparison', 'to', 'other', 'models', '.']","(19, 20)","(81, 86)",0,shows,['The analysis'],['.'],0,13,19,13,19
64,https://www.semanticscholar.org/paper/Comparative-Study-of-Machine-Learning-Models-and-on-Patel-Raval/12867c63f39cbed83713dd0d68a56cda66554790,9,Abstract,2,"However, BERT requires a greater execution time even when only 100 samples are used.","However ,",BERT,requires a greater execution time even when only 100 samples are used .,"['However', ',', 'BERT', 'requires', 'a', 'greater', 'execution', 'time', 'even', 'when', 'only', '100', 'samples', 'are', 'used', '.']","(2, 3)","(9, 13)",-1,requires,"['However', ',', 'BERT']","['a greater execution', '.']",2,15,24,0,9
65,https://www.semanticscholar.org/paper/Comparative-Study-of-Machine-Learning-Models-and-on-Patel-Raval/12867c63f39cbed83713dd0d68a56cda66554790,9,Abstract,3,This shows that with increasing accuracy more amount of time is invested in training the data.,This shows that with increasing accuracy more amount of time is invested in training the data.,,,"['This', 'shows', 'that', 'with', 'increasing', 'accuracy', 'more', 'amount', 'of', 'time', 'is', 'invested', 'in', 'training', 'the', 'data.']",,,,shows,['This'],[],0,5,11,5,11
66,https://www.semanticscholar.org/paper/Comparative-Study-of-Machine-Learning-Models-and-on-Patel-Raval/12867c63f39cbed83713dd0d68a56cda66554790,9,Abstract,4,"Whereas in case of preliminary machine learning models, execution time for full data is lower but accuracy is compromised","Whereas in case of preliminary machine learning models, execution time for full data is lower but accuracy is compromised",,,"['Whereas', 'in', 'case', 'of', 'preliminary', 'machine', 'learning', 'models,', 'execution', 'time', 'for', 'full', 'data', 'is', 'lower', 'but', 'accuracy', 'is', 'compromised']",,,,case,['Whereas'],[],0,11,16,11,16
67,https://www.semanticscholar.org/paper/Comparative-Study-of-Machine-Learning-Models-and-on-Patel-Raval/12867c63f39cbed83713dd0d68a56cda66554790,9,Title,0,Comparative Study of Machine Learning Models and BERT on SQuAD.,Comparative Study of Machine Learning Models and BERT on,SQuAD,.,"['Comparative', 'Study', 'of', 'Machine', 'Learning', 'Models', 'and', 'BERT', 'on', 'SQuAD', '.']","(9, 10)","(56, 61)",0,Comparative,[],['Study'],0,0,12,0,12
68,https://www.semanticscholar.org/paper/Reproducing-a-Neural-Question-Answering-Applied-to-D%C3%BCr-Rauber/191421cc19206e82b199113fa252546f16cf5114,10,Abstract,0,Reproducibility is one of the pillars of scientific research.,Reproducibility is one of the pillars of scientific research.,,,"['Reproducibility', 'is', 'one', 'of', 'the', 'pillars', 'of', 'scientific', 'research.']",,,,is,"['Reproducibility', 'one of the pillars of scientific research']",['.'],0,16,19,16,19
69,https://www.semanticscholar.org/paper/Reproducing-a-Neural-Question-Answering-Applied-to-D%C3%BCr-Rauber/191421cc19206e82b199113fa252546f16cf5114,10,Abstract,1,"This study attempts to reproduce the Gated Self-Matching Network, which is the basis of one of the best performing models on the SQuAD dataset.",This study attempts to reproduce,"the Gated Self - Matching Network , which is the basis of one of the best performing models on the SQuAD dataset",.,"['This', 'study', 'attempts', 'to', 'reproduce', 'the', 'Gated', 'Self', '-', 'Matching', 'Network', ',', 'which', 'is', 'the', 'basis', 'of', 'one', 'of', 'the', 'best', 'performing', 'models', 'on', 'the', 'SQuAD', 'dataset', '.']","(5, 27)","(32, 144)",99,study,['This'],"['attempts to reproduce the Gated Self - Matching Network , which is the basis of one of the best performing models on the SQuAD dataset']",0,5,11,5,11
70,https://www.semanticscholar.org/paper/Reproducing-a-Neural-Question-Answering-Applied-to-D%C3%BCr-Rauber/191421cc19206e82b199113fa252546f16cf5114,10,Abstract,2,We reimplement the neural network model and highlight ambiguities in the original architectural description.,We reimplement,the neural network model,and highlight ambiguities in the original architectural description .,"['We', 'reimplement', 'the', 'neural', 'network', 'model', 'and', 'highlight', 'ambiguities', 'in', 'the', 'original', 'architectural', 'description', '.']","(2, 6)","(14, 38)",-1,ambiguities,['We reimplement the neural network model and highlight'],[],2,54,66,14,26
71,https://www.semanticscholar.org/paper/Reproducing-a-Neural-Question-Answering-Applied-to-D%C3%BCr-Rauber/191421cc19206e82b199113fa252546f16cf5114,10,Abstract,3,"We show that due to uncertainty about only two components of the neural network model and no precise description of the training process, it is not possible to reproduce the experimental results obtained by the original implementation.",We show that due to uncertainty about only two components of,the neural network model,"and no precise description of the training process , it is not possible to reproduce the experimental results obtained by the original implementation .","['We', 'show', 'that', 'due', 'to', 'uncertainty', 'about', 'only', 'two', 'components', 'of', 'the', 'neural', 'network', 'model', 'and', 'no', 'precise', 'description', 'of', 'the', 'training', 'process', ',', 'it', 'is', 'not', 'possible', 'to', 'reproduce', 'the', 'experimental', 'results', 'obtained', 'by', 'the', 'original', 'implementation', '.']","(11, 15)","(60, 84)",-1,is possible,"['We show', 'it']",[],2,142,158,56,72
72,https://www.semanticscholar.org/paper/Reproducing-a-Neural-Question-Answering-Applied-to-D%C3%BCr-Rauber/191421cc19206e82b199113fa252546f16cf5114,10,Abstract,4,"Finally we summarize what we learned from this reproduction process about writing precise neural network architecture descriptions, providing our implementation as a basis for future exploration","Finally we summarize what we learned from this reproduction process about writing precise neural network architecture descriptions, providing our implementation as a basis for future exploration",,,"['Finally', 'we', 'summarize', 'what', 'we', 'learned', 'from', 'this', 'reproduction', 'process', 'about', 'writing', 'precise', 'neural', 'network', 'architecture', 'descriptions,', 'providing', 'our', 'implementation', 'as', 'a', 'basis', 'for', 'future', 'exploration']",,,,summarize,"['Finally', 'we', 'what']",[],0,11,21,11,21
73,https://www.semanticscholar.org/paper/Reproducing-a-Neural-Question-Answering-Applied-to-D%C3%BCr-Rauber/191421cc19206e82b199113fa252546f16cf5114,10,Title,0,Reproducing a Neural Question Answering Architecture Applied to the SQuAD Benchmark Dataset: Challenges and Lessons Learned.,Reproducing a Neural Question Answering Architecture Applied to the,SQuAD,Benchmark Dataset : Challenges and Lessons Learned .,"['Reproducing', 'a', 'Neural', 'Question', 'Answering', 'Architecture', 'Applied', 'to', 'the', 'SQuAD', 'Benchmark', 'Dataset', ':', 'Challenges', 'and', 'Lessons', 'Learned', '.']","(9, 10)","(67, 72)",0,Reproducing,[],"['a', 'Neural Question Answering Architecture Applied to the SQuAD Benchmark Dataset : Challenges and Lessons Learned .']",0,0,12,0,12
74,https://www.semanticscholar.org/paper/Automatic-Spanish-Translation-of-the-SQuAD-Dataset-Carrino-Costa-juss%C3%A0/da9082edcac8fb83289371a77aecdfcddf0a7aa5,11,Abstract,0,"Recently, multilingual question answering became a crucial research topic, and it is receiving increased interest in the NLP community.","Recently, multilingual question answering became a crucial research topic, and it is receiving increased interest in the NLP community.",,,"['Recently,', 'multilingual', 'question', 'answering', 'became', 'a', 'crucial', 'research', 'topic,', 'and', 'it', 'is', 'receiving', 'increased', 'interest', 'in', 'the', 'NLP', 'community.']",,,,Recently,[],"[', multilingual question answering became a crucial research topic , and it is receiving increased interest in the NLP community']",0,0,9,0,9
75,https://www.semanticscholar.org/paper/Automatic-Spanish-Translation-of-the-SQuAD-Dataset-Carrino-Costa-juss%C3%A0/da9082edcac8fb83289371a77aecdfcddf0a7aa5,11,Abstract,1,"However, the unavailability of large-scale datasets makes it challenging to train multilingual QA systems with performance comparable to the English ones.","However, the unavailability of large-scale datasets makes it challenging to train multilingual QA systems with performance comparable to the English ones.",,,"['However,', 'the', 'unavailability', 'of', 'large-scale', 'datasets', 'makes', 'it', 'challenging', 'to', 'train', 'multilingual', 'QA', 'systems', 'with', 'performance', 'comparable', 'to', 'the', 'English', 'ones.']",,,,makes,"['However', ',', 'the unavailability of large - scale datasets']",['it challenging to train multilingual QA systems with performance comparable to the English ones'],0,55,61,55,61
76,https://www.semanticscholar.org/paper/Automatic-Spanish-Translation-of-the-SQuAD-Dataset-Carrino-Costa-juss%C3%A0/da9082edcac8fb83289371a77aecdfcddf0a7aa5,11,Abstract,2,"In this work, we develop the Translate Align Retrieve (TAR) method to automatically translate the Stanford Question Answering Dataset (SQuAD) v1.1 to Spanish.","In this work , we develop the Translate Align Retrieve ( TAR ) method to automatically translate",the Stanford Question Answering Dataset ( SQuAD ) v1.1,to Spanish .,"['In', 'this', 'work', ',', 'we', 'develop', 'the', 'Translate', 'Align', 'Retrieve', '(', 'TAR', ')', 'method', 'to', 'automatically', 'translate', 'the', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', 'v1.1', 'to', 'Spanish', '.']","(17, 26)","(96, 150)",42,develop,[],['the Translate Align Retrieve ( TAR ) method to automatically translate the Stanford Question Answering Dataset ( SQuAD ) v1.1 to Spanish .'],0,18,26,18,26
77,https://www.semanticscholar.org/paper/Automatic-Spanish-Translation-of-the-SQuAD-Dataset-Carrino-Costa-juss%C3%A0/da9082edcac8fb83289371a77aecdfcddf0a7aa5,11,Abstract,3,We then used this dataset to train Spanish QA systems by fine-tuning a Multilingual-BERT model.,We then used,this dataset,to train Spanish QA systems by fine - tuning a Multilingual - BERT model .,"['We', 'then', 'used', 'this', 'dataset', 'to', 'train', 'Spanish', 'QA', 'systems', 'by', 'fine', '-', 'tuning', 'a', 'Multilingual', '-', 'BERT', 'model', '.']","(3, 5)","(12, 24)",-1,used,"['We', 'then']","['this', 'dataset to train Spanish QA systems by fine - tuning a Multilingual - BERT model']",0,8,13,8,13
78,https://www.semanticscholar.org/paper/Automatic-Spanish-Translation-of-the-SQuAD-Dataset-Carrino-Costa-juss%C3%A0/da9082edcac8fb83289371a77aecdfcddf0a7aa5,11,Abstract,4,"Finally, we evaluated our QA models with the recently proposed MLQA and XQuAD benchmarks for cross-lingual Extractive QA.","Finally, we evaluated our QA models with the recently proposed MLQA and XQuAD benchmarks for cross-lingual Extractive QA.",,,"['Finally,', 'we', 'evaluated', 'our', 'QA', 'models', 'with', 'the', 'recently', 'proposed', 'MLQA', 'and', 'XQuAD', 'benchmarks', 'for', 'cross-lingual', 'Extractive', 'QA.']",,,,evaluated,['we'],['our QA models with the recently proposed MLQA and XQuAD benchmarks for cross - lingual Extractive QA .'],0,13,23,13,23
79,https://www.semanticscholar.org/paper/Automatic-Spanish-Translation-of-the-SQuAD-Dataset-Carrino-Costa-juss%C3%A0/da9082edcac8fb83289371a77aecdfcddf0a7aa5,11,Abstract,5,Experimental results show that our models outperform the previous Multilingual-BERT baselines achieving the new state-of-the-art value of 68.1 F1 points on the Spanish MLQA corpus and 77.6 F1 and 61.8 Exact Match points on the Spanish XQuAD corpus.,Experimental results show that our models outperform the previous Multilingual-BERT baselines achieving the new state-of-the-art value of 68.1 F1 points on the Spanish MLQA corpus and 77.6 F1 and 61.8 Exact Match points on the Spanish XQuAD corpus.,,,"['Experimental', 'results', 'show', 'that', 'our', 'models', 'outperform', 'the', 'previous', 'Multilingual-BERT', 'baselines', 'achieving', 'the', 'new', 'state-of-the-art', 'value', 'of', '68.1', 'F1', 'points', 'on', 'the', 'Spanish', 'MLQA', 'corpus', 'and', '77.6', 'F1', 'and', '61.8', 'Exact', 'Match', 'points', 'on', 'the', 'Spanish', 'XQuAD', 'corpus.']",,,,show,"['Experimental', 'results']",[],0,21,26,21,26
80,https://www.semanticscholar.org/paper/Automatic-Spanish-Translation-of-the-SQuAD-Dataset-Carrino-Costa-juss%C3%A0/da9082edcac8fb83289371a77aecdfcddf0a7aa5,11,Abstract,6,"The resulting, synthetically generated SQuAD-es v1.1 corpora, with almost 100% of data contained in the original English version, to the best of our knowledge, is the first large-scale QA training resource for Spanish","The resulting , synthetically generated",SQuAD,"-es v1.1 corpora , with almost 100 % of data contained in the original English version , to the best of our knowledge , is the first large-scale QA training resource for Spanish","['The', 'resulting', ',', 'synthetically', 'generated', 'SQuAD', '-es', 'v1.1', 'corpora', ',', 'with', 'almost', '100', '%', 'of', 'data', 'contained', 'in', 'the', 'original', 'English', 'version', ',', 'to', 'the', 'best', 'of', 'our', 'knowledge', ',', 'is', 'the', 'first', 'large-scale', 'QA', 'training', 'resource', 'for', 'Spanish']","(5, 6)","(39, 44)",0,synthetically,[],[],0,16,30,16,30
81,https://www.semanticscholar.org/paper/Automatic-Spanish-Translation-of-the-SQuAD-Dataset-Carrino-Costa-juss%C3%A0/da9082edcac8fb83289371a77aecdfcddf0a7aa5,11,Title,0,Automatic Spanish Translation of the SQuAD Dataset for Multilingual Question Answering.,Automatic Spanish Translation of the,SQuAD,Dataset for Multilingual Question Answering .,"['Automatic', 'Spanish', 'Translation', 'of', 'the', 'SQuAD', 'Dataset', 'for', 'Multilingual', 'Question', 'Answering', '.']","(5, 6)","(36, 41)",0,Spanish,[],['Translation of the SQuAD Dataset for Multilingual Question Answering .'],0,10,18,10,18
82,https://www.semanticscholar.org/paper/SQuAD2-CR%3A-Semi-supervised-Annotation-for-Cause-and-Lee-Hwang/27faa3ad168a64ad46a2aecfdb8b302d8217d811,12,Abstract,0,"Existing machine reading comprehension models are reported to be brittle for adversarially perturbed questions when optimizing only for accuracy, which led to the creation of new reading comprehension benchmarks, such as SQuAD 2.0 which contains such type of questions.","Existing machine reading comprehension models are reported to be brittle for adversarially perturbed questions when optimizing only for accuracy , which led to the creation of new reading comprehension benchmarks , such as",SQuAD,2.0 which contains such type of questions .,"['Existing', 'machine', 'reading', 'comprehension', 'models', 'are', 'reported', 'to', 'be', 'brittle', 'for', 'adversarially', 'perturbed', 'questions', 'when', 'optimizing', 'only', 'for', 'accuracy', ',', 'which', 'led', 'to', 'the', 'creation', 'of', 'new', 'reading', 'comprehension', 'benchmarks', ',', 'such', 'as', 'SQuAD', '2.0', 'which', 'contains', 'such', 'type', 'of', 'questions', '.']","(33, 34)","(222, 227)",0,are reported,['Existing machine reading comprehension models'],[],0,46,59,46,59
83,https://www.semanticscholar.org/paper/SQuAD2-CR%3A-Semi-supervised-Annotation-for-Cause-and-Lee-Hwang/27faa3ad168a64ad46a2aecfdb8b302d8217d811,12,Abstract,1,"However, despite the super-human accuracy of existing models on such datasets, it is still unclear how the model predicts the answerability of the question, potentially due to the absence of a shared annotation for the explanation.","However, despite the super-human accuracy of existing models on such datasets, it is still unclear how the model predicts the answerability of the question, potentially due to the absence of a shared annotation for the explanation.",,,"['However,', 'despite', 'the', 'super-human', 'accuracy', 'of', 'existing', 'models', 'on', 'such', 'datasets,', 'it', 'is', 'still', 'unclear', 'how', 'the', 'model', 'predicts', 'the', 'answerability', 'of', 'the', 'question,', 'potentially', 'due', 'to', 'the', 'absence', 'of', 'a', 'shared', 'annotation', 'for', 'the', 'explanation.']",,,,is unclear,"['However despite the super - human accuracy of existing models on such datasets , it']","['the model predicts the answerability of the question , potentially due to the absence of a shared annotation for the explanation .']",0,86,103,86,103
84,https://www.semanticscholar.org/paper/SQuAD2-CR%3A-Semi-supervised-Annotation-for-Cause-and-Lee-Hwang/27faa3ad168a64ad46a2aecfdb8b302d8217d811,12,Abstract,2,"To address such absence, we release SQuAD2-CR dataset, which contains annotations on unanswerable questions from the SQuAD 2.0 dataset, to enable an explanatory analysis of the model prediction.","To address such absence , we release",SQuAD2,"-CR dataset , which contains annotations on unanswerable questions from the SQuAD 2.0 dataset , to enable an explanatory analysis of the model prediction .","['To', 'address', 'such', 'absence', ',', 'we', 'release', 'SQuAD2', '-CR', 'dataset', ',', 'which', 'contains', 'annotations', 'on', 'unanswerable', 'questions', 'from', 'the', 'SQuAD', '2.0', 'dataset', ',', 'to', 'enable', 'an', 'explanatory', 'analysis', 'of', 'the', 'model', 'prediction', '.']","(7, 8)","(36, 42)",0,",",['To address such absence'],"['we release SQuAD2 -CR dataset , which contains annotations on unanswerable questions from the SQuAD 2.0 dataset , to enable an explanatory analysis of the model prediction .']",0,24,26,24,26
85,https://www.semanticscholar.org/paper/SQuAD2-CR%3A-Semi-supervised-Annotation-for-Cause-and-Lee-Hwang/27faa3ad168a64ad46a2aecfdb8b302d8217d811,12,Abstract,3,"Specifically, we annotate (1) explanation on why the most plausible answer span cannot be the answer and (2) which part of the question causes unanswerability.","Specifically, we annotate (1) explanation on why the most plausible answer span cannot be the answer and (2) which part of the question causes unanswerability.",,,"['Specifically,', 'we', 'annotate', '(1)', 'explanation', 'on', 'why', 'the', 'most', 'plausible', 'answer', 'span', 'cannot', 'be', 'the', 'answer', 'and', '(2)', 'which', 'part', 'of', 'the', 'question', 'causes', 'unanswerability.']",,,,),"['we annotate', '(', '1']",[],0,31,33,31,33
86,https://www.semanticscholar.org/paper/SQuAD2-CR%3A-Semi-supervised-Annotation-for-Cause-and-Lee-Hwang/27faa3ad168a64ad46a2aecfdb8b302d8217d811,12,Abstract,4,We share intuitions and experimental results that how this dataset can be used to analyze and improve the interpretability of existing reading comprehension model behavior,We share intuitions and experimental results that how this dataset can be used to analyze and improve the interpretability of existing reading comprehension model behavior,,,"['We', 'share', 'intuitions', 'and', 'experimental', 'results', 'that', 'how', 'this', 'dataset', 'can', 'be', 'used', 'to', 'analyze', 'and', 'improve', 'the', 'interpretability', 'of', 'existing', 'reading', 'comprehension', 'model', 'behavior']",,,,We,[],"['share intuitions and', 'experimental results that how this dataset can be used to analyze and improve the interpretability of existing reading comprehension model behavior']",0,0,3,0,3
87,https://www.semanticscholar.org/paper/SQuAD2-CR%3A-Semi-supervised-Annotation-for-Cause-and-Lee-Hwang/27faa3ad168a64ad46a2aecfdb8b302d8217d811,12,Title,0,SQuAD2-CR: Semi-supervised Annotation for Cause and Rationales for Unanswerability in SQuAD 2.0.,,SQuAD2,-CR : Semi-supervised Annotation for Cause and Rationales for Unanswerability in SQuAD 2.0 .,"['SQuAD2', '-CR', ':', 'Semi-supervised', 'Annotation', 'for', 'Cause', 'and', 'Rationales', 'for', 'Unanswerability', 'in', 'SQuAD', '2.0', '.']","(0, 1)","(0, 6)",0,-CR,['SQuAD2'],[': Semi - supervised Annotation for Cause and Rationales for Unanswerability in SQuAD 2.0 .'],2,7,11,0,4
88,https://www.semanticscholar.org/paper/BART%3A-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481,13,Abstract,0,"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models.","We present BART, a denoising autoencoder for pretraining sequence-to-sequence models.",,,"['We', 'present', 'BART,', 'a', 'denoising', 'autoencoder', 'for', 'pretraining', 'sequence-to-sequence', 'models.']",,,,present,['We .'],"['BART', ', a denoising autoencoder for pretraining sequence - to - sequence models']",0,3,11,3,11
89,https://www.semanticscholar.org/paper/BART%3A-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481,13,Abstract,1,"BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.","BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.",,,"['BART', 'is', 'trained', 'by', '(1)', 'corrupting', 'text', 'with', 'an', 'arbitrary', 'noising', 'function,', 'and', '(2)', 'learning', 'a', 'model', 'to', 'reconstruct', 'the', 'original', 'text.']",,,,is trained,['BART'],"['( ) corrupting text with an arbitrary noising function , and ( 2 ) learning a model to reconstruct the original text']",0,5,16,5,16
90,https://www.semanticscholar.org/paper/BART%3A-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481,13,Abstract,2,"It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.","It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.",,,"['It', 'uses', 'a', 'standard', 'Tranformer-based', 'neural', 'machine', 'translation', 'architecture', 'which,', 'despite', 'its', 'simplicity,', 'can', 'be', 'seen', 'as', 'generalizing', 'BERT', '(due', 'to', 'the', 'bidirectional', 'encoder),', 'GPT', '(with', 'the', 'left-to-right', 'decoder),', 'and', 'many', 'other', 'more', 'recent', 'pretraining', 'schemes.']",,,,uses,['It'],"['a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes .']",0,3,8,3,8
91,https://www.semanticscholar.org/paper/BART%3A-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481,13,Abstract,3,"We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token.","We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token.",,,"['We', 'evaluate', 'a', 'number', 'of', 'noising', 'approaches,', 'finding', 'the', 'best', 'performance', 'by', 'both', 'randomly', 'shuffling', 'the', 'order', 'of', 'the', 'original', 'sentences', 'and', 'using', 'a', 'novel', 'in-filling', 'scheme,', 'where', 'spans', 'of', 'text', 'are', 'replaced', 'with', 'a', 'single', 'mask', 'token.']",,,,are replaced,"['where spans of text', 'We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of the original sentences and using a novel in - filling scheme', ',']",['.'],0,197,210,197,210
92,https://www.semanticscholar.org/paper/BART%3A-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481,13,Abstract,4,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks.,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks.,,,"['BART', 'is', 'particularly', 'effective', 'when', 'fine', 'tuned', 'for', 'text', 'generation', 'but', 'also', 'works', 'well', 'for', 'comprehension', 'tasks.']",,,,is effective,"['BART', 'when fine tuned for text generation but also works well for comprehension tasks', 'particularly']",['.'],0,5,31,5,31
93,https://www.semanticscholar.org/paper/BART%3A-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481,13,Abstract,5,"It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.",It matches the performance of RoBERTa with comparable training resources on GLUE and,SQuAD,", achieves new state-of-the-art results on a range of abstractive dialogue , question answering , and summarization tasks , with gains of up to 6 ROUGE .","['It', 'matches', 'the', 'performance', 'of', 'RoBERTa', 'with', 'comparable', 'training', 'resources', 'on', 'GLUE', 'and', 'SQuAD', ',', 'achieves', 'new', 'state-of-the-art', 'results', 'on', 'a', 'range', 'of', 'abstractive', 'dialogue', ',', 'question', 'answering', ',', 'and', 'summarization', 'tasks', ',', 'with', 'gains', 'of', 'up', 'to', '6', 'ROUGE', '.']","(13, 14)","(84, 89)",0,question,"['It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD , achieves new state - of - the - art results on a range of abstractive dialogue', ',']","['answering and summarization', 'tasks', ',']",2,174,183,83,92
94,https://www.semanticscholar.org/paper/BART%3A-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481,13,Abstract,6,"BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.","BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.",,,"['BART', 'also', 'provides', 'a', '1.1', 'BLEU', 'increase', 'over', 'a', 'back-translation', 'system', 'for', 'machine', 'translation,', 'with', 'only', 'target', 'language', 'pretraining.']",,,,provides,['BART'],"['a 1.1 BLEU', 'increase over a back - translation system for machine translation , with only target language pretraining']",0,10,19,10,19
95,https://www.semanticscholar.org/paper/BART%3A-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481,13,Abstract,7,"We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance","We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance",,,"['We', 'also', 'report', 'ablation', 'experiments', 'that', 'replicate', 'other', 'pretraining', 'schemes', 'within', 'the', 'BART', 'framework,', 'to', 'better', 'measure', 'which', 'factors', 'most', 'influence', 'end-task', 'performance']",,,,report,"['We', 'experiments that replicate other pretraining schemes within the BART framework , to better measure which factors most influence end - task performance']",[],0,8,15,8,15
96,https://www.semanticscholar.org/paper/BART%3A-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481,13,Title,0,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.","BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.",,,"['BART:', 'Denoising', 'Sequence-to-Sequence', 'Pre-training', 'for', 'Natural', 'Language', 'Generation,', 'Translation,', 'and', 'Comprehension.']",,,,BART,[],"[':', 'Denoising to Pre training for Natural Language Generation , Translation , and Comprehension', 'Sequence - - Sequence -']",0,0,5,0,5
97,https://www.semanticscholar.org/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88,14,Abstract,0,Machine reading comprehension with unanswerable questions aims to abstain from answering when no answer can be inferred.,Machine reading comprehension with unanswerable questions aims to abstain from answering when no answer can be inferred.,,,"['Machine', 'reading', 'comprehension', 'with', 'unanswerable', 'questions', 'aims', 'to', 'abstain', 'from', 'answering', 'when', 'no', 'answer', 'can', 'be', 'inferred.']",,,,Machine,[],['reading comprehension with unanswerable questions aims to abstain from answering when no answer can be inferred .'],0,0,8,0,8
98,https://www.semanticscholar.org/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88,14,Abstract,1,"In addition to extract answers, previous works usually predict an additional ""no-answer"" probability to detect unanswerable cases.","In addition to extract answers, previous works usually predict an additional ""no-answer"" probability to detect unanswerable cases.",,,"['In', 'addition', 'to', 'extract', 'answers,', 'previous', 'works', 'usually', 'predict', 'an', 'additional', '""no-answer""', 'probability', 'to', 'detect', 'unanswerable', 'cases.']",,,,predict,"[',', 'previous works', 'usually']","['an additional "" no - answer "" probability to detect unanswerable cases', '.']",0,56,64,56,64
99,https://www.semanticscholar.org/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88,14,Abstract,2,"However, they fail to validate the answerability of the question by verifying the legitimacy of the predicted answer.","However, they fail to validate the answerability of the question by verifying the legitimacy of the predicted answer.",,,"['However,', 'they', 'fail', 'to', 'validate', 'the', 'answerability', 'of', 'the', 'question', 'by', 'verifying', 'the', 'legitimacy', 'of', 'the', 'predicted', 'answer.']",,,,validate,['to'],['the answerability of the question'],0,23,32,23,32
100,https://www.semanticscholar.org/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88,14,Abstract,3,"To address this problem, we propose a novel read-then-verify system, which not only utilizes a neural reader to extract candidate answers and produce no-answer probabilities, but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets.","To address this problem, we propose a novel read-then-verify system, which not only utilizes a neural reader to extract candidate answers and produce no-answer probabilities, but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets.",,,"['To', 'address', 'this', 'problem,', 'we', 'propose', 'a', 'novel', 'read-then-verify', 'system,', 'which', 'not', 'only', 'utilizes', 'a', 'neural', 'reader', 'to', 'extract', 'candidate', 'answers', 'and', 'produce', 'no-answer', 'probabilities,', 'but', 'also', 'leverages', 'an', 'answer', 'verifier', 'to', 'decide', 'whether', 'the', 'predicted', 'answer', 'is', 'entailed', 'by', 'the', 'input', 'snippets.']",,,,problem,"['this', 'To address']","['we propose a novel read - then - verify system , which not only utilizes a neural reader to extract candidate answers and produce no - answer probabilities , but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets .']",0,16,24,16,24
101,https://www.semanticscholar.org/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88,14,Abstract,4,"Moreover, we introduce two auxiliary losses to help the reader better handle answer extraction as well as no-answer detection, and investigate three different architectures for the answer verifier.","Moreover, we introduce two auxiliary losses to help the reader better handle answer extraction as well as no-answer detection, and investigate three different architectures for the answer verifier.",,,"['Moreover,', 'we', 'introduce', 'two', 'auxiliary', 'losses', 'to', 'help', 'the', 'reader', 'better', 'handle', 'answer', 'extraction', 'as', 'well', 'as', 'no-answer', 'detection,', 'and', 'investigate', 'three', 'different', 'architectures', 'for', 'the', 'answer', 'verifier.']",,,,introduce,['we'],['two auxiliary losses'],0,14,24,14,24
102,https://www.semanticscholar.org/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88,14,Abstract,5,"Our experiments on the SQuAD 2.0 dataset show that our system achieves a score of 74.2 F1 on the test set, achieving state-of-the-art results at the time of submission (Aug. 28th, 2018).",Our experiments on the,SQuAD,"2.0 dataset show that our system achieves a score of 74.2 F1 on the test set , achieving state-of-the-art results at the time of submission ( Aug. 28th , 2018 ) .","['Our', 'experiments', 'on', 'the', 'SQuAD', '2.0', 'dataset', 'show', 'that', 'our', 'system', 'achieves', 'a', 'score', 'of', '74.2', 'F1', 'on', 'the', 'test', 'set', ',', 'achieving', 'state-of-the-art', 'results', 'at', 'the', 'time', 'of', 'submission', '(', 'Aug.', '28th', ',', '2018', ')', '.']","(4, 5)","(22, 27)",0,show,['Our experiments on the SQuAD 2.0 dataset'],[],2,41,46,12,17
103,https://www.semanticscholar.org/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88,14,Title,0,Read + Verify: Machine Reading Comprehension with Unanswerable Questions.,Read + Verify: Machine Reading Comprehension with Unanswerable Questions.,,,"['Read', '+', 'Verify:', 'Machine', 'Reading', 'Comprehension', 'with', 'Unanswerable', 'Questions.']",,,,Verify,['Read +'],[': Machine Reading Comprehension with Unanswerable Questions'],0,7,14,7,14
104,https://www.semanticscholar.org/paper/Multi-step-Retriever-Reader-Interaction-for-Das-Dhuliawala/e7512b84e923372ae410d7614e71224d573ed2ef,15,Abstract,0,This paper introduces a new framework for open-domain question answering in which the retriever and the reader iteratively interact with each other.,This paper introduces a new framework for open-domain question answering in which the retriever and the reader iteratively interact with each other.,,,"['This', 'paper', 'introduces', 'a', 'new', 'framework', 'for', 'open-domain', 'question', 'answering', 'in', 'which', 'the', 'retriever', 'and', 'the', 'reader', 'iteratively', 'interact', 'with', 'each', 'other.']",,,,question,"['This paper introduces a new framework for', 'open - domain']",['answering in which the retriever and the reader iteratively interact with each other'],0,56,65,56,65
105,https://www.semanticscholar.org/paper/Multi-step-Retriever-Reader-Interaction-for-Das-Dhuliawala/e7512b84e923372ae410d7614e71224d573ed2ef,15,Abstract,1,"The framework is agnostic to the architecture of the machine reading model, only requiring access to the token-level hidden representations of the reader.","The framework is agnostic to the architecture of the machine reading model, only requiring access to the token-level hidden representations of the reader.",,,"['The', 'framework', 'is', 'agnostic', 'to', 'the', 'architecture', 'of', 'the', 'machine', 'reading', 'model,', 'only', 'requiring', 'access', 'to', 'the', 'token-level', 'hidden', 'representations', 'of', 'the', 'reader.']",,,,is agnostic,['The framework'],"[',']",0,14,26,14,26
106,https://www.semanticscholar.org/paper/Multi-step-Retriever-Reader-Interaction-for-Das-Dhuliawala/e7512b84e923372ae410d7614e71224d573ed2ef,15,Abstract,2,The retriever uses fast nearest neighbor search to scale to corpora containing millions of paragraphs.,The retriever uses fast nearest neighbor search to scale to corpora containing millions of paragraphs.,,,"['The', 'retriever', 'uses', 'fast', 'nearest', 'neighbor', 'search', 'to', 'scale', 'to', 'corpora', 'containing', 'millions', 'of', 'paragraphs.']",,,,uses,"['The', 'retriever']","['fast', 'nearest', 'neighbor search to scale to corpora containing millions of paragraphs']",0,14,19,14,19
107,https://www.semanticscholar.org/paper/Multi-step-Retriever-Reader-Interaction-for-Das-Dhuliawala/e7512b84e923372ae410d7614e71224d573ed2ef,15,Abstract,3,A gated recurrent unit updates the query at each step conditioned on the state of the reader and the reformulated query is used to re-rank the paragraphs by the retriever.,A gated recurrent unit updates the query at each step conditioned on the state of the reader and the reformulated query is used to re-rank the paragraphs by the retriever.,,,"['A', 'gated', 'recurrent', 'unit', 'updates', 'the', 'query', 'at', 'each', 'step', 'conditioned', 'on', 'the', 'state', 'of', 'the', 'reader', 'and', 'the', 'reformulated', 'query', 'is', 'used', 'to', 're-rank', 'the', 'paragraphs', 'by', 'the', 'retriever.']",,,,is used,['A gated recurrent unit updates the query at each step conditioned on the state of the reader and the reformulated query'],[],0,120,128,120,128
108,https://www.semanticscholar.org/paper/Multi-step-Retriever-Reader-Interaction-for-Das-Dhuliawala/e7512b84e923372ae410d7614e71224d573ed2ef,15,Abstract,4,We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus.,We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus.,,,"['We', 'conduct', 'analysis', 'and', 'show', 'that', 'iterative', 'interaction', 'helps', 'in', 'retrieving', 'informative', 'paragraphs', 'from', 'the', 'corpus.']",,,,analysis,[],['show that iterative interaction helps in retrieving informative paragraphs from the corpus .'],0,11,20,11,20
109,https://www.semanticscholar.org/paper/Multi-step-Retriever-Reader-Interaction-for-Das-Dhuliawala/e7512b84e923372ae410d7614e71224d573ed2ef,15,Abstract,5,"Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures DrQA and BiDAF on various large open-domain datasets --- TriviaQA-unfiltered, QuasarT, SearchQA, and SQuAD-Open.","Finally , we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures DrQA and BiDAF on various large open-domain datasets -- - TriviaQA-unfiltered , QuasarT , SearchQA , and",SQuAD,-Open .,"['Finally', ',', 'we', 'show', 'that', 'our', 'multi-step-reasoning', 'framework', 'brings', 'consistent', 'improvement', 'when', 'applied', 'to', 'two', 'widely', 'used', 'reader', 'architectures', 'DrQA', 'and', 'BiDAF', 'on', 'various', 'large', 'open-domain', 'datasets', '--', '-', 'TriviaQA-unfiltered', ',', 'QuasarT', ',', 'SearchQA', ',', 'and', 'SQuAD', '-Open', '.']","(36, 37)","(245, 250)",0,show,"[',', 'we']",[],0,13,18,13,18
110,https://www.semanticscholar.org/paper/Multi-step-Retriever-Reader-Interaction-for-Das-Dhuliawala/e7512b84e923372ae410d7614e71224d573ed2ef,15,Title,0,Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering.,Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering.,,,"['Multi-step', 'Retriever-Reader', 'Interaction', 'for', 'Scalable', 'Open-domain', 'Question', 'Answering.']",,,,step,"['Multi', '-']","['Retriever - Reader', 'Interaction for Scalable Open - domain Question Answering']",0,8,13,8,13
111,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,0,In recent years researchers have achieved considerable success applying neural network methods to question answering (QA).,In recent years researchers have achieved considerable success applying neural network methods to question answering (QA).,,,"['In', 'recent', 'years', 'researchers', 'have', 'achieved', 'considerable', 'success', 'applying', 'neural', 'network', 'methods', 'to', 'question', 'answering', '(QA).']",,,,have achieved,['researchers'],['considerable success applying neural network methods to question answering ( QA ) .'],0,28,42,28,42
112,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,1,These approaches have achieved state of the art results in simplified closed-domain settings such as the SQuAD (Rajpurkar et al.,These approaches have achieved state of the art results in simplified closed-domain settings such as the,SQuAD,( Rajpurkar et al .,"['These', 'approaches', 'have', 'achieved', 'state', 'of', 'the', 'art', 'results', 'in', 'simplified', 'closed-domain', 'settings', 'such', 'as', 'the', 'SQuAD', '(', 'Rajpurkar', 'et', 'al', '.']","(16, 17)","(104, 109)",0,have achieved,['These approaches'],['state of the art results in simplified closed - domain settings such as the SQuAD ( Rajpurkar et al .'],0,17,31,17,31
113,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,2,"2016) dataset, which provides a preselected passage, from which the answer to a given question may be extracted.","2016) dataset, which provides a preselected passage, from which the answer to a given question may be extracted.",,,"['2016)', 'dataset,', 'which', 'provides', 'a', 'preselected', 'passage,', 'from', 'which', 'the', 'answer', 'to', 'a', 'given', 'question', 'may', 'be', 'extracted.']",,,,provides,"['which', '2016 ) dataset ,']","['a preselected passage', ',']",0,23,32,23,32
114,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,3,"More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al.","More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al.",,,"['More', 'recently,', 'researchers', 'have', 'begun', 'to', 'tackle', 'open-domain', 'QA,', 'in', 'which', 'the', 'model', 'is', 'given', 'a', 'question', 'and', 'access', 'to', 'a', 'large', 'corpus', '(e.g.,', 'wikipedia)', 'instead', 'of', 'a', 'pre-selected', 'passage', '(Chen', 'et', 'al.']",,,,recently,[],[],0,5,14,5,14
115,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,4,2017a).,2017a).,,,['2017a).'],,,,2017a,[],[') .'],0,0,6,0,6
116,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,5,"This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that “reads” the passages to generate an answer to the question.","This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that “reads” the passages to generate an answer to the question.",,,"['This', 'setting', 'is', 'more', 'complex', 'as', 'it', 'requires', 'large-scale', 'search', 'for', 'relevant', 'passages', 'by', 'an', 'information', 'retrieval', 'component,', 'combined', 'with', 'a', 'reading', 'comprehension', 'model', 'that', '“reads”', 'the', 'passages', 'to', 'generate', 'an', 'answer', 'to', 'the', 'question.']",,,,is complex,"['This', 'setting', 'more']","['it requires large - scale search for relevant passages by an information retrieval component , combined with a reading comprehension model that “ reads ” the passages to generate an answer to the question']",0,13,29,13,29
117,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,6,Performance in this setting lags well behind closed-domain performance.,Performance in this setting lags well behind closed-domain performance.,,,"['Performance', 'in', 'this', 'setting', 'lags', 'well', 'behind', 'closed-domain', 'performance.']",,,,behind,"['Performance in this setting lags', 'well']","['closed', '- domain performance .']",0,38,45,38,45
118,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,7,"In this paper, we present a novel open-domain QA system called Reinforced Ranker-Reader (R), based on two algorithmic innovations.","In this paper, we present a novel open-domain QA system called Reinforced Ranker-Reader (R), based on two algorithmic innovations.",,,"['In', 'this', 'paper,', 'we', 'present', 'a', 'novel', 'open-domain', 'QA', 'system', 'called', 'Reinforced', 'Ranker-Reader', '(R),', 'based', 'on', 'two', 'algorithmic', 'innovations.']",,,,present,"['we', ',']","['a novel open - domain QA system called Reinforced Ranker - Reader ( R ) , based on two algorithmic innovations .']",0,19,27,19,27
119,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,8,"First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of extracting the ground-truth answer to a given question.","First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of extracting the ground-truth answer to a given question.",,,"['First,', 'we', 'propose', 'a', 'new', 'pipeline', 'for', 'open-domain', 'QA', 'with', 'a', 'Ranker', 'component,', 'which', 'learns', 'to', 'rank', 'retrieved', 'passages', 'in', 'terms', 'of', 'likelihood', 'of', 'extracting', 'the', 'ground-truth', 'answer', 'to', 'a', 'given', 'question.']",,,,propose,"[',', 'we']","['a new pipeline for open - domain QA with a Ranker component', ',', 'which learns to rank retrieved passages in terms of likelihood of extracting the ground - truth answer to a given question .']",0,11,19,11,19
120,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,9,"Second, we propose a novel method that jointly trains the Ranker along with an answer-extraction Reader model, based on reinforcement learning.","Second, we propose a novel method that jointly trains the Ranker along with an answer-extraction Reader model, based on reinforcement learning.",,,"['Second,', 'we', 'propose', 'a', 'novel', 'method', 'that', 'jointly', 'trains', 'the', 'Ranker', 'along', 'with', 'an', 'answer-extraction', 'Reader', 'model,', 'based', 'on', 'reinforcement', 'learning.']",,,,propose,"[',', 'we', 'that']","['a novel method', 'jointly trains the Ranker along with an answer - extraction Reader model , based on reinforcement learning']",0,12,20,12,20
121,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,10,We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets.,We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets.,,,"['We', 'report', 'extensive', 'experimental', 'results', 'showing', 'that', 'our', 'method', 'significantly', 'improves', 'on', 'the', 'state', 'of', 'the', 'art', 'for', 'multiple', 'open-domain', 'QA', 'datasets.']",,,,report,['We'],['extensive experimental results showing that our method significantly improves on the state of the art for multiple open - domain QA datasets'],0,3,10,3,10
122,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Abstract,11,2,2,,,['2'],,,,2,[],[],0,0,2,0,2
123,https://www.semanticscholar.org/paper/R3%3A-Reinforced-Ranker-Reader-for-Open-Domain-Wang-Yu/f7df82c5417b9ec7582def05b79ca080a07c4f3b,16,Title,0,R3: Reinforced Ranker-Reader for Open-Domain Question Answering.,R3: Reinforced Ranker-Reader for Open-Domain Question Answering.,,,"['R3:', 'Reinforced', 'Ranker-Reader', 'for', 'Open-Domain', 'Question', 'Answering.']",,,,R3,[],[': Reinforced Ranker - Reader for Open - Domain Question Answering'],0,0,3,0,3
124,https://www.semanticscholar.org/paper/Inoculation-by-Fine-Tuning%3A-A-Method-for-Analyzing-Liu-Schwartz/9fed0a5813d5c7ea87c48bf3ce7862b0c2fbf0f7,17,Abstract,0,Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks.,Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks.,,,"['Several', 'datasets', 'have', 'recently', 'been', 'constructed', 'to', 'expose', 'brittleness', 'in', 'models', 'trained', 'on', 'existing', 'benchmarks.']",,,,have been constructed,"['Several', 'datasets']",[],0,17,48,17,48
125,https://www.semanticscholar.org/paper/Inoculation-by-Fine-Tuning%3A-A-Method-for-Analyzing-Liu-Schwartz/9fed0a5813d5c7ea87c48bf3ce7862b0c2fbf0f7,17,Abstract,1,"While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal.","While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal.",,,"['While', 'model', 'performance', 'on', 'these', 'challenge', 'datasets', 'is', 'significantly', 'lower', 'compared', 'to', 'the', 'original', 'benchmark,', 'it', 'is', 'unclear', 'what', 'particular', 'weaknesses', 'they', 'reveal.']",,,,is unclear,"['it', 'While model performance on these challenge datasets is significantly lower compared to the original benchmark ,']",[],0,115,126,115,126
126,https://www.semanticscholar.org/paper/Inoculation-by-Fine-Tuning%3A-A-Method-for-Analyzing-Liu-Schwartz/9fed0a5813d5c7ea87c48bf3ce7862b0c2fbf0f7,17,Abstract,2,"For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model's specific training set.","For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model's specific training set.",,,"['For', 'example,', 'a', 'challenge', 'dataset', 'may', 'be', 'difficult', 'because', 'it', 'targets', 'phenomena', 'that', 'current', 'models', 'cannot', 'capture,', 'or', 'because', 'it', 'simply', 'exploits', 'blind', 'spots', 'in', 'a', ""model's"", 'specific', 'training', 'set.']",,,,may be difficult,['For'],"[""because it targets phenomena that current models can not capture , or because it simply exploits blind spots in a model 's specific training set""]",0,34,51,34,51
127,https://www.semanticscholar.org/paper/Inoculation-by-Fine-Tuning%3A-A-Method-for-Analyzing-Liu-Schwartz/9fed0a5813d5c7ea87c48bf3ce7862b0c2fbf0f7,17,Abstract,3,"We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt.","We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt.",,,"['We', 'introduce', 'inoculation', 'by', 'fine-tuning,', 'a', 'new', 'analysis', 'method', 'for', 'studying', 'challenge', 'datasets', 'by', 'exposing', 'models', '(the', 'metaphorical', 'patient)', 'to', 'a', 'small', 'amount', 'of', 'data', 'from', 'the', 'challenge', 'dataset', '(a', 'metaphorical', 'pathogen)', 'and', 'assessing', 'how', 'well', 'they', 'can', 'adapt.']",,,,introduce,['We'],"['inoculation by fine - tuning , a new analysis method for studying challenge datasets by exposing models ( the metaphorical patient ) to a small amount of data from the challenge dataset ( a metaphorical pathogen ) and assessing how well they can adapt .']",0,3,13,3,13
128,https://www.semanticscholar.org/paper/Inoculation-by-Fine-Tuning%3A-A-Method-for-Analyzing-Liu-Schwartz/9fed0a5813d5c7ea87c48bf3ce7862b0c2fbf0f7,17,Abstract,4,"We apply our method to analyze the NLI ""stress tests"" (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017).",We apply our method to analyze,"the NLI "" stress tests "" ( Naik et al . , 2018 ) and the Adversarial SQuAD dataset ( Jia and Liang , 2017 )",.,"['We', 'apply', 'our', 'method', 'to', 'analyze', 'the', 'NLI', '""', 'stress', 'tests', '""', '(', 'Naik', 'et', 'al', '.', ',', '2018', ')', 'and', 'the', 'Adversarial', 'SQuAD', 'dataset', '(', 'Jia', 'and', 'Liang', ',', '2017', ')', '.']","(6, 32)","(30, 137)",69,method,[],"['to analyze the NLI "" stress tests "" ( Naik et al . , 2018 ) and the Adversarial SQuAD dataset ( Jia and Liang , 2017 ) .']",0,13,20,13,20
129,https://www.semanticscholar.org/paper/Inoculation-by-Fine-Tuning%3A-A-Method-for-Analyzing-Liu-Schwartz/9fed0a5813d5c7ea87c48bf3ce7862b0c2fbf0f7,17,Abstract,5,"We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult.","We show that after slight exposure , some of",these datasets,"are no longer challenging , while others remain difficult .","['We', 'show', 'that', 'after', 'slight', 'exposure', ',', 'some', 'of', 'these', 'datasets', 'are', 'no', 'longer', 'challenging', ',', 'while', 'others', 'remain', 'difficult', '.']","(9, 11)","(44, 58)",-1,show,['We'],[],0,3,8,3,8
130,https://www.semanticscholar.org/paper/Inoculation-by-Fine-Tuning%3A-A-Method-for-Analyzing-Liu-Schwartz/9fed0a5813d5c7ea87c48bf3ce7862b0c2fbf0f7,17,Abstract,6,"Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.","Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.",,,"['Our', 'results', 'indicate', 'that', 'failures', 'on', 'challenge', 'datasets', 'may', 'lead', 'to', 'very', 'different', 'conclusions', 'about', 'models,', 'training', 'datasets,', 'and', 'the', 'challenge', 'datasets', 'themselves.']",,,,results,['Our'],"['indicate that failures on challenge datasets may lead to very different conclusions about models , training datasets , and the challenge datasets themselves']",0,4,12,4,12
131,https://www.semanticscholar.org/paper/Inoculation-by-Fine-Tuning%3A-A-Method-for-Analyzing-Liu-Schwartz/9fed0a5813d5c7ea87c48bf3ce7862b0c2fbf0f7,17,Title,0,Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets.,Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets.,,,"['Inoculation', 'by', 'Fine-Tuning:', 'A', 'Method', 'for', 'Analyzing', 'Challenge', 'Datasets.']",,,,Tuning,"['Inoculation by Fine', '-']",[': A Method for Analyzing Challenge Datasets .'],0,22,29,22,29
132,https://www.semanticscholar.org/paper/A-Simple-Method-for-Commonsense-Reasoning-Trinh-Le/d7b6753a2d4a2b286c396854063bde3a91b75535,18,Abstract,0,Commonsense reasoning is a long-standing challenge for deep learning.,Commonsense reasoning is a long-standing challenge for deep learning.,,,"['Commonsense', 'reasoning', 'is', 'a', 'long-standing', 'challenge', 'for', 'deep', 'learning.']",,,,is,['Commonsense reasoning'],['a long - standing challenge for deep learning'],0,22,25,22,25
133,https://www.semanticscholar.org/paper/A-Simple-Method-for-Commonsense-Reasoning-Trinh-Le/d7b6753a2d4a2b286c396854063bde3a91b75535,18,Abstract,1,"For example, it is difficult to use neural networks to tackle the Winograd Schema dataset~\cite{levesque2011winograd}.","For example, it is difficult to use neural networks to tackle the Winograd Schema dataset~\cite{levesque2011winograd}.",,,"['For', 'example,', 'it', 'is', 'difficult', 'to', 'use', 'neural', 'networks', 'to', 'tackle', 'the', 'Winograd', 'Schema', 'dataset~\\cite{levesque2011winograd}.']",,,,is difficult,"[',', 'example it']",[],0,17,30,17,30
134,https://www.semanticscholar.org/paper/A-Simple-Method-for-Commonsense-Reasoning-Trinh-Le/d7b6753a2d4a2b286c396854063bde3a91b75535,18,Abstract,2,"In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning.","In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning.",,,"['In', 'this', 'paper,', 'we', 'present', 'a', 'simple', 'method', 'for', 'commonsense', 'reasoning', 'with', 'neural', 'networks,', 'using', 'unsupervised', 'learning.']",,,,present,"['we', ',']","['a simple method for commonsense reasoning with neural networks', ', using unsupervised learning', '.']",0,19,27,19,27
135,https://www.semanticscholar.org/paper/A-Simple-Method-for-Commonsense-Reasoning-Trinh-Le/d7b6753a2d4a2b286c396854063bde3a91b75535,18,Abstract,3,"Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests.","Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests.",,,"['Key', 'to', 'our', 'method', 'is', 'the', 'use', 'of', 'language', 'models,', 'trained', 'on', 'a', 'massive', 'amount', 'of', 'unlabled', 'data,', 'to', 'score', 'multiple', 'choice', 'questions', 'posed', 'by', 'commonsense', 'reasoning', 'tests.']",,,,is,"['the use of language models , trained on a massive amount of unlabled data ,', 'Key to our method']",['to score multiple choice questions posed by commonsense reasoning tests .'],0,18,21,18,21
136,https://www.semanticscholar.org/paper/A-Simple-Method-for-Commonsense-Reasoning-Trinh-Le/d7b6753a2d4a2b286c396854063bde3a91b75535,18,Abstract,4,"On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features.","On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features.",,,"['On', 'both', 'Pronoun', 'Disambiguation', 'and', 'Winograd', 'Schema', 'challenges,', 'our', 'models', 'outperform', 'previous', 'state-of-the-art', 'methods', 'by', 'a', 'large', 'margin,', 'without', 'using', 'expensive', 'annotated', 'knowledge', 'bases', 'or', 'hand-engineered', 'features.']",,,,challenges,['On both Pronoun Disambiguation and Winograd Schema'],"[',', 'our models outperform previous state - of - the - art methods by a large margin , without using expensive annotated knowledge bases or hand - engineered features .']",0,51,62,51,62
137,https://www.semanticscholar.org/paper/A-Simple-Method-for-Commonsense-Reasoning-Trinh-Le/d7b6753a2d4a2b286c396854063bde3a91b75535,18,Abstract,5,"We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance.","We train an array of large RNN language models that operate at word or character level on LM-1-Billion , CommonCrawl ,",SQuAD,", Gutenberg Books , and a customized corpus for this task and show that diversity of training data plays an important role in test performance .","['We', 'train', 'an', 'array', 'of', 'large', 'RNN', 'language', 'models', 'that', 'operate', 'at', 'word', 'or', 'character', 'level', 'on', 'LM-1-Billion', ',', 'CommonCrawl', ',', 'SQuAD', ',', 'Gutenberg', 'Books', ',', 'and', 'a', 'customized', 'corpus', 'for', 'this', 'task', 'and', 'show', 'that', 'diversity', 'of', 'training', 'data', 'plays', 'an', 'important', 'role', 'in', 'test', 'performance', '.']","(21, 22)","(118, 123)",0,We,[],"['train an array of large RNN language models that operate at word or character level on LM-1-Billion , CommonCrawl , SQuAD , Gutenberg Books , and a customized corpus for this task and show that diversity of training data plays an important role in test performance .']",0,0,3,0,3
138,https://www.semanticscholar.org/paper/A-Simple-Method-for-Commonsense-Reasoning-Trinh-Le/d7b6753a2d4a2b286c396854063bde3a91b75535,18,Abstract,6,"Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.","Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.",,,"['Further', 'analysis', 'also', 'shows', 'that', 'our', 'system', 'successfully', 'discovers', 'important', 'features', 'of', 'the', 'context', 'that', 'decide', 'the', 'correct', 'answer,', 'indicating', 'a', 'good', 'grasp', 'of', 'commonsense', 'knowledge.']",,,,shows,['Further analysis'],"['that our system successfully discovers important features of the context that decide the correct answer , indicating a good grasp of commonsense knowledge']",0,22,28,22,28
139,https://www.semanticscholar.org/paper/A-Simple-Method-for-Commonsense-Reasoning-Trinh-Le/d7b6753a2d4a2b286c396854063bde3a91b75535,18,Title,0,A Simple Method for Commonsense Reasoning.,A Simple Method for Commonsense Reasoning.,,,"['A', 'Simple', 'Method', 'for', 'Commonsense', 'Reasoning.']",,,,Method,[],[],0,9,16,9,16
140,https://www.semanticscholar.org/paper/Universal-Adversarial-Triggers-for-NLP-Wallace-Feng/33b0a849138062b3806bc72f9769bc8dfe434193,19,Abstract,0,Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation.,Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation.,,,"['Adversarial', 'examples', 'highlight', 'model', 'vulnerabilities', 'and', 'are', 'useful', 'for', 'evaluation', 'and', 'interpretation.']",,,,are useful,['Adversarial examples highlight model vulnerabilities and'],[],0,57,68,57,68
141,https://www.semanticscholar.org/paper/Universal-Adversarial-Triggers-for-NLP-Wallace-Feng/33b0a849138062b3806bc72f9769bc8dfe434193,19,Abstract,1,We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset.,We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset.,,,"['We', 'define', 'universal', 'adversarial', 'triggers:', 'input-agnostic', 'sequences', 'of', 'tokens', 'that', 'trigger', 'a', 'model', 'to', 'produce', 'a', 'specific', 'prediction', 'when', 'concatenated', 'to', 'any', 'input', 'from', 'a', 'dataset.']",,,,define,['We'],"['universal', 'adversarial triggers : input - agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset .']",0,3,10,3,10
142,https://www.semanticscholar.org/paper/Universal-Adversarial-Triggers-for-NLP-Wallace-Feng/33b0a849138062b3806bc72f9769bc8dfe434193,19,Abstract,2,"We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction.","We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction.",,,"['We', 'propose', 'a', 'gradient-guided', 'search', 'over', 'tokens', 'which', 'finds', 'short', 'trigger', 'sequences', '(e.g.,', 'one', 'word', 'for', 'classification', 'and', 'four', 'words', 'for', 'language', 'modeling)', 'that', 'successfully', 'trigger', 'the', 'target', 'prediction.']",,,,finds,"['which', 'trigger sequences ( e.g. , one word for classification and four words for language modeling ) that successfully trigger the target prediction .', 'We', 'propose', 'a gradient - guided search over tokens']",['short'],0,56,62,56,62
143,https://www.semanticscholar.org/paper/Universal-Adversarial-Triggers-for-NLP-Wallace-Feng/33b0a849138062b3806bc72f9769bc8dfe434193,19,Abstract,3,"For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of ""why"" questions in SQuAD to be answered ""to kill american people"", and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts.","For example , triggers cause SNLI entailment accuracy to drop from 89.94 % to 0.55 % , 72 % of `` why '' questions in",SQuAD,"to be answered `` to kill american people '' , and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts .","['For', 'example', ',', 'triggers', 'cause', 'SNLI', 'entailment', 'accuracy', 'to', 'drop', 'from', '89.94', '%', 'to', '0.55', '%', ',', '72', '%', 'of', '``', 'why', ""''"", 'questions', 'in', 'SQuAD', 'to', 'be', 'answered', '``', 'to', 'kill', 'american', 'people', ""''"", ',', 'and', 'the', 'GPT-2', 'language', 'model', 'to', 'spew', 'racist', 'output', 'even', 'when', 'conditioned', 'on', 'non-racial', 'contexts', '.']","(25, 26)","(117, 122)",0,conditioned,['even'],[],2,234,246,110,122
144,https://www.semanticscholar.org/paper/Universal-Adversarial-Triggers-for-NLP-Wallace-Feng/33b0a849138062b3806bc72f9769bc8dfe434193,19,Abstract,4,"Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider.","Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider.",,,"['Furthermore,', 'although', 'the', 'triggers', 'are', 'optimized', 'using', 'white-box', 'access', 'to', 'a', 'specific', 'model,', 'they', 'transfer', 'to', 'other', 'models', 'for', 'all', 'tasks', 'we', 'consider.']",,,,transfer,['they'],[],0,102,111,102,111
145,https://www.semanticscholar.org/paper/Universal-Adversarial-Triggers-for-NLP-Wallace-Feng/33b0a849138062b3806bc72f9769bc8dfe434193,19,Abstract,5,"Finally, since triggers are input-agnostic, they provide an analysis of global model behavior.","Finally, since triggers are input-agnostic, they provide an analysis of global model behavior.",,,"['Finally,', 'since', 'triggers', 'are', 'input-agnostic,', 'they', 'provide', 'an', 'analysis', 'of', 'global', 'model', 'behavior.']",,,,are input,[],"['-', 'agnostic', 'they provide an analysis of global model behavior .']",0,25,35,25,35
146,https://www.semanticscholar.org/paper/Universal-Adversarial-Triggers-for-NLP-Wallace-Feng/33b0a849138062b3806bc72f9769bc8dfe434193,19,Abstract,6,"For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.","For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",,,"['For', 'instance,', 'they', 'confirm', 'that', 'SNLI', 'models', 'exploit', 'dataset', 'biases', 'and', 'help', 'to', 'diagnose', 'heuristics', 'learned', 'by', 'reading', 'comprehension', 'models.']",,,,confirm,"['instance ,', 'they']",[],0,20,28,20,28
147,https://www.semanticscholar.org/paper/Universal-Adversarial-Triggers-for-NLP-Wallace-Feng/33b0a849138062b3806bc72f9769bc8dfe434193,19,Title,0,Universal Adversarial Triggers for NLP.,Universal Adversarial Triggers for NLP.,,,"['Universal', 'Adversarial', 'Triggers', 'for', 'NLP.']",,,,Adversarial,[],['Universal Triggers for NLP'],0,10,22,10,22
148,https://www.semanticscholar.org/paper/Multi-Hop-Paragraph-Retrieval-for-Open-Domain-Feldman-El-Yaniv/03ffb4b19fef9ae483e526c2b6608ae8e1f65561,20,Abstract,0,This paper is concerned with the task of multi-hop open-domain Question Answering (QA).,This paper is concerned with the task of multi-hop open-domain Question Answering (QA).,,,"['This', 'paper', 'is', 'concerned', 'with', 'the', 'task', 'of', 'multi-hop', 'open-domain', 'Question', 'Answering', '(QA).']",,,,is concerned,['This paper'],[],0,11,24,11,24
149,https://www.semanticscholar.org/paper/Multi-Hop-Paragraph-Retrieval-for-Open-Domain-Feldman-El-Yaniv/03ffb4b19fef9ae483e526c2b6608ae8e1f65561,20,Abstract,1,This task is particularly challenging since it requires the simultaneous performance of textual reasoning and efficient searching.,This task is particularly challenging since it requires the simultaneous performance of textual reasoning and efficient searching.,,,"['This', 'task', 'is', 'particularly', 'challenging', 'since', 'it', 'requires', 'the', 'simultaneous', 'performance', 'of', 'textual', 'reasoning', 'and', 'efficient', 'searching.']",,,,is challenging,"['This task', 'particularly']","['since', 'it requires the simultaneous performance of textual reasoning and efficient searching', '.']",0,10,38,10,38
150,https://www.semanticscholar.org/paper/Multi-Hop-Paragraph-Retrieval-for-Open-Domain-Feldman-El-Yaniv/03ffb4b19fef9ae483e526c2b6608ae8e1f65561,20,Abstract,2,"We present a method for retrieving multiple supporting paragraphs, nested amidst a large knowledge base, which contain the necessary evidence to answer a given question.","We present a method for retrieving multiple supporting paragraphs, nested amidst a large knowledge base, which contain the necessary evidence to answer a given question.",,,"['We', 'present', 'a', 'method', 'for', 'retrieving', 'multiple', 'supporting', 'paragraphs,', 'nested', 'amidst', 'a', 'large', 'knowledge', 'base,', 'which', 'contain', 'the', 'necessary', 'evidence', 'to', 'answer', 'a', 'given', 'question.']",,,,contain,"['which', 'We present a method for retrieving multiple supporting paragraphs , nested amidst a large knowledge base ,']",['the necessary evidence to answer a given question .'],0,113,121,113,121
151,https://www.semanticscholar.org/paper/Multi-Hop-Paragraph-Retrieval-for-Open-Domain-Feldman-El-Yaniv/03ffb4b19fef9ae483e526c2b6608ae8e1f65561,20,Abstract,3,Our method iteratively retrieves supporting paragraphs by forming a joint vector representation of both a question and a paragraph.,Our method iteratively retrieves supporting paragraphs by forming a joint vector representation of both a question and a paragraph.,,,"['Our', 'method', 'iteratively', 'retrieves', 'supporting', 'paragraphs', 'by', 'forming', 'a', 'joint', 'vector', 'representation', 'of', 'both', 'a', 'question', 'and', 'a', 'paragraph.']",,,,method,['Our'],"['iteratively', 'retrieves', 'supporting paragraphs by forming a joint vector representation of both a question and a paragraph']",0,4,11,4,11
152,https://www.semanticscholar.org/paper/Multi-Hop-Paragraph-Retrieval-for-Open-Domain-Feldman-El-Yaniv/03ffb4b19fef9ae483e526c2b6608ae8e1f65561,20,Abstract,4,The retrieval is performed by considering contextualized sentence-level representations of the paragraphs in the knowledge source.,The retrieval is performed by considering contextualized sentence-level representations of the paragraphs in the knowledge source.,,,"['The', 'retrieval', 'is', 'performed', 'by', 'considering', 'contextualized', 'sentence-level', 'representations', 'of', 'the', 'paragraphs', 'in', 'the', 'knowledge', 'source.']",,,,is performed,['The retrieval'],[],0,14,27,14,27
153,https://www.semanticscholar.org/paper/Multi-Hop-Paragraph-Retrieval-for-Open-Domain-Feldman-El-Yaniv/03ffb4b19fef9ae483e526c2b6608ae8e1f65561,20,Abstract,5,"Our method achieves state-of-the-art performance over two well-known datasets, SQuAD-Open and HotpotQA, which serve as our single- and multi-hop open-domain QA benchmarks, respectively.","Our method achieves state-of-the-art performance over two well-known datasets ,",SQuAD,"-Open and HotpotQA , which serve as our single- and multi-hop open-domain QA benchmarks , respectively .","['Our', 'method', 'achieves', 'state-of-the-art', 'performance', 'over', 'two', 'well-known', 'datasets', ',', 'SQuAD', '-Open', 'and', 'HotpotQA', ',', 'which', 'serve', 'as', 'our', 'single-', 'and', 'multi-hop', 'open-domain', 'QA', 'benchmarks', ',', 'respectively', '.']","(10, 11)","(79, 84)",0,domain,"['Our method achieves state - of - the - art performance over two well - known datasets , SQuAD -Open and HotpotQA ,', 'which', 'serve', 'as our', 'single-', 'and', 'multi', '-', 'hop', 'open']","['QA benchmarks , respectively .']",2,165,172,79,86
154,https://www.semanticscholar.org/paper/Multi-Hop-Paragraph-Retrieval-for-Open-Domain-Feldman-El-Yaniv/03ffb4b19fef9ae483e526c2b6608ae8e1f65561,20,Title,0,Multi-Hop Paragraph Retrieval for Open-Domain Question Answering.,Multi-Hop Paragraph Retrieval for Open-Domain Question Answering.,,,"['Multi-Hop', 'Paragraph', 'Retrieval', 'for', 'Open-Domain', 'Question', 'Answering.']",,,,Multi,[],"['-', 'Hop Paragraph Retrieval']",0,0,6,0,6
155,https://www.semanticscholar.org/paper/Assessing-the-Benchmarking-Capacity-of-Machine-Sugawara-Stenetorp/9148f4bb8ebdcc75beaddc875d6de857bbe85ba3,21,Abstract,0,Existing analysis work in machine reading comprehension (MRC) is largely concerned with evaluating the capabilities of systems.,Existing analysis work in machine reading comprehension (MRC) is largely concerned with evaluating the capabilities of systems.,,,"['Existing', 'analysis', 'work', 'in', 'machine', 'reading', 'comprehension', '(MRC)', 'is', 'largely', 'concerned', 'with', 'evaluating', 'the', 'capabilities', 'of', 'systems.']",,,,is concerned,['Existing analysis work in machine reading comprehension ( MRC )'],['.'],0,64,85,64,85
156,https://www.semanticscholar.org/paper/Assessing-the-Benchmarking-Capacity-of-Machine-Sugawara-Stenetorp/9148f4bb8ebdcc75beaddc875d6de857bbe85ba3,21,Abstract,1,"However, the capabilities of datasets are not assessed for benchmarking language understanding precisely.","However, the capabilities of datasets are not assessed for benchmarking language understanding precisely.",,,"['However,', 'the', 'capabilities', 'of', 'datasets', 'are', 'not', 'assessed', 'for', 'benchmarking', 'language', 'understanding', 'precisely.']",,,,are assessed,"['However', 'the capabilities of datasets']",[],0,39,56,39,56
157,https://www.semanticscholar.org/paper/Assessing-the-Benchmarking-Capacity-of-Machine-Sugawara-Stenetorp/9148f4bb8ebdcc75beaddc875d6de857bbe85ba3,21,Abstract,2,"We propose a semi-automated, ablation-based methodology for this challenge; By checking whether questions can be solved even after removing features associated with a skill requisite for language understanding, we evaluate to what degree the questions do not require the skill.","We propose a semi-automated, ablation-based methodology for this challenge; By checking whether questions can be solved even after removing features associated with a skill requisite for language understanding, we evaluate to what degree the questions do not require the skill.",,,"['We', 'propose', 'a', 'semi-automated,', 'ablation-based', 'methodology', 'for', 'this', 'challenge;', 'By', 'checking', 'whether', 'questions', 'can', 'be', 'solved', 'even', 'after', 'removing', 'features', 'associated', 'with', 'a', 'skill', 'requisite', 'for', 'language', 'understanding,', 'we', 'evaluate', 'to', 'what', 'degree', 'the', 'questions', 'do', 'not', 'require', 'the', 'skill.']",,,,do require,"['we', 'evaluate', 'to', 'degree the questions']","['what', 'the skill']",0,259,274,259,274
158,https://www.semanticscholar.org/paper/Assessing-the-Benchmarking-Capacity-of-Machine-Sugawara-Stenetorp/9148f4bb8ebdcc75beaddc875d6de857bbe85ba3,21,Abstract,3,"Experiments on 10 datasets (e.g., CoQA, SQuAD v2.0, and RACE) with a strong baseline model show that, for example, the relative scores of a baseline model provided with content words only and with shuffled sentence words in the context are on average 89.2% and 78.5% of the original score, respectively.","Experiments on 10 datasets ( e.g. , CoQA ,",SQuAD,"v2.0 , and RACE ) with a strong baseline model show that , for example , the relative scores of a baseline model provided with content words only and with shuffled sentence words in the context are on average 89.2 % and 78.5 % of the original score , respectively .","['Experiments', 'on', '10', 'datasets', '(', 'e.g.', ',', 'CoQA', ',', 'SQuAD', 'v2.0', ',', 'and', 'RACE', ')', 'with', 'a', 'strong', 'baseline', 'model', 'show', 'that', ',', 'for', 'example', ',', 'the', 'relative', 'scores', 'of', 'a', 'baseline', 'model', 'provided', 'with', 'content', 'words', 'only', 'and', 'with', 'shuffled', 'sentence', 'words', 'in', 'the', 'context', 'are', 'on', 'average', '89.2', '%', 'and', '78.5', '%', 'of', 'the', 'original', 'score', ',', 'respectively', '.']","(9, 10)","(42, 47)",0,datasets,['10'],"['( e.g. CoQA , SQuAD v2.0 , and RACE ) with a strong baseline model show that , for example , the relative scores of a baseline model provided with content words only and with shuffled sentence words in the context are on average 89.2 % and 78.5 % of the original score , respectively .', ',']",0,18,27,18,27
159,https://www.semanticscholar.org/paper/Assessing-the-Benchmarking-Capacity-of-Machine-Sugawara-Stenetorp/9148f4bb8ebdcc75beaddc875d6de857bbe85ba3,21,Abstract,4,These results suggest that most of the questions already answered correctly by the model do not necessarily require grammatical and complex reasoning.,These results suggest that most of the questions already answered correctly by the model do not necessarily require grammatical and complex reasoning.,,,"['These', 'results', 'suggest', 'that', 'most', 'of', 'the', 'questions', 'already', 'answered', 'correctly', 'by', 'the', 'model', 'do', 'not', 'necessarily', 'require', 'grammatical', 'and', 'complex', 'reasoning.']",,,,suggest,"['These', 'results']",['.'],0,14,22,14,22
160,https://www.semanticscholar.org/paper/Assessing-the-Benchmarking-Capacity-of-Machine-Sugawara-Stenetorp/9148f4bb8ebdcc75beaddc875d6de857bbe85ba3,21,Abstract,5,"For precise benchmarking, MRC datasets will need to take extra care in their design to ensure that questions can correctly evaluate the intended skills.","For precise benchmarking, MRC datasets will need to take extra care in their design to ensure that questions can correctly evaluate the intended skills.",,,"['For', 'precise', 'benchmarking,', 'MRC', 'datasets', 'will', 'need', 'to', 'take', 'extra', 'care', 'in', 'their', 'design', 'to', 'ensure', 'that', 'questions', 'can', 'correctly', 'evaluate', 'the', 'intended', 'skills.']",,,,will need,[],['to ensure that questions can correctly evaluate the intended skills .'],0,40,50,40,50
161,https://www.semanticscholar.org/paper/Assessing-the-Benchmarking-Capacity-of-Machine-Sugawara-Stenetorp/9148f4bb8ebdcc75beaddc875d6de857bbe85ba3,21,Title,0,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets.,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets.,,,"['Assessing', 'the', 'Benchmarking', 'Capacity', 'of', 'Machine', 'Reading', 'Comprehension', 'Datasets.']",,,,Assessing,[],"['the', 'Benchmarking Capacity of Machine Reading Comprehension Datasets .']",0,0,10,0,10
162,https://www.semanticscholar.org/paper/XLDA%3A-Cross-Lingual-Data-Augmentation-for-Natural-Singh-McCann/c846cbb24866af99a8d02d4c73aa4d7dd1831538,22,Abstract,0,"While natural language processing systems often focus on a single language, multilingual transfer learning has the potential to improve performance, especially for low-resource languages.","While natural language processing systems often focus on a single language, multilingual transfer learning has the potential to improve performance, especially for low-resource languages.",,,"['While', 'natural', 'language', 'processing', 'systems', 'often', 'focus', 'on', 'a', 'single', 'language,', 'multilingual', 'transfer', 'learning', 'has', 'the', 'potential', 'to', 'improve', 'performance,', 'especially', 'for', 'low-resource', 'languages.']",,,,focus,['natural language processing systems'],[],0,48,54,48,54
163,https://www.semanticscholar.org/paper/XLDA%3A-Cross-Lingual-Data-Augmentation-for-Natural-Singh-McCann/c846cbb24866af99a8d02d4c73aa4d7dd1831538,22,Abstract,1,"We introduce XLDA, cross-lingual data augmentation, a method that replaces a segment of the input text with its translation in another language.","We introduce XLDA, cross-lingual data augmentation, a method that replaces a segment of the input text with its translation in another language.",,,"['We', 'introduce', 'XLDA,', 'cross-lingual', 'data', 'augmentation,', 'a', 'method', 'that', 'replaces', 'a', 'segment', 'of', 'the', 'input', 'text', 'with', 'its', 'translation', 'in', 'another', 'language.']",,,,introduce,['We'],"['XLDA', ',', 'cross - lingual data augmentation , a method that replaces a segment of the input text with its translation in another language']",0,3,13,3,13
164,https://www.semanticscholar.org/paper/XLDA%3A-Cross-Lingual-Data-Augmentation-for-Natural-Singh-McCann/c846cbb24866af99a8d02d4c73aa4d7dd1831538,22,Abstract,2,XLDA enhances performance of all 14 tested languages of the cross-lingual natural language inference (XNLI) benchmark.,XLDA enhances performance of all 14 tested languages of the cross-lingual natural language inference (XNLI) benchmark.,,,"['XLDA', 'enhances', 'performance', 'of', 'all', '14', 'tested', 'languages', 'of', 'the', 'cross-lingual', 'natural', 'language', 'inference', '(XNLI)', 'benchmark.']",,,,benchmark,"['(', 'XNLI', 'XLDA enhances performance of all 14 tested languages of the cross - lingual natural language inference ) .']",[],0,112,122,112,122
165,https://www.semanticscholar.org/paper/XLDA%3A-Cross-Lingual-Data-Augmentation-for-Natural-Singh-McCann/c846cbb24866af99a8d02d4c73aa4d7dd1831538,22,Abstract,3,"With improvements of up to $4.8\%$, training with XLDA achieves state-of-the-art performance for Greek, Turkish, and Urdu.","With improvements of up to $4.8\%$, training with XLDA achieves state-of-the-art performance for Greek, Turkish, and Urdu.",,,"['With', 'improvements', 'of', 'up', 'to', '$4.8\\%$,', 'training', 'with', 'XLDA', 'achieves', 'state-of-the-art', 'performance', 'for', 'Greek,', 'Turkish,', 'and', 'Urdu.']",,,,With,[],"['improvements of up to $ 4.8\\%$', ', training with XLDA achieves state - of - the - art performance for Greek , Turkish , and Urdu .']",0,0,5,0,5
166,https://www.semanticscholar.org/paper/XLDA%3A-Cross-Lingual-Data-Augmentation-for-Natural-Singh-McCann/c846cbb24866af99a8d02d4c73aa4d7dd1831538,22,Abstract,4,"XLDA is in contrast to, and performs markedly better than, a more naive approach that aggregates examples in various languages in a way that each example is solely in one language.","XLDA is in contrast to, and performs markedly better than, a more naive approach that aggregates examples in various languages in a way that each example is solely in one language.",,,"['XLDA', 'is', 'in', 'contrast', 'to,', 'and', 'performs', 'markedly', 'better', 'than,', 'a', 'more', 'naive', 'approach', 'that', 'aggregates', 'examples', 'in', 'various', 'languages', 'in', 'a', 'way', 'that', 'each', 'example', 'is', 'solely', 'in', 'one', 'language.']",,,,is,['XLDA'],[],0,5,8,5,8
167,https://www.semanticscholar.org/paper/XLDA%3A-Cross-Lingual-Data-Augmentation-for-Natural-Singh-McCann/c846cbb24866af99a8d02d4c73aa4d7dd1831538,22,Abstract,5,"On the SQuAD question answering task, we see that XLDA provides a $1.0\%$ performance increase on the English evaluation set.",On the,SQuAD,"question answering task , we see that XLDA provides a $ 1.0\ % $ performance increase on the English evaluation set .","['On', 'the', 'SQuAD', 'question', 'answering', 'task', ',', 'we', 'see', 'that', 'XLDA', 'provides', 'a', '$', '1.0\\', '%', '$', 'performance', 'increase', 'on', 'the', 'English', 'evaluation', 'set', '.']","(2, 3)","(6, 11)",0,question,[],['On we see that XLDA provides a $ 1.0\\ % $ performance increase on the English evaluation set .'],2,13,22,0,9
168,https://www.semanticscholar.org/paper/XLDA%3A-Cross-Lingual-Data-Augmentation-for-Natural-Singh-McCann/c846cbb24866af99a8d02d4c73aa4d7dd1831538,22,Abstract,6,"Comprehensive experiments suggest that most languages are effective as cross-lingual augmentors, that XLDA is robust to a wide range of translation quality, and that XLDA is even more effective for randomly initialized models than for pretrained models.","Comprehensive experiments suggest that most languages are effective as cross-lingual augmentors, that XLDA is robust to a wide range of translation quality, and that XLDA is even more effective for randomly initialized models than for pretrained models.",,,"['Comprehensive', 'experiments', 'suggest', 'that', 'most', 'languages', 'are', 'effective', 'as', 'cross-lingual', 'augmentors,', 'that', 'XLDA', 'is', 'robust', 'to', 'a', 'wide', 'range', 'of', 'translation', 'quality,', 'and', 'that', 'XLDA', 'is', 'even', 'more', 'effective', 'for', 'randomly', 'initialized', 'models', 'than', 'for', 'pretrained', 'models.']",,,,suggest,['Comprehensive experiments'],[],0,26,34,26,34
169,https://www.semanticscholar.org/paper/XLDA%3A-Cross-Lingual-Data-Augmentation-for-Natural-Singh-McCann/c846cbb24866af99a8d02d4c73aa4d7dd1831538,22,Title,0,XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering.,XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering.,,,"['XLDA:', 'Cross-Lingual', 'Data', 'Augmentation', 'for', 'Natural', 'Language', 'Inference', 'and', 'Question', 'Answering.']",,,,XLDA,[],"[':', 'Cross - Lingual Data Augmentation for Natural Language Inference and Question Answering']",0,0,5,0,5
170,https://www.semanticscholar.org/paper/Difficulty-Controllable-Generation-of-Reading-Gao-Bing/bd39ac452ebede84fb849cf1df86269e330309cc,23,Abstract,0,"We investigate the difficulty levels of questions in reading comprehension datasets such as SQuAD, and propose a new question generation setting, named Difficulty-controllable Question Generation (DQG).",We investigate the difficulty levels of questions in reading comprehension datasets such as,SQuAD,", and propose a new question generation setting , named Difficulty-controllable Question Generation ( DQG ) .","['We', 'investigate', 'the', 'difficulty', 'levels', 'of', 'questions', 'in', 'reading', 'comprehension', 'datasets', 'such', 'as', 'SQuAD', ',', 'and', 'propose', 'a', 'new', 'question', 'generation', 'setting', ',', 'named', 'Difficulty-controllable', 'Question', 'Generation', '(', 'DQG', ')', '.']","(13, 14)","(91, 96)",0,Generation,"['Difficulty', 'We investigate the difficulty levels of questions in reading comprehension datasets such as SQuAD , and propose a new question generation setting , named - controllable', 'Question']","['(', 'DQG )']",2,189,200,91,102
171,https://www.semanticscholar.org/paper/Difficulty-Controllable-Generation-of-Reading-Gao-Bing/bd39ac452ebede84fb849cf1df86269e330309cc,23,Abstract,1,"Taking as input a sentence in the reading comprehension paragraph and some of its text fragments (i.e., answers) that we want to ask questions about, a DQG method needs to generate questions each of which has a given text fragment as its answer, and meanwhile the generation is under the control of specified difficulty labels---the output questions should satisfy the specified difficulty as much as possible.","Taking as input a sentence in the reading comprehension paragraph and some of its text fragments (i.e., answers) that we want to ask questions about, a DQG method needs to generate questions each of which has a given text fragment as its answer, and meanwhile the generation is under the control of specified difficulty labels---the output questions should satisfy the specified difficulty as much as possible.",,,"['Taking', 'as', 'input', 'a', 'sentence', 'in', 'the', 'reading', 'comprehension', 'paragraph', 'and', 'some', 'of', 'its', 'text', 'fragments', '(i.e.,', 'answers)', 'that', 'we', 'want', 'to', 'ask', 'questions', 'about,', 'a', 'DQG', 'method', 'needs', 'to', 'generate', 'questions', 'each', 'of', 'which', 'has', 'a', 'given', 'text', 'fragment', 'as', 'its', 'answer,', 'and', 'meanwhile', 'the', 'generation', 'is', 'under', 'the', 'control', 'of', 'specified', 'difficulty', 'labels---the', 'output', 'questions', 'should', 'satisfy', 'the', 'specified', 'difficulty', 'as', 'much', 'as', 'possible.']",,,,),[],[],0,114,116,114,116
172,https://www.semanticscholar.org/paper/Difficulty-Controllable-Generation-of-Reading-Gao-Bing/bd39ac452ebede84fb849cf1df86269e330309cc,23,Abstract,2,"To solve this task, we propose an end-to-end framework to generate questions of designated difficulty levels by exploring a few important intuitions.","To solve this task, we propose an end-to-end framework to generate questions of designated difficulty levels by exploring a few important intuitions.",,,"['To', 'solve', 'this', 'task,', 'we', 'propose', 'an', 'end-to-end', 'framework', 'to', 'generate', 'questions', 'of', 'designated', 'difficulty', 'levels', 'by', 'exploring', 'a', 'few', 'important', 'intuitions.']",,,,solve,['To'],"['this', 'task', 'we propose an end - to - end framework to generate questions of designated difficulty levels by exploring a few important intuitions']",0,3,9,3,9
173,https://www.semanticscholar.org/paper/Difficulty-Controllable-Generation-of-Reading-Gao-Bing/bd39ac452ebede84fb849cf1df86269e330309cc,23,Abstract,3,"For evaluation, we prepared the first dataset of reading comprehension questions with difficulty labels.","For evaluation, we prepared the first dataset of reading comprehension questions with difficulty labels.",,,"['For', 'evaluation,', 'we', 'prepared', 'the', 'first', 'dataset', 'of', 'reading', 'comprehension', 'questions', 'with', 'difficulty', 'labels.']",,,,prepared,"[',', 'we']",['the first dataset of reading comprehension questions with difficulty labels'],0,20,29,20,29
174,https://www.semanticscholar.org/paper/Difficulty-Controllable-Generation-of-Reading-Gao-Bing/bd39ac452ebede84fb849cf1df86269e330309cc,23,Abstract,4,"The results show that the question generated by our framework not only have better quality under the metrics like BLEU, but also comply with the specified difficulty labels.","The results show that the question generated by our framework not only have better quality under the metrics like BLEU, but also comply with the specified difficulty labels.",,,"['The', 'results', 'show', 'that', 'the', 'question', 'generated', 'by', 'our', 'framework', 'not', 'only', 'have', 'better', 'quality', 'under', 'the', 'metrics', 'like', 'BLEU,', 'but', 'also', 'comply', 'with', 'the', 'specified', 'difficulty', 'labels.']",,,,show,['The results'],['.'],0,12,17,12,17
175,https://www.semanticscholar.org/paper/Difficulty-Controllable-Generation-of-Reading-Gao-Bing/bd39ac452ebede84fb849cf1df86269e330309cc,23,Title,0,Difficulty Controllable Generation of Reading Comprehension Questions.,Difficulty Controllable Generation of Reading Comprehension Questions.,,,"['Difficulty', 'Controllable', 'Generation', 'of', 'Reading', 'Comprehension', 'Questions.']",,,,Controllable,['Difficulty .'],['Generation of Reading Comprehension Questions'],0,11,24,11,24
176,https://www.semanticscholar.org/paper/Learning-to-Ask-Unanswerable-Questions-for-Machine-Zhu-Dong/02dbc43fb947eda8f3b83bc085b4deb0f07010f5,24,Abstract,0,Machine reading comprehension with unanswerable questions is a challenging task.,Machine reading comprehension with unanswerable questions is a challenging task.,,,"['Machine', 'reading', 'comprehension', 'with', 'unanswerable', 'questions', 'is', 'a', 'challenging', 'task.']",,,,is task,['Machine reading comprehension with unanswerable questions'],[],0,58,80,58,80
177,https://www.semanticscholar.org/paper/Learning-to-Ask-Unanswerable-Questions-for-Machine-Zhu-Dong/02dbc43fb947eda8f3b83bc085b4deb0f07010f5,24,Abstract,1,"In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding paragraph that contains the answer.","In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding paragraph that contains the answer.",,,"['In', 'this', 'work,', 'we', 'propose', 'a', 'data', 'augmentation', 'technique', 'by', 'automatically', 'generating', 'relevant', 'unanswerable', 'questions', 'according', 'to', 'an', 'answerable', 'question', 'paired', 'with', 'its', 'corresponding', 'paragraph', 'that', 'contains', 'the', 'answer.']",,,,propose,['we'],['a data augmentation technique'],0,18,26,18,26
178,https://www.semanticscholar.org/paper/Learning-to-Ask-Unanswerable-Questions-for-Machine-Zhu-Dong/02dbc43fb947eda8f3b83bc085b4deb0f07010f5,24,Abstract,2,"We introduce a pair-to-sequence model for unanswerable question generation, which effectively captures the interactions between the question and the paragraph.","We introduce a pair-to-sequence model for unanswerable question generation, which effectively captures the interactions between the question and the paragraph.",,,"['We', 'introduce', 'a', 'pair-to-sequence', 'model', 'for', 'unanswerable', 'question', 'generation,', 'which', 'effectively', 'captures', 'the', 'interactions', 'between', 'the', 'question', 'and', 'the', 'paragraph.']",,,,model,['We introduce a pair - to - sequence'],[],0,36,42,36,42
179,https://www.semanticscholar.org/paper/Learning-to-Ask-Unanswerable-Questions-for-Machine-Zhu-Dong/02dbc43fb947eda8f3b83bc085b4deb0f07010f5,24,Abstract,3,We also present a way to construct training data for our question generation models by leveraging the existing reading comprehension dataset.,We also present a way to construct training data for our question generation models by leveraging the existing reading comprehension dataset.,,,"['We', 'also', 'present', 'a', 'way', 'to', 'construct', 'training', 'data', 'for', 'our', 'question', 'generation', 'models', 'by', 'leveraging', 'the', 'existing', 'reading', 'comprehension', 'dataset.']",,,,present,"['We', 'also']","['a', 'way to construct training data for our question generation models by leveraging the existing reading comprehension dataset .']",0,8,16,8,16
180,https://www.semanticscholar.org/paper/Learning-to-Ask-Unanswerable-Questions-for-Machine-Zhu-Dong/02dbc43fb947eda8f3b83bc085b4deb0f07010f5,24,Abstract,4,Experimental results show that the pair-to-sequence model performs consistently better compared with the sequence-to-sequence baseline.,Experimental results show that the pair-to-sequence model performs consistently better compared with the sequence-to-sequence baseline.,,,"['Experimental', 'results', 'show', 'that', 'the', 'pair-to-sequence', 'model', 'performs', 'consistently', 'better', 'compared', 'with', 'the', 'sequence-to-sequence', 'baseline.']",,,,show,"['Experimental', 'results']",['.'],0,21,26,21,26
181,https://www.semanticscholar.org/paper/Learning-to-Ask-Unanswerable-Questions-for-Machine-Zhu-Dong/02dbc43fb947eda8f3b83bc085b4deb0f07010f5,24,Abstract,5,"We further use the automatically generated unanswerable questions as a means of data augmentation on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model.",We further use the automatically generated unanswerable questions as a means of data augmentation on the,SQuAD,"2.0 dataset , yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model .","['We', 'further', 'use', 'the', 'automatically', 'generated', 'unanswerable', 'questions', 'as', 'a', 'means', 'of', 'data', 'augmentation', 'on', 'the', 'SQuAD', '2.0', 'dataset', ',', 'yielding', '1.9', 'absolute', 'F1', 'improvement', 'with', 'BERT-base', 'model', 'and', '1.7', 'absolute', 'F1', 'improvement', 'with', 'BERT-large', 'model', '.']","(16, 17)","(104, 109)",0,questions,['We further use the automatically generated unanswerable'],[],0,56,66,56,66
182,https://www.semanticscholar.org/paper/Learning-to-Ask-Unanswerable-Questions-for-Machine-Zhu-Dong/02dbc43fb947eda8f3b83bc085b4deb0f07010f5,24,Title,0,Learning to Ask Unanswerable Questions for Machine Reading Comprehension.,Learning to Ask Unanswerable Questions for Machine Reading Comprehension.,,,"['Learning', 'to', 'Ask', 'Unanswerable', 'Questions', 'for', 'Machine', 'Reading', 'Comprehension.']",,,,Learning,[],[],0,0,9,0,9
183,https://www.semanticscholar.org/paper/Training-a-Ranking-Function-for-Open-Domain-Htut-Bowman/2af0040b92c2ec3d07d09ed9a8eda6047fe3fed2,25,Abstract,0,"In recent years, there have been amazing advances in deep learning methods for machine reading.","In recent years, there have been amazing advances in deep learning methods for machine reading.",,,"['In', 'recent', 'years,', 'there', 'have', 'been', 'amazing', 'advances', 'in', 'deep', 'learning', 'methods', 'for', 'machine', 'reading.']",,,,have been amazing,['there'],['.'],0,24,42,24,42
184,https://www.semanticscholar.org/paper/Training-a-Ranking-Function-for-Open-Domain-Htut-Bowman/2af0040b92c2ec3d07d09ed9a8eda6047fe3fed2,25,Abstract,1,"In machine reading, the machine reader has to extract the answer from the given ground truth paragraph.","In machine reading, the machine reader has to extract the answer from the given ground truth paragraph.",,,"['In', 'machine', 'reading,', 'the', 'machine', 'reader', 'has', 'to', 'extract', 'the', 'answer', 'from', 'the', 'given', 'ground', 'truth', 'paragraph.']",,,,has,['the machine reader'],[],0,40,44,40,44
185,https://www.semanticscholar.org/paper/Training-a-Ranking-Function-for-Open-Domain-Htut-Bowman/2af0040b92c2ec3d07d09ed9a8eda6047fe3fed2,25,Abstract,2,"Recently, the state-of-the-art machine reading models achieve human level performance in SQuAD which is a reading comprehension-style question answering (QA) task.","Recently , the state-of-the-art machine reading models achieve human level performance in",SQuAD,which is a reading comprehension-style question answering ( QA ) task .,"['Recently', ',', 'the', 'state-of-the-art', 'machine', 'reading', 'models', 'achieve', 'human', 'level', 'performance', 'in', 'SQuAD', 'which', 'is', 'a', 'reading', 'comprehension-style', 'question', 'answering', '(', 'QA', ')', 'task', '.']","(12, 13)","(89, 94)",0,is question,"['Recently , the state - of - the - art machine reading models achieve human level performance in SQuAD which', 'a', 'reading', 'comprehension', '-', 'style']",['answering ( QA ) task'],2,108,152,12,56
186,https://www.semanticscholar.org/paper/Training-a-Ranking-Function-for-Open-Domain-Htut-Bowman/2af0040b92c2ec3d07d09ed9a8eda6047fe3fed2,25,Abstract,3,The success of machine reading has inspired researchers to combine information retrieval with machine reading to tackle open-domain QA.,The success of machine reading has inspired researchers to combine information retrieval with machine reading to tackle open-domain QA.,,,"['The', 'success', 'of', 'machine', 'reading', 'has', 'inspired', 'researchers', 'to', 'combine', 'information', 'retrieval', 'with', 'machine', 'reading', 'to', 'tackle', 'open-domain', 'QA.']",,,,has,['The success of machine reading'],['inspired researchers to combine information retrieval with machine reading to tackle open - domain QA .'],0,31,35,31,35
187,https://www.semanticscholar.org/paper/Training-a-Ranking-Function-for-Open-Domain-Htut-Bowman/2af0040b92c2ec3d07d09ed9a8eda6047fe3fed2,25,Abstract,4,"However, these systems perform poorly compared to reading comprehension-style QA because it is difficult to retrieve the pieces of paragraphs that contain the answer to the question.","However, these systems perform poorly compared to reading comprehension-style QA because it is difficult to retrieve the pieces of paragraphs that contain the answer to the question.",,,"['However,', 'these', 'systems', 'perform', 'poorly', 'compared', 'to', 'reading', 'comprehension-style', 'QA', 'because', 'it', 'is', 'difficult', 'to', 'retrieve', 'the', 'pieces', 'of', 'paragraphs', 'that', 'contain', 'the', 'answer', 'to', 'the', 'question.']",,,,perform,"['However', ', these systems']",[],0,24,32,24,32
188,https://www.semanticscholar.org/paper/Training-a-Ranking-Function-for-Open-Domain-Htut-Bowman/2af0040b92c2ec3d07d09ed9a8eda6047fe3fed2,25,Abstract,5,"In this study, we propose two neural network rankers that assign scores to different passages based on their likelihood of containing the answer to a given question.","In this study, we propose two neural network rankers that assign scores to different passages based on their likelihood of containing the answer to a given question.",,,"['In', 'this', 'study,', 'we', 'propose', 'two', 'neural', 'network', 'rankers', 'that', 'assign', 'scores', 'to', 'different', 'passages', 'based', 'on', 'their', 'likelihood', 'of', 'containing', 'the', 'answer', 'to', 'a', 'given', 'question.']",,,,propose,"[',', 'we']",['two neural network rankers that assign scores to different passages based on their likelihood of containing the answer to a given question .'],0,19,27,19,27
189,https://www.semanticscholar.org/paper/Training-a-Ranking-Function-for-Open-Domain-Htut-Bowman/2af0040b92c2ec3d07d09ed9a8eda6047fe3fed2,25,Abstract,6,"Additionally, we analyze the relative importance of semantic similarity and word level relevance matching in open-domain QA.","Additionally, we analyze the relative importance of semantic similarity and word level relevance matching in open-domain QA.",,,"['Additionally,', 'we', 'analyze', 'the', 'relative', 'importance', 'of', 'semantic', 'similarity', 'and', 'word', 'level', 'relevance', 'matching', 'in', 'open-domain', 'QA.']",,,,analyze,"['Additionally', ',', 'we']",['the relative importance of semantic similarity and word level relevance matching in open - domain QA'],0,18,26,18,26
190,https://www.semanticscholar.org/paper/Training-a-Ranking-Function-for-Open-Domain-Htut-Bowman/2af0040b92c2ec3d07d09ed9a8eda6047fe3fed2,25,Title,0,Training a Ranking Function for Open-Domain Question Answering.,Training a Ranking Function for Open-Domain Question Answering.,,,"['Training', 'a', 'Ranking', 'Function', 'for', 'Open-Domain', 'Question', 'Answering.']",,,,Training,[],['a Ranking Function for Open - Domain Question Answering'],0,0,9,0,9
191,https://www.semanticscholar.org/paper/Self-Training-for-Jointly-Learning-to-Ask-and-Sachan-Xing/a1e79bc3717486b311488bc67b319b3f6a44da14,26,Abstract,0,Building curious machines that can answer as well as ask questions is an important challenge for AI.,Building curious machines that can answer as well as ask questions is an important challenge for AI.,,,"['Building', 'curious', 'machines', 'that', 'can', 'answer', 'as', 'well', 'as', 'ask', 'questions', 'is', 'an', 'important', 'challenge', 'for', 'AI.']",,,,is challenge,['Building curious machines that can answer as well as ask questions'],[],0,67,93,67,93
192,https://www.semanticscholar.org/paper/Self-Training-for-Jointly-Learning-to-Ask-and-Sachan-Xing/a1e79bc3717486b311488bc67b319b3f6a44da14,26,Abstract,1,The two tasks of question answering and question generation are usually tackled separately in the NLP literature.,The two tasks of question answering and question generation are usually tackled separately in the NLP literature.,,,"['The', 'two', 'tasks', 'of', 'question', 'answering', 'and', 'question', 'generation', 'are', 'usually', 'tackled', 'separately', 'in', 'the', 'NLP', 'literature.']",,,,are tackled,"['The two tasks of question answering and question generation', 'usually']",[],0,60,80,60,80
193,https://www.semanticscholar.org/paper/Self-Training-for-Jointly-Learning-to-Ask-and-Sachan-Xing/a1e79bc3717486b311488bc67b319b3f6a44da14,26,Abstract,2,"At the same time, both require significant amounts of supervised data which is hard to obtain in many domains.","At the same time, both require significant amounts of supervised data which is hard to obtain in many domains.",,,"['At', 'the', 'same', 'time,', 'both', 'require', 'significant', 'amounts', 'of', 'supervised', 'data', 'which', 'is', 'hard', 'to', 'obtain', 'in', 'many', 'domains.']",,,,require,['both'],['significant amounts of supervised data'],0,24,32,24,32
194,https://www.semanticscholar.org/paper/Self-Training-for-Jointly-Learning-to-Ask-and-Sachan-Xing/a1e79bc3717486b311488bc67b319b3f6a44da14,26,Abstract,3,"To alleviate these issues, we propose a self-training method for jointly learning to ask as well as answer questions, leveraging unlabeled text along with labeled question answer pairs for learning.","To alleviate these issues, we propose a self-training method for jointly learning to ask as well as answer questions, leveraging unlabeled text along with labeled question answer pairs for learning.",,,"['To', 'alleviate', 'these', 'issues,', 'we', 'propose', 'a', 'self-training', 'method', 'for', 'jointly', 'learning', 'to', 'ask', 'as', 'well', 'as', 'answer', 'questions,', 'leveraging', 'unlabeled', 'text', 'along', 'with', 'labeled', 'question', 'answer', 'pairs', 'for', 'learning.']",,,,propose,"['we', 'To alleviate these issues']","['a self - training method for jointly learning to ask as well as answer questions , leveraging unlabeled text along with labeled question answer pairs for learning .']",0,31,39,31,39
195,https://www.semanticscholar.org/paper/Self-Training-for-Jointly-Learning-to-Ask-and-Sachan-Xing/a1e79bc3717486b311488bc67b319b3f6a44da14,26,Abstract,4,"We evaluate our approach on four benchmark datasets: SQUAD, MS MARCO, WikiQA and TrecQA, and show significant improvements over a number of established baselines on both question answering and question generation tasks.",We evaluate our approach on four benchmark datasets :,SQUAD,", MS MARCO , WikiQA and TrecQA , and show significant improvements over a number of established baselines on both question answering and question generation tasks .","['We', 'evaluate', 'our', 'approach', 'on', 'four', 'benchmark', 'datasets', ':', 'SQUAD', ',', 'MS', 'MARCO', ',', 'WikiQA', 'and', 'TrecQA', ',', 'and', 'show', 'significant', 'improvements', 'over', 'a', 'number', 'of', 'established', 'baselines', 'on', 'both', 'question', 'answering', 'and', 'question', 'generation', 'tasks', '.']","(9, 10)","(53, 58)",0,evaluate,['We'],"['our approach on four benchmark datasets : SQUAD , MS MARCO , WikiQA and TrecQA , and show significant improvements over a number of established baselines on both question answering and question generation tasks .']",0,3,12,3,12
196,https://www.semanticscholar.org/paper/Self-Training-for-Jointly-Learning-to-Ask-and-Sachan-Xing/a1e79bc3717486b311488bc67b319b3f6a44da14,26,Abstract,5,We also achieved new state-of-the-art results on two competitive answer sentence selection tasks: WikiQA and TrecQA.,We also achieved new state-of-the-art results on two competitive answer sentence selection tasks: WikiQA and TrecQA.,,,"['We', 'also', 'achieved', 'new', 'state-of-the-art', 'results', 'on', 'two', 'competitive', 'answer', 'sentence', 'selection', 'tasks:', 'WikiQA', 'and', 'TrecQA.']",,,,state,"['also achieved', 'new']","['-', 'We of - art results on two competitive answer sentence selection tasks : WikiQA and TrecQA .', 'the']",0,21,27,21,27
197,https://www.semanticscholar.org/paper/Self-Training-for-Jointly-Learning-to-Ask-and-Sachan-Xing/a1e79bc3717486b311488bc67b319b3f6a44da14,26,Title,0,Self-Training for Jointly Learning to Ask and Answer Questions.,Self-Training for Jointly Learning to Ask and Answer Questions.,,,"['Self-Training', 'for', 'Jointly', 'Learning', 'to', 'Ask', 'and', 'Answer', 'Questions.']",,,,Self,[],['- Training for Jointly Learning to Ask and Answer Questions'],0,0,5,0,5
198,https://www.semanticscholar.org/paper/Pruning-a-BERT-based-Question-Answering-Model-McCarley/ffc01ec3d950a54b165b0d6b773ef49e4e354d0c,27,Abstract,0,We investigate compressing a BERT-based question answering system by pruning parameters from the underlying BERT model.,We investigate compressing a BERT-based question answering system by pruning parameters from the underlying BERT model.,,,"['We', 'investigate', 'compressing', 'a', 'BERT-based', 'question', 'answering', 'system', 'by', 'pruning', 'parameters', 'from', 'the', 'underlying', 'BERT', 'model.']",,,,investigate,['We'],"['compressing a BERT - based question answering system by pruning parameters from the underlying BERT model', '.']",0,3,15,3,15
199,https://www.semanticscholar.org/paper/Pruning-a-BERT-based-Question-Answering-Model-McCarley/ffc01ec3d950a54b165b0d6b773ef49e4e354d0c,27,Abstract,1,We start from models trained for SQuAD 2.0 and introduce gates that allow selected parts of transformers to be individually eliminated.,We start from models trained for,SQuAD,2.0 and introduce gates that allow selected parts of transformers to be individually eliminated .,"['We', 'start', 'from', 'models', 'trained', 'for', 'SQuAD', '2.0', 'and', 'introduce', 'gates', 'that', 'allow', 'selected', 'parts', 'of', 'transformers', 'to', 'be', 'individually', 'eliminated', '.']","(6, 7)","(32, 37)",0,allow be eliminated,['We start from models trained for SQuAD 2.0 and introduce gates'],['.'],2,68,135,29,96
200,https://www.semanticscholar.org/paper/Pruning-a-BERT-based-Question-Answering-Model-McCarley/ffc01ec3d950a54b165b0d6b773ef49e4e354d0c,27,Abstract,2,"Specifically, we investigate (1) reducing the number of attention heads in each transformer, (2) reducing the intermediate width of the feed-forward sublayer of each transformer, and (3) reducing the embedding dimension.","Specifically, we investigate (1) reducing the number of attention heads in each transformer, (2) reducing the intermediate width of the feed-forward sublayer of each transformer, and (3) reducing the embedding dimension.",,,"['Specifically,', 'we', 'investigate', '(1)', 'reducing', 'the', 'number', 'of', 'attention', 'heads', 'in', 'each', 'transformer,', '(2)', 'reducing', 'the', 'intermediate', 'width', 'of', 'the', 'feed-forward', 'sublayer', 'of', 'each', 'transformer,', 'and', '(3)', 'reducing', 'the', 'embedding', 'dimension.']",,,,investigate,"['we', '( 1 ) reducing the number of attention heads in each transformer', 'Specifically']","[', ( 2 ) reducing the intermediate width of the feed - forward sublayer of each transformer , and ( 3 ) reducing the embedding dimension .']",0,18,30,18,30
201,https://www.semanticscholar.org/paper/Pruning-a-BERT-based-Question-Answering-Model-McCarley/ffc01ec3d950a54b165b0d6b773ef49e4e354d0c,27,Abstract,3,We compare several approaches for determining the values of these gates.,We compare several approaches for determining the values of these gates.,,,"['We', 'compare', 'several', 'approaches', 'for', 'determining', 'the', 'values', 'of', 'these', 'gates.']",,,,compare,['We'],['several approaches for determining the values of these gates'],0,3,11,3,11
202,https://www.semanticscholar.org/paper/Pruning-a-BERT-based-Question-Answering-Model-McCarley/ffc01ec3d950a54b165b0d6b773ef49e4e354d0c,27,Abstract,4,"We find that a combination of pruning attention heads and the feed-forward layer almost doubles the decoding speed, with only a 1.5 f-point loss in accuracy.","We find that a combination of pruning attention heads and the feed-forward layer almost doubles the decoding speed, with only a 1.5 f-point loss in accuracy.",,,"['We', 'find', 'that', 'a', 'combination', 'of', 'pruning', 'attention', 'heads', 'and', 'the', 'feed-forward', 'layer', 'almost', 'doubles', 'the', 'decoding', 'speed,', 'with', 'only', 'a', '1.5', 'f-point', 'loss', 'in', 'accuracy.']",,,,find,['We'],"['that a combination of pruning attention heads and the feed - forward layer almost doubles the decoding speed , with only a 1.5 f - point loss in accuracy', '.']",0,3,8,3,8
203,https://www.semanticscholar.org/paper/Pruning-a-BERT-based-Question-Answering-Model-McCarley/ffc01ec3d950a54b165b0d6b773ef49e4e354d0c,27,Title,0,Pruning a BERT-based Question Answering Model.,Pruning a BERT-based Question Answering Model.,,,"['Pruning', 'a', 'BERT-based', 'Question', 'Answering', 'Model.']",,,,Pruning,[],"['a', 'BERT - based Question Answering Model']",0,0,8,0,8
204,https://www.semanticscholar.org/paper/Frustratingly-Easy-Natural-Question-Answering-Pan-Chakravarti/c7e04335452e988e2be5f1d132e7f6eadad13fd3,28,Abstract,0,"Existing literature on Question Answering (QA) mostly focuses on algorithmic novelty, data augmentation, or increasingly large pre-trained language models like XLNet and RoBERTa.","Existing literature on Question Answering (QA) mostly focuses on algorithmic novelty, data augmentation, or increasingly large pre-trained language models like XLNet and RoBERTa.",,,"['Existing', 'literature', 'on', 'Question', 'Answering', '(QA)', 'mostly', 'focuses', 'on', 'algorithmic', 'novelty,', 'data', 'augmentation,', 'or', 'increasingly', 'large', 'pre-trained', 'language', 'models', 'like', 'XLNet', 'and', 'RoBERTa.']",,,,focuses,"['Existing literature on', 'Question Answering ( QA ) mostly']",[],0,56,64,56,64
205,https://www.semanticscholar.org/paper/Frustratingly-Easy-Natural-Question-Answering-Pan-Chakravarti/c7e04335452e988e2be5f1d132e7f6eadad13fd3,28,Abstract,1,"Additionally, a lot of systems on the QA leaderboards do not have associated research documentation in order to successfully replicate their experiments.","Additionally, a lot of systems on the QA leaderboards do not have associated research documentation in order to successfully replicate their experiments.",,,"['Additionally,', 'a', 'lot', 'of', 'systems', 'on', 'the', 'QA', 'leaderboards', 'do', 'not', 'have', 'associated', 'research', 'documentation', 'in', 'order', 'to', 'successfully', 'replicate', 'their', 'experiments.']",,,,research,"['Additionally , a lot of systems on the QA leaderboards do not have .']",['documentation in order to successfully replicate their experiments'],0,78,87,78,87
206,https://www.semanticscholar.org/paper/Frustratingly-Easy-Natural-Question-Answering-Pan-Chakravarti/c7e04335452e988e2be5f1d132e7f6eadad13fd3,28,Abstract,2,"In this paper, we outline these algorithmic components such as Attention-over-Attention, coupled with data augmentation and ensembling strategies that have shown to yield state-of-the-art results on benchmark datasets like SQuAD, even achieving super-human performance.","In this paper , we outline these algorithmic components such as Attention-over-Attention , coupled with data augmentation and ensembling strategies that have shown to yield state-of-the-art results on benchmark datasets like",SQuAD,", even achieving super-human performance .","['In', 'this', 'paper', ',', 'we', 'outline', 'these', 'algorithmic', 'components', 'such', 'as', 'Attention-over-Attention', ',', 'coupled', 'with', 'data', 'augmentation', 'and', 'ensembling', 'strategies', 'that', 'have', 'shown', 'to', 'yield', 'state-of-the-art', 'results', 'on', 'benchmark', 'datasets', 'like', 'SQuAD', ',', 'even', 'achieving', 'super-human', 'performance', '.']","(31, 32)","(224, 229)",0,outline,"[',', 'we']","['these algorithmic components such as Attention - over - Attention , coupled with data augmentation and ensembling strategies that have shown', 'to yield state - of - the - art results on benchmark datasets like SQuAD , even achieving super - human performance .']",0,19,27,19,27
207,https://www.semanticscholar.org/paper/Frustratingly-Easy-Natural-Question-Answering-Pan-Chakravarti/c7e04335452e988e2be5f1d132e7f6eadad13fd3,28,Abstract,3,"Contrary to these prior results, when we evaluate on the recently proposed Natural Questions benchmark dataset, we find that an incredibly simple approach of transfer learning from BERT outperforms the previous state-of-the-art system trained on 4 million more examples than ours by 1.9 F1 points.","Contrary to these prior results, when we evaluate on the recently proposed Natural Questions benchmark dataset, we find that an incredibly simple approach of transfer learning from BERT outperforms the previous state-of-the-art system trained on 4 million more examples than ours by 1.9 F1 points.",,,"['Contrary', 'to', 'these', 'prior', 'results,', 'when', 'we', 'evaluate', 'on', 'the', 'recently', 'proposed', 'Natural', 'Questions', 'benchmark', 'dataset,', 'we', 'find', 'that', 'an', 'incredibly', 'simple', 'approach', 'of', 'transfer', 'learning', 'from', 'BERT', 'outperforms', 'the', 'previous', 'state-of-the-art', 'system', 'trained', 'on', '4', 'million', 'more', 'examples', 'than', 'ours', 'by', '1.9', 'F1', 'points.']",,,,find,"[',', 'we', 'an incredibly simple approach of transfer learning from BERT outperforms the previous state - of - the - art system trained on 4 million more examples than ours by 1.9 F1 points .', 'when we evaluate on', 'the recently proposed Natural Questions benchmark dataset ,']",[],0,117,122,117,122
208,https://www.semanticscholar.org/paper/Frustratingly-Easy-Natural-Question-Answering-Pan-Chakravarti/c7e04335452e988e2be5f1d132e7f6eadad13fd3,28,Abstract,4,Adding ensembling strategies further improves that number by 2.3 F1 points.,Adding ensembling strategies further improves that number by 2.3 F1 points.,,,"['Adding', 'ensembling', 'strategies', 'further', 'improves', 'that', 'number', 'by', '2.3', 'F1', 'points.']",,,,improves,"['Adding ensembling strategies', 'that', 'number by 2.3 F1 points']",['.'],0,37,46,37,46
209,https://www.semanticscholar.org/paper/Frustratingly-Easy-Natural-Question-Answering-Pan-Chakravarti/c7e04335452e988e2be5f1d132e7f6eadad13fd3,28,Title,0,Frustratingly Easy Natural Question Answering.,Frustratingly Easy Natural Question Answering.,,,"['Frustratingly', 'Easy', 'Natural', 'Question', 'Answering.']",,,,Frustratingly,[],"['Easy', 'Natural Question Answering .']",0,0,14,0,14
210,https://www.semanticscholar.org/paper/MPNet%3A-Masked-and-Permuted-Pre-training-for-Song-Tan/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,29,Abstract,0,BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models.,BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models.,,,"['BERT', 'adopts', 'masked', 'language', 'modeling', '(MLM)', 'for', 'pre-training', 'and', 'is', 'one', 'of', 'the', 'most', 'successful', 'pre-training', 'models.']",,,,is one,['BERT adopts masked language modeling ( MLM ) for pre - training and'],[],0,68,75,68,75
211,https://www.semanticscholar.org/paper/MPNet%3A-Masked-and-Permuted-Pre-training-for-Song-Tan/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,29,Abstract,1,"Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem.","Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem.",,,"['Since', 'BERT', 'neglects', 'dependency', 'among', 'predicted', 'tokens,', 'XLNet', 'introduces', 'permuted', 'language', 'modeling', '(PLM)', 'for', 'pre-training', 'to', 'address', 'this', 'problem.']",,,,predicted,"['Since BERT neglects dependency', 'among tokens , XLNet introduces permuted language modeling ( PLM ) for pre - training to address this problem .']",[],0,37,47,37,47
212,https://www.semanticscholar.org/paper/MPNet%3A-Masked-and-Permuted-Pre-training-for-Song-Tan/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,29,Abstract,2,We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning.,We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning.,,,"['We', 'argue', 'that', 'XLNet', 'does', 'not', 'leverage', 'the', 'full', 'position', 'information', 'of', 'a', 'sentence', 'and', 'thus', 'suffers', 'from', 'position', 'discrepancy', 'between', 'pre-training', 'and', 'fine-tuning.']",,,,suffers,['We argue that XLNet does not leverage the full position information of a sentence and thus'],[],0,91,99,91,99
213,https://www.semanticscholar.org/paper/MPNet%3A-Masked-and-Permuted-Pre-training-for-Song-Tan/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,29,Abstract,3,"In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations.","In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations.",,,"['In', 'this', 'paper,', 'we', 'propose', 'MPNet,', 'a', 'novel', 'pre-training', 'method', 'that', 'inherits', 'the', 'advantages', 'of', 'BERT', 'and', 'XLNet', 'and', 'avoids', 'their', 'limitations.']",,,,propose,"[',', 'we']","['MPNet ,', 'a novel pre - training method inherits the advantages of BERT and XLNet and']",0,19,27,19,27
214,https://www.semanticscholar.org/paper/MPNet%3A-Masked-and-Permuted-Pre-training-for-Song-Tan/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,29,Abstract,4,"MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet).","MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet).",,,"['MPNet', 'leverages', 'the', 'dependency', 'among', 'predicted', 'tokens', 'through', 'permuted', 'language', 'modeling', '(vs.', 'MLM', 'in', 'BERT),', 'and', 'takes', 'auxiliary', 'position', 'information', 'as', 'input', 'to', 'make', 'the', 'model', 'see', 'a', 'full', 'sentence', 'and', 'thus', 'reducing', 'the', 'position', 'discrepancy', '(vs.', 'PLM', 'in', 'XLNet).']",,,,leverages,[],"['the dependency predicted', 'among tokens through permuted language modeling ( vs. MLM in BERT ) , and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy ( vs. PLM in XLNet ) .']",0,6,16,6,16
215,https://www.semanticscholar.org/paper/MPNet%3A-Masked-and-Permuted-Pre-training-for-Song-Tan/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,29,Abstract,5,"We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc).",We pre - train MPNet on a large - scale dataset ( over 160 GB text corpora ) and fine - tune on,"a variety of down - streaming tasks ( GLUE , SQuAD , etc )",.,"['We', 'pre', '-', 'train', 'MPNet', 'on', 'a', 'large', '-', 'scale', 'dataset', '(', 'over', '160', 'GB', 'text', 'corpora', ')', 'and', 'fine', '-', 'tune', 'on', 'a', 'variety', 'of', 'down', '-', 'streaming', 'tasks', '(', 'GLUE', ',', 'SQuAD', ',', 'etc', ')', '.']","(23, 37)","(95, 153)",45,pre,['We'],"['-', 'train MPNet on a large - scale dataset ( over 160 GB text corpora ) and fine - tune on a variety of down - streaming tasks ( GLUE , SQuAD , etc ) .']",0,3,7,3,7
216,https://www.semanticscholar.org/paper/MPNet%3A-Masked-and-Permuted-Pre-training-for-Song-Tan/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,29,Abstract,6,"Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting.","Experimental results show that MPNet outperforms MLM and PLM by a large margin , and achieves better results on",these tasks,"compared with previous state - of - the - art pre - trained methods ( e.g. , BERT , XLNet , RoBERTa ) under the same model setting .","['Experimental', 'results', 'show', 'that', 'MPNet', 'outperforms', 'MLM', 'and', 'PLM', 'by', 'a', 'large', 'margin', ',', 'and', 'achieves', 'better', 'results', 'on', 'these', 'tasks', 'compared', 'with', 'previous', 'state', '-', 'of', '-', 'the', '-', 'art', 'pre', '-', 'trained', 'methods', '(', 'e.g.', ',', 'BERT', ',', 'XLNet', ',', 'RoBERTa', ')', 'under', 'the', 'same', 'model', 'setting', '.']","(19, 21)","(111, 122)",-1,results,[],"['Experimental show that MPNet outperforms MLM and PLM by a large margin , and achieves better results on these tasks compared with previous state - of - the - art pre - trained methods ( e.g. , BERT , XLNet , RoBERTa ) under the same model setting .']",0,13,21,13,21
217,https://www.semanticscholar.org/paper/MPNet%3A-Masked-and-Permuted-Pre-training-for-Song-Tan/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,29,Abstract,7,We release the code and pre-trained model in GitHub\footnote{\url{this https URL}}.,We release the code and pre-trained model in GitHub\footnote{\url{this https URL}}.,,,"['We', 'release', 'the', 'code', 'and', 'pre-trained', 'model', 'in', 'GitHub\\footnote{\\url{this', 'https', 'URL}}.']",,,,},[],['}'],3,83,85,-1,1
218,https://www.semanticscholar.org/paper/MPNet%3A-Masked-and-Permuted-Pre-training-for-Song-Tan/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,29,Title,0,MPNet: Masked and Permuted Pre-training for Language Understanding.,MPNet: Masked and Permuted Pre-training for Language Understanding.,,,"['MPNet:', 'Masked', 'and', 'Permuted', 'Pre-training', 'for', 'Language', 'Understanding.']",,,,MPNet,[],[': Masked and Permuted Pre - training for Language Understanding'],0,0,6,0,6
219,https://www.semanticscholar.org/paper/What-do-Models-Learn-from-Question-Answering-Sen-Saffari/8890b53b1f8513b43e6c0e289b6ae260f3d09023,30,Abstract,0,"While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself.",While models have reached superhuman performance on popular question answering ( QA ) datasets such as,SQuAD,", they have yet to outperform humans on the task of question answering itself .","['While', 'models', 'have', 'reached', 'superhuman', 'performance', 'on', 'popular', 'question', 'answering', '(', 'QA', ')', 'datasets', 'such', 'as', 'SQuAD', ',', 'they', 'have', 'yet', 'to', 'outperform', 'humans', 'on', 'the', 'task', 'of', 'question', 'answering', 'itself', '.']","(16, 17)","(102, 107)",0,have reached,"['While', 'models']",[],0,13,26,13,26
220,https://www.semanticscholar.org/paper/What-do-Models-Learn-from-Question-Answering-Sen-Saffari/8890b53b1f8513b43e6c0e289b6ae260f3d09023,30,Abstract,1,"In this paper, we investigate what models are really learning from QA datasets by evaluating BERT-based models across five popular QA datasets.","In this paper, we investigate what models are really learning from QA datasets by evaluating BERT-based models across five popular QA datasets.",,,"['In', 'this', 'paper,', 'we', 'investigate', 'what', 'models', 'are', 'really', 'learning', 'from', 'QA', 'datasets', 'by', 'evaluating', 'BERT-based', 'models', 'across', 'five', 'popular', 'QA', 'datasets.']",,,,investigate,"[',', 'we']",[],0,19,31,19,31
221,https://www.semanticscholar.org/paper/What-do-Models-Learn-from-Question-Answering-Sen-Saffari/8890b53b1f8513b43e6c0e289b6ae260f3d09023,30,Abstract,2,"We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect information in datasets, and ability to handle variations in questions.","We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect information in datasets, and ability to handle variations in questions.",,,"['We', 'evaluate', 'models', 'on', 'their', 'generalizability', 'to', 'out-of-domain', 'examples,', 'responses', 'to', 'missing', 'or', 'incorrect', 'information', 'in', 'datasets,', 'and', 'ability', 'to', 'handle', 'variations', 'in', 'questions.']",,,,We,[],"['evaluate models on their generalizability to out - of - domain examples , responses to missing or incorrect information in datasets , and ability to handle variations in questions .']",0,0,3,0,3
222,https://www.semanticscholar.org/paper/What-do-Models-Learn-from-Question-Answering-Sen-Saffari/8890b53b1f8513b43e6c0e289b6ae260f3d09023,30,Abstract,3,We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods.,We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods.,,,"['We', 'find', 'that', 'no', 'single', 'dataset', 'is', 'robust', 'to', 'all', 'of', 'our', 'experiments', 'and', 'identify', 'shortcomings', 'in', 'both', 'datasets', 'and', 'evaluation', 'methods.']",,,,find,['We'],['.'],0,3,8,3,8
223,https://www.semanticscholar.org/paper/What-do-Models-Learn-from-Question-Answering-Sen-Saffari/8890b53b1f8513b43e6c0e289b6ae260f3d09023,30,Abstract,4,"Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering.","Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering.",,,"['Following', 'our', 'analysis,', 'we', 'make', 'recommendations', 'for', 'building', 'future', 'QA', 'datasets', 'that', 'better', 'evaluate', 'the', 'task', 'of', 'question', 'answering.']",,,,make,['we'],['recommendations for building future QA datasets'],0,28,33,28,33
224,https://www.semanticscholar.org/paper/What-do-Models-Learn-from-Question-Answering-Sen-Saffari/8890b53b1f8513b43e6c0e289b6ae260f3d09023,30,Title,0,What do Models Learn from Question Answering Datasets?.,What do Models Learn from Question Answering Datasets?.,,,"['What', 'do', 'Models', 'Learn', 'from', 'Question', 'Answering', 'Datasets?.']",,,,Models,"['What', 'do']",['Learn'],0,8,15,8,15
225,https://www.semanticscholar.org/paper/What-Happens-To-BERT-Embeddings-During-Fine-tuning-Merchant-Rahimtoroghi/b2fd96a52ded7a64f60c1e54f5bb488c787629c0,31,Abstract,0,"While there has been much recent work studying how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks.","While there has been much recent work studying how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks.",,,"['While', 'there', 'has', 'been', 'much', 'recent', 'work', 'studying', 'how', 'linguistic', 'information', 'is', 'encoded', 'in', 'pre-trained', 'sentence', 'representations,', 'comparatively', 'little', 'is', 'understood', 'about', 'how', 'these', 'models', 'change', 'when', 'adapted', 'to', 'solve', 'downstream', 'tasks.']",,,,work,[],"['studying how linguistic information is encoded in pre - trained sentence representations , comparatively little is understood about how these models change when adapted to solve downstream tasks .']",0,33,38,33,38
226,https://www.semanticscholar.org/paper/What-Happens-To-BERT-Embeddings-During-Fine-tuning-Merchant-Rahimtoroghi/b2fd96a52ded7a64f60c1e54f5bb488c787629c0,31,Abstract,1,"Using a suite of analysis techniques (probing classifiers, Representational Similarity Analysis, and model ablations), we investigate how fine-tuning affects the representations of the BERT model.","Using a suite of analysis techniques (probing classifiers, Representational Similarity Analysis, and model ablations), we investigate how fine-tuning affects the representations of the BERT model.",,,"['Using', 'a', 'suite', 'of', 'analysis', 'techniques', '(probing', 'classifiers,', 'Representational', 'Similarity', 'Analysis,', 'and', 'model', 'ablations),', 'we', 'investigate', 'how', 'fine-tuning', 'affects', 'the', 'representations', 'of', 'the', 'BERT', 'model.']",,,,investigate,"[',', 'we', 'Using a suite of analysis techniques ( probing classifiers , Representational Similarity Analysis , and model ablations )']",[],0,127,139,127,139
227,https://www.semanticscholar.org/paper/What-Happens-To-BERT-Embeddings-During-Fine-tuning-Merchant-Rahimtoroghi/b2fd96a52ded7a64f60c1e54f5bb488c787629c0,31,Abstract,2,"We find that while fine-tuning necessarily makes significant changes, it does not lead to catastrophic forgetting of linguistic phenomena.","We find that while fine-tuning necessarily makes significant changes, it does not lead to catastrophic forgetting of linguistic phenomena.",,,"['We', 'find', 'that', 'while', 'fine-tuning', 'necessarily', 'makes', 'significant', 'changes,', 'it', 'does', 'not', 'lead', 'to', 'catastrophic', 'forgetting', 'of', 'linguistic', 'phenomena.']",,,,find,['We'],['.'],0,3,8,3,8
228,https://www.semanticscholar.org/paper/What-Happens-To-BERT-Embeddings-During-Fine-tuning-Merchant-Rahimtoroghi/b2fd96a52ded7a64f60c1e54f5bb488c787629c0,31,Abstract,3,"We instead find that fine-tuning primarily affects the top layers of BERT, but with noteworthy variation across tasks.","We instead find that fine-tuning primarily affects the top layers of BERT, but with noteworthy variation across tasks.",,,"['We', 'instead', 'find', 'that', 'fine-tuning', 'primarily', 'affects', 'the', 'top', 'layers', 'of', 'BERT,', 'but', 'with', 'noteworthy', 'variation', 'across', 'tasks.']",,,,We,[],"['instead find that fine - tuning primarily affects the top layers of BERT , but with noteworthy variation across tasks', '.']",0,0,3,0,3
229,https://www.semanticscholar.org/paper/What-Happens-To-BERT-Embeddings-During-Fine-tuning-Merchant-Rahimtoroghi/b2fd96a52ded7a64f60c1e54f5bb488c787629c0,31,Abstract,4,"In particular, dependency parsing reconfigures most of the model, whereas SQuAD and MNLI appear to involve much shallower processing.","In particular , dependency parsing reconfigures most of the model , whereas",SQuAD,and MNLI appear to involve much shallower processing .,"['In', 'particular', ',', 'dependency', 'parsing', 'reconfigures', 'most', 'of', 'the', 'model', ',', 'whereas', 'SQuAD', 'and', 'MNLI', 'appear', 'to', 'involve', 'much', 'shallower', 'processing', '.']","(12, 13)","(75, 80)",0,involve,"['In particular , dependency parsing reconfigures most of the model , whereas SQuAD and MNLI appear to']","['much shallower processing', '.']",2,101,109,19,27
230,https://www.semanticscholar.org/paper/What-Happens-To-BERT-Embeddings-During-Fine-tuning-Merchant-Rahimtoroghi/b2fd96a52ded7a64f60c1e54f5bb488c787629c0,31,Abstract,5,"Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.","Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.",,,"['Finally,', 'we', 'also', 'find', 'that', 'fine-tuning', 'has', 'a', 'weaker', 'effect', 'on', 'representations', 'of', 'out-of-domain', 'sentences,', 'suggesting', 'room', 'for', 'improvement', 'in', 'model', 'generalization.']",,,,also,[],"['Finally , we find that fine - tuning has a weaker effect on representations of out - of - domain sentences , suggesting room for improvement in model generalization .']",0,13,18,13,18
231,https://www.semanticscholar.org/paper/What-Happens-To-BERT-Embeddings-During-Fine-tuning-Merchant-Rahimtoroghi/b2fd96a52ded7a64f60c1e54f5bb488c787629c0,31,Title,0,What Happens To BERT Embeddings During Fine-tuning?.,What Happens To BERT Embeddings During Fine-tuning?.,,,"['What', 'Happens', 'To', 'BERT', 'Embeddings', 'During', 'Fine-tuning?.']",,,,Embeddings,"['What Happens To Fine - tuning ? .', 'BERT']",['During'],0,21,32,21,32
