,URL,ID,Type,Index,Text,split_0,split_1,split_2,split_tokens,split_anchor_span,split_anchor_indices,within_anchor_index,group,group2,group3,group4,group5,group6,group7,group8,group9,group10,group11,group12,group13,group14
0,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,0,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP).",,Transfer learning,", where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task , has emerged as a powerful technique in natural language processing ( NLP ) .","['Transfer', 'learning', ',', 'where', 'a', 'model', 'is', 'first', 'pre-trained', 'on', 'a', 'data-rich', 'task', 'before', 'being', 'fine-tuned', 'on', 'a', 'downstream', 'task', ',', 'has', 'emerged', 'as', 'a', 'powerful', 'technique', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '.']","(0, 2)","(0, 17)",0,Transfer learning,1,0,17,0,17,has emerged,2,128,139,110,121,"['Transfer', 'learning']","['has', 'emerged']"
1,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,0,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP).",,Transfer,"learning , where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task , has emerged as a powerful technique in natural language processing ( NLP ) .","['Transfer', 'learning', ',', 'where', 'a', 'model', 'is', 'first', 'pre-trained', 'on', 'a', 'data-rich', 'task', 'before', 'being', 'fine-tuned', 'on', 'a', 'downstream', 'task', ',', 'has', 'emerged', 'as', 'a', 'powerful', 'technique', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', '.']","(0, 1)","(0, 8)",0,Transfer learning,1,0,17,0,17,has emerged,2,128,139,119,130,"['Transfer', 'learning']","['has', 'emerged']"
2,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,1,"The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.",The effectiveness of,transfer learning,"has given rise to a diversity of approaches , methodology , and practice .","['The', 'effectiveness', 'of', 'transfer', 'learning', 'has', 'given', 'rise', 'to', 'a', 'diversity', 'of', 'approaches', ',', 'methodology', ',', 'and', 'practice', '.']","(3, 5)","(20, 37)",0,The effectiveness,0,0,17,0,17,has given,2,39,48,0,9,"['The', 'effectiveness']","['has', 'given']"
3,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,1,"The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.",The effectiveness of,transfer,"learning has given rise to a diversity of approaches , methodology , and practice .","['The', 'effectiveness', 'of', 'transfer', 'learning', 'has', 'given', 'rise', 'to', 'a', 'diversity', 'of', 'approaches', ',', 'methodology', ',', 'and', 'practice', '.']","(3, 4)","(20, 28)",0,The effectiveness,0,0,17,0,17,has given,2,39,48,9,18,"['The', 'effectiveness']","['has', 'given']"
4,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,2,"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format.","In this paper , we explore the landscape of",transfer learning,techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format .,"['In', 'this', 'paper', ',', 'we', 'explore', 'the', 'landscape', 'of', 'transfer', 'learning', 'techniques', 'for', 'NLP', 'by', 'introducing', 'a', 'unified', 'framework', 'that', 'converts', 'every', 'language', 'problem', 'into', 'a', 'text-to-text', 'format', '.']","(9, 11)","(43, 60)",0,we,0,16,18,16,18,explore,0,19,26,19,26,['we'],['explore']
5,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,2,"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format.","In this paper , we explore the landscape of",transfer,learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format .,"['In', 'this', 'paper', ',', 'we', 'explore', 'the', 'landscape', 'of', 'transfer', 'learning', 'techniques', 'for', 'NLP', 'by', 'introducing', 'a', 'unified', 'framework', 'that', 'converts', 'every', 'language', 'problem', 'into', 'a', 'text-to-text', 'format', '.']","(9, 10)","(43, 51)",0,we,0,16,18,16,18,explore,0,19,26,19,26,['we'],['explore']
6,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,3,"Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.","Our systematic study compares pre-training objectives , architectures , unlabeled datasets ,",transfer,"approaches , and other factors on dozens of language understanding tasks .","['Our', 'systematic', 'study', 'compares', 'pre-training', 'objectives', ',', 'architectures', ',', 'unlabeled', 'datasets', ',', 'transfer', 'approaches', ',', 'and', 'other', 'factors', 'on', 'dozens', 'of', 'language', 'understanding', 'tasks', '.']","(12, 13)","(92, 100)",0,systematic study,0,4,20,4,20,compares,0,21,29,21,29,"['systematic', 'study']",['compares']
7,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,4,"By combining the insights from our exploration with scale and our new ""Colossal Clean Crawled Corpus"", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.","By combining the insights from our exploration with scale and our new ""Colossal Clean Crawled Corpus"", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.",,,"['By', 'combining', 'the', 'insights', 'from', 'our', 'exploration', 'with', 'scale', 'and', 'our', 'new', '""Colossal', 'Clean', 'Crawled', 'Corpus"",', 'we', 'achieve', 'state-of-the-art', 'results', 'on', 'many', 'benchmarks', 'covering', 'summarization,', 'question', 'answering,', 'text', 'classification,', 'and', 'more.']",,,,we,0,106,108,106,108,achieve,0,109,116,109,116,['we'],['achieve']
8,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,5,"To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",To facilitate future work on,transfer learning,"for NLP , we release our dataset , pre-trained models , and code .","['To', 'facilitate', 'future', 'work', 'on', 'transfer', 'learning', 'for', 'NLP', ',', 'we', 'release', 'our', 'dataset', ',', 'pre-trained', 'models', ',', 'and', 'code', '.']","(5, 7)","(28, 45)",0,we,2,57,59,10,12,release,2,60,67,13,20,['we'],"['facilitate', 'release']"
9,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,5,"To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",To facilitate future work on,transfer,"learning for NLP , we release our dataset , pre-trained models , and code .","['To', 'facilitate', 'future', 'work', 'on', 'transfer', 'learning', 'for', 'NLP', ',', 'we', 'release', 'our', 'dataset', ',', 'pre-trained', 'models', ',', 'and', 'code', '.']","(5, 6)","(28, 36)",0,we,2,57,59,19,21,release,2,60,67,22,29,['we'],"['facilitate', 'release']"
10,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Title,0,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.,Exploring the Limits of Transfer Learning with a Unified Text-to-Text,Transformer,.,"['Exploring', 'the', 'Limits', 'of', 'Transfer', 'Learning', 'with', 'a', 'Unified', 'Text-to-Text', 'Transformer', '.']","(10, 11)","(69, 80)",0,the Limits Text Transformer,0,10,85,10,85,Exploring,0,0,9,0,9,"['the', 'Limits', 'Text', 'Transformer']",['Exploring']
11,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Title,0,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.,Exploring the Limits of,Transfer Learning,with a Unified Text-to-Text Transformer .,"['Exploring', 'the', 'Limits', 'of', 'Transfer', 'Learning', 'with', 'a', 'Unified', 'Text-to-Text', 'Transformer', '.']","(4, 6)","(23, 40)",0,the Limits Text Transformer,0,10,85,10,85,Exploring,0,0,9,0,9,"['the', 'Limits', 'Text', 'Transformer']",['Exploring']
12,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Title,0,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.,Exploring the Limits of,Transfer,Learning with a Unified Text-to-Text Transformer .,"['Exploring', 'the', 'Limits', 'of', 'Transfer', 'Learning', 'with', 'a', 'Unified', 'Text-to-Text', 'Transformer', '.']","(4, 5)","(23, 31)",0,the Limits Text Transformer,0,10,85,10,85,Exploring,0,0,9,0,9,"['the', 'Limits', 'Text', 'Transformer']",['Exploring']
13,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,0,"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.","Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.",,,"['Large', 'pre-trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP,', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data.']",,,,Large pre - trained neural networks,0,0,35,0,35,have had,0,49,57,49,57,"['Large', 'pre', '-', 'trained', 'neural', 'networks']","['have', 'had', 'motivating']"
14,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,1,"Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers).","Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers).",,,"['Most', 'recent', 'analysis', 'has', 'focused', 'on', 'model', 'outputs', '(e.g.,', 'language', 'model', 'surprisal)', 'or', 'internal', 'vector', 'representations', '(e.g.,', 'probing', 'classifiers).']",,,,Most recent analysis,0,0,20,0,20,has focused,0,21,32,21,32,"['Most', 'recent', 'analysis']","['has', 'focused']"
15,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,2,"Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT.","Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT.",,,"['Complementary', 'to', 'these', 'works,', 'we', 'propose', 'methods', 'for', 'analyzing', 'the', 'attention', 'mechanisms', 'of', 'pre-trained', 'models', 'and', 'apply', 'them', 'to', 'BERT.']",,,,we,0,31,33,31,33,propose,0,34,41,34,41,['we'],['propose']
16,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,3,"BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors.","BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors.",,,"[""BERT's"", 'attention', 'heads', 'exhibit', 'patterns', 'such', 'as', 'attending', 'to', 'delimiter', 'tokens,', 'specific', 'positional', 'offsets,', 'or', 'broadly', 'attending', 'over', 'the', 'whole', 'sentence,', 'with', 'heads', 'in', 'the', 'same', 'layer', 'often', 'exhibiting', 'similar', 'behaviors.']",,,,BERT 's attention,0,0,17,0,17,,,,,,,"['BERT', 'attention']",[]
17,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,4,We further show that certain attention heads correspond well to linguistic notions of syntax and coreference.,We further show that certain attention heads correspond well to linguistic notions of syntax and coreference.,,,"['We', 'further', 'show', 'that', 'certain', 'attention', 'heads', 'correspond', 'well', 'to', 'linguistic', 'notions', 'of', 'syntax', 'and', 'coreference.']",,,,We,0,0,2,0,2,show,0,11,15,11,15,['We'],['show']
18,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,5,"For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy.","For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy.",,,"['For', 'example,', 'we', 'find', 'heads', 'that', 'attend', 'to', 'the', 'direct', 'objects', 'of', 'verbs,', 'determiners', 'of', 'nouns,', 'objects', 'of', 'prepositions,', 'and', 'coreferent', 'mentions', 'with', 'remarkably', 'high', 'accuracy.']",,,,we,0,14,16,14,16,find,0,17,21,17,21,['we'],['find']
19,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,6,"Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.","Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.",,,"['Lastly,', 'we', 'propose', 'an', 'attention-based', 'probing', 'classifier', 'and', 'use', 'it', 'to', 'further', 'demonstrate', 'that', 'substantial', 'syntactic', 'information', 'is', 'captured', 'in', ""BERT's"", 'attention.']",,,,we,0,9,11,9,11,propose and use,0,12,67,12,67,['we'],"['propose', 'use', 'demonstrate']"
20,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Title,0,Transformers: State-of-the-art Natural Language Processing.,,Transformers,: State-of-the-art Natural Language Processing .,"['Transformers', ':', 'State-of-the-art', 'Natural', 'Language', 'Processing', '.']","(0, 1)","(0, 12)",0,Transformers State the art Natural Language Processing,1,0,65,0,65,,,,,,,"['Transformers', 'State', 'the', 'art', 'Natural', 'Language', 'Processing']",[]
21,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,0,Transfer learning has fundamentally changed the landscape of natural language processing (NLP) research.,,Transfer learning,has fundamentally changed the landscape of natural language processing ( NLP ) research .,"['Transfer', 'learning', 'has', 'fundamentally', 'changed', 'the', 'landscape', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', 'research', '.']","(0, 2)","(0, 17)",0,Transfer learning,1,0,17,0,17,has changed,2,18,43,0,25,"['Transfer', 'learning']","['has', 'changed']"
22,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,0,Transfer learning has fundamentally changed the landscape of natural language processing (NLP) research.,,Transfer,learning has fundamentally changed the landscape of natural language processing ( NLP ) research .,"['Transfer', 'learning', 'has', 'fundamentally', 'changed', 'the', 'landscape', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', 'research', '.']","(0, 1)","(0, 8)",0,Transfer learning,1,0,17,0,17,has changed,2,18,43,9,34,"['Transfer', 'learning']","['has', 'changed']"
23,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,1,Many existing state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks.,Many existing state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks.,,,"['Many', 'existing', 'state-of-the-art', 'models', 'are', 'first', 'pre-trained', 'on', 'a', 'large', 'text', 'corpus', 'and', 'then', 'fine-tuned', 'on', 'downstream', 'tasks.']",,,,Many state art models,0,0,43,0,43,are - trained and,0,44,94,44,94,"['Many', 'state', 'art', 'models']","['are', '-', 'trained']"
24,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,2,"However, due to limited data resources from downstream tasks and the extremely large capacity of pre-trained models, aggressive fine-tuning often causes the adapted model to overfit the data of downstream tasks and forget the knowledge of the pre-trained model.","However, due to limited data resources from downstream tasks and the extremely large capacity of pre-trained models, aggressive fine-tuning often causes the adapted model to overfit the data of downstream tasks and forget the knowledge of the pre-trained model.",,,"['However,', 'due', 'to', 'limited', 'data', 'resources', 'from', 'downstream', 'tasks', 'and', 'the', 'extremely', 'large', 'capacity', 'of', 'pre-trained', 'models,', 'aggressive', 'fine-tuning', 'often', 'causes', 'the', 'adapted', 'model', 'to', 'overfit', 'the', 'data', 'of', 'downstream', 'tasks', 'and', 'forget', 'the', 'knowledge', 'of', 'the', 'pre-trained', 'model.']",,,,,,,,,,causes,0,152,158,152,158,[],"['causes', 'overfit', 'forget']"
25,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,3,"To address the above issue in a more principled manner, we propose a new computational framework for robust and efficient fine-tuning for pre-trained language models.","To address the above issue in a more principled manner, we propose a new computational framework for robust and efficient fine-tuning for pre-trained language models.",,,"['To', 'address', 'the', 'above', 'issue', 'in', 'a', 'more', 'principled', 'manner,', 'we', 'propose', 'a', 'new', 'computational', 'framework', 'for', 'robust', 'and', 'efficient', 'fine-tuning', 'for', 'pre-trained', 'language', 'models.']",,,,we,0,57,59,57,59,propose,0,60,67,60,67,['we'],"['address', 'propose']"
26,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,4,"Specifically, our proposed framework contains two important ingredients: 1.","Specifically, our proposed framework contains two important ingredients: 1.",,,"['Specifically,', 'our', 'proposed', 'framework', 'contains', 'two', 'important', 'ingredients:', '1.']",,,,framework,0,28,37,28,37,contains,0,38,46,38,46,['framework'],['contains']
27,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,5,"Smoothness-inducing regularization, which effectively manages the capacity of the model; 2.","Smoothness-inducing regularization, which effectively manages the capacity of the model; 2.",,,"['Smoothness-inducing', 'regularization,', 'which', 'effectively', 'manages', 'the', 'capacity', 'of', 'the', 'model;', '2.']",,,,Smoothness,0,0,10,0,10,,,,,,,['Smoothness'],['inducing']
28,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,6,"Bregman proximal point optimization, which is a class of trust-region methods and can prevent knowledge forgetting.","Bregman proximal point optimization, which is a class of trust-region methods and can prevent knowledge forgetting.",,,"['Bregman', 'proximal', 'point', 'optimization,', 'which', 'is', 'a', 'class', 'of', 'trust-region', 'methods', 'and', 'can', 'prevent', 'knowledge', 'forgetting.']",,,,Bregman proximal point optimization,0,0,35,0,35,,,,,,,"['Bregman', 'proximal', 'point', 'optimization']",[]
29,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,7,Our experiments demonstrate that our proposed method achieves the state-of-the-art performance on multiple NLP benchmarks.,Our experiments demonstrate that our proposed method achieves the state-of-the-art performance on multiple NLP benchmarks.,,,"['Our', 'experiments', 'demonstrate', 'that', 'our', 'proposed', 'method', 'achieves', 'the', 'state-of-the-art', 'performance', 'on', 'multiple', 'NLP', 'benchmarks.']",,,,experiments,0,4,15,4,15,demonstrate,0,16,27,16,27,['experiments'],['demonstrate']
30,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Title,0,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization.,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization.,,,"['SMART:', 'Robust', 'and', 'Efficient', 'Fine-Tuning', 'for', 'Pre-trained', 'Natural', 'Language', 'Models', 'through', 'Principled', 'Regularized', 'Optimization.']",,,,Fine Tuning Pre - Natural Language Models,0,29,84,29,84,,,,,,,"['Fine', 'Tuning', 'Pre', '-', 'Natural', 'Language', 'Models']",[]
31,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,0,Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks.,Fine-tuning of pre-trained,transformer,models has become the standard approach for solving common NLP tasks .,"['Fine-tuning', 'of', 'pre-trained', 'transformer', 'models', 'has', 'become', 'the', 'standard', 'approach', 'for', 'solving', 'common', 'NLP', 'tasks', '.']","(3, 4)","(26, 37)",0,Fine trained transformer models,0,0,49,0,49,has become,2,50,60,11,21,"['Fine', 'trained', 'transformer', 'models']","['has', 'become']"
32,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,1,Most of the existing approaches rely on a randomly initialized classifier on top of such networks.,Most of the existing approaches rely on a randomly initialized classifier on top of such networks.,,,"['Most', 'of', 'the', 'existing', 'approaches', 'rely', 'on', 'a', 'randomly', 'initialized', 'classifier', 'on', 'top', 'of', 'such', 'networks.']",,,,Most,0,0,4,0,4,rely,0,32,36,32,36,['Most'],['rely']
33,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,2,"We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task.","We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task.",,,"['We', 'argue', 'that', 'this', 'fine-tuning', 'procedure', 'is', 'sub-optimal', 'as', 'the', 'pre-trained', 'model', 'has', 'no', 'prior', 'on', 'the', 'specific', 'classifier', 'labels,', 'while', 'it', 'might', 'have', 'already', 'learned', 'an', 'intrinsic', 'textual', 'representation', 'of', 'the', 'task.']",,,,We,0,0,2,0,2,argue,0,3,8,3,8,['We'],['argue']
34,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,3,"In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase.","In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase.",,,"['In', 'this', 'paper,', 'we', 'introduce', 'a', 'new', 'scoring', 'method', 'that', 'casts', 'a', 'plausibility', 'ranking', 'task', 'in', 'a', 'full-text', 'format', 'and', 'leverages', 'the', 'masked', 'language', 'modeling', 'head', 'tuned', 'during', 'the', 'pre-training', 'phase.']",,,,we,0,16,18,16,18,introduce,0,19,28,19,28,['we'],['introduce']
35,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,4,"We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets.","We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets.",,,"['We', 'study', 'commonsense', 'reasoning', 'tasks', 'where', 'the', 'model', 'must', 'rank', 'a', 'set', 'of', 'hypotheses', 'given', 'a', 'premise,', 'focusing', 'on', 'the', 'COPA,', 'Swag,', 'HellaSwag', 'and', 'CommonsenseQA', 'datasets.']",,,,We,0,0,2,0,2,study,0,3,8,3,8,['We'],['study']
36,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,5,"By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g.","By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g.",,,"['By', 'exploiting', 'our', 'scoring', 'method', 'without', 'fine-tuning,', 'we', 'are', 'able', 'to', 'produce', 'strong', 'baselines', '(e.g.']",,,,we,0,57,59,57,59,are produce,0,60,79,60,79,['we'],"['are', 'produce']"
37,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,6,80% test accuracy on COPA) that are comparable to supervised approaches.,80% test accuracy on COPA) that are comparable to supervised approaches.,,,"['80%', 'test', 'accuracy', 'on', 'COPA)', 'that', 'are', 'comparable', 'to', 'supervised', 'approaches.']",,,,% test accuracy,0,3,18,3,18,,,,,,,"['%', 'test', 'accuracy']",[]
38,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,7,"Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g $\times 10$ standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.","Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g $\times 10$ standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.",,,"['Moreover,', 'when', 'fine-tuning', 'directly', 'on', 'the', 'proposed', 'scoring', 'function,', 'we', 'show', 'that', 'our', 'method', 'provides', 'a', 'much', 'more', 'stable', 'training', 'phase', 'across', 'random', 'restarts', '(e.g', '$\\times', '10$', 'standard', 'deviation', 'reduction', 'on', 'COPA', 'test', 'accuracy)', 'and', 'requires', 'less', 'annotated', 'data', 'than', 'the', 'standard', 'classifier', 'approach', 'to', 'reach', 'equivalent', 'performances.']",,,,we,0,74,76,74,76,show,0,77,81,77,81,['we'],['show']
39,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Title,0,Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning.,Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning.,,,"['Pre-training', 'Is', '(Almost)', 'All', 'You', 'Need:', 'An', 'Application', 'to', 'Commonsense', 'Reasoning.']",,,,Pre - training,0,0,14,0,14,Is,0,15,17,15,17,"['Pre', '-', 'training']",['Is']
40,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,0,Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP.,Recent developments in unsupervised representation learning have successfully established the concept of,transfer learning,in NLP .,"['Recent', 'developments', 'in', 'unsupervised', 'representation', 'learning', 'have', 'successfully', 'established', 'the', 'concept', 'of', 'transfer', 'learning', 'in', 'NLP', '.']","(12, 14)","(104, 121)",0,Recent developments,0,0,19,0,19,have established,0,60,89,60,89,"['Recent', 'developments']","['have', 'established']"
41,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,0,Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP.,Recent developments in unsupervised representation learning have successfully established the concept of,transfer,learning in NLP .,"['Recent', 'developments', 'in', 'unsupervised', 'representation', 'learning', 'have', 'successfully', 'established', 'the', 'concept', 'of', 'transfer', 'learning', 'in', 'NLP', '.']","(12, 13)","(104, 112)",0,Recent developments,0,0,19,0,19,have established,0,60,89,60,89,"['Recent', 'developments']","['have', 'established']"
42,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,1,Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information.,Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information.,,,"['Mainly', 'three', 'forces', 'are', 'driving', 'the', 'improvements', 'in', 'this', 'area', 'of', 'research:', 'More', 'elaborated', 'architectures', 'are', 'making', 'better', 'use', 'of', 'contextual', 'information.']",,,,,,,,,,,,,,,,"['forces', 'architectures']","['are', 'driving', 'are', 'making']"
43,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,2,"Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives.","Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives.",,,"['Instead', 'of', 'simply', 'plugging', 'in', 'static', 'pre-trained', 'representations,', 'these', 'are', 'learned', 'based', 'on', 'surrounding', 'context', 'in', 'end-to-end', 'trainable', 'models', 'with', 'more', 'intelligently', 'designed', 'language', 'modelling', 'objectives.']",,,,static pre - representations these,0,30,74,30,74,plugging are learned based,0,18,92,18,92,"['static', 'pre', '-', 'representations', 'these']","['plugging', 'are', 'learned', 'based']"
44,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,3,"Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks.","Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks.",,,"['Along', 'with', 'this,', 'larger', 'corpora', 'are', 'used', 'as', 'resources', 'for', 'pre-training', 'large', 'language', 'models', 'in', 'a', 'self-supervised', 'fashion', 'which', 'are', 'afterwards', 'fine-tuned', 'on', 'supervised', 'tasks.']",,,,larger corpora,0,18,32,18,32,are used,0,33,41,33,41,"['larger', 'corpora']","['are', 'used']"
45,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,4,"Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models.","Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models.",,,"['Advances', 'in', 'parallel', 'computing', 'as', 'well', 'as', 'in', 'cloud', 'computing,', 'made', 'it', 'possible', 'to', 'train', 'these', 'models', 'with', 'growing', 'capacities', 'in', 'the', 'same', 'or', 'even', 'in', 'shorter', 'time', 'than', 'previously', 'established', 'models.']",,,,Advances,0,0,8,0,8,made,0,63,67,63,67,['Advances'],"['made', 'train']"
46,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,5,These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency.,These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency.,,,"['These', 'three', 'developments', 'agglomerate', 'in', 'new', 'state-of-the-art', '(SOTA)', 'results', 'being', 'revealed', 'in', 'a', 'higher', 'and', 'higher', 'frequency.']",,,,These developments,0,0,24,0,24,agglomerate,0,25,36,25,36,"['These', 'developments']",['agglomerate']
47,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,6,"It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces.","It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces.",,,"['It', 'is', 'not', 'always', 'obvious', 'where', 'these', 'improvements', 'originate', 'from,', 'as', 'it', 'is', 'not', 'possible', 'to', 'completely', 'disentangle', 'the', 'contributions', 'of', 'the', 'three', 'driving', 'forces.']",,,,It,0,0,2,0,2,is,0,3,5,3,5,['It'],['is']
48,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,7,"We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources.","We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources.",,,"['We', 'set', 'ourselves', 'to', 'providing', 'a', 'clear', 'and', 'concise', 'overview', 'on', 'several', 'large', 'pre-trained', 'language', 'models,', 'which', 'achieved', 'SOTA', 'results', 'in', 'the', 'last', 'two', 'years,', 'with', 'respect', 'to', 'their', 'use', 'of', 'new', 'architectures', 'and', 'resources.']",,,,We,0,0,2,0,2,set,0,3,6,3,6,['We'],['set']
49,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,8,We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes.,We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes.,,,"['We', 'want', 'to', 'clarify', 'for', 'the', 'reader', 'where', 'the', 'differences', 'between', 'the', 'models', 'are', 'and', 'we', 'furthermore', 'attempt', 'to', 'gain', 'some', 'insight', 'into', 'the', 'single', 'contributions', 'of', 'lexical/computational', 'improvements', 'as', 'well', 'as', 'of', 'architectural', 'changes.']",,,,We,0,0,2,0,2,want,0,3,7,3,7,['We'],"['want', 'clarify', 'attempt', 'gain']"
50,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,9,"We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons.","We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons.",,,"['We', 'explicitly', 'do', 'not', 'intend', 'to', 'quantify', 'these', 'contributions,', 'but', 'rather', 'see', 'our', 'work', 'as', 'an', 'overview', 'in', 'order', 'to', 'identify', 'potential', 'starting', 'points', 'for', 'benchmark', 'comparisons.']",,,,We,0,0,2,0,2,do intend but see,0,14,76,14,76,['We'],"['do', 'intend', 'quantify', 'see']"
51,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,10,"Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.","Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.",,,"['Furthermore,', 'we', 'tentatively', 'want', 'to', 'point', 'at', 'potential', 'possibilities', 'for', 'improvement', 'in', 'the', 'field', 'of', 'open-sourcing', 'and', 'reproducible', 'research.']",,,,we,0,14,16,14,16,want,0,29,33,29,33,['we'],"['want', 'point']"
52,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Title,0,On the Comparability of Pre-trained Language Models.,On the Comparability of Pre-trained Language Models.,,,"['On', 'the', 'Comparability', 'of', 'Pre-trained', 'Language', 'Models.']",,,,,,,,,,,,,,,,[],[]
53,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,0,"Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering.","Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering.",,,"['Language', 'model', 'pre-training', 'has', 'been', 'shown', 'to', 'capture', 'a', 'surprising', 'amount', 'of', 'world', 'knowledge,', 'crucial', 'for', 'NLP', 'tasks', 'such', 'as', 'question', 'answering.']",,,,Language model - training,0,0,29,0,29,has been shown,0,30,44,30,44,"['Language', 'model', '-', 'training']","['has', 'been', 'shown', 'capture']"
54,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,1,"However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.","However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.",,,"['However,', 'this', 'knowledge', 'is', 'stored', 'implicitly', 'in', 'the', 'parameters', 'of', 'a', 'neural', 'network,', 'requiring', 'ever-larger', 'networks', 'to', 'cover', 'more', 'facts.']",,,,this knowledge,0,10,24,10,24,is stored requiring,0,25,95,25,95,"['this', 'knowledge']","['is', 'stored', 'requiring', 'cover']"
55,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,2,"To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference.","To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference.",,,"['To', 'capture', 'knowledge', 'in', 'a', 'more', 'modular', 'and', 'interpretable', 'way,', 'we', 'augment', 'language', 'model', 'pre-training', 'with', 'a', 'latent', 'knowledge', 'retriever,', 'which', 'allows', 'the', 'model', 'to', 'retrieve', 'and', 'attend', 'over', 'documents', 'from', 'a', 'large', 'corpus', 'such', 'as', 'Wikipedia,', 'used', 'during', 'pre-training,', 'fine-tuning', 'and', 'inference.']",,,,we,0,63,65,63,65,augment,0,66,73,66,73,['we'],"['capture', 'augment']"
56,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,3,"For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.","For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.",,,"['For', 'the', 'first', 'time,', 'we', 'show', 'how', 'to', 'pre-train', 'such', 'a', 'knowledge', 'retriever', 'in', 'an', 'unsupervised', 'manner,', 'using', 'masked', 'language', 'modeling', 'as', 'the', 'learning', 'signal', 'and', 'backpropagating', 'through', 'a', 'retrieval', 'step', 'that', 'considers', 'millions', 'of', 'documents.']",,,,we,0,21,23,21,23,show pre - train,0,24,47,24,47,['we'],"['show', 'pre', '-', 'train', 'using', 'backpropagating']"
57,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,4,We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA).,We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA).,,,"['We', 'demonstrate', 'the', 'effectiveness', 'of', 'Retrieval-Augmented', 'Language', 'Model', 'pre-training', '(REALM)', 'by', 'fine-tuning', 'on', 'the', 'challenging', 'task', 'of', 'Open-domain', 'Question', 'Answering', '(Open-QA).']",,,,We,0,0,2,0,2,demonstrate,0,3,14,3,14,['We'],['demonstrate']
58,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,5,"We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.","We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",,,"['We', 'compare', 'against', 'state-of-the-art', 'models', 'for', 'both', 'explicit', 'and', 'implicit', 'knowledge', 'storage', 'on', 'three', 'popular', 'Open-QA', 'benchmarks,', 'and', 'find', 'that', 'we', 'outperform', 'all', 'previous', 'methods', 'by', 'a', 'significant', 'margin', '(4-16%', 'absolute', 'accuracy),', 'while', 'also', 'providing', 'qualitative', 'benefits', 'such', 'as', 'interpretability', 'and', 'modularity.']",,,,We,0,0,2,0,2,compare and find,0,3,146,3,146,['We'],"['compare', 'find']"
59,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Title,0,REALM: Retrieval-Augmented Language Model Pre-Training.,REALM: Retrieval-Augmented Language Model Pre-Training.,,,"['REALM:', 'Retrieval-Augmented', 'Language', 'Model', 'Pre-Training.']",,,,REALM Retrieval Augmented Language Model Pre - Training,0,0,59,0,59,,,,,,,"['REALM', 'Retrieval', 'Augmented', 'Language', 'Model', 'Pre', '-', 'Training']",[]
60,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,0,"There has been recent success in pre-training on monolingual data and fine-tuning on Machine Translation (MT), but it remains unclear how to best leverage a pre-trained model for a given MT task.","There has been recent success in pre-training on monolingual data and fine-tuning on Machine Translation (MT), but it remains unclear how to best leverage a pre-trained model for a given MT task.",,,"['There', 'has', 'been', 'recent', 'success', 'in', 'pre-training', 'on', 'monolingual', 'data', 'and', 'fine-tuning', 'on', 'Machine', 'Translation', '(MT),', 'but', 'it', 'remains', 'unclear', 'how', 'to', 'best', 'leverage', 'a', 'pre-trained', 'model', 'for', 'a', 'given', 'MT', 'task.']",,,,,,,,,,but,0,118,121,118,121,['it'],"['has', 'been', 'remains']"
61,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,1,"This paper investigates the benefits and drawbacks of freezing parameters, and adding new ones, when fine-tuning a pre-trained model on MT.","This paper investigates the benefits and drawbacks of freezing parameters, and adding new ones, when fine-tuning a pre-trained model on MT.",,,"['This', 'paper', 'investigates', 'the', 'benefits', 'and', 'drawbacks', 'of', 'freezing', 'parameters,', 'and', 'adding', 'new', 'ones,', 'when', 'fine-tuning', 'a', 'pre-trained', 'model', 'on', 'MT.']",,,,This paper,0,0,10,0,10,investigates and adding,0,11,86,11,86,"['This', 'paper']","['investigates', 'adding']"
62,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,2,"We focus on 1) Fine-tuning a model trained only on English monolingual data, BART.","We focus on 1) Fine-tuning a model trained only on English monolingual data, BART.",,,"['We', 'focus', 'on', '1)', 'Fine-tuning', 'a', 'model', 'trained', 'only', 'on', 'English', 'monolingual', 'data,', 'BART.']",,,,We,0,0,2,0,2,focus,0,3,8,3,8,['We'],"['focus', 'tuning']"
63,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,3,"2) Fine-tuning a model trained on monolingual data from 25 languages, mBART.","2) Fine-tuning a model trained on monolingual data from 25 languages, mBART.",,,"['2)', 'Fine-tuning', 'a', 'model', 'trained', 'on', 'monolingual', 'data', 'from', '25', 'languages,', 'mBART.']",,,,,,,,,,,,,,,,[],['tuning']
64,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,4,"For BART we get the best performance by freezing most of the model parameters, and adding extra positional embeddings.","For BART we get the best performance by freezing most of the model parameters, and adding extra positional embeddings.",,,"['For', 'BART', 'we', 'get', 'the', 'best', 'performance', 'by', 'freezing', 'most', 'of', 'the', 'model', 'parameters,', 'and', 'adding', 'extra', 'positional', 'embeddings.']",,,,we,0,9,11,9,11,get,0,12,15,12,15,['we'],['get']
65,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,5,"For mBART we match the performance of naive fine-tuning for most language pairs, and outperform it for Nepali to English (0.5 BLEU) and Czech to English (0.6 BLEU), all with a lower memory cost at training time.","For mBART we match the performance of naive fine-tuning for most language pairs, and outperform it for Nepali to English (0.5 BLEU) and Czech to English (0.6 BLEU), all with a lower memory cost at training time.",,,"['For', 'mBART', 'we', 'match', 'the', 'performance', 'of', 'naive', 'fine-tuning', 'for', 'most', 'language', 'pairs,', 'and', 'outperform', 'it', 'for', 'Nepali', 'to', 'English', '(0.5', 'BLEU)', 'and', 'Czech', 'to', 'English', '(0.6', 'BLEU),', 'all', 'with', 'a', 'lower', 'memory', 'cost', 'at', 'training', 'time.']",,,,we,0,10,12,10,12,match and outperform and,0,13,140,13,140,['we'],"['match', 'outperform']"
66,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,6,When constraining ourselves to an out-of-domain training set for Vietnamese to English we outperform the fine-tuning baseline by 0.9 BLEU.,When constraining ourselves to an out-of-domain training set for Vietnamese to English we outperform the fine-tuning baseline by 0.9 BLEU.,,,"['When', 'constraining', 'ourselves', 'to', 'an', 'out-of-domain', 'training', 'set', 'for', 'Vietnamese', 'to', 'English', 'we', 'outperform', 'the', 'fine-tuning', 'baseline', 'by', '0.9', 'BLEU.']",,,,we,0,91,93,91,93,outperform,0,94,104,94,104,['we'],['outperform']
67,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Title,0,Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation.,Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation.,,,"['Recipes', 'for', 'Adapting', 'Pre-trained', 'Monolingual', 'and', 'Multilingual', 'Models', 'to', 'Machine', 'Translation.']",,,,Recipes Monolingual Multilingual Models,0,0,70,0,70,,,,,,,"['Recipes', 'Monolingual', 'Multilingual', 'Models']",[]
68,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,0,"While transformer-based finetuning techniques have proven effective in tasks that involve low-resource, low-data environments, a lack of properly established baselines and benchmark datasets make it hard to compare different approaches that are aimed at tackling the low-resource setting.",While,transformer,"-based finetuning techniques have proven effective in tasks that involve low-resource , low-data environments , a lack of properly established baselines and benchmark datasets make it hard to compare different approaches that are aimed at tackling the low-resource setting .","['While', 'transformer', '-based', 'finetuning', 'techniques', 'have', 'proven', 'effective', 'in', 'tasks', 'that', 'involve', 'low-resource', ',', 'low-data', 'environments', ',', 'a', 'lack', 'of', 'properly', 'established', 'baselines', 'and', 'benchmark', 'datasets', 'make', 'it', 'hard', 'to', 'compare', 'different', 'approaches', 'that', 'are', 'aimed', 'at', 'tackling', 'the', 'low-resource', 'setting', '.']","(1, 2)","(5, 16)",0,a lack,2,134,140,116,122,make,2,198,202,180,184,"['a', 'lack']","['make', 'compare']"
69,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,1,"In this work, we provide three contributions.","In this work, we provide three contributions.",,,"['In', 'this', 'work,', 'we', 'provide', 'three', 'contributions.']",,,,we,0,15,17,15,17,provide,0,18,25,18,25,['we'],['provide']
70,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,2,"First, we introduce two previously unreleased datasets as benchmark datasets for text classification and low-resource multilabel text classification for the low-resource language Filipino.","First, we introduce two previously unreleased datasets as benchmark datasets for text classification and low-resource multilabel text classification for the low-resource language Filipino.",,,"['First,', 'we', 'introduce', 'two', 'previously', 'unreleased', 'datasets', 'as', 'benchmark', 'datasets', 'for', 'text', 'classification', 'and', 'low-resource', 'multilabel', 'text', 'classification', 'for', 'the', 'low-resource', 'language', 'Filipino.']",,,,we,0,8,10,8,10,introduce,0,11,20,11,20,['we'],['introduce']
71,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,3,"Second, we pretrain better BERT and DistilBERT models for use within the Filipino setting.","Second, we pretrain better BERT and DistilBERT models for use within the Filipino setting.",,,"['Second,', 'we', 'pretrain', 'better', 'BERT', 'and', 'DistilBERT', 'models', 'for', 'use', 'within', 'the', 'Filipino', 'setting.']",,,,we,0,9,11,9,11,pretrain,0,12,20,12,20,['we'],['pretrain']
72,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,4,"Third, we introduce a simple degradation test that benchmarks a model's resistance to performance degradation as the number of training samples are reduced.","Third, we introduce a simple degradation test that benchmarks a model's resistance to performance degradation as the number of training samples are reduced.",,,"['Third,', 'we', 'introduce', 'a', 'simple', 'degradation', 'test', 'that', 'benchmarks', 'a', ""model's"", 'resistance', 'to', 'performance', 'degradation', 'as', 'the', 'number', 'of', 'training', 'samples', 'are', 'reduced.']",,,,we,0,8,10,8,10,introduce,0,11,20,11,20,['we'],['introduce']
73,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,5,We analyze our pretrained model's degradation speeds and look towards the use of this method for comparing models aimed at operating within the low-resource setting.,We analyze our pretrained model's degradation speeds and look towards the use of this method for comparing models aimed at operating within the low-resource setting.,,,"['We', 'analyze', 'our', 'pretrained', ""model's"", 'degradation', 'speeds', 'and', 'look', 'towards', 'the', 'use', 'of', 'this', 'method', 'for', 'comparing', 'models', 'aimed', 'at', 'operating', 'within', 'the', 'low-resource', 'setting.']",,,,We,0,0,2,0,2,analyze and look,0,3,62,3,62,['We'],"['analyze', 'look']"
74,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,6,We release all our models and datasets for the research community to use.,We release all our models and datasets for the research community to use.,,,"['We', 'release', 'all', 'our', 'models', 'and', 'datasets', 'for', 'the', 'research', 'community', 'to', 'use.']",,,,We,0,0,2,0,2,release,0,3,10,3,10,['We'],['release']
75,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Title,0,Establishing Baselines for Text Classification in Low-Resource Languages.,Establishing Baselines for Text Classification in Low-Resource Languages.,,,"['Establishing', 'Baselines', 'for', 'Text', 'Classification', 'in', 'Low-Resource', 'Languages.']",,,,,,,,,,Establishing,0,0,12,0,12,[],['Establishing']
76,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,0,"Neural Network based models have been state-of-the-art models for various Natural Language Processing tasks, however, the input and output dimension problem in the networks has still not been fully resolved, especially in text generation tasks (e.g.","Neural Network based models have been state-of-the-art models for various Natural Language Processing tasks, however, the input and output dimension problem in the networks has still not been fully resolved, especially in text generation tasks (e.g.",,,"['Neural', 'Network', 'based', 'models', 'have', 'been', 'state-of-the-art', 'models', 'for', 'various', 'Natural', 'Language', 'Processing', 'tasks,', 'however,', 'the', 'input', 'and', 'output', 'dimension', 'problem', 'in', 'the', 'networks', 'has', 'still', 'not', 'been', 'fully', 'resolved,', 'especially', 'in', 'text', 'generation', 'tasks', '(e.g.']",,,,Neural Network models the input output dimension problem,0,0,164,0,164,have been has been resolved,0,28,214,28,214,"['Neural', 'Network', 'models', 'the', 'input', 'output', 'dimension', 'problem']","['have', 'been', 'has', 'been', 'resolved']"
77,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,1,"Machine Translation, Text Summarization), in which input and output both have huge sizes of vocabularies.","Machine Translation, Text Summarization), in which input and output both have huge sizes of vocabularies.",,,"['Machine', 'Translation,', 'Text', 'Summarization),', 'in', 'which', 'input', 'and', 'output', 'both', 'have', 'huge', 'sizes', 'of', 'vocabularies.']",,,,Machine Translation Text Summarization,0,0,40,0,40,,,,,,,"['Machine', 'Translation', 'Text', 'Summarization']",[]
78,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,2,"Therefore, input-output embedding weight sharing has been introduced and adopted widely, which remains to be improved.","Therefore, input-output embedding weight sharing has been introduced and adopted widely, which remains to be improved.",,,"['Therefore,', 'input-output', 'embedding', 'weight', 'sharing', 'has', 'been', 'introduced', 'and', 'adopted', 'widely,', 'which', 'remains', 'to', 'be', 'improved.']",,,,input output,0,12,26,12,26,has been introduced and adopted,0,52,83,52,83,"['input', 'output']","['has', 'been', 'introduced', 'adopted']"
79,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,3,"Based on linear algebra and statistical theories, this paper locates the shortcoming of existed input-output embedding weight sharing method, then raises methods for improving input-output weight shared embedding, among which methods of normalization of embedding weight matrices show best performance.","Based on linear algebra and statistical theories, this paper locates the shortcoming of existed input-output embedding weight sharing method, then raises methods for improving input-output weight shared embedding, among which methods of normalization of embedding weight matrices show best performance.",,,"['Based', 'on', 'linear', 'algebra', 'and', 'statistical', 'theories,', 'this', 'paper', 'locates', 'the', 'shortcoming', 'of', 'existed', 'input-output', 'embedding', 'weight', 'sharing', 'method,', 'then', 'raises', 'methods', 'for', 'improving', 'input-output', 'weight', 'shared', 'embedding,', 'among', 'which', 'methods', 'of', 'normalization', 'of', 'embedding', 'weight', 'matrices', 'show', 'best', 'performance.']",,,,this paper,0,51,61,51,61,locates,0,62,69,62,69,"['this', 'paper']","['locates', 'raises']"
80,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,4,"These methods are nearly computational cost-free, can get combined with other embedding techniques, and show good effectiveness when applied on state-of-the-art Neural Network models.","These methods are nearly computational cost-free, can get combined with other embedding techniques, and show good effectiveness when applied on state-of-the-art Neural Network models.",,,"['These', 'methods', 'are', 'nearly', 'computational', 'cost-free,', 'can', 'get', 'combined', 'with', 'other', 'embedding', 'techniques,', 'and', 'show', 'good', 'effectiveness', 'when', 'applied', 'on', 'state-of-the-art', 'Neural', 'Network', 'models.']",,,,These methods,0,0,13,0,13,are,0,14,17,14,17,"['These', 'methods']",['are']
81,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,5,"For Transformer-big models, the normalization techniques can get at best 0.6 BLEU improvement compared to the original version of model on WMT'16 En-De dataset, and similar BLEU improvements on IWSLT 14' datasets.",For,Transformer,"-big models , the normalization techniques can get at best 0.6 BLEU improvement compared to the original version of model on WMT'16 En-De dataset , and similar BLEU improvements on IWSLT 14 ' datasets .","['For', 'Transformer', '-big', 'models', ',', 'the', 'normalization', 'techniques', 'can', 'get', 'at', 'best', '0.6', 'BLEU', 'improvement', 'compared', 'to', 'the', 'original', 'version', 'of', 'model', 'on', ""WMT'16"", 'En-De', 'dataset', ',', 'and', 'similar', 'BLEU', 'improvements', 'on', 'IWSLT', '14', ""'"", 'datasets', '.']","(1, 2)","(3, 14)",0,the normalization techniques,2,30,58,14,42,can get,2,59,66,43,50,"['the', 'normalization', 'techniques']","['can', 'get']"
82,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,6,"For DynamicConv models, 0.5 BLEU improvement can be attained on WMT'16 En-De dataset, and 0.41 BLEU improvement on IWSLT 14' De-En translation task is achieved.","For DynamicConv models, 0.5 BLEU improvement can be attained on WMT'16 En-De dataset, and 0.41 BLEU improvement on IWSLT 14' De-En translation task is achieved.",,,"['For', 'DynamicConv', 'models,', '0.5', 'BLEU', 'improvement', 'can', 'be', 'attained', 'on', ""WMT'16"", 'En-De', 'dataset,', 'and', '0.41', 'BLEU', 'improvement', 'on', 'IWSLT', ""14'"", 'De-En', 'translation', 'task', 'is', 'achieved.']",,,,BLEU improvement IWSLT De En translation task,0,99,154,99,154,and is achieved,0,90,166,90,166,"['BLEU', 'improvement', 'BLEU', 'improvement', 'IWSLT', 'De', 'En', 'translation', 'task']","['can', 'be', 'attained', 'is', 'achieved']"
83,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Title,0,Normalization of Input-output Shared Embeddings in Text Generation Models.,Normalization of Input-output Shared Embeddings in Text Generation Models.,,,"['Normalization', 'of', 'Input-output', 'Shared', 'Embeddings', 'in', 'Text', 'Generation', 'Models.']",,,,Normalization output Shared Embeddings,0,0,49,0,49,,,,,,,"['Normalization', 'output', 'Shared', 'Embeddings']",[]
84,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,0,"Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era.","Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era.",,,"['Recently,', 'the', 'emergence', 'of', 'pre-trained', 'models', '(PTMs)', 'has', 'brought', 'natural', 'language', 'processing', '(NLP)', 'to', 'a', 'new', 'era.']",,,,the emergence,0,11,24,11,24,has brought,0,58,69,58,69,"['the', 'emergence']","['has', 'brought']"
85,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,1,"In this survey, we provide a comprehensive review of PTMs for NLP.","In this survey, we provide a comprehensive review of PTMs for NLP.",,,"['In', 'this', 'survey,', 'we', 'provide', 'a', 'comprehensive', 'review', 'of', 'PTMs', 'for', 'NLP.']",,,,we,0,17,19,17,19,provide,0,20,27,20,27,['we'],['provide']
86,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,2,We first briefly introduce language representation learning and its research progress.,We first briefly introduce language representation learning and its research progress.,,,"['We', 'first', 'briefly', 'introduce', 'language', 'representation', 'learning', 'and', 'its', 'research', 'progress.']",,,,We,0,0,2,0,2,introduce,0,17,26,17,26,['We'],['introduce']
87,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,3,Then we systematically categorize existing PTMs based on a taxonomy with four perspectives.,Then we systematically categorize existing PTMs based on a taxonomy with four perspectives.,,,"['Then', 'we', 'systematically', 'categorize', 'existing', 'PTMs', 'based', 'on', 'a', 'taxonomy', 'with', 'four', 'perspectives.']",,,,we,0,5,7,5,7,categorize,0,23,33,23,33,['we'],['categorize']
88,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,4,"Next, we describe how to adapt the knowledge of PTMs to the downstream tasks.","Next, we describe how to adapt the knowledge of PTMs to the downstream tasks.",,,"['Next,', 'we', 'describe', 'how', 'to', 'adapt', 'the', 'knowledge', 'of', 'PTMs', 'to', 'the', 'downstream', 'tasks.']",,,,we,0,7,9,7,9,describe,0,10,18,10,18,['we'],['describe']
89,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,5,"Finally, we outline some potential directions of PTMs for future research.","Finally, we outline some potential directions of PTMs for future research.",,,"['Finally,', 'we', 'outline', 'some', 'potential', 'directions', 'of', 'PTMs', 'for', 'future', 'research.']",,,,we,0,10,12,10,12,outline,0,13,20,13,20,['we'],['outline']
90,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,6,"This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.","This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.",,,"['This', 'survey', 'is', 'purposed', 'to', 'be', 'a', 'hands-on', 'guide', 'for', 'understanding,', 'using,', 'and', 'developing', 'PTMs', 'for', 'various', 'NLP', 'tasks.']",,,,This survey,0,0,11,0,11,is purposed,0,12,23,12,23,"['This', 'survey']","['is', 'purposed', 'be']"
91,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Title,0,Pre-trained Models for Natural Language Processing: A Survey.,Pre-trained Models for Natural Language Processing: A Survey.,,,"['Pre-trained', 'Models', 'for', 'Natural', 'Language', 'Processing:', 'A', 'Survey.']",,,,Pre - Models A Survey,0,0,63,0,63,,,,,,,"['Pre', '-', 'Models', 'A', 'Survey']",[]
92,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,0,"Large pretrained natural language representations such as BERT, ALBERT, and other variants (BERT-models) have achieved state of the art performance on a diverse set of NLP tasks after fine tuning.","Large pretrained natural language representations such as BERT, ALBERT, and other variants (BERT-models) have achieved state of the art performance on a diverse set of NLP tasks after fine tuning.",,,"['Large', 'pretrained', 'natural', 'language', 'representations', 'such', 'as', 'BERT,', 'ALBERT,', 'and', 'other', 'variants', '(BERT-models)', 'have', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'a', 'diverse', 'set', 'of', 'NLP', 'tasks', 'after', 'fine', 'tuning.']",,,,Large natural language representations,0,0,49,0,49,have achieved,0,111,124,111,124,"['Large', 'natural', 'language', 'representations']","['have', 'achieved']"
93,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,1,"This suggests these BERT-models learn to extract signal-rich, transferable language features.","This suggests these BERT-models learn to extract signal-rich ,",transferable,language features .,"['This', 'suggests', 'these', 'BERT-models', 'learn', 'to', 'extract', 'signal-rich', ',', 'transferable', 'language', 'features', '.']","(9, 10)","(62, 74)",0,This,0,0,4,0,4,suggests,0,5,13,5,13,['This'],['suggests']
94,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,2,"We investigate whether there is an appreciable difference in feature quality at various depths of pretrained and fine tuned BERT-models by training softmax regression probes to perform a semantic level task, question answering (QA) on SQuAD 2.0.","We investigate whether there is an appreciable difference in feature quality at various depths of pretrained and fine tuned BERT-models by training softmax regression probes to perform a semantic level task, question answering (QA) on SQuAD 2.0.",,,"['We', 'investigate', 'whether', 'there', 'is', 'an', 'appreciable', 'difference', 'in', 'feature', 'quality', 'at', 'various', 'depths', 'of', 'pretrained', 'and', 'fine', 'tuned', 'BERT-models', 'by', 'training', 'softmax', 'regression', 'probes', 'to', 'perform', 'a', 'semantic', 'level', 'task,', 'question', 'answering', '(QA)', 'on', 'SQuAD', '2.0.']",,,,We,0,0,2,0,2,investigate,0,3,14,3,14,['We'],['investigate']
95,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,3,"We find that the feature quality of fine tuned BERT-models improves with each successive layer, while the feature quality of pretrained BERT-models remains constant.","We find that the feature quality of fine tuned BERT-models improves with each successive layer, while the feature quality of pretrained BERT-models remains constant.",,,"['We', 'find', 'that', 'the', 'feature', 'quality', 'of', 'fine', 'tuned', 'BERT-models', 'improves', 'with', 'each', 'successive', 'layer,', 'while', 'the', 'feature', 'quality', 'of', 'pretrained', 'BERT-models', 'remains', 'constant.']",,,,We,0,0,2,0,2,find,0,3,7,3,7,['We'],['find']
96,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,4,"We also find that pretrained BERT-models feature quality is relatively poor in comparison to high layers in the fine tuned models, suggesting that the fine tuning process is key for extracting high quality features.","We also find that pretrained BERT-models feature quality is relatively poor in comparison to high layers in the fine tuned models, suggesting that the fine tuning process is key for extracting high quality features.",,,"['We', 'also', 'find', 'that', 'pretrained', 'BERT-models', 'feature', 'quality', 'is', 'relatively', 'poor', 'in', 'comparison', 'to', 'high', 'layers', 'in', 'the', 'fine', 'tuned', 'models,', 'suggesting', 'that', 'the', 'fine', 'tuning', 'process', 'is', 'key', 'for', 'extracting', 'high', 'quality', 'features.']",,,,We,0,0,2,0,2,find,0,8,12,8,12,['We'],['find']
97,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,5,"One particularly interesting finding is that the early to middle layers in fine tuned BERT-models begin to perform well on questions with answers, at the cost of performance on questions with no answer.","One particularly interesting finding is that the early to middle layers in fine tuned BERT-models begin to perform well on questions with answers, at the cost of performance on questions with no answer.",,,"['One', 'particularly', 'interesting', 'finding', 'is', 'that', 'the', 'early', 'to', 'middle', 'layers', 'in', 'fine', 'tuned', 'BERT-models', 'begin', 'to', 'perform', 'well', 'on', 'questions', 'with', 'answers,', 'at', 'the', 'cost', 'of', 'performance', 'on', 'questions', 'with', 'no', 'answer.']",,,,finding,0,29,36,29,36,is,0,37,39,37,39,['finding'],['is']
98,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,6,Higher layers in fine tuned BERT-models are able to perform well on both questions with and without answers.,Higher layers in fine tuned BERT-models are able to perform well on both questions with and without answers.,,,"['Higher', 'layers', 'in', 'fine', 'tuned', 'BERT-models', 'are', 'able', 'to', 'perform', 'well', 'on', 'both', 'questions', 'with', 'and', 'without', 'answers.']",,,,Higher layers models,0,0,41,0,41,are,0,42,45,42,45,"['Higher', 'layers', 'models']",['are']
99,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,7,Code available at https://github.com/travismcguire/cs224nfinalproject,Code available at https://github.com/travismcguire/cs224nfinalproject,,,"['Code', 'available', 'at', 'https://github.com/travismcguire/cs224nfinalproject']",,,,Code available,0,0,14,0,14,,,,,,,"['Code', 'available']",[]
100,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Title,0,Transferability of Contextual Representations for Question Answering.,,Transferability,of Contextual Representations for Question Answering .,"['Transferability', 'of', 'Contextual', 'Representations', 'for', 'Question', 'Answering', '.']","(0, 1)","(0, 15)",0,Transferability,1,0,15,0,15,,,,,,,['Transferability'],[]
101,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,0,"Large pre-trained contextual word representations have transformed the field of natural language processing, obtaining impressive results on a wide range of tasks.","Large pre-trained contextual word representations have transformed the field of natural language processing, obtaining impressive results on a wide range of tasks.",,,"['Large', 'pre-trained', 'contextual', 'word', 'representations', 'have', 'transformed', 'the', 'field', 'of', 'natural', 'language', 'processing,', 'obtaining', 'impressive', 'results', 'on', 'a', 'wide', 'range', 'of', 'tasks.']",,,,Large pre contextual word representations,0,0,51,0,51,have transformed,0,52,68,52,68,"['Large', 'pre', 'contextual', 'word', 'representations']","['have', 'transformed', 'obtaining']"
102,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,1,"However, as models increase in size, computational limitations make them impractical for researchers and practitioners alike.","However, as models increase in size, computational limitations make them impractical for researchers and practitioners alike.",,,"['However,', 'as', 'models', 'increase', 'in', 'size,', 'computational', 'limitations', 'make', 'them', 'impractical', 'for', 'researchers', 'and', 'practitioners', 'alike.']",,,,computational limitations,0,39,64,39,64,make,0,65,69,65,69,"['computational', 'limitations']",['make']
103,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,2,We hypothesize that contextual representations have both intrinsic and task-specific redundancies.,We hypothesize that contextual representations have both intrinsic and task-specific redundancies.,,,"['We', 'hypothesize', 'that', 'contextual', 'representations', 'have', 'both', 'intrinsic', 'and', 'task-specific', 'redundancies.']",,,,We,0,0,2,0,2,hypothesize,0,3,14,3,14,['We'],['hypothesize']
104,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,3,"We propose a novel feature selection method, which takes advantage of these redundancies to reduce the size of the pre-trained features.","We propose a novel feature selection method, which takes advantage of these redundancies to reduce the size of the pre-trained features.",,,"['We', 'propose', 'a', 'novel', 'feature', 'selection', 'method,', 'which', 'takes', 'advantage', 'of', 'these', 'redundancies', 'to', 'reduce', 'the', 'size', 'of', 'the', 'pre-trained', 'features.']",,,,We,0,0,2,0,2,propose,0,3,10,3,10,['We'],['propose']
105,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,4,"In a comprehensive evaluation on two pre-trained models, BERT and XLNet, using a diverse suite of sequence labeling and sequence classification tasks, our method reduces the feature set down to 1--7% of the original size, while maintaining more than 97% of the performance.","In a comprehensive evaluation on two pre-trained models, BERT and XLNet, using a diverse suite of sequence labeling and sequence classification tasks, our method reduces the feature set down to 1--7% of the original size, while maintaining more than 97% of the performance.",,,"['In', 'a', 'comprehensive', 'evaluation', 'on', 'two', 'pre-trained', 'models,', 'BERT', 'and', 'XLNet,', 'using', 'a', 'diverse', 'suite', 'of', 'sequence', 'labeling', 'and', 'sequence', 'classification', 'tasks,', 'our', 'method', 'reduces', 'the', 'feature', 'set', 'down', 'to', '1--7%', 'of', 'the', 'original', 'size,', 'while', 'maintaining', 'more', 'than', '97%', 'of', 'the', 'performance.']",,,,method,0,160,166,160,166,reduces set,0,167,190,167,190,['method'],"['reduces', 'set']"
106,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Title,0,Exploiting Redundancy in Pre-trained Language Models for Efficient Transfer Learning.,Exploiting Redundancy in Pre-trained Language Models for Efficient,Transfer Learning,.,"['Exploiting', 'Redundancy', 'in', 'Pre-trained', 'Language', 'Models', 'for', 'Efficient', 'Transfer', 'Learning', '.']","(8, 10)","(66, 83)",0,Redundancy Language Models,0,11,54,11,54,Exploiting,0,0,10,0,10,"['Redundancy', 'Language', 'Models']",['Exploiting']
107,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Title,0,Exploiting Redundancy in Pre-trained Language Models for Efficient Transfer Learning.,Exploiting Redundancy in Pre-trained Language Models for Efficient,Transfer,Learning .,"['Exploiting', 'Redundancy', 'in', 'Pre-trained', 'Language', 'Models', 'for', 'Efficient', 'Transfer', 'Learning', '.']","(8, 9)","(66, 74)",0,Redundancy Language Models,0,11,54,11,54,Exploiting,0,0,10,0,10,"['Redundancy', 'Language', 'Models']",['Exploiting']
108,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,0,The success of pretrained transformer language models in natural language processing has led to a wide range of different pretraining setups.,The success of,pretrained transformer language models,in natural language processing has led to a wide range of different pretraining setups .,"['The', 'success', 'of', 'pretrained', 'transformer', 'language', 'models', 'in', 'natural', 'language', 'processing', 'has', 'led', 'to', 'a', 'wide', 'range', 'of', 'different', 'pretraining', 'setups', '.']","(3, 7)","(14, 52)",11,The success,0,0,11,0,11,has led,2,85,92,31,38,"['The', 'success']","['has', 'led']"
109,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,1,"These models employ a variety of subword tokenization methods, most notably byte pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text.",,These models,"employ a variety of subword tokenization methods , most notably byte pair encoding ( BPE ) ( Sennrich et al . , 2016 ; Gage , 1994 ) , the WordPiece method ( Schuster and Nakajima , 2012 ) , and unigram language modeling ( Kudo , 2018 ) , to segment text .","['These', 'models', 'employ', 'a', 'variety', 'of', 'subword', 'tokenization', 'methods', ',', 'most', 'notably', 'byte', 'pair', 'encoding', '(', 'BPE', ')', '(', 'Sennrich', 'et', 'al', '.', ',', '2016', ';', 'Gage', ',', '1994', ')', ',', 'the', 'WordPiece', 'method', '(', 'Schuster', 'and', 'Nakajima', ',', '2012', ')', ',', 'and', 'unigram', 'language', 'modeling', '(', 'Kudo', ',', '2018', ')', ',', 'to', 'segment', 'text', '.']","(0, 2)","(0, 12)",-1,These models,1,0,12,0,12,employ,2,13,19,0,6,"['These', 'models']",['employ']
110,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,2,"However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining.","However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining.",,,"['However,', 'to', 'the', 'best', 'of', 'our', 'knowledge,', 'the', 'literature', 'does', 'not', 'contain', 'a', 'direct', 'evaluation', 'of', 'the', 'impact', 'of', 'tokenization', 'on', 'language', 'model', 'pretraining.']",,,,the literature,0,41,55,41,55,does contain,0,56,72,56,72,"['the', 'literature']","['does', 'contain']"
111,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,3,"First, we analyze differences between BPE and unigram LM tokenization, and find that the unigram LM method is able to recover subword units that more strongly align with underlying morphology, in addition to avoiding several shortcomings of BPE stemming from its greedy construction procedure.","First, we analyze differences between BPE and unigram LM tokenization, and find that the unigram LM method is able to recover subword units that more strongly align with underlying morphology, in addition to avoiding several shortcomings of BPE stemming from its greedy construction procedure.",,,"['First,', 'we', 'analyze', 'differences', 'between', 'BPE', 'and', 'unigram', 'LM', 'tokenization,', 'and', 'find', 'that', 'the', 'unigram', 'LM', 'method', 'is', 'able', 'to', 'recover', 'subword', 'units', 'that', 'more', 'strongly', 'align', 'with', 'underlying', 'morphology,', 'in', 'addition', 'to', 'avoiding', 'several', 'shortcomings', 'of', 'BPE', 'stemming', 'from', 'its', 'greedy', 'construction', 'procedure.']",,,,we,0,8,10,8,10,analyze and find,0,11,81,11,81,['we'],"['analyze', 'find']"
112,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,4,We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations.,We then compare the fine-tuned task performance of identical,transformer,masked language models pretrained with these tokenizations .,"['We', 'then', 'compare', 'the', 'fine-tuned', 'task', 'performance', 'of', 'identical', 'transformer', 'masked', 'language', 'models', 'pretrained', 'with', 'these', 'tokenizations', '.']","(9, 10)","(60, 71)",0,We,0,0,2,0,2,compare,0,8,15,8,15,['We'],['compare']
113,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,5,"Across downstream tasks, we find that the unigram LM tokenization method consistently matches or outperforms BPE.","Across downstream tasks, we find that the unigram LM tokenization method consistently matches or outperforms BPE.",,,"['Across', 'downstream', 'tasks,', 'we', 'find', 'that', 'the', 'unigram', 'LM', 'tokenization', 'method', 'consistently', 'matches', 'or', 'outperforms', 'BPE.']",,,,we,0,26,28,26,28,find,0,29,33,29,33,['we'],['find']
114,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,6,We hope that developers of future pretrained language models will consider adopting the unigram LM method over the more common BPE.,We hope that developers of future pretrained language models will consider adopting the unigram LM method over the more common BPE.,,,"['We', 'hope', 'that', 'developers', 'of', 'future', 'pretrained', 'language', 'models', 'will', 'consider', 'adopting', 'the', 'unigram', 'LM', 'method', 'over', 'the', 'more', 'common', 'BPE.']",,,,We,0,0,2,0,2,hope,0,3,7,3,7,['We'],['hope']
115,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Title,0,Byte Pair Encoding is Suboptimal for Language Model Pretraining.,Byte Pair Encoding is Suboptimal for Language Model Pretraining.,,,"['Byte', 'Pair', 'Encoding', 'is', 'Suboptimal', 'for', 'Language', 'Model', 'Pretraining.']",,,,Byte Pair Encoding,0,0,18,0,18,is,0,19,21,19,21,"['Byte', 'Pair', 'Encoding']",['is']
116,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,0,A recent introduction of Transformer deep learning architecture made breakthroughs in various natural language processing tasks.,A recent introduction of,Transformer,deep learning architecture made breakthroughs in various natural language processing tasks .,"['A', 'recent', 'introduction', 'of', 'Transformer', 'deep', 'learning', 'architecture', 'made', 'breakthroughs', 'in', 'various', 'natural', 'language', 'processing', 'tasks', '.']","(4, 5)","(24, 35)",0,A recent introduction,0,0,21,0,21,made,2,64,68,27,31,"['A', 'recent', 'introduction']",['made']
117,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,1,"However, non-English languages could not leverage such new opportunities with the English text pre-trained models.","However, non-English languages could not leverage such new opportunities with the English text pre-trained models.",,,"['However,', 'non-English', 'languages', 'could', 'not', 'leverage', 'such', 'new', 'opportunities', 'with', 'the', 'English', 'text', 'pre-trained', 'models.']",,,,non - English languages,0,10,33,10,33,could leverage,0,34,52,34,52,"['non', '-', 'English', 'languages']","['could', 'leverage']"
118,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,2,"This changed with research focusing on multilingual models, where less-spoken languages are the main beneficiaries.","This changed with research focusing on multilingual models, where less-spoken languages are the main beneficiaries.",,,"['This', 'changed', 'with', 'research', 'focusing', 'on', 'multilingual', 'models,', 'where', 'less-spoken', 'languages', 'are', 'the', 'main', 'beneficiaries.']",,,,This,0,0,4,0,4,changed,0,5,12,5,12,['This'],['changed']
119,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,3,"We compare pre-trained multilingual BERT, XLM-R, and older learned text representation methods as encodings for the task of Lithuanian news clustering.","We compare pre-trained multilingual BERT, XLM-R, and older learned text representation methods as encodings for the task of Lithuanian news clustering.",,,"['We', 'compare', 'pre-trained', 'multilingual', 'BERT,', 'XLM-R,', 'and', 'older', 'learned', 'text', 'representation', 'methods', 'as', 'encodings', 'for', 'the', 'task', 'of', 'Lithuanian', 'news', 'clustering.']",,,,We,0,0,2,0,2,compare,0,3,10,3,10,['We'],['compare']
120,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,4,Our results indicate that publicly available pre-trained multilingual Transformer models can be fine-tuned to surpass word vectors but still score much lower than specially trained doc2vec embeddings.,Our results indicate that publicly available pre-trained multilingual,Transformer,models can be fine-tuned to surpass word vectors but still score much lower than specially trained doc2vec embeddings .,"['Our', 'results', 'indicate', 'that', 'publicly', 'available', 'pre-trained', 'multilingual', 'Transformer', 'models', 'can', 'be', 'fine-tuned', 'to', 'surpass', 'word', 'vectors', 'but', 'still', 'score', 'much', 'lower', 'than', 'specially', 'trained', 'doc2vec', 'embeddings', '.']","(8, 9)","(69, 80)",0,results,0,4,11,4,11,indicate,0,12,20,12,20,['results'],['indicate']
121,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Title,0,Testing pre-trained Transformer models for Lithuanian news clustering.,Testing pre-trained,Transformer,models for Lithuanian news clustering .,"['Testing', 'pre-trained', 'Transformer', 'models', 'for', 'Lithuanian', 'news', 'clustering', '.']","(2, 3)","(19, 30)",0,Transformer models,1,22,40,2,20,-,0,12,13,12,13,"['Transformer', 'models']","['pre', '-']"
122,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,0,"Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence.","Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence.",,,"['Textual', 'representation', 'learners', 'trained', 'on', 'large', 'amounts', 'of', 'data', 'have', 'achieved', 'notable', 'success', 'on', 'downstream', 'tasks;', 'intriguingly,', 'they', 'have', 'also', 'performed', 'well', 'on', 'challenging', 'tests', 'of', 'syntactic', 'competence.']",,,,,,,,,,,,,,,,"['Textual', 'representation', 'learners', 'they']","['have', 'achieved', 'have', 'performed']"
123,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,1,"Given this success, it remains an open question whether scalable learners like BERT can become fully proficient in the syntax of natural language by virtue of data scale alone, or whether they still benefit from more explicit syntactic biases.","Given this success, it remains an open question whether scalable learners like BERT can become fully proficient in the syntax of natural language by virtue of data scale alone, or whether they still benefit from more explicit syntactic biases.",,,"['Given', 'this', 'success,', 'it', 'remains', 'an', 'open', 'question', 'whether', 'scalable', 'learners', 'like', 'BERT', 'can', 'become', 'fully', 'proficient', 'in', 'the', 'syntax', 'of', 'natural', 'language', 'by', 'virtue', 'of', 'data', 'scale', 'alone,', 'or', 'whether', 'they', 'still', 'benefit', 'from', 'more', 'explicit', 'syntactic', 'biases.']",,,,it,0,21,23,21,23,remains,0,24,31,24,31,['it'],['remains']
124,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,2,"To answer this question, we introduce a knowledge distillation strategy for injecting syntactic biases into BERT pretraining, by distilling the syntactically informative predictions of a hierarchical---albeit harder to scale---syntactic language model.","To answer this question, we introduce a knowledge distillation strategy for injecting syntactic biases into BERT pretraining, by distilling the syntactically informative predictions of a hierarchical---albeit harder to scale---syntactic language model.",,,"['To', 'answer', 'this', 'question,', 'we', 'introduce', 'a', 'knowledge', 'distillation', 'strategy', 'for', 'injecting', 'syntactic', 'biases', 'into', 'BERT', 'pretraining,', 'by', 'distilling', 'the', 'syntactically', 'informative', 'predictions', 'of', 'a', 'hierarchical---albeit', 'harder', 'to', 'scale---syntactic', 'language', 'model.']",,,,we,0,26,28,26,28,introduce,0,29,38,29,38,['we'],"['answer', 'introduce']"
125,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,3,"Since BERT models masked words in bidirectional context, we propose to distill the approximate marginal distribution over words in context from the syntactic LM.","Since BERT models masked words in bidirectional context, we propose to distill the approximate marginal distribution over words in context from the syntactic LM.",,,"['Since', 'BERT', 'models', 'masked', 'words', 'in', 'bidirectional', 'context,', 'we', 'propose', 'to', 'distill', 'the', 'approximate', 'marginal', 'distribution', 'over', 'words', 'in', 'context', 'from', 'the', 'syntactic', 'LM.']",,,,we,0,58,60,58,60,propose,0,61,68,61,68,['we'],"['propose', 'distill']"
126,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,4,"Our approach reduces relative error by 2-21% on a diverse set of structured prediction tasks, although we obtain mixed results on the GLUE benchmark.","Our approach reduces relative error by 2-21% on a diverse set of structured prediction tasks, although we obtain mixed results on the GLUE benchmark.",,,"['Our', 'approach', 'reduces', 'relative', 'error', 'by', '2-21%', 'on', 'a', 'diverse', 'set', 'of', 'structured', 'prediction', 'tasks,', 'although', 'we', 'obtain', 'mixed', 'results', 'on', 'the', 'GLUE', 'benchmark.']",,,,approach,0,4,12,4,12,reduces,0,13,20,13,20,['approach'],['reduces']
127,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,5,"Our findings demonstrate the benefits of syntactic biases, even in representation learners that exploit large amounts of data, and contribute to a better understanding of where syntactic biases are most helpful in benchmarks of natural language understanding.","Our findings demonstrate the benefits of syntactic biases, even in representation learners that exploit large amounts of data, and contribute to a better understanding of where syntactic biases are most helpful in benchmarks of natural language understanding.",,,"['Our', 'findings', 'demonstrate', 'the', 'benefits', 'of', 'syntactic', 'biases,', 'even', 'in', 'representation', 'learners', 'that', 'exploit', 'large', 'amounts', 'of', 'data,', 'and', 'contribute', 'to', 'a', 'better', 'understanding', 'of', 'where', 'syntactic', 'biases', 'are', 'most', 'helpful', 'in', 'benchmarks', 'of', 'natural', 'language', 'understanding.']",,,,findings,0,4,12,4,12,demonstrate and contribute,0,13,143,13,143,['findings'],"['demonstrate', 'contribute']"
128,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Title,0,Syntactic Structure Distillation Pretraining For Bidirectional Encoders.,Syntactic Structure Distillation Pretraining For Bidirectional Encoders.,,,"['Syntactic', 'Structure', 'Distillation', 'Pretraining', 'For', 'Bidirectional', 'Encoders.']",,,,Syntactic Structure Distillation,0,0,32,0,32,Pretraining,0,33,44,33,44,"['Syntactic', 'Structure', 'Distillation']",['Pretraining']
129,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,0,"The notion of ""in-domain data"" in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality.","The notion of ""in-domain data"" in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality.",,,"['The', 'notion', 'of', '""in-domain', 'data""', 'in', 'NLP', 'is', 'often', 'over-simplistic', 'and', 'vague,', 'as', 'textual', 'data', 'varies', 'in', 'many', 'nuanced', 'linguistic', 'aspects', 'such', 'as', 'topic,', 'style', 'or', 'level', 'of', 'formality.']",,,,The notion,0,0,10,0,10,is,0,42,44,42,44,"['The', 'notion']",['is']
130,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,1,"In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems.","In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems.",,,"['In', 'addition,', 'domain', 'labels', 'are', 'many', 'times', 'unavailable,', 'making', 'it', 'challenging', 'to', 'build', 'domain-specific', 'systems.']",,,,domain labels,0,14,27,14,27,are,0,28,31,28,31,"['domain', 'labels']","['are', 'making', 'build']"
131,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,2,We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision -- suggesting a simple data-driven definition of domains in textual data.,We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision -- suggesting a simple data-driven definition of domains in textual data.,,,"['We', 'show', 'that', 'massive', 'pre-trained', 'language', 'models', 'implicitly', 'learn', 'sentence', 'representations', 'that', 'cluster', 'by', 'domains', 'without', 'supervision', '--', 'suggesting', 'a', 'simple', 'data-driven', 'definition', 'of', 'domains', 'in', 'textual', 'data.']",,,,We,0,0,2,0,2,show,0,3,7,3,7,['We'],['show']
132,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,3,"We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data.","We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data.",,,"['We', 'harness', 'this', 'property', 'and', 'propose', 'domain', 'data', 'selection', 'methods', 'based', 'on', 'such', 'models,', 'which', 'require', 'only', 'a', 'small', 'set', 'of', 'in-domain', 'monolingual', 'data.']",,,,We,0,0,2,0,2,harness and propose,0,3,36,3,36,['We'],"['harness', 'propose']"
133,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,4,"We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and by precision and recall of sentence selection with respect to an oracle.","We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and by precision and recall of sentence selection with respect to an oracle.",,,"['We', 'evaluate', 'our', 'data', 'selection', 'methods', 'for', 'neural', 'machine', 'translation', 'across', 'five', 'diverse', 'domains,', 'where', 'they', 'outperform', 'an', 'established', 'approach', 'as', 'measured', 'by', 'both', 'BLEU', 'and', 'by', 'precision', 'and', 'recall', 'of', 'sentence', 'selection', 'with', 'respect', 'to', 'an', 'oracle.']",,,,We,0,0,2,0,2,evaluate,0,3,11,3,11,['We'],['evaluate']
134,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Title,0,Unsupervised Domain Clusters in Pretrained Language Models.,Unsupervised Domain Clusters in Pretrained Language Models.,,,"['Unsupervised', 'Domain', 'Clusters', 'in', 'Pretrained', 'Language', 'Models.']",,,,Unsupervised Domain Clusters,0,0,28,0,28,,,,,,,"['Unsupervised', 'Domain', 'Clusters']",[]
135,https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,0,"Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of a trained model to downstream natural language processing tasks, such as named entity recognition (NER) and question answering.",Recent advances in language representation using neural networks have made it viable to,transfer,"the learned internal states of a trained model to downstream natural language processing tasks , such as named entity recognition ( NER ) and question answering .","['Recent', 'advances', 'in', 'language', 'representation', 'using', 'neural', 'networks', 'have', 'made', 'it', 'viable', 'to', 'transfer', 'the', 'learned', 'internal', 'states', 'of', 'a', 'trained', 'model', 'to', 'downstream', 'natural', 'language', 'processing', 'tasks', ',', 'such', 'as', 'named', 'entity', 'recognition', '(', 'NER', ')', 'and', 'question', 'answering', '.']","(13, 14)","(87, 95)",0,Recent advances,0,0,15,0,15,have made,0,65,74,65,74,"['Recent', 'advances']","['have', 'made', 'transfer']"
136,https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,1,It has been shown that the leverage of pre-trained language models improves the overall performance on many tasks and is highly beneficial when labeled data is scarce.,It has been shown that the leverage of pre-trained language models improves the overall performance on many tasks and is highly beneficial when labeled data is scarce.,,,"['It', 'has', 'been', 'shown', 'that', 'the', 'leverage', 'of', 'pre-trained', 'language', 'models', 'improves', 'the', 'overall', 'performance', 'on', 'many', 'tasks', 'and', 'is', 'highly', 'beneficial', 'when', 'labeled', 'data', 'is', 'scarce.']",,,,It,0,0,2,0,2,has been shown,0,3,17,3,17,['It'],"['has', 'been', 'shown']"
137,https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,2,"In this work, we employ a pre-trained BERT with Conditional Random Fields (CRF) architecture to the NER task on the Portuguese language, combining the transfer capabilities of BERT with the structured predictions of CRF.","In this work , we employ a pre-trained BERT with Conditional Random Fields ( CRF ) architecture to the NER task on the Portuguese language , combining the",transfer,capabilities of BERT with the structured predictions of CRF .,"['In', 'this', 'work', ',', 'we', 'employ', 'a', 'pre-trained', 'BERT', 'with', 'Conditional', 'Random', 'Fields', '(', 'CRF', ')', 'architecture', 'to', 'the', 'NER', 'task', 'on', 'the', 'Portuguese', 'language', ',', 'combining', 'the', 'transfer', 'capabilities', 'of', 'BERT', 'with', 'the', 'structured', 'predictions', 'of', 'CRF', '.']","(28, 29)","(154, 162)",0,we,0,15,17,15,17,employ,0,18,24,18,24,['we'],"['employ', 'combining']"
138,https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,3,We explore feature-based and fine-tuning training strategies for the BERT model.,We explore feature-based and fine-tuning training strategies for the BERT model.,,,"['We', 'explore', 'feature-based', 'and', 'fine-tuning', 'training', 'strategies', 'for', 'the', 'BERT', 'model.']",,,,We,0,0,2,0,2,explore,0,3,10,3,10,['We'],['explore']
139,https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,4,"Our fine-tuning approach obtains new state-of-the-art results on the HAREM I dataset, improving the F1-score by 3.2 points on the selective scenario (5 NE classes) and by 3.8 points on the total scenario (10 NE classes).","Our fine-tuning approach obtains new state-of-the-art results on the HAREM I dataset, improving the F1-score by 3.2 points on the selective scenario (5 NE classes) and by 3.8 points on the total scenario (10 NE classes).",,,"['Our', 'fine-tuning', 'approach', 'obtains', 'new', 'state-of-the-art', 'results', 'on', 'the', 'HAREM', 'I', 'dataset,', 'improving', 'the', 'F1-score', 'by', '3.2', 'points', 'on', 'the', 'selective', 'scenario', '(5', 'NE', 'classes)', 'and', 'by', '3.8', 'points', 'on', 'the', 'total', 'scenario', '(10', 'NE', 'classes).']",,,,fine tuning approach,0,4,26,4,26,obtains,0,27,34,27,34,"['fine', 'tuning', 'approach']","['obtains', 'improving']"
140,https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Title,0,Portuguese Named Entity Recognition using BERT-CRF.,Portuguese Named Entity Recognition using BERT-CRF.,,,"['Portuguese', 'Named', 'Entity', 'Recognition', 'using', 'BERT-CRF.']",,,,Portuguese Named Entity Recognition,0,0,35,0,35,using,0,36,41,36,41,"['Portuguese', 'Named', 'Entity', 'Recognition']",['using']
141,https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,0,"Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and RoBERTa, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks.",,"Transformer based Very Large Language Models ( VLLMs ) like BERT , XLNet and RoBERTa",", have recently shown tremendous performance on a large variety of Natural Language Understanding ( NLU ) tasks .","['Transformer', 'based', 'Very', 'Large', 'Language', 'Models', '(', 'VLLMs', ')', 'like', 'BERT', ',', 'XLNet', 'and', 'RoBERTa', ',', 'have', 'recently', 'shown', 'tremendous', 'performance', 'on', 'a', 'large', 'variety', 'of', 'Natural', 'Language', 'Understanding', '(', 'NLU', ')', 'tasks', '.']","(0, 15)","(0, 84)",0,Large Language Models VLLMs,1,23,52,23,52,have shown,2,87,106,2,21,"['Large', 'Language', 'Models', 'VLLMs']","['have', 'shown']"
142,https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,1,"However, due to their size, these VLLMs are extremely resource intensive and cumbersome to deploy at production time.","However , due to",their,"size , these VLLMs are extremely resource intensive and cumbersome to deploy at production time .","['However', ',', 'due', 'to', 'their', 'size', ',', 'these', 'VLLMs', 'are', 'extremely', 'resource', 'intensive', 'and', 'cumbersome', 'to', 'deploy', 'at', 'production', 'time', '.']","(4, 5)","(16, 21)",-1,these VLLMs,2,30,41,7,18,are,2,42,45,19,22,"['these', 'VLLMs']",['are']
143,https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,1,"However, due to their size, these VLLMs are extremely resource intensive and cumbersome to deploy at production time.","However , due to their size ,",these VLLMs,are extremely resource intensive and cumbersome to deploy at production time .,"['However', ',', 'due', 'to', 'their', 'size', ',', 'these', 'VLLMs', 'are', 'extremely', 'resource', 'intensive', 'and', 'cumbersome', 'to', 'deploy', 'at', 'production', 'time', '.']","(7, 9)","(29, 40)",-1,these VLLMs,1,30,41,0,11,are,2,42,45,0,3,"['these', 'VLLMs']",['are']
144,https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,2,Several recent publications have looked into various ways to distil knowledge from a transformer based VLLM (most commonly BERT-Base) into a smaller model which can run much faster at inference time.,Several recent publications have looked into various ways to distil knowledge from a,transformer,based VLLM ( most commonly BERT-Base ) into a smaller model which can run much faster at inference time .,"['Several', 'recent', 'publications', 'have', 'looked', 'into', 'various', 'ways', 'to', 'distil', 'knowledge', 'from', 'a', 'transformer', 'based', 'VLLM', '(', 'most', 'commonly', 'BERT-Base', ')', 'into', 'a', 'smaller', 'model', 'which', 'can', 'run', 'much', 'faster', 'at', 'inference', 'time', '.']","(13, 14)","(84, 95)",0,Several recent publications,0,0,27,0,27,have looked,0,28,39,28,39,"['Several', 'recent', 'publications']","['have', 'looked']"
145,https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,3,"Here, we propose a novel set of techniques which together produce a task-specific hybrid convolutional and transformer model, WaLDORf, that achieves state-of-the-art inference speed while still being more accurate than previous distilled models.","Here , we propose a novel set of techniques which together produce a task-specific hybrid convolutional and",transformer,"model , WaLDORf , that achieves state-of-the-art inference speed while still being more accurate than previous distilled models .","['Here', ',', 'we', 'propose', 'a', 'novel', 'set', 'of', 'techniques', 'which', 'together', 'produce', 'a', 'task-specific', 'hybrid', 'convolutional', 'and', 'transformer', 'model', ',', 'WaLDORf', ',', 'that', 'achieves', 'state-of-the-art', 'inference', 'speed', 'while', 'still', 'being', 'more', 'accurate', 'than', 'previous', 'distilled', 'models', '.']","(17, 18)","(107, 118)",0,we,0,7,9,7,9,propose,0,10,17,10,17,['we'],['propose']
146,https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Title,0,WaLDORf: Wasteless Language-model Distillation On Reading-comprehension.,WaLDORf: Wasteless Language-model Distillation On Reading-comprehension.,,,"['WaLDORf:', 'Wasteless', 'Language-model', 'Distillation', 'On', 'Reading-comprehension.']",,,,Wasteless Language comprehension,0,10,76,10,76,,,,,,,"['Wasteless', 'Language', 'comprehension']",[]
147,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,0,"Many visual scenes contain text that carries crucial information, and it is thus essential to understand text in images for downstream reasoning tasks.","Many visual scenes contain text that carries crucial information, and it is thus essential to understand text in images for downstream reasoning tasks.",,,"['Many', 'visual', 'scenes', 'contain', 'text', 'that', 'carries', 'crucial', 'information,', 'and', 'it', 'is', 'thus', 'essential', 'to', 'understand', 'text', 'in', 'images', 'for', 'downstream', 'reasoning', 'tasks.']",,,,,,,,,,and,0,67,70,67,70,"['Many', 'visual', 'scenes', 'it']","['contain', 'is']"
148,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,1,"For example, a deep water label on a warning sign warns people about the danger in the scene.","For example, a deep water label on a warning sign warns people about the danger in the scene.",,,"['For', 'example,', 'a', 'deep', 'water', 'label', 'on', 'a', 'warning', 'sign', 'warns', 'people', 'about', 'the', 'danger', 'in', 'the', 'scene.']",,,,a deep water label,0,14,32,14,32,warns,0,51,56,51,56,"['a', 'deep', 'water', 'label']",['warns']
149,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,2,Recent work has explored the TextVQA task that requires reading and understanding text in images to answer a question.,Recent work has explored the TextVQA task that requires reading and understanding text in images to answer a question.,,,"['Recent', 'work', 'has', 'explored', 'the', 'TextVQA', 'task', 'that', 'requires', 'reading', 'and', 'understanding', 'text', 'in', 'images', 'to', 'answer', 'a', 'question.']",,,,Recent work,0,0,11,0,11,has explored,0,12,24,12,24,"['Recent', 'work']","['has', 'explored']"
150,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,3,"However, existing approaches for TextVQA are mostly based on custom pairwise fusion mechanisms between a pair of two modalities and are restricted to a single prediction step by casting TextVQA as a classification task.","However, existing approaches for TextVQA are mostly based on custom pairwise fusion mechanisms between a pair of two modalities and are restricted to a single prediction step by casting TextVQA as a classification task.",,,"['However,', 'existing', 'approaches', 'for', 'TextVQA', 'are', 'mostly', 'based', 'on', 'custom', 'pairwise', 'fusion', 'mechanisms', 'between', 'a', 'pair', 'of', 'two', 'modalities', 'and', 'are', 'restricted', 'to', 'a', 'single', 'prediction', 'step', 'by', 'casting', 'TextVQA', 'as', 'a', 'classification', 'task.']",,,,approaches,0,19,29,19,29,are based and are restricted,0,42,147,42,147,['approaches'],"['are', 'based', 'are', 'restricted']"
151,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,4,"In this work, we propose a novel model for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images.","In this work , we propose",a novel model for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images,.,"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'novel', 'model', 'for', 'the', 'TextVQA', 'task', 'based', 'on', 'a', 'multimodal', 'transformer', 'architecture', 'accompanied', 'by', 'a', 'rich', 'representation', 'for', 'text', 'in', 'images', '.']","(6, 28)","(25, 162)",57,we,0,15,17,15,17,propose,0,18,25,18,25,['we'],['propose']
152,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,5,Our model naturally fuses different modalities homogeneously by embedding them into a common semantic space where self-attention is applied to model inter- and intra- modality context.,,Our model,naturally fuses different modalities homogeneously by embedding them into a common semantic space where self - attention is applied to model inter- and intra- modality context .,"['Our', 'model', 'naturally', 'fuses', 'different', 'modalities', 'homogeneously', 'by', 'embedding', 'them', 'into', 'a', 'common', 'semantic', 'space', 'where', 'self', '-', 'attention', 'is', 'applied', 'to', 'model', 'inter-', 'and', 'intra-', 'modality', 'context', '.']","(0, 2)","(0, 9)",-1,model,1,4,9,4,9,fuses,2,20,25,10,15,['model'],['fuses']
153,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,6,"Furthermore, it enables iterative answer decoding with a dynamic pointer network, allowing the model to form an answer through multi-step prediction instead of one-step classification.","Furthermore ,",it,"enables iterative answer decoding with a dynamic pointer network , allowing the model to form an answer through multi - step prediction instead of one - step classification .","['Furthermore', ',', 'it', 'enables', 'iterative', 'answer', 'decoding', 'with', 'a', 'dynamic', 'pointer', 'network', ',', 'allowing', 'the', 'model', 'to', 'form', 'an', 'answer', 'through', 'multi', '-', 'step', 'prediction', 'instead', 'of', 'one', '-', 'step', 'classification', '.']","(2, 3)","(13, 15)",-1,it,1,14,16,0,2,enables,2,17,24,0,7,['it'],"['enables', 'allowing', 'form']"
154,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,6,"Furthermore, it enables iterative answer decoding with a dynamic pointer network, allowing the model to form an answer through multi-step prediction instead of one-step classification.","Furthermore , it enables iterative answer decoding with a dynamic pointer network , allowing",the model,to form an answer through multi - step prediction instead of one - step classification .,"['Furthermore', ',', 'it', 'enables', 'iterative', 'answer', 'decoding', 'with', 'a', 'dynamic', 'pointer', 'network', ',', 'allowing', 'the', 'model', 'to', 'form', 'an', 'answer', 'through', 'multi', '-', 'step', 'prediction', 'instead', 'of', 'one', '-', 'step', 'classification', '.']","(14, 16)","(92, 101)",-1,it,0,14,16,14,16,enables,0,17,24,17,24,['it'],"['enables', 'allowing', 'form']"
155,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,7,Our model outperforms existing approaches on three benchmark datasets for the TextVQA task by a large margin.,,Our model,outperforms existing approaches on three benchmark datasets for the TextVQA task by a large margin .,"['Our', 'model', 'outperforms', 'existing', 'approaches', 'on', 'three', 'benchmark', 'datasets', 'for', 'the', 'TextVQA', 'task', 'by', 'a', 'large', 'margin', '.']","(0, 2)","(0, 9)",-1,model,1,4,9,4,9,,,,,,,['model'],[]
156,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Title,0,Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA.,Iterative Answer Prediction with Pointer-Augmented Multimodal,Transformers,for TextVQA .,"['Iterative', 'Answer', 'Prediction', 'with', 'Pointer-Augmented', 'Multimodal', 'Transformers', 'for', 'TextVQA', '.']","(6, 7)","(61, 73)",0,Iterative Answer Prediction Pointer Multimodal Transformers,0,0,76,0,76,,,,,,,"['Iterative', 'Answer', 'Prediction', 'Pointer', 'Multimodal', 'Transformers']",[]
157,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,0,Learning knowledge graph embeddings (KGEs) is an efficient approach to knowledge graph completion.,Learning knowledge graph embeddings (KGEs) is an efficient approach to knowledge graph completion.,,,"['Learning', 'knowledge', 'graph', 'embeddings', '(KGEs)', 'is', 'an', 'efficient', 'approach', 'to', 'knowledge', 'graph', 'completion.']",,,,knowledge graph embeddings KGEs,0,9,42,9,42,is,0,45,47,45,47,"['knowledge', 'graph', 'embeddings', 'KGEs']",['is']
158,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,1,"Conventional KGEs often suffer from limited knowledge representation, which causes less accuracy especially when training on sparse knowledge graphs.","Conventional KGEs often suffer from limited knowledge representation, which causes less accuracy especially when training on sparse knowledge graphs.",,,"['Conventional', 'KGEs', 'often', 'suffer', 'from', 'limited', 'knowledge', 'representation,', 'which', 'causes', 'less', 'accuracy', 'especially', 'when', 'training', 'on', 'sparse', 'knowledge', 'graphs.']",,,,Conventional KGEs,0,0,17,0,17,suffer,0,24,30,24,30,"['Conventional', 'KGEs']",['suffer']
159,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,2,"To remedy this, we present Pretrain-KGEs, a training framework for learning better knowledgeable entity and relation embeddings, leveraging the abundant linguistic knowledge from pretrained language models.","To remedy this, we present Pretrain-KGEs, a training framework for learning better knowledgeable entity and relation embeddings, leveraging the abundant linguistic knowledge from pretrained language models.",,,"['To', 'remedy', 'this,', 'we', 'present', 'Pretrain-KGEs,', 'a', 'training', 'framework', 'for', 'learning', 'better', 'knowledgeable', 'entity', 'and', 'relation', 'embeddings,', 'leveraging', 'the', 'abundant', 'linguistic', 'knowledge', 'from', 'pretrained', 'language', 'models.']",,,,we,0,17,19,17,19,present,0,20,27,20,27,['we'],"['remedy', 'present']"
160,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,3,"Specifically, we propose a unified approach in which we first learn entity and relation representations via pretrained language models and use the representations to initialize entity and relation embeddings for training KGE models.","Specifically, we propose a unified approach in which we first learn entity and relation representations via pretrained language models and use the representations to initialize entity and relation embeddings for training KGE models.",,,"['Specifically,', 'we', 'propose', 'a', 'unified', 'approach', 'in', 'which', 'we', 'first', 'learn', 'entity', 'and', 'relation', 'representations', 'via', 'pretrained', 'language', 'models', 'and', 'use', 'the', 'representations', 'to', 'initialize', 'entity', 'and', 'relation', 'embeddings', 'for', 'training', 'KGE', 'models.']",,,,we,0,15,17,15,17,propose,0,18,25,18,25,['we'],['propose']
161,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,4,Our proposed method is model agnostic in the sense that it can be applied to any variant of KGE models.,Our proposed method is model agnostic in the sense that it can be applied to any variant of KGE models.,,,"['Our', 'proposed', 'method', 'is', 'model', 'agnostic', 'in', 'the', 'sense', 'that', 'it', 'can', 'be', 'applied', 'to', 'any', 'variant', 'of', 'KGE', 'models.']",,,,method,0,13,19,13,19,is,0,20,22,20,22,['method'],['is']
162,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,5,"Experimental results show that our method can consistently improve results and achieve state-of-the-art performance using different KGE models such as TransE and QuatE, across four benchmark KG datasets in link prediction and triplet classification tasks.","Experimental results show that our method can consistently improve results and achieve state-of-the-art performance using different KGE models such as TransE and QuatE, across four benchmark KG datasets in link prediction and triplet classification tasks.",,,"['Experimental', 'results', 'show', 'that', 'our', 'method', 'can', 'consistently', 'improve', 'results', 'and', 'achieve', 'state-of-the-art', 'performance', 'using', 'different', 'KGE', 'models', 'such', 'as', 'TransE', 'and', 'QuatE,', 'across', 'four', 'benchmark', 'KG', 'datasets', 'in', 'link', 'prediction', 'and', 'triplet', 'classification', 'tasks.']",,,,Experimental results,0,0,20,0,20,show,0,21,25,21,25,"['Experimental', 'results']",['show']
163,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Title,0,Pretrain-KGEs: Learning Knowledge Representation from Pretrained Models for Knowledge Graph Embeddings.,Pretrain-KGEs: Learning Knowledge Representation from Pretrained Models for Knowledge Graph Embeddings.,,,"['Pretrain-KGEs:', 'Learning', 'Knowledge', 'Representation', 'from', 'Pretrained', 'Models', 'for', 'Knowledge', 'Graph', 'Embeddings.']",,,,Pretrain KGEs Knowledge Representation,0,0,51,0,51,,,,,,,"['Pretrain', 'KGEs', 'Knowledge', 'Representation']",[]
164,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,0,"As single-task accuracy on individual language and image tasks has improved substantially in the last few years, the long-term goal of a generally skilled agent that can both see and talk becomes more feasible to explore.","As single-task accuracy on individual language and image tasks has improved substantially in the last few years, the long-term goal of a generally skilled agent that can both see and talk becomes more feasible to explore.",,,"['As', 'single-task', 'accuracy', 'on', 'individual', 'language', 'and', 'image', 'tasks', 'has', 'improved', 'substantially', 'in', 'the', 'last', 'few', 'years,', 'the', 'long-term', 'goal', 'of', 'a', 'generally', 'skilled', 'agent', 'that', 'can', 'both', 'see', 'and', 'talk', 'becomes', 'more', 'feasible', 'to', 'explore.']",,,,the goal,0,116,136,116,136,becomes,0,193,200,193,200,"['task', 'accuracy', 'the', 'goal']","['has', 'improved', 'becomes']"
165,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,1,"In this work, we focus on leveraging individual language and image tasks, along with resources that incorporate both vision and language towards that objective.","In this work, we focus on leveraging individual language and image tasks, along with resources that incorporate both vision and language towards that objective.",,,"['In', 'this', 'work,', 'we', 'focus', 'on', 'leveraging', 'individual', 'language', 'and', 'image', 'tasks,', 'along', 'with', 'resources', 'that', 'incorporate', 'both', 'vision', 'and', 'language', 'towards', 'that', 'objective.']",,,,we,0,15,17,15,17,focus,0,18,23,18,23,['we'],['focus']
166,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,2,We design an architecture that combines state-of-the-art Transformer and ResNeXt modules fed into a novel attentive multimodal module to produce a combined model trained on many tasks.,We design an architecture that combines state-of-the-art,Transformer,and ResNeXt modules fed into a novel attentive multimodal module to produce a combined model trained on many tasks .,"['We', 'design', 'an', 'architecture', 'that', 'combines', 'state-of-the-art', 'Transformer', 'and', 'ResNeXt', 'modules', 'fed', 'into', 'a', 'novel', 'attentive', 'multimodal', 'module', 'to', 'produce', 'a', 'combined', 'model', 'trained', 'on', 'many', 'tasks', '.']","(7, 8)","(56, 67)",0,We,0,0,2,0,2,design,0,3,9,3,9,['We'],['design']
167,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,3,"We provide a thorough analysis of the components of the model, and transfer performance when training on one, some, or all of the tasks.","We provide a thorough analysis of the components of the model , and",transfer,"performance when training on one , some , or all of the tasks .","['We', 'provide', 'a', 'thorough', 'analysis', 'of', 'the', 'components', 'of', 'the', 'model', ',', 'and', 'transfer', 'performance', 'when', 'training', 'on', 'one', ',', 'some', ',', 'or', 'all', 'of', 'the', 'tasks', '.']","(13, 14)","(67, 75)",0,We,0,0,2,0,2,provide,0,3,10,3,10,['We'],['provide']
168,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,4,"Our final models provide a single system that obtains good results on all vision and language tasks considered, and improves the state-of-the-art in image-grounded conversational applications.","Our final models provide a single system that obtains good results on all vision and language tasks considered, and improves the state-of-the-art in image-grounded conversational applications.",,,"['Our', 'final', 'models', 'provide', 'a', 'single', 'system', 'that', 'obtains', 'good', 'results', 'on', 'all', 'vision', 'and', 'language', 'tasks', 'considered,', 'and', 'improves', 'the', 'state-of-the-art', 'in', 'image-grounded', 'conversational', 'applications.']",,,,final models,0,4,16,4,16,provide,0,17,24,17,24,"['final', 'models']",['provide']
169,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Title,0,All-in-One Image-Grounded Conversational Agents.,All-in-One Image-Grounded Conversational Agents.,,,"['All-in-One', 'Image-Grounded', 'Conversational', 'Agents.']",,,,Image Conversational Agents,0,15,53,15,53,,,,,,,"['Image', 'Conversational', 'Agents']",[]
170,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,0,Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications.,Recent work in language modeling demonstrates that training large,transformer,models advances the state of the art in Natural Language Processing applications .,"['Recent', 'work', 'in', 'language', 'modeling', 'demonstrates', 'that', 'training', 'large', 'transformer', 'models', 'advances', 'the', 'state', 'of', 'the', 'art', 'in', 'Natural', 'Language', 'Processing', 'applications', '.']","(9, 10)","(65, 76)",0,Recent work,0,0,11,0,11,demonstrates,0,33,45,33,45,"['Recent', 'work']",['demonstrates']
171,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,1,"However, very large models can be quite difficult to train due to memory constraints.","However, very large models can be quite difficult to train due to memory constraints.",,,"['However,', 'very', 'large', 'models', 'can', 'be', 'quite', 'difficult', 'to', 'train', 'due', 'to', 'memory', 'constraints.']",,,,models,0,21,27,21,27,can be,0,28,34,28,34,['models'],"['can', 'be']"
172,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,2,"In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters.","In this work , we present our techniques for training very large transformer models and implement","a simple , efficient intra - layer model parallel approach that enables training transformer models with billions of parameters",.,"['In', 'this', 'work', ',', 'we', 'present', 'our', 'techniques', 'for', 'training', 'very', 'large', 'transformer', 'models', 'and', 'implement', 'a', 'simple', ',', 'efficient', 'intra', '-', 'layer', 'model', 'parallel', 'approach', 'that', 'enables', 'training', 'transformer', 'models', 'with', 'billions', 'of', 'parameters', '.']","(16, 35)","(97, 224)",81,we,0,15,17,15,17,present,0,18,25,18,25,['we'],['present']
173,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,3,"Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch.",,Our approach,"does not require a new compiler or library changes , is orthogonal and complimentary to pipeline model parallelism , and can be fully implemented with the insertion of a few communication operations in native PyTorch .","['Our', 'approach', 'does', 'not', 'require', 'a', 'new', 'compiler', 'or', 'library', 'changes', ',', 'is', 'orthogonal', 'and', 'complimentary', 'to', 'pipeline', 'model', 'parallelism', ',', 'and', 'can', 'be', 'fully', 'implemented', 'with', 'the', 'insertion', 'of', 'a', 'few', 'communication', 'operations', 'in', 'native', 'PyTorch', '.']","(0, 2)","(0, 12)",-1,approach,1,4,12,4,12,does require is and and can be implemented,2,13,158,0,145,['approach'],"['does', 'require', 'is', 'can', 'be', 'implemented']"
174,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,4,We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs.,We illustrate,this approach,by converging transformer based models up to 8.3 billion parameters using 512 GPUs .,"['We', 'illustrate', 'this', 'approach', 'by', 'converging', 'transformer', 'based', 'models', 'up', 'to', '8.3', 'billion', 'parameters', 'using', '512', 'GPUs', '.']","(2, 4)","(13, 26)",-1,We,0,0,2,0,2,illustrate,0,3,13,3,13,['We'],['illustrate']
175,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,5,"We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs.","We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs.",,,"['We', 'sustain', '15.1', 'PetaFLOPs', 'across', 'the', 'entire', 'application', 'with', '76%', 'scaling', 'efficiency', 'when', 'compared', 'to', 'a', 'strong', 'single', 'GPU', 'baseline', 'that', 'sustains', '39', 'TeraFLOPs,', 'which', 'is', '30%', 'of', 'peak', 'FLOPs.']",,,,We,0,0,2,0,2,sustain,0,3,10,3,10,['We'],['sustain']
176,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,6,"To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT.","To demonstrate that large language models can further advance the state of the art ( SOTA ) , we train an 8.3 billion parameter",transformer,language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT .,"['To', 'demonstrate', 'that', 'large', 'language', 'models', 'can', 'further', 'advance', 'the', 'state', 'of', 'the', 'art', '(', 'SOTA', ')', ',', 'we', 'train', 'an', '8.3', 'billion', 'parameter', 'transformer', 'language', 'model', 'similar', 'to', 'GPT-2', 'and', 'a', '3.9', 'billion', 'parameter', 'model', 'similar', 'to', 'BERT', '.']","(24, 25)","(127, 138)",0,we,0,94,96,94,96,train,0,97,102,97,102,['we'],"['demonstrate', 'train']"
177,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,7,We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows.,We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows.,,,"['We', 'show', 'that', 'careful', 'attention', 'to', 'the', 'placement', 'of', 'layer', 'normalization', 'in', 'BERT-like', 'models', 'is', 'critical', 'to', 'achieving', 'increased', 'performance', 'as', 'the', 'model', 'size', 'grows.']",,,,We,0,0,2,0,2,show is,0,3,95,3,95,['We'],"['show', 'is']"
178,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,8,Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets.,Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets.,,,"['Using', 'the', 'GPT-2', 'model', 'we', 'achieve', 'SOTA', 'results', 'on', 'the', 'WikiText103', '(10.8', 'compared', 'to', 'SOTA', 'perplexity', 'of', '15.8)', 'and', 'LAMBADA', '(66.5%', 'compared', 'to', 'SOTA', 'accuracy', 'of', '63.2%)', 'datasets.']",,,,we,0,22,24,22,24,achieve,0,25,32,25,32,['we'],"['Using', 'achieve']"
179,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,9,Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).,Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).,,,"['Our', 'BERT', 'model', 'achieves', 'SOTA', 'results', 'on', 'the', 'RACE', 'dataset', '(90.9%', 'compared', 'to', 'SOTA', 'accuracy', 'of', '89.4%).']",,,,BERT model,0,4,14,4,14,achieves,0,15,23,15,23,"['BERT', 'model']",['achieves']
180,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Title,0,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.,,,"['Megatron-LM:', 'Training', 'Multi-Billion', 'Parameter', 'Language', 'Models', 'Using', 'Model', 'Parallelism.']",,,,Megatron LM Training Multi Billion Parameter Language Models,0,0,66,0,66,,,,,,,"['Megatron', 'LM', 'Training', 'Multi', 'Billion', 'Parameter', 'Language', 'Models']",[]
181,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,0,"Transfer learning has become the de facto standard in computer vision and natural language processing, especially where labeled data is scarce.",,Transfer learning,"has become the de facto standard in computer vision and natural language processing , especially where labeled data is scarce .","['Transfer', 'learning', 'has', 'become', 'the', 'de', 'facto', 'standard', 'in', 'computer', 'vision', 'and', 'natural', 'language', 'processing', ',', 'especially', 'where', 'labeled', 'data', 'is', 'scarce', '.']","(0, 2)","(0, 17)",0,Transfer learning,1,0,17,0,17,has become,2,18,28,0,10,"['Transfer', 'learning']","['has', 'become']"
182,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,0,"Transfer learning has become the de facto standard in computer vision and natural language processing, especially where labeled data is scarce.",,Transfer,"learning has become the de facto standard in computer vision and natural language processing , especially where labeled data is scarce .","['Transfer', 'learning', 'has', 'become', 'the', 'de', 'facto', 'standard', 'in', 'computer', 'vision', 'and', 'natural', 'language', 'processing', ',', 'especially', 'where', 'labeled', 'data', 'is', 'scarce', '.']","(0, 1)","(0, 8)",0,Transfer learning,1,0,17,0,17,has become,2,18,28,9,19,"['Transfer', 'learning']","['has', 'become']"
183,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,1,Accuracy can be significantly improved by using pre-trained models and subsequent fine-tuning.,Accuracy can be significantly improved by using pre-trained models and subsequent fine-tuning.,,,"['Accuracy', 'can', 'be', 'significantly', 'improved', 'by', 'using', 'pre-trained', 'models', 'and', 'subsequent', 'fine-tuning.']",,,,Accuracy,0,0,8,0,8,can be improved,0,9,38,9,38,['Accuracy'],"['can', 'be', 'improved']"
184,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,2,"In visual reasoning tasks, such as image question answering, transfer learning is more complex.","In visual reasoning tasks , such as image question answering ,",transfer learning,is more complex .,"['In', 'visual', 'reasoning', 'tasks', ',', 'such', 'as', 'image', 'question', 'answering', ',', 'transfer', 'learning', 'is', 'more', 'complex', '.']","(11, 13)","(62, 79)",0,visual reasoning tasks transfer learning,0,3,80,3,80,is,2,81,83,0,2,"['visual', 'reasoning', 'tasks', 'transfer', 'learning']",['is']
185,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,2,"In visual reasoning tasks, such as image question answering, transfer learning is more complex.","In visual reasoning tasks , such as image question answering ,",transfer,learning is more complex .,"['In', 'visual', 'reasoning', 'tasks', ',', 'such', 'as', 'image', 'question', 'answering', ',', 'transfer', 'learning', 'is', 'more', 'complex', '.']","(11, 12)","(62, 70)",0,visual reasoning tasks transfer learning,0,3,80,3,80,is,2,81,83,9,11,"['visual', 'reasoning', 'tasks', 'transfer', 'learning']",['is']
186,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,3,"In addition to transferring the capability to recognize visual features, we also expect to transfer the system's ability to reason.",In addition to,transferring,"the capability to recognize visual features , we also expect to transfer the system 's ability to reason .","['In', 'addition', 'to', 'transferring', 'the', 'capability', 'to', 'recognize', 'visual', 'features', ',', 'we', 'also', 'expect', 'to', 'transfer', 'the', 'system', ""'s"", 'ability', 'to', 'reason', '.']","(3, 4)","(14, 26)",0,we,2,74,76,46,48,expect,2,82,88,54,60,['we'],"['expect', 'transfer']"
187,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,4,"Moreover, for video data, temporal reasoning adds another dimension.","Moreover, for video data, temporal reasoning adds another dimension.",,,"['Moreover,', 'for', 'video', 'data,', 'temporal', 'reasoning', 'adds', 'another', 'dimension.']",,,,temporal reasoning,0,28,46,28,46,adds,0,47,51,47,51,"['temporal', 'reasoning']",['adds']
188,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,5,"In this work, we formalize these unique aspects of transfer learning and propose a theoretical framework for visual reasoning, exemplified by the well-established CLEVR and COG datasets.","In this work , we formalize these unique aspects of",transfer learning,"and propose a theoretical framework for visual reasoning , exemplified by the well-established CLEVR and COG datasets .","['In', 'this', 'work', ',', 'we', 'formalize', 'these', 'unique', 'aspects', 'of', 'transfer', 'learning', 'and', 'propose', 'a', 'theoretical', 'framework', 'for', 'visual', 'reasoning', ',', 'exemplified', 'by', 'the', 'well-established', 'CLEVR', 'and', 'COG', 'datasets', '.']","(10, 12)","(51, 68)",0,we,0,15,17,15,17,formalize and propose,0,18,81,18,81,['we'],"['formalize', 'propose']"
189,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,5,"In this work, we formalize these unique aspects of transfer learning and propose a theoretical framework for visual reasoning, exemplified by the well-established CLEVR and COG datasets.","In this work , we formalize these unique aspects of",transfer,"learning and propose a theoretical framework for visual reasoning , exemplified by the well-established CLEVR and COG datasets .","['In', 'this', 'work', ',', 'we', 'formalize', 'these', 'unique', 'aspects', 'of', 'transfer', 'learning', 'and', 'propose', 'a', 'theoretical', 'framework', 'for', 'visual', 'reasoning', ',', 'exemplified', 'by', 'the', 'well-established', 'CLEVR', 'and', 'COG', 'datasets', '.']","(10, 11)","(51, 59)",0,we,0,15,17,15,17,formalize and propose,0,18,81,18,81,['we'],"['formalize', 'propose']"
190,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,6,"Furthermore, we introduce a new, end-to-end differentiable recurrent model (SAMNet), which shows state-of-the-art accuracy and better performance in transfer learning on both datasets.","Furthermore , we introduce a new , end-to-end differentiable recurrent model ( SAMNet ) , which shows state-of-the-art accuracy and better performance in",transfer learning,on both datasets .,"['Furthermore', ',', 'we', 'introduce', 'a', 'new', ',', 'end-to-end', 'differentiable', 'recurrent', 'model', '(', 'SAMNet', ')', ',', 'which', 'shows', 'state-of-the-art', 'accuracy', 'and', 'better', 'performance', 'in', 'transfer', 'learning', 'on', 'both', 'datasets', '.']","(23, 25)","(153, 170)",0,we,0,14,16,14,16,introduce,0,17,26,17,26,['we'],['introduce']
191,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,6,"Furthermore, we introduce a new, end-to-end differentiable recurrent model (SAMNet), which shows state-of-the-art accuracy and better performance in transfer learning on both datasets.","Furthermore , we introduce a new , end-to-end differentiable recurrent model ( SAMNet ) , which shows state-of-the-art accuracy and better performance in",transfer,learning on both datasets .,"['Furthermore', ',', 'we', 'introduce', 'a', 'new', ',', 'end-to-end', 'differentiable', 'recurrent', 'model', '(', 'SAMNet', ')', ',', 'which', 'shows', 'state-of-the-art', 'accuracy', 'and', 'better', 'performance', 'in', 'transfer', 'learning', 'on', 'both', 'datasets', '.']","(23, 24)","(153, 161)",0,we,0,14,16,14,16,introduce,0,17,26,17,26,['we'],['introduce']
192,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,7,The improved performance of SAMNet stems from its capability to decouple the abstract multi-step reasoning from the length of the sequence and its selective attention enabling to store only the question-relevant objects in the external memory.,The improved performance of SAMNet stems from its capability to decouple the abstract multi-step reasoning from the length of the sequence and its selective attention enabling to store only the question-relevant objects in the external memory.,,,"['The', 'improved', 'performance', 'of', 'SAMNet', 'stems', 'from', 'its', 'capability', 'to', 'decouple', 'the', 'abstract', 'multi-step', 'reasoning', 'from', 'the', 'length', 'of', 'the', 'sequence', 'and', 'its', 'selective', 'attention', 'enabling', 'to', 'store', 'only', 'the', 'question-relevant', 'objects', 'in', 'the', 'external', 'memory.']",,,,The improved performance,0,0,24,0,24,stems,0,35,40,35,40,"['The', 'improved', 'performance']",['stems']
193,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Title,0,Transfer Learning in Visual and Relational Reasoning.,,Transfer Learning,in Visual and Relational Reasoning .,"['Transfer', 'Learning', 'in', 'Visual', 'and', 'Relational', 'Reasoning', '.']","(0, 2)","(0, 17)",0,Transfer,1,0,8,0,8,,,,,,,['Transfer'],[]
194,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Title,0,Transfer Learning in Visual and Relational Reasoning.,,Transfer,Learning in Visual and Relational Reasoning .,"['Transfer', 'Learning', 'in', 'Visual', 'and', 'Relational', 'Reasoning', '.']","(0, 1)","(0, 8)",0,Transfer,1,0,8,0,8,,,,,,,['Transfer'],[]
195,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,0,"Speculation is a naturally occurring phenomena in textual data, forming an integral component of many systems, especially in the biomedical information retrieval domain.","Speculation is a naturally occurring phenomena in textual data, forming an integral component of many systems, especially in the biomedical information retrieval domain.",,,"['Speculation', 'is', 'a', 'naturally', 'occurring', 'phenomena', 'in', 'textual', 'data,', 'forming', 'an', 'integral', 'component', 'of', 'many', 'systems,', 'especially', 'in', 'the', 'biomedical', 'information', 'retrieval', 'domain.']",,,,Speculation,0,0,11,0,11,is,0,12,14,12,14,['Speculation'],"['is', 'forming']"
196,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,1,Previous work addressing cue detection and scope resolution (the two subtasks of speculation detection) have ranged from rule-based systems to deep learning-based approaches.,Previous work addressing cue detection and scope resolution (the two subtasks of speculation detection) have ranged from rule-based systems to deep learning-based approaches.,,,"['Previous', 'work', 'addressing', 'cue', 'detection', 'and', 'scope', 'resolution', '(the', 'two', 'subtasks', 'of', 'speculation', 'detection)', 'have', 'ranged', 'from', 'rule-based', 'systems', 'to', 'deep', 'learning-based', 'approaches.']",,,,Previous work,0,0,13,0,13,have ranged,0,106,117,106,117,"['Previous', 'work']","['have', 'ranged']"
197,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,2,"In this paper, we apply three popular transformer-based architectures, BERT, XLNet and RoBERTa to this task, on two publicly available datasets, BioScope Corpus and SFU Review Corpus, reporting substantial improvements over previously reported results (by at least 0.29 F1 points on cue detection and 4.27 F1 points on scope resolution).","In this paper , we apply three popular",transformer,"-based architectures , BERT , XLNet and RoBERTa to this task , on two publicly available datasets , BioScope Corpus and SFU Review Corpus , reporting substantial improvements over previously reported results ( by at least 0.29 F1 points on cue detection and 4.27 F1 points on scope resolution ) .","['In', 'this', 'paper', ',', 'we', 'apply', 'three', 'popular', 'transformer', '-based', 'architectures', ',', 'BERT', ',', 'XLNet', 'and', 'RoBERTa', 'to', 'this', 'task', ',', 'on', 'two', 'publicly', 'available', 'datasets', ',', 'BioScope', 'Corpus', 'and', 'SFU', 'Review', 'Corpus', ',', 'reporting', 'substantial', 'improvements', 'over', 'previously', 'reported', 'results', '(', 'by', 'at', 'least', '0.29', 'F1', 'points', 'on', 'cue', 'detection', 'and', '4.27', 'F1', 'points', 'on', 'scope', 'resolution', ')', '.']","(8, 9)","(38, 49)",0,we,0,16,18,16,18,apply,0,19,24,19,24,['we'],['apply']
198,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,3,"We also experiment with joint training of the model on multiple datasets, which outperforms the single dataset training approach by a good margin.","We also experiment with joint training of the model on multiple datasets, which outperforms the single dataset training approach by a good margin.",,,"['We', 'also', 'experiment', 'with', 'joint', 'training', 'of', 'the', 'model', 'on', 'multiple', 'datasets,', 'which', 'outperforms', 'the', 'single', 'dataset', 'training', 'approach', 'by', 'a', 'good', 'margin.']",,,,We,0,0,2,0,2,experiment,0,8,18,8,18,['We'],['experiment']
199,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,4,"We observe that XLNet consistently outperforms BERT and RoBERTa, contrary to results on other benchmark datasets.","We observe that XLNet consistently outperforms BERT and RoBERTa, contrary to results on other benchmark datasets.",,,"['We', 'observe', 'that', 'XLNet', 'consistently', 'outperforms', 'BERT', 'and', 'RoBERTa,', 'contrary', 'to', 'results', 'on', 'other', 'benchmark', 'datasets.']",,,,We,0,0,2,0,2,observe,0,3,10,3,10,['We'],['observe']
200,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,5,"To confirm this observation, we apply XLNet and RoBERTa to negation detection and scope resolution, reporting state-of-the-art results on negation scope resolution for the BioScope Corpus (increase of 3.16 F1 points on the BioScope Full Papers, 0.06 F1 points on the BioScope Abstracts) and the SFU Review Corpus (increase of 0.3 F1 points).","To confirm this observation, we apply XLNet and RoBERTa to negation detection and scope resolution, reporting state-of-the-art results on negation scope resolution for the BioScope Corpus (increase of 3.16 F1 points on the BioScope Full Papers, 0.06 F1 points on the BioScope Abstracts) and the SFU Review Corpus (increase of 0.3 F1 points).",,,"['To', 'confirm', 'this', 'observation,', 'we', 'apply', 'XLNet', 'and', 'RoBERTa', 'to', 'negation', 'detection', 'and', 'scope', 'resolution,', 'reporting', 'state-of-the-art', 'results', 'on', 'negation', 'scope', 'resolution', 'for', 'the', 'BioScope', 'Corpus', '(increase', 'of', '3.16', 'F1', 'points', 'on', 'the', 'BioScope', 'Full', 'Papers,', '0.06', 'F1', 'points', 'on', 'the', 'BioScope', 'Abstracts)', 'and', 'the', 'SFU', 'Review', 'Corpus', '(increase', 'of', '0.3', 'F1', 'points).']",,,,we,0,30,32,30,32,apply,0,33,38,33,38,['we'],"['confirm', 'apply']"
201,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Title,0,Resolving the Scope of Speculation and Negation using Transformer-Based Architectures.,Resolving the Scope of Speculation and Negation using,Transformer,-Based Architectures .,"['Resolving', 'the', 'Scope', 'of', 'Speculation', 'and', 'Negation', 'using', 'Transformer', '-Based', 'Architectures', '.']","(8, 9)","(53, 64)",0,,,,,,,using,0,48,53,48,53,[],"['Resolving', 'using']"
202,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,0,Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data.,Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data.,,,"['Recent', 'years', 'have', 'witnessed', 'the', 'emerging', 'success', 'of', 'graph', 'neural', 'networks', '(GNNs)', 'for', 'modeling', 'structured', 'data.']",,,,Recent years,0,0,12,0,12,have witnessed,0,13,27,13,27,"['Recent', 'years']","['have', 'witnessed']"
203,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,1,"However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making it infeasible to represent heterogeneous structures.","However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making it infeasible to represent heterogeneous structures.",,,"['However,', 'most', 'GNNs', 'are', 'designed', 'for', 'homogeneous', 'graphs,', 'in', 'which', 'all', 'nodes', 'and', 'edges', 'belong', 'to', 'the', 'same', 'types,', 'making', 'it', 'infeasible', 'to', 'represent', 'heterogeneous', 'structures.']",,,,most GNNs,0,10,19,10,19,are designed,0,20,32,20,32,"['most', 'GNNs']","['are', 'designed']"
204,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,2,"In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs.","In this paper , we present",the Heterogeneous Graph Transformer ( HGT ),architecture for modeling Web - scale heterogeneous graphs .,"['In', 'this', 'paper', ',', 'we', 'present', 'the', 'Heterogeneous', 'Graph', 'Transformer', '(', 'HGT', ')', 'architecture', 'for', 'modeling', 'Web', '-', 'scale', 'heterogeneous', 'graphs', '.']","(6, 13)","(26, 69)",24,we,0,16,18,16,18,present,0,19,26,19,26,['we'],['present']
205,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,2,"In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs.","In this paper , we present",the Heterogeneous Graph Transformer ( HGT ) architecture for modeling Web - scale heterogeneous graphs,.,"['In', 'this', 'paper', ',', 'we', 'present', 'the', 'Heterogeneous', 'Graph', 'Transformer', '(', 'HGT', ')', 'architecture', 'for', 'modeling', 'Web', '-', 'scale', 'heterogeneous', 'graphs', '.']","(6, 21)","(26, 128)",24,we,0,16,18,16,18,present,0,19,26,19,26,['we'],['present']
206,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,3,"To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges.","To model heterogeneity , we design node- and edge - type dependent parameters to characterize the heterogeneous attention over each edge , empowering",HGT,to maintain dedicated representations for different types of nodes and edges .,"['To', 'model', 'heterogeneity', ',', 'we', 'design', 'node-', 'and', 'edge', '-', 'type', 'dependent', 'parameters', 'to', 'characterize', 'the', 'heterogeneous', 'attention', 'over', 'each', 'edge', ',', 'empowering', 'HGT', 'to', 'maintain', 'dedicated', 'representations', 'for', 'different', 'types', 'of', 'nodes', 'and', 'edges', '.']","(23, 24)","(149, 152)",-1,we,0,25,27,25,27,design,0,28,34,28,34,['we'],"['model', 'design', 'characterize', 'empowering', 'maintain']"
207,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,4,"To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithmHGSamplingfor efficient and scalable training.","To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithmHGSamplingfor efficient and scalable training.",,,"['To', 'handle', 'Web-scale', 'graph', 'data,', 'we', 'design', 'the', 'heterogeneous', 'mini-batch', 'graph', 'sampling', 'algorithmHGSamplingfor', 'efficient', 'and', 'scalable', 'training.']",,,,we,0,35,37,35,37,design,0,38,44,38,44,['we'],"['handle', 'design']"
208,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,5,Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 921 on various downstream tasks.,Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed,HGT,model consistently outperforms all the state - of - the - art GNN baselines by 921 on various downstream tasks .,"['Extensive', 'experiments', 'on', 'the', 'Open', 'Academic', 'Graph', 'of', '179', 'million', 'nodes', 'and', '2', 'billion', 'edges', 'show', 'that', 'the', 'proposed', 'HGT', 'model', 'consistently', 'outperforms', 'all', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'GNN', 'baselines', 'by', '921', 'on', 'various', 'downstream', 'tasks', '.']","(19, 20)","(112, 115)",-1,Extensive experiments,0,0,21,0,21,show,0,90,94,90,94,"['Extensive', 'experiments']",['show']
209,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,5,Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 921 on various downstream tasks.,Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that,the proposed HGT model,consistently outperforms all the state - of - the - art GNN baselines by 921 on various downstream tasks .,"['Extensive', 'experiments', 'on', 'the', 'Open', 'Academic', 'Graph', 'of', '179', 'million', 'nodes', 'and', '2', 'billion', 'edges', 'show', 'that', 'the', 'proposed', 'HGT', 'model', 'consistently', 'outperforms', 'all', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'GNN', 'baselines', 'by', '921', 'on', 'various', 'downstream', 'tasks', '.']","(17, 21)","(99, 121)",-1,Extensive experiments,0,0,21,0,21,show,0,90,94,90,94,"['Extensive', 'experiments']",['show']
210,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,6,The dataset and source code of HGT are publicly available at https://github.com/acbull/pyHGT.,The dataset and source code of,HGT,are publicly available at https://github.com/acbull/pyHGT .,"['The', 'dataset', 'and', 'source', 'code', 'of', 'HGT', 'are', 'publicly', 'available', 'at', 'https://github.com/acbull/pyHGT', '.']","(6, 7)","(30, 33)",-1,The dataset source code,0,0,27,0,27,are,2,35,38,0,3,"['The', 'dataset', 'source', 'code']",['are']
211,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Title,0,Heterogeneous Graph Transformer.,Heterogeneous Graph,Transformer,.,"['Heterogeneous', 'Graph', 'Transformer', '.']","(2, 3)","(19, 30)",0,Heterogeneous Graph Transformer,0,0,31,0,31,,,,,,,"['Heterogeneous', 'Graph', 'Transformer']",[]
212,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,0,A growing body of knowledge about biological mechanisms and interaction of biological components is contained in the peer-reviewed scientific literature.,A growing body of knowledge about biological mechanisms and interaction of biological components is contained in the peer-reviewed scientific literature.,,,"['A', 'growing', 'body', 'of', 'knowledge', 'about', 'biological', 'mechanisms', 'and', 'interaction', 'of', 'biological', 'components', 'is', 'contained', 'in', 'the', 'peer-reviewed', 'scientific', 'literature.']",,,,A body,0,0,14,0,14,is contained,0,97,109,97,109,"['A', 'body']","['is', 'contained']"
213,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,1,"In order to leverage this knowledge towards the development of predictive models, one must first extract these relationships from the text.","In order to leverage this knowledge towards the development of predictive models, one must first extract these relationships from the text.",,,"['In', 'order', 'to', 'leverage', 'this', 'knowledge', 'towards', 'the', 'development', 'of', 'predictive', 'models,', 'one', 'must', 'first', 'extract', 'these', 'relationships', 'from', 'the', 'text.']",,,,one,0,83,86,83,86,must extract,0,87,105,87,105,['one'],"['must', 'extract']"
214,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,2,"However, the context in which the interaction was reported is critical in ensuring that it is used in a manner consistent with the model's intended application.","However, the context in which the interaction was reported is critical in ensuring that it is used in a manner consistent with the model's intended application.",,,"['However,', 'the', 'context', 'in', 'which', 'the', 'interaction', 'was', 'reported', 'is', 'critical', 'in', 'ensuring', 'that', 'it', 'is', 'used', 'in', 'a', 'manner', 'consistent', 'with', 'the', ""model's"", 'intended', 'application.']",,,,the context,0,10,21,10,21,is,0,60,62,60,62,"['the', 'context']",['is']
215,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,3,Here we assess the applicability of two generic automated methods for leveraging a broader contextual structure in the more specific domain of a biological experiment using only the paper's title and abstract.,Here we assess the applicability of two generic automated methods for leveraging a broader contextual structure in the more specific domain of a biological experiment using only the paper's title and abstract.,,,"['Here', 'we', 'assess', 'the', 'applicability', 'of', 'two', 'generic', 'automated', 'methods', 'for', 'leveraging', 'a', 'broader', 'contextual', 'structure', 'in', 'the', 'more', 'specific', 'domain', 'of', 'a', 'biological', 'experiment', 'using', 'only', 'the', ""paper's"", 'title', 'and', 'abstract.']",,,,we,0,5,7,5,7,assess,0,8,14,8,14,['we'],['assess']
216,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,4,"In an example use case, a Support Vector Machine (SVM) and two variants of the broadly-used Bidirectional Encoder Representations from Transformers (BERT) neural network model, serve to distinguish mouse from human subject experiments in a corpus of over 12,000 papers documenting mechanistic interactions in a regulatory model of of mucosal immune signaling.","In an example use case , a Support Vector Machine ( SVM ) and two variants of",the broadly - used Bidirectional Encoder Representations from Transformers ( BERT ) neural network model,", serve to distinguish mouse from human subject experiments in a corpus of over 12,000 papers documenting mechanistic interactions in a regulatory model of of mucosal immune signaling .","['In', 'an', 'example', 'use', 'case', ',', 'a', 'Support', 'Vector', 'Machine', '(', 'SVM', ')', 'and', 'two', 'variants', 'of', 'the', 'broadly', '-', 'used', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '(', 'BERT', ')', 'neural', 'network', 'model', ',', 'serve', 'to', 'distinguish', 'mouse', 'from', 'human', 'subject', 'experiments', 'in', 'a', 'corpus', 'of', 'over', '12,000', 'papers', 'documenting', 'mechanistic', 'interactions', 'in', 'a', 'regulatory', 'model', 'of', 'of', 'mucosal', 'immune', 'signaling', '.']","(17, 32)","(77, 181)",62,an example use case a Support Vector Machine SVM variants,0,3,74,3,74,serve,2,185,190,2,7,"['an', 'example', 'use', 'case', 'a', 'Support', 'Vector', 'Machine', 'SVM', 'variants']","['serve', 'distinguish']"
217,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,5,The BERT and domain-specific BioBERT yielded essentially equivalent classification accuracy with both neural network models performing only marginally better than the SVM.,The,BERT,and domain - specific BioBERT yielded essentially equivalent classification accuracy with both neural network models performing only marginally better than the SVM .,"['The', 'BERT', 'and', 'domain', '-', 'specific', 'BioBERT', 'yielded', 'essentially', 'equivalent', 'classification', 'accuracy', 'with', 'both', 'neural', 'network', 'models', 'performing', 'only', 'marginally', 'better', 'than', 'the', 'SVM', '.']","(1, 2)","(3, 7)",-1,The BERT domain specific BioBERT,0,0,38,0,38,yielded,2,39,46,30,37,"['The', 'BERT', 'domain', 'specific', 'BioBERT']",['yielded']
218,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,6,"Words occurring frequently in abstracts were largely non-specific, whereas words unique to each class were used in 4% or less of the abstracts.","Words occurring frequently in abstracts were largely non-specific, whereas words unique to each class were used in 4% or less of the abstracts.",,,"['Words', 'occurring', 'frequently', 'in', 'abstracts', 'were', 'largely', 'non-specific,', 'whereas', 'words', 'unique', 'to', 'each', 'class', 'were', 'used', 'in', '4%', 'or', 'less', 'of', 'the', 'abstracts.']",,,,Words,0,0,5,0,5,were,0,40,44,40,44,['Words'],['were']
219,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,7,These high-specificity words were used in very similar contexts that separated mouse and human study abstracts on the basis of study design and experimental procedure rather than species or basic biological markers.,These high-specificity words were used in very similar contexts that separated mouse and human study abstracts on the basis of study design and experimental procedure rather than species or basic biological markers.,,,"['These', 'high-specificity', 'words', 'were', 'used', 'in', 'very', 'similar', 'contexts', 'that', 'separated', 'mouse', 'and', 'human', 'study', 'abstracts', 'on', 'the', 'basis', 'of', 'study', 'design', 'and', 'experimental', 'procedure', 'rather', 'than', 'species', 'or', 'basic', 'biological', 'markers.']",,,,These specificity words,0,0,30,0,30,were used,0,31,40,31,40,"['These', 'specificity', 'words']","['were', 'used']"
220,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Title,0,When the How Outweighs the What: the Pivotal Importance of Context.,When the How Outweighs the What: the Pivotal Importance of Context.,,,"['When', 'the', 'How', 'Outweighs', 'the', 'What:', 'the', 'Pivotal', 'Importance', 'of', 'Context.']",,,,the the the Pivotal Importance,0,5,56,5,56,,,,,,,"['the', 'the', 'the', 'Pivotal', 'Importance']",[]
221,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,0,"We present our approach for the identification of cited text spans in scientific literature, using pre-trained encoders (BERT) in combination with different neural networks.","We present our approach for the identification of cited text spans in scientific literature, using pre-trained encoders (BERT) in combination with different neural networks.",,,"['We', 'present', 'our', 'approach', 'for', 'the', 'identification', 'of', 'cited', 'text', 'spans', 'in', 'scientific', 'literature,', 'using', 'pre-trained', 'encoders', '(BERT)', 'in', 'combination', 'with', 'different', 'neural', 'networks.']",,,,We,0,0,2,0,2,present,0,3,10,3,10,['We'],"['present', 'using']"
222,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,1,We further experiment to assess the impact of using these cited text spans as input in BERT-based extractive summarisation methods.,We further experiment to assess the impact of using these cited text spans as input in BERT-based extractive summarisation methods.,,,"['We', 'further', 'experiment', 'to', 'assess', 'the', 'impact', 'of', 'using', 'these', 'cited', 'text', 'spans', 'as', 'input', 'in', 'BERT-based', 'extractive', 'summarisation', 'methods.']",,,,We experiment,0,0,21,0,21,,,,,,,"['We', 'experiment']",['assess']
223,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,2,"Inspired and motivated by the CL-SciSumm shared tasks, we explore different methods to adapt pre-trained models which are tuned for generic domain to scientific literature.","Inspired and motivated by the CL-SciSumm shared tasks, we explore different methods to adapt pre-trained models which are tuned for generic domain to scientific literature.",,,"['Inspired', 'and', 'motivated', 'by', 'the', 'CL-SciSumm', 'shared', 'tasks,', 'we', 'explore', 'different', 'methods', 'to', 'adapt', 'pre-trained', 'models', 'which', 'are', 'tuned', 'for', 'generic', 'domain', 'to', 'scientific', 'literature.']",,,,we,0,58,60,58,60,explore,0,61,68,61,68,"['the', 'CL', 'SciSumm', 'we']","['Inspired', 'motivated', 'shared', 'explore']"
224,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,3,"For the identification of cited text spans, we assess the impact of different configurations in terms of learning from augmented data and using different features and network architectures (BERT, XLNET, CNN, and BiMPM) for training.","For the identification of cited text spans, we assess the impact of different configurations in terms of learning from augmented data and using different features and network architectures (BERT, XLNET, CNN, and BiMPM) for training.",,,"['For', 'the', 'identification', 'of', 'cited', 'text', 'spans,', 'we', 'assess', 'the', 'impact', 'of', 'different', 'configurations', 'in', 'terms', 'of', 'learning', 'from', 'augmented', 'data', 'and', 'using', 'different', 'features', 'and', 'network', 'architectures', '(BERT,', 'XLNET,', 'CNN,', 'and', 'BiMPM)', 'for', 'training.']",,,,we,0,45,47,45,47,assess and using,0,48,144,48,144,['we'],"['assess', 'using']"
225,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,4,We show that identifying and fine-tuning the language models on unlabelled or augmented domain specific data can improve the performance of cited text span identification models.,We show that identifying and fine-tuning the language models on unlabelled or augmented domain specific data can improve the performance of cited text span identification models.,,,"['We', 'show', 'that', 'identifying', 'and', 'fine-tuning', 'the', 'language', 'models', 'on', 'unlabelled', 'or', 'augmented', 'domain', 'specific', 'data', 'can', 'improve', 'the', 'performance', 'of', 'cited', 'text', 'span', 'identification', 'models.']",,,,We,0,0,2,0,2,show,0,3,7,3,7,['We'],['show']
226,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,5,For the scientific summarisation we implement an extractive summarisation model adapted from BERT.,For the scientific summarisation we implement an extractive summarisation model adapted from BERT.,,,"['For', 'the', 'scientific', 'summarisation', 'we', 'implement', 'an', 'extractive', 'summarisation', 'model', 'adapted', 'from', 'BERT.']",,,,we,0,33,35,33,35,implement,0,36,45,36,45,['we'],['implement']
227,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,6,"With respect to the input sentences taken from the cited paper, we explore two different scenarios: (1) consider all the sentences (full-text) of the referenced article as input and (2) consider only the text spans that have been identified to be cited by other publications.","With respect to the input sentences taken from the cited paper, we explore two different scenarios: (1) consider all the sentences (full-text) of the referenced article as input and (2) consider only the text spans that have been identified to be cited by other publications.",,,"['With', 'respect', 'to', 'the', 'input', 'sentences', 'taken', 'from', 'the', 'cited', 'paper,', 'we', 'explore', 'two', 'different', 'scenarios:', '(1)', 'consider', 'all', 'the', 'sentences', '(full-text)', 'of', 'the', 'referenced', 'article', 'as', 'input', 'and', '(2)', 'consider', 'only', 'the', 'text', 'spans', 'that', 'have', 'been', 'identified', 'to', 'be', 'cited', 'by', 'other', 'publications.']",,,,we,0,65,67,65,67,explore,0,68,75,68,75,['we'],['explore']
228,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,7,"We observe that in certain experiments, by using only the cited text-spans we can achieve better performance, while minimising the input size needed.","We observe that in certain experiments, by using only the cited text-spans we can achieve better performance, while minimising the input size needed.",,,"['We', 'observe', 'that', 'in', 'certain', 'experiments,', 'by', 'using', 'only', 'the', 'cited', 'text-spans', 'we', 'can', 'achieve', 'better', 'performance,', 'while', 'minimising', 'the', 'input', 'size', 'needed.']",,,,We,0,0,2,0,2,observe,0,3,10,3,10,['We'],['observe']
229,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Title,0,Cited text span identification for scientific summarisation using pre-trained encoders.,Cited text span identification for scientific summarisation using pre-trained encoders.,,,"['Cited', 'text', 'span', 'identification', 'for', 'scientific', 'summarisation', 'using', 'pre-trained', 'encoders.']",,,,text span identification,0,6,30,6,30,using,0,60,65,60,65,"['text', 'span', 'identification']",['using']
230,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,0,The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters.,The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters.,,,"['The', 'current', 'modus', 'operandi', 'in', 'NLP', 'involves', 'downloading', 'and', 'fine-tuning', 'pre-trained', 'models', 'consisting', 'of', 'millions', 'or', 'billions', 'of', 'parameters.']",,,,The current,0,0,11,0,11,involves,0,34,42,34,42,"['The', 'current']",['involves']
231,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,1,"Storing and sharing such large trained models is expensive, slow, and timeconsuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks.","Storing and sharing such large trained models is expensive, slow, and timeconsuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks.",,,"['Storing', 'and', 'sharing', 'such', 'large', 'trained', 'models', 'is', 'expensive,', 'slow,', 'and', 'timeconsuming,', 'which', 'impedes', 'progress', 'towards', 'more', 'general', 'and', 'versatile', 'NLP', 'methods', 'that', 'learn', 'from', 'and', 'for', 'many', 'tasks.']",,,,,,,,,,is,0,46,48,46,48,[],"['Storing', 'sharing', 'is']"
232,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,2,Adapters small learnt bottleneck layers inserted within each layer of a pre-trained model ameliorate this issue by avoiding full fine-tuning of the entire model.,Adapters small learnt bottleneck layers inserted within each layer of a pre-trained model ameliorate this issue by avoiding full fine-tuning of the entire model.,,,"['Adapters', 'small', 'learnt', 'bottleneck', 'layers', 'inserted', 'within', 'each', 'layer', 'of', 'a', 'pre-trained', 'model', 'ameliorate', 'this', 'issue', 'by', 'avoiding', 'full', 'fine-tuning', 'of', 'the', 'entire', 'model.']",,,,Adapters bottleneck layers,0,0,41,0,41,ameliorate,0,96,106,96,106,"['Adapters', 'bottleneck', 'layers']",['ameliorate']
233,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,3,"However, sharing and integrating adapter layers is not straightforward.","However, sharing and integrating adapter layers is not straightforward.",,,"['However,', 'sharing', 'and', 'integrating', 'adapter', 'layers', 'is', 'not', 'straightforward.']",,,,,,,,,,is,0,49,51,49,51,[],"['sharing', 'integrating', 'is']"
234,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,4,"We propose AdapterHub, a framework that allows dynamic stiching-in of pre-trained adapters for different tasks and languages.",We propose,"AdapterHub , a framework that allows dynamic  stiching - in  of pre - trained adapters for different tasks and languages",.,"['We', 'propose', 'AdapterHub', ',', 'a', 'framework', 'that', 'allows', 'dynamic', '', 'stiching', '-', 'in', '', 'of', 'pre', '-', 'trained', 'adapters', 'for', 'different', 'tasks', 'and', 'languages', '.']","(2, 24)","(10, 132)",-1,We,0,0,2,0,2,propose,0,3,10,3,10,['We'],['propose']
235,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,5,"The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pretrained models (e.g., BERT, RoBERTa, XLMR) across tasks and languages.",,"The framework , built on top of the popular HuggingFace Transformers library ,","enables extremely easy and quick adaptations of state - of - the - art pretrained models ( e.g. , BERT , RoBERTa , XLMR ) across tasks and languages .","['The', 'framework', ',', 'built', 'on', 'top', 'of', 'the', 'popular', 'HuggingFace', 'Transformers', 'library', ',', 'enables', 'extremely', 'easy', 'and', 'quick', 'adaptations', 'of', 'state', '-', 'of', '-', 'the', '-', 'art', 'pretrained', 'models', '(', 'e.g.', ',', 'BERT', ',', 'RoBERTa', ',', 'XLMR', ')', 'across', 'tasks', 'and', 'languages', '.']","(0, 13)","(0, 78)",56,The framework,1,0,13,0,13,enables,2,79,86,0,7,"['The', 'framework']",['enables']
236,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,6,"Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure.","Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure.",,,"['Downloading,', 'sharing,', 'and', 'training', 'adapters', 'is', 'as', 'seamless', 'as', 'possible', 'using', 'minimal', 'changes', 'to', 'the', 'training', 'scripts', 'and', 'a', 'specialized', 'infrastructure.']",,,,sharing training adapters,0,14,45,14,45,is,0,46,48,46,48,"['sharing', 'training', 'adapters']",['is']
237,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,7,"Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios.",,Our framework,"enables scalable and easy access to sharing of task - specific models , particularly in low - resource scenarios .","['Our', 'framework', 'enables', 'scalable', 'and', 'easy', 'access', 'to', 'sharing', 'of', 'task', '-', 'specific', 'models', ',', 'particularly', 'in', 'low', '-', 'resource', 'scenarios', '.']","(0, 2)","(0, 13)",-1,framework,1,4,13,4,13,enables,2,14,21,0,7,['framework'],['enables']
238,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,8,AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml.,,AdapterHub,includes all recent adapter architectures and can be found at AdapterHub.ml .,"['AdapterHub', 'includes', 'all', 'recent', 'adapter', 'architectures', 'and', 'can', 'be', 'found', 'at', 'AdapterHub.ml', '.']","(0, 1)","(0, 10)",-1,AdapterHub,1,0,10,0,10,includes and can be found,2,11,69,0,58,['AdapterHub'],"['includes', 'can', 'be', 'found']"
239,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Title,0,AdapterHub: A Framework for Adapting Transformers.,AdapterHub : A Framework for Adapting,Transformers,.,"['AdapterHub', ':', 'A', 'Framework', 'for', 'Adapting', 'Transformers', '.']","(6, 7)","(37, 49)",0,AdapterHub A Framework,0,0,24,0,24,,,,,,,"['AdapterHub', 'A', 'Framework']",[]
240,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,0,Anonymization of clinical data is a crucial prerequisite for its many uses for both scientific analysis and extrapolation using modern research methods such as deep learning.,Anonymization of clinical data is a crucial prerequisite for its many uses for both scientific analysis and extrapolation using modern research methods such as deep learning.,,,"['Anonymization', 'of', 'clinical', 'data', 'is', 'a', 'crucial', 'prerequisite', 'for', 'its', 'many', 'uses', 'for', 'both', 'scientific', 'analysis', 'and', 'extrapolation', 'using', 'modern', 'research', 'methods', 'such', 'as', 'deep', 'learning.']",,,,Anonymization,0,0,13,0,13,is,0,31,33,31,33,['Anonymization'],['is']
241,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,1,"State-of-the-art Natural Language Processing (NLP) techniques have been found useful for anonymization of patient notes prescribed by doctors, which aims to remove Personal Health Identifiers (PHIs) from medical datasets.","State-of-the-art Natural Language Processing (NLP) techniques have been found useful for anonymization of patient notes prescribed by doctors, which aims to remove Personal Health Identifiers (PHIs) from medical datasets.",,,"['State-of-the-art', 'Natural', 'Language', 'Processing', '(NLP)', 'techniques', 'have', 'been', 'found', 'useful', 'for', 'anonymization', 'of', 'patient', 'notes', 'prescribed', 'by', 'doctors,', 'which', 'aims', 'to', 'remove', 'Personal', 'Health', 'Identifiers', '(PHIs)', 'from', 'medical', 'datasets.']",,,,State the art Natural Language Processing NLP techniques,0,0,69,0,69,have been found,0,70,85,70,85,"['State', 'the', 'art', 'Natural', 'Language', 'Processing', 'NLP', 'techniques']","['have', 'been', 'found']"
242,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,2,Our goal is to identify and extract named entities present as PHIs within discharge summaries with NLP-based techniques.,Our goal is to identify and extract named entities present as PHIs within discharge summaries with NLP-based techniques.,,,"['Our', 'goal', 'is', 'to', 'identify', 'and', 'extract', 'named', 'entities', 'present', 'as', 'PHIs', 'within', 'discharge', 'summaries', 'with', 'NLP-based', 'techniques.']",,,,goal,0,4,8,4,8,is,0,9,11,9,11,['goal'],"['is', 'identify', 'extract']"
243,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,3,"We experiment with an existing Artificial Neural Network (ANN) model, for which we implement a random search to tune hyperparameters.","We experiment with an existing Artificial Neural Network (ANN) model, for which we implement a random search to tune hyperparameters.",,,"['We', 'experiment', 'with', 'an', 'existing', 'Artificial', 'Neural', 'Network', '(ANN)', 'model,', 'for', 'which', 'we', 'implement', 'a', 'random', 'search', 'to', 'tune', 'hyperparameters.']",,,,We,0,0,2,0,2,experiment,0,3,13,3,13,['We'],['experiment']
244,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,4,We find that models with higher learning rates yield better results on average and that increasing dropout rate increases accuracy as well.,We find that models with higher learning rates yield better results on average and that increasing dropout rate increases accuracy as well.,,,"['We', 'find', 'that', 'models', 'with', 'higher', 'learning', 'rates', 'yield', 'better', 'results', 'on', 'average', 'and', 'that', 'increasing', 'dropout', 'rate', 'increases', 'accuracy', 'as', 'well.']",,,,We,0,0,2,0,2,find,0,3,7,3,7,['We'],['find']
245,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,5,"Increasing number of character embedding dimensions for the bi-directional LSTM increased the accuracy as well, albeit insignificantly.","Increasing number of character embedding dimensions for the bi-directional LSTM increased the accuracy as well, albeit insignificantly.",,,"['Increasing', 'number', 'of', 'character', 'embedding', 'dimensions', 'for', 'the', 'bi-directional', 'LSTM', 'increased', 'the', 'accuracy', 'as', 'well,', 'albeit', 'insignificantly.']",,,,number,0,11,17,11,17,increased,0,82,91,82,91,['number'],['increased']
246,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,6,"Notably, we implement a Transformer model and train on a large corpus of patient notes, for which we modify an existing model built on top of HuggingFaces transformers package to adapt it to a medical context.","Notably , we implement a Transformer model and train on a large corpus of patient notes , for which we modify an existing model built on top of HuggingFace  s",transformers,package to adapt it to a medical context .,"['Notably', ',', 'we', 'implement', 'a', 'Transformer', 'model', 'and', 'train', 'on', 'a', 'large', 'corpus', 'of', 'patient', 'notes', ',', 'for', 'which', 'we', 'modify', 'an', 'existing', 'model', 'built', 'on', 'top', 'of', 'HuggingFace', '', 's', 'transformers', 'package', 'to', 'adapt', 'it', 'to', 'a', 'medical', 'context', '.']","(31, 32)","(159, 171)",0,we,0,10,12,10,12,implement and,0,13,46,13,46,['we'],['implement']
247,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,6,"Notably, we implement a Transformer model and train on a large corpus of patient notes, for which we modify an existing model built on top of HuggingFaces transformers package to adapt it to a medical context.","Notably , we implement a",Transformer,"model and train on a large corpus of patient notes , for which we modify an existing model built on top of HuggingFace  s transformers package to adapt it to a medical context .","['Notably', ',', 'we', 'implement', 'a', 'Transformer', 'model', 'and', 'train', 'on', 'a', 'large', 'corpus', 'of', 'patient', 'notes', ',', 'for', 'which', 'we', 'modify', 'an', 'existing', 'model', 'built', 'on', 'top', 'of', 'HuggingFace', '', 's', 'transformers', 'package', 'to', 'adapt', 'it', 'to', 'a', 'medical', 'context', '.']","(5, 6)","(24, 35)",0,we,0,10,12,10,12,implement and,0,13,46,13,46,['we'],['implement']
248,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,7,"With just a few training iterations, we are able to achieve near state-of-theart results.","With just a few training iterations, we are able to achieve near state-of-theart results.",,,"['With', 'just', 'a', 'few', 'training', 'iterations,', 'we', 'are', 'able', 'to', 'achieve', 'near', 'state-of-theart', 'results.']",,,,we,0,38,40,38,40,are,0,41,44,41,44,['we'],['are']
249,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,8,"Though there is precedent for disease predictions from EHR data, such utilization of a Transformer for de-identification of personal health indicators, with near-state-of-the-art accuracy results, is, to the best of our knowledge, the first of its kind.","Though there is precedent for disease predictions from EHR data ,","such utilization of a Transformer for de - identification of personal health indicators , with near - state - of - the - art accuracy results",", is , to the best of our knowledge , the first of its kind .","['Though', 'there', 'is', 'precedent', 'for', 'disease', 'predictions', 'from', 'EHR', 'data', ',', 'such', 'utilization', 'of', 'a', 'Transformer', 'for', 'de', '-', 'identification', 'of', 'personal', 'health', 'indicators', ',', 'with', 'near', '-', 'state', '-', 'of', '-', 'the', '-', 'art', 'accuracy', 'results', ',', 'is', ',', 'to', 'the', 'best', 'of', 'our', 'knowledge', ',', 'the', 'first', 'of', 'its', 'kind', '.']","(11, 37)","(65, 206)",22,such utilization a Transformer de - identification,1,66,123,0,57,is,2,210,212,2,4,"['such', 'utilization', 'a', 'Transformer', 'de', '-', 'identification']",['is']
250,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,8,"Though there is precedent for disease predictions from EHR data, such utilization of a Transformer for de-identification of personal health indicators, with near-state-of-the-art accuracy results, is, to the best of our knowledge, the first of its kind.","Though there is precedent for disease predictions from EHR data , such utilization of a Transformer for de - identification of personal health indicators , with near - state - of - the - art accuracy results , is , to the best of our knowledge , the first of",its,kind .,"['Though', 'there', 'is', 'precedent', 'for', 'disease', 'predictions', 'from', 'EHR', 'data', ',', 'such', 'utilization', 'of', 'a', 'Transformer', 'for', 'de', '-', 'identification', 'of', 'personal', 'health', 'indicators', ',', 'with', 'near', '-', 'state', '-', 'of', '-', 'the', '-', 'art', 'accuracy', 'results', ',', 'is', ',', 'to', 'the', 'best', 'of', 'our', 'knowledge', ',', 'the', 'first', 'of', 'its', 'kind', '.']","(50, 51)","(258, 261)",-1,such utilization a Transformer de - identification,0,66,123,66,123,is,0,210,212,210,212,"['such', 'utilization', 'a', 'Transformer', 'de', '-', 'identification']",['is']
251,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Title,0,Simple Transformers for PHI De-identification.,Simple,Transformers,for PHI De-identification .,"['Simple', 'Transformers', 'for', 'PHI', 'De-identification', '.']","(1, 2)","(6, 18)",0,Simple Transformers PHI De - identification,0,0,47,0,47,,,,,,,"['Simple', 'Transformers', 'PHI', 'De', '-', 'identification']",[]
252,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,0,"Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length.",,Transformers,"are powerful sequence models , but require time and memory that grows quadratically with the sequence length .","['Transformers', 'are', 'powerful', 'sequence', 'models', ',', 'but', 'require', 'time', 'and', 'memory', 'that', 'grows', 'quadratically', 'with', 'the', 'sequence', 'length', '.']","(0, 1)","(0, 12)",0,Transformers,1,0,12,0,12,are but require,2,13,55,0,42,['Transformers'],"['are', 'require']"
253,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,1,In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$.,In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$.,,,"['In', 'this', 'paper', 'we', 'introduce', 'sparse', 'factorizations', 'of', 'the', 'attention', 'matrix', 'which', 'reduce', 'this', 'to', '$O(n', '\\sqrt{n})$.']",,,,we,0,14,16,14,16,introduce,0,17,26,17,26,['we'],['introduce']
254,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,2,"We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training.","We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training.",,,"['We', 'also', 'introduce', 'a)', 'a', 'variation', 'on', 'architecture', 'and', 'initialization', 'to', 'train', 'deeper', 'networks,', 'b)', 'the', 'recomputation', 'of', 'attention', 'matrices', 'to', 'save', 'memory,', 'and', 'c)', 'fast', 'attention', 'kernels', 'for', 'training.']",,,,We,0,0,2,0,2,introduce,0,8,17,8,17,['We'],['introduce']
255,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,3,"We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers.",We call networks with these changes Sparse,Transformers,", and show they can model sequences tens of thousands of timesteps long using hundreds of layers .","['We', 'call', 'networks', 'with', 'these', 'changes', 'Sparse', 'Transformers', ',', 'and', 'show', 'they', 'can', 'model', 'sequences', 'tens', 'of', 'thousands', 'of', 'timesteps', 'long', 'using', 'hundreds', 'of', 'layers', '.']","(7, 8)","(42, 54)",0,We,0,0,2,0,2,call and show,0,3,66,3,66,['We'],"['call', 'show']"
256,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,4,"We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64.","We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64.",,,"['We', 'use', 'the', 'same', 'architecture', 'to', 'model', 'images,', 'audio,', 'and', 'text', 'from', 'raw', 'bytes,', 'setting', 'a', 'new', 'state', 'of', 'the', 'art', 'for', 'density', 'modeling', 'of', 'Enwik8,', 'CIFAR-10,', 'and', 'ImageNet-64.']",,,,We,0,0,2,0,2,use,0,3,6,3,6,['We'],"['use', 'model', 'setting']"
257,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,5,"We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.","We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",,,"['We', 'generate', 'unconditional', 'samples', 'that', 'demonstrate', 'global', 'coherence', 'and', 'great', 'diversity,', 'and', 'show', 'it', 'is', 'possible', 'in', 'principle', 'to', 'use', 'self-attention', 'to', 'model', 'sequences', 'of', 'length', 'one', 'million', 'or', 'more.']",,,,We,0,0,2,0,2,generate and show,0,3,98,3,98,['We'],"['generate', 'show']"
258,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Title,0,Generating Long Sequences with Sparse Transformers.,Generating Long Sequences with Sparse,Transformers,.,"['Generating', 'Long', 'Sequences', 'with', 'Sparse', 'Transformers', '.']","(5, 6)","(37, 49)",0,,,,,,,Generating,0,0,10,0,10,[],['Generating']
259,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,0,"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.","Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.",,,"['Recurrent', 'neural', 'networks', '(RNNs)', 'sequentially', 'process', 'data', 'by', 'updating', 'their', 'state', 'with', 'each', 'new', 'data', 'point,', 'and', 'have', 'long', 'been', 'the', 'de', 'facto', 'choice', 'for', 'sequence', 'modeling', 'tasks.']",,,,Recurrent neural networks RNNs,0,0,32,0,32,process and have been,0,48,130,48,130,"['Recurrent', 'neural', 'networks', 'RNNs']","['process', 'have', 'been']"
260,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,1,"However, their inherently sequential computation makes them slow to train.","However, their inherently sequential computation makes them slow to train.",,,"['However,', 'their', 'inherently', 'sequential', 'computation', 'makes', 'them', 'slow', 'to', 'train.']",,,,computation,0,38,49,38,49,makes,0,50,55,50,55,['computation'],['makes']
261,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,2,"Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.","Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.",,,"['Feed-forward', 'and', 'convolutional', 'architectures', 'have', 'recently', 'been', 'shown', 'to', 'achieve', 'superior', 'results', 'on', 'some', 'sequence', 'modeling', 'tasks', 'such', 'as', 'machine', 'translation,', 'with', 'the', 'added', 'advantage', 'that', 'they', 'concurrently', 'process', 'all', 'inputs', 'in', 'the', 'sequence,', 'leading', 'to', 'easy', 'parallelization', 'and', 'faster', 'training', 'times.']",,,,Feed architectures,0,0,46,0,46,have been shown,0,47,71,47,71,"['Feed', 'architectures']","['have', 'been', 'shown', 'achieve']"
262,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,3,"Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g.","Despite these successes , however , popular feed - forward sequence models like",the Transformer,"fail to generalize in many simple tasks that recurrent models handle with ease , e.g.","['Despite', 'these', 'successes', ',', 'however', ',', 'popular', 'feed', '-', 'forward', 'sequence', 'models', 'like', 'the', 'Transformer', 'fail', 'to', 'generalize', 'in', 'many', 'simple', 'tasks', 'that', 'recurrent', 'models', 'handle', 'with', 'ease', ',', 'e.g.']","(13, 15)","(79, 94)",4,popular feed forward sequence models,0,36,74,36,74,fail,2,96,100,0,4,"['popular', 'feed', 'forward', 'sequence', 'models']","['fail', 'generalize']"
263,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,4,copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.,copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.,,,"['copying', 'strings', 'or', 'even', 'simple', 'logical', 'inference', 'when', 'the', 'string', 'or', 'formula', 'lengths', 'exceed', 'those', 'observed', 'at', 'training', 'time.']",,,,strings simple logical inference,0,8,48,8,48,copying,0,0,7,0,7,"['strings', 'simple', 'logical', 'inference']",['copying']
264,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,5,"We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.",We propose the Universal,Transformer,"( UT ) , a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues .","['We', 'propose', 'the', 'Universal', 'Transformer', '(', 'UT', ')', ',', 'a', 'parallel-in-time', 'self-attentive', 'recurrent', 'sequence', 'model', 'which', 'can', 'be', 'cast', 'as', 'a', 'generalization', 'of', 'the', 'Transformer', 'model', 'and', 'which', 'addresses', 'these', 'issues', '.']","(4, 5)","(24, 35)",0,We,0,0,2,0,2,propose,0,3,10,3,10,['We'],['propose']
265,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,5,"We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.","We propose the Universal Transformer ( UT ) , a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the",Transformer,model and which addresses these issues .,"['We', 'propose', 'the', 'Universal', 'Transformer', '(', 'UT', ')', ',', 'a', 'parallel-in-time', 'self-attentive', 'recurrent', 'sequence', 'model', 'which', 'can', 'be', 'cast', 'as', 'a', 'generalization', 'of', 'the', 'Transformer', 'model', 'and', 'which', 'addresses', 'these', 'issues', '.']","(24, 25)","(149, 160)",0,We,0,0,2,0,2,propose,0,3,10,3,10,['We'],['propose']
266,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,6,UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs.,UTs combine the parallelizability and global receptive field of feed - forward sequence models like,the Transformer,with the recurrent inductive bias of RNNs .,"['UTs', 'combine', 'the', 'parallelizability', 'and', 'global', 'receptive', 'field', 'of', 'feed', '-', 'forward', 'sequence', 'models', 'like', 'the', 'Transformer', 'with', 'the', 'recurrent', 'inductive', 'bias', 'of', 'RNNs', '.']","(15, 17)","(99, 114)",4,UTs,0,0,3,0,3,combine,0,4,11,4,11,['UTs'],['combine']
267,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,7,We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.,We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.,,,"['We', 'also', 'add', 'a', 'dynamic', 'per-position', 'halting', 'mechanism', 'and', 'find', 'that', 'it', 'improves', 'accuracy', 'on', 'several', 'tasks.']",,,,We,0,0,2,0,2,add and find,0,8,63,8,63,['We'],"['add', 'find']"
268,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,8,"In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete.",In contrast to,the standard Transformer,", under certain assumptions , UTs can be shown to be Turing - complete .","['In', 'contrast', 'to', 'the', 'standard', 'Transformer', ',', 'under', 'certain', 'assumptions', ',', 'UTs', 'can', 'be', 'shown', 'to', 'be', 'Turing', '-', 'complete', '.']","(3, 6)","(14, 38)",13,UTs,2,70,73,30,33,can be shown,2,74,86,34,46,['UTs'],"['can', 'be', 'shown', 'be']"
269,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,9,"Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",Our experiments show that UTs outperform standard,Transformers,"on a wide range of algorithmic and language understanding tasks , including the challenging LAMBADA language modeling task where UTs achieve a new state of the art , and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset .","['Our', 'experiments', 'show', 'that', 'UTs', 'outperform', 'standard', 'Transformers', 'on', 'a', 'wide', 'range', 'of', 'algorithmic', 'and', 'language', 'understanding', 'tasks', ',', 'including', 'the', 'challenging', 'LAMBADA', 'language', 'modeling', 'task', 'where', 'UTs', 'achieve', 'a', 'new', 'state', 'of', 'the', 'art', ',', 'and', 'machine', 'translation', 'where', 'UTs', 'achieve', 'a', '0.9', 'BLEU', 'improvement', 'over', 'Transformers', 'on', 'the', 'WMT14', 'En-De', 'dataset', '.']","(7, 8)","(49, 61)",0,experiments,0,4,15,4,15,show,0,16,20,16,20,['experiments'],['show']
270,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,9,"Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.","Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks , including the challenging LAMBADA language modeling task where UTs achieve a new state of the art , and machine translation where UTs achieve a 0.9 BLEU improvement over",Transformers,on the WMT14 En-De dataset .,"['Our', 'experiments', 'show', 'that', 'UTs', 'outperform', 'standard', 'Transformers', 'on', 'a', 'wide', 'range', 'of', 'algorithmic', 'and', 'language', 'understanding', 'tasks', ',', 'including', 'the', 'challenging', 'LAMBADA', 'language', 'modeling', 'task', 'where', 'UTs', 'achieve', 'a', 'new', 'state', 'of', 'the', 'art', ',', 'and', 'machine', 'translation', 'where', 'UTs', 'achieve', 'a', '0.9', 'BLEU', 'improvement', 'over', 'Transformers', 'on', 'the', 'WMT14', 'En-De', 'dataset', '.']","(47, 48)","(298, 310)",0,experiments,0,4,15,4,15,show,0,16,20,16,20,['experiments'],['show']
271,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Title,0,Universal Transformers.,Universal,Transformers,.,"['Universal', 'Transformers', '.']","(1, 2)","(9, 21)",0,Universal Transformers,0,0,22,0,22,,,,,,,"['Universal', 'Transformers']",[]
272,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,0,The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition.,The recent success of,transformer networks,for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition .,"['The', 'recent', 'success', 'of', 'transformer', 'networks', 'for', 'neural', 'machine', 'translation', 'and', 'other', 'NLP', 'tasks', 'has', 'led', 'to', 'a', 'surge', 'in', 'research', 'work', 'trying', 'to', 'apply', 'it', 'for', 'speech', 'recognition', '.']","(4, 6)","(21, 41)",0,The recent success,0,0,18,0,18,has led,2,94,101,51,58,"['The', 'recent', 'success']","['has', 'led']"
273,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,0,The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition.,The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply,it,for speech recognition .,"['The', 'recent', 'success', 'of', 'transformer', 'networks', 'for', 'neural', 'machine', 'translation', 'and', 'other', 'NLP', 'tasks', 'has', 'led', 'to', 'a', 'surge', 'in', 'research', 'work', 'trying', 'to', 'apply', 'it', 'for', 'speech', 'recognition', '.']","(25, 26)","(145, 147)",-1,The recent success,0,0,18,0,18,has led,0,94,101,94,101,"['The', 'recent', 'success']","['has', 'led']"
274,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,1,"Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks.","Recent efforts studied key research questions around ways of combining positional embedding with speech features , and stability of optimization for large scale learning of",transformer,networks .,"['Recent', 'efforts', 'studied', 'key', 'research', 'questions', 'around', 'ways', 'of', 'combining', 'positional', 'embedding', 'with', 'speech', 'features', ',', 'and', 'stability', 'of', 'optimization', 'for', 'large', 'scale', 'learning', 'of', 'transformer', 'networks', '.']","(25, 26)","(172, 183)",0,Recent efforts,0,0,14,0,14,studied,0,15,22,15,22,"['Recent', 'efforts']",['studied']
275,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,2,"In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations.","In this paper , we propose replacing the sinusoidal positional embedding for",transformers,with convolutionally learned input representations .,"['In', 'this', 'paper', ',', 'we', 'propose', 'replacing', 'the', 'sinusoidal', 'positional', 'embedding', 'for', 'transformers', 'with', 'convolutionally', 'learned', 'input', 'representations', '.']","(12, 13)","(76, 88)",0,we,0,16,18,16,18,propose,0,19,26,19,26,['we'],"['propose', 'replacing']"
276,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,3,These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts.,These contextual representations provide subsequent,transformer,blocks with relative positional information needed for discovering long-range relationships between local concepts .,"['These', 'contextual', 'representations', 'provide', 'subsequent', 'transformer', 'blocks', 'with', 'relative', 'positional', 'information', 'needed', 'for', 'discovering', 'long-range', 'relationships', 'between', 'local', 'concepts', '.']","(5, 6)","(51, 62)",0,These contextual representations,0,0,32,0,32,provide,0,33,40,33,40,"['These', 'contextual', 'representations']",['provide']
277,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,4,The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps.,The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps.,,,"['The', 'proposed', 'system', 'has', 'favorable', 'optimization', 'characteristics', 'where', 'our', 'reported', 'results', 'are', 'produced', 'with', 'fixed', 'learning', 'rate', 'of', '1.0', 'and', 'no', 'warmup', 'steps.']",,,,The system,0,0,19,0,19,has,0,20,23,20,23,"['The', 'system']",['has']
278,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,5,The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.,The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.,,,"['The', 'proposed', 'model', 'achieves', 'a', 'competitive', '4.7%', 'and', '12.9%', 'WER', 'on', 'the', 'Librispeech', '``test', ""clean''"", 'and', '``test', ""other''"", 'subsets', 'when', 'no', 'extra', 'LM', 'text', 'is', 'provided.']",,,,The model,0,0,18,0,18,achieves,0,19,27,19,27,"['The', 'model']",['achieves']
279,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Title,0,Transformers with convolutional context for ASR.,,Transformers,with convolutional context for ASR .,"['Transformers', 'with', 'convolutional', 'context', 'for', 'ASR', '.']","(0, 1)","(0, 12)",0,Transformers,1,0,12,0,12,,,,,,,['Transformers'],[]
280,https://www.semanticscholar.org/paper/Adaptive-Attention-Span-in-Transformers-Sukhbaatar-Grave/f4238bd2385a52413ccbacfd9e409a650235bd13,32,Abstract,0,We propose a novel self-attention mechanism that can learn its optimal attention span.,We propose a novel self-attention mechanism that can learn its optimal attention span.,,,"['We', 'propose', 'a', 'novel', 'self-attention', 'mechanism', 'that', 'can', 'learn', 'its', 'optimal', 'attention', 'span.']",,,,We,0,0,2,0,2,propose,0,3,10,3,10,['We'],['propose']
281,https://www.semanticscholar.org/paper/Adaptive-Attention-Span-in-Transformers-Sukhbaatar-Grave/f4238bd2385a52413ccbacfd9e409a650235bd13,32,Abstract,1,"This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time.",This allows us to extend significantly the maximum context size used in,Transformer,", while maintaining control over their memory footprint and computational time .","['This', 'allows', 'us', 'to', 'extend', 'significantly', 'the', 'maximum', 'context', 'size', 'used', 'in', 'Transformer', ',', 'while', 'maintaining', 'control', 'over', 'their', 'memory', 'footprint', 'and', 'computational', 'time', '.']","(12, 13)","(71, 82)",0,This,0,0,4,0,4,allows,0,5,11,5,11,['This'],"['allows', 'extend']"
282,https://www.semanticscholar.org/paper/Adaptive-Attention-Span-in-Transformers-Sukhbaatar-Grave/f4238bd2385a52413ccbacfd9e409a650235bd13,32,Abstract,1,"This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time.","This allows us to extend significantly the maximum context size used in Transformer , while maintaining control over",their,memory footprint and computational time .,"['This', 'allows', 'us', 'to', 'extend', 'significantly', 'the', 'maximum', 'context', 'size', 'used', 'in', 'Transformer', ',', 'while', 'maintaining', 'control', 'over', 'their', 'memory', 'footprint', 'and', 'computational', 'time', '.']","(18, 19)","(116, 121)",-1,This,0,0,4,0,4,allows,0,5,11,5,11,['This'],"['allows', 'extend']"
283,https://www.semanticscholar.org/paper/Adaptive-Attention-Span-in-Transformers-Sukhbaatar-Grave/f4238bd2385a52413ccbacfd9e409a650235bd13,32,Abstract,2,"We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.","We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",,,"['We', 'show', 'the', 'effectiveness', 'of', 'our', 'approach', 'on', 'the', 'task', 'of', 'character', 'level', 'language', 'modeling,', 'where', 'we', 'achieve', 'state-of-the-art', 'performances', 'on', 'text8', 'and', 'enwiki8', 'by', 'using', 'a', 'maximum', 'context', 'of', '8k', 'characters.']",,,,We,0,0,2,0,2,show,0,3,7,3,7,['We'],['show']
284,https://www.semanticscholar.org/paper/Adaptive-Attention-Span-in-Transformers-Sukhbaatar-Grave/f4238bd2385a52413ccbacfd9e409a650235bd13,32,Title,0,Adaptive Attention Span in Transformers.,Adaptive Attention Span in,Transformers,.,"['Adaptive', 'Attention', 'Span', 'in', 'Transformers', '.']","(4, 5)","(26, 38)",0,Adaptive Attention Span,0,0,23,0,23,,,,,,,"['Adaptive', 'Attention', 'Span']",[]
285,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,0,We learn models to generate the immediate future in video.,We learn models to generate the immediate future in video.,,,"['We', 'learn', 'models', 'to', 'generate', 'the', 'immediate', 'future', 'in', 'video.']",,,,We,0,0,2,0,2,learn,0,3,8,3,8,['We'],['learn']
286,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,1,This problem has two main challenges.,This problem has two main challenges.,,,"['This', 'problem', 'has', 'two', 'main', 'challenges.']",,,,This problem,0,0,12,0,12,has,0,13,16,13,16,"['This', 'problem']",['has']
287,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,2,"Firstly, since the future is uncertain, models should be multi-modal, which can be difficult to learn.","Firstly, since the future is uncertain, models should be multi-modal, which can be difficult to learn.",,,"['Firstly,', 'since', 'the', 'future', 'is', 'uncertain,', 'models', 'should', 'be', 'multi-modal,', 'which', 'can', 'be', 'difficult', 'to', 'learn.']",,,,models,0,42,48,42,48,should be,0,49,58,49,58,['models'],"['should', 'be']"
288,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,3,"Secondly, since the future is similar to the past, models store low-level details, which complicates learning of high-level semantics.","Secondly, since the future is similar to the past, models store low-level details, which complicates learning of high-level semantics.",,,"['Secondly,', 'since', 'the', 'future', 'is', 'similar', 'to', 'the', 'past,', 'models', 'store', 'low-level', 'details,', 'which', 'complicates', 'learning', 'of', 'high-level', 'semantics.']",,,,models,0,53,59,53,59,store,0,60,65,60,65,['models'],['store']
289,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,4,We propose a framework to tackle both of these challenges.,We propose a framework to tackle both of these challenges.,,,"['We', 'propose', 'a', 'framework', 'to', 'tackle', 'both', 'of', 'these', 'challenges.']",,,,We,0,0,2,0,2,propose,0,3,10,3,10,['We'],['propose']
290,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,5,We present a model that generates the future by transforming pixels in the past.,We present a model that generates the future by transforming pixels in the past.,,,"['We', 'present', 'a', 'model', 'that', 'generates', 'the', 'future', 'by', 'transforming', 'pixels', 'in', 'the', 'past.']",,,,We,0,0,2,0,2,present,0,3,10,3,10,['We'],['present']
291,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,6,"Our approach explicitly disentangles the models memory from the prediction, which helps the model learn desirable invariances.","Our approach explicitly disentangles the models memory from the prediction, which helps the model learn desirable invariances.",,,"['Our', 'approach', 'explicitly', 'disentangles', 'the', 'models', 'memory', 'from', 'the', 'prediction,', 'which', 'helps', 'the', 'model', 'learn', 'desirable', 'invariances.']",,,,approach,0,4,12,4,12,disentangles,0,24,36,24,36,['approach'],['disentangles']
292,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,7,Experiments suggest that this model can generate short videos of plausible futures.,Experiments suggest that this model can generate short videos of plausible futures.,,,"['Experiments', 'suggest', 'that', 'this', 'model', 'can', 'generate', 'short', 'videos', 'of', 'plausible', 'futures.']",,,,Experiments,0,0,11,0,11,suggest,0,12,19,12,19,['Experiments'],['suggest']
293,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,8,"We believe predictive models have many applications in robotics, health-care, and video understanding.","We believe predictive models have many applications in robotics, health-care, and video understanding.",,,"['We', 'believe', 'predictive', 'models', 'have', 'many', 'applications', 'in', 'robotics,', 'health-care,', 'and', 'video', 'understanding.']",,,,We,0,0,2,0,2,believe,0,3,10,3,10,['We'],['believe']
294,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Title,0,Generating the Future with Adversarial Transformers.,Generating the Future with Adversarial,Transformers,.,"['Generating', 'the', 'Future', 'with', 'Adversarial', 'Transformers', '.']","(5, 6)","(38, 50)",0,,,,,,,Generating,0,0,10,0,10,[],['Generating']
