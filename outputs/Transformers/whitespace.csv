,split_0,split_1,split_2,averb,averb-s,averb-o,root,root-full,root-s,root-o,fverb,fword,apos,apos-w,URL,ID,Type,Index,Text,split_tokens,split_anchor_span
0,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP).",,,,,,first,first,"['Transfer learning , where a model']",[],,,,,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,0,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP).","['Transfer', 'learning,', 'where', 'a', 'model', 'is', 'first', 'pre-trained', 'on', 'a', 'data-rich', 'task', 'before', 'being', 'fine-tuned', 'on', 'a', 'downstream', 'task,', 'has', 'emerged', 'as', 'a', 'powerful', 'technique', 'in', 'natural', 'language', 'processing', '(NLP).']",
1,"The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.",,,,,,given,has given,[],[],,,,,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,1,"The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.","['The', 'effectiveness', 'of', 'transfer', 'learning', 'has', 'given', 'rise', 'to', 'a', 'diversity', 'of', 'approaches,', 'methodology,', 'and', 'practice.']",
2,"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format.",,,,,,explore,explore,"[',']",['the landscape of transfer learning techniques for NLP'],,,,,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,2,"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format.","['In', 'this', 'paper,', 'we', 'explore', 'the', 'landscape', 'of', 'transfer', 'learning', 'techniques', 'for', 'NLP', 'by', 'introducing', 'a', 'unified', 'framework', 'that', 'converts', 'every', 'language', 'problem', 'into', 'a', 'text-to-text', 'format.']",
3,"Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.",,,,,,compares,compares,"['Our', 'systematic study']","['pre - training objectives , architectures , unlabeled datasets , transfer approaches , and other factors on dozens of language understanding tasks .']",,,,,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,3,"Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.","['Our', 'systematic', 'study', 'compares', 'pre-training', 'objectives,', 'architectures,', 'unlabeled', 'datasets,', 'transfer', 'approaches,', 'and', 'other', 'factors', 'on', 'dozens', 'of', 'language', 'understanding', 'tasks.']",
4,"By combining the insights from our exploration with scale and our new ""Colossal Clean Crawled Corpus"", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.",,,,,,question,question,[],[],,,,,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,4,"By combining the insights from our exploration with scale and our new ""Colossal Clean Crawled Corpus"", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.","['By', 'combining', 'the', 'insights', 'from', 'our', 'exploration', 'with', 'scale', 'and', 'our', 'new', '""Colossal', 'Clean', 'Crawled', 'Corpus"",', 'we', 'achieve', 'state-of-the-art', 'results', 'on', 'many', 'benchmarks', 'covering', 'summarization,', 'question', 'answering,', 'text', 'classification,', 'and', 'more.']",
5,"To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",,,,,,release,release,"['To', 'we']",[],,,,,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Abstract,5,"To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.","['To', 'facilitate', 'future', 'work', 'on', 'transfer', 'learning', 'for', 'NLP,', 'we', 'release', 'our', 'dataset,', 'pre-trained', 'models,', 'and', 'code.']",
6,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.,,,,,,Exploring,Exploring,[],[],,,,,https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/c7be76475b98ca7ab9ce182c4b57e16ca9e2976b,0,Title,0,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.,"['Exploring', 'the', 'Limits', 'of', 'Transfer', 'Learning', 'with', 'a', 'Unified', 'Text-to-Text', 'Transformer.']",
7,Large pre-trained neural networks such as,BERT,"have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.",have had,[],[],success,success,[],[],have,have,"['PROPN', 'SCONJ', 'NOUN']","['BERT', 'as', 'networks']",https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,0,"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.","['Large', 'pre-trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP,', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data.']","(6, 7)"
8,"Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers).",,,,,,outputs,outputs,[],[],,,,,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,1,"Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers).","['Most', 'recent', 'analysis', 'has', 'focused', 'on', 'model', 'outputs', '(e.g.,', 'language', 'model', 'surprisal)', 'or', 'internal', 'vector', 'representations', '(e.g.,', 'probing', 'classifiers).']",
9,"Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to",BERT,.,apply,[],[],propose,propose,"['Complementary to these works', ',', 'we']",[],,,"['PROPN', 'ADP', 'VERB']","['BERT', 'to', 'apply']",https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,2,"Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT.","['Complementary', 'to', 'these', 'works,', 'we', 'propose', 'methods', 'for', 'analyzing', 'the', 'attention', 'mechanisms', 'of', 'pre-trained', 'models', 'and', 'apply', 'them', 'to', 'BERT', '.']","(19, 20)"
10,,BERT,"'s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors.",,,,BERT,BERT,[],[],exhibit,attention,['PROPN'],['BERT'],https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,3,"BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors.","['BERT', ""'s"", 'attention', 'heads', 'exhibit', 'patterns', 'such', 'as', 'attending', 'to', 'delimiter', 'tokens,', 'specific', 'positional', 'offsets,', 'or', 'broadly', 'attending', 'over', 'the', 'whole', 'sentence,', 'with', 'heads', 'in', 'the', 'same', 'layer', 'often', 'exhibiting', 'similar', 'behaviors.']","(0, 1)"
11,We further show that certain attention heads correspond well to linguistic notions of syntax and coreference.,,,,,,show,show,"['We', 'further']",[],,,,,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,4,We further show that certain attention heads correspond well to linguistic notions of syntax and coreference.,"['We', 'further', 'show', 'that', 'certain', 'attention', 'heads', 'correspond', 'well', 'to', 'linguistic', 'notions', 'of', 'syntax', 'and', 'coreference.']",
12,"For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy.",,,,,,find,find,"['example ,', 'we']",[],,,,,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,5,"For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy.","['For', 'example,', 'we', 'find', 'heads', 'that', 'attend', 'to', 'the', 'direct', 'objects', 'of', 'verbs,', 'determiners', 'of', 'nouns,', 'objects', 'of', 'prepositions,', 'and', 'coreferent', 'mentions', 'with', 'remarkably', 'high', 'accuracy.']",
13,"Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in",BERT,'s attention.,is captured,['substantial syntactic information'],[],Lastly,Lastly,[],[],,attention,"['PROPN', 'ADP', 'VERB']","['BERT', 'in', 'captured']",https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Abstract,6,"Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.","['Lastly,', 'we', 'propose', 'an', 'attention-based', 'probing', 'classifier', 'and', 'use', 'it', 'to', 'further', 'demonstrate', 'that', 'substantial', 'syntactic', 'information', 'is', 'captured', 'in', 'BERT', ""'s"", 'attention.']","(20, 21)"
14,Transformers: State-of-the-art Natural Language Processing.,,,,,,Transformers,Transformers,[],[],,,,,https://www.semanticscholar.org/paper/Transformers%3A-State-of-the-art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2,1,Title,0,Transformers: State-of-the-art Natural Language Processing.,"['Transformers:', 'State-of-the-art', 'Natural', 'Language', 'Processing.']",
15,Transfer learning has fundamentally changed the landscape of natural language processing (NLP) research.,,,,,,changed,has changed,[],[],,,,,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,0,Transfer learning has fundamentally changed the landscape of natural language processing (NLP) research.,"['Transfer', 'learning', 'has', 'fundamentally', 'changed', 'the', 'landscape', 'of', 'natural', 'language', 'processing', '(NLP)', 'research.']",
16,Many existing state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks.,,,,,,pre,are pre,[],[],,,,,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,1,Many existing state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks.,"['Many', 'existing', 'state-of-the-art', 'models', 'are', 'first', 'pre-trained', 'on', 'a', 'large', 'text', 'corpus', 'and', 'then', 'fine-tuned', 'on', 'downstream', 'tasks.']",
17,"However, due to limited data resources from downstream tasks and the extremely large capacity of pre-trained models, aggressive fine-tuning often causes the adapted model to overfit the data of downstream tasks and forget the knowledge of the pre-trained model.",,,,,,causes,causes,"['However , due to limited data resources from downstream tasks and the extremely large capacity of pre - trained models , aggressive fine - tuning', 'the adapted model to overfit the data of downstream tasks and forget the knowledge of the pre - trained model .']",[],,,,,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,2,"However, due to limited data resources from downstream tasks and the extremely large capacity of pre-trained models, aggressive fine-tuning often causes the adapted model to overfit the data of downstream tasks and forget the knowledge of the pre-trained model.","['However,', 'due', 'to', 'limited', 'data', 'resources', 'from', 'downstream', 'tasks', 'and', 'the', 'extremely', 'large', 'capacity', 'of', 'pre-trained', 'models,', 'aggressive', 'fine-tuning', 'often', 'causes', 'the', 'adapted', 'model', 'to', 'overfit', 'the', 'data', 'of', 'downstream', 'tasks', 'and', 'forget', 'the', 'knowledge', 'of', 'the', 'pre-trained', 'model.']",
18,"To address the above issue in a more principled manner, we propose a new computational framework for robust and efficient fine-tuning for pre-trained language models.",,,,,,propose,propose,['we'],['a new computational framework for robust and efficient fine - tuning for pre - trained language models'],,,,,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,3,"To address the above issue in a more principled manner, we propose a new computational framework for robust and efficient fine-tuning for pre-trained language models.","['To', 'address', 'the', 'above', 'issue', 'in', 'a', 'more', 'principled', 'manner,', 'we', 'propose', 'a', 'new', 'computational', 'framework', 'for', 'robust', 'and', 'efficient', 'fine-tuning', 'for', 'pre-trained', 'language', 'models.']",
19,"Specifically, our proposed framework contains two important ingredients: 1.",,,,,,contains,contains,['our proposed framework'],['two important ingredients : 1'],,,,,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,4,"Specifically, our proposed framework contains two important ingredients: 1.","['Specifically,', 'our', 'proposed', 'framework', 'contains', 'two', 'important', 'ingredients:', '1.']",
20,"Smoothness-inducing regularization, which effectively manages the capacity of the model; 2.",,,,,,Smoothness,Smoothness,[],[],,,,,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,5,"Smoothness-inducing regularization, which effectively manages the capacity of the model; 2.","['Smoothness-inducing', 'regularization,', 'which', 'effectively', 'manages', 'the', 'capacity', 'of', 'the', 'model;', '2.']",
21,"Bregman proximal point optimization, which is a class of trust-region methods and can prevent knowledge forgetting.",,,,,,proximal,proximal,[],[],,,,,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,6,"Bregman proximal point optimization, which is a class of trust-region methods and can prevent knowledge forgetting.","['Bregman', 'proximal', 'point', 'optimization,', 'which', 'is', 'a', 'class', 'of', 'trust-region', 'methods', 'and', 'can', 'prevent', 'knowledge', 'forgetting.']",
22,Our experiments demonstrate that our proposed method achieves the state-of-the-art performance on multiple NLP benchmarks.,,,,,,demonstrate,demonstrate,['Our experiments'],[],,,,,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Abstract,7,Our experiments demonstrate that our proposed method achieves the state-of-the-art performance on multiple NLP benchmarks.,"['Our', 'experiments', 'demonstrate', 'that', 'our', 'proposed', 'method', 'achieves', 'the', 'state-of-the-art', 'performance', 'on', 'multiple', 'NLP', 'benchmarks.']",
23,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization.,,,,,,SMART,SMART,[],[],,,,,https://www.semanticscholar.org/paper/SMART%3A-Robust-and-Efficient-Fine-Tuning-for-Natural-Jiang-He/7234b6be1573b3e552b7a54b46452599ff869a94,2,Title,0,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization.,"['SMART:', 'Robust', 'and', 'Efficient', 'Fine-Tuning', 'for', 'Pre-trained', 'Natural', 'Language', 'Models', 'through', 'Principled', 'Regularized', 'Optimization.']",
24,Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks.,,,,,,approach,has become approach,['Fine - tuning of pre - trained transformer models'],[],,,,,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,0,Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks.,"['Fine-tuning', 'of', 'pre-trained', 'transformer', 'models', 'has', 'become', 'the', 'standard', 'approach', 'for', 'solving', 'common', 'NLP', 'tasks.']",
25,Most of the existing approaches rely on a randomly initialized classifier on top of such networks.,,,,,,rely,rely,['Most of the existing approaches'],[],,,,,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,1,Most of the existing approaches rely on a randomly initialized classifier on top of such networks.,"['Most', 'of', 'the', 'existing', 'approaches', 'rely', 'on', 'a', 'randomly', 'initialized', 'classifier', 'on', 'top', 'of', 'such', 'networks.']",
26,"We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task.",,,,,,We,We,[],[],,,,,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,2,"We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task.","['We', 'argue', 'that', 'this', 'fine-tuning', 'procedure', 'is', 'sub-optimal', 'as', 'the', 'pre-trained', 'model', 'has', 'no', 'prior', 'on', 'the', 'specific', 'classifier', 'labels,', 'while', 'it', 'might', 'have', 'already', 'learned', 'an', 'intrinsic', 'textual', 'representation', 'of', 'the', 'task.']",
27,"In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase.",,,,,,introduce,introduce,"[',', 'we']",['a new scoring method that casts a plausibility ranking task in a full - text format and leverages the masked language modeling head tuned during the pre - training phase'],,,,,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,3,"In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase.","['In', 'this', 'paper,', 'we', 'introduce', 'a', 'new', 'scoring', 'method', 'that', 'casts', 'a', 'plausibility', 'ranking', 'task', 'in', 'a', 'full-text', 'format', 'and', 'leverages', 'the', 'masked', 'language', 'modeling', 'head', 'tuned', 'during', 'the', 'pre-training', 'phase.']",
28,"We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets.",,,,,,rank,must rank,['We study commonsense reasoning tasks where the model'],[],,,,,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,4,"We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets.","['We', 'study', 'commonsense', 'reasoning', 'tasks', 'where', 'the', 'model', 'must', 'rank', 'a', 'set', 'of', 'hypotheses', 'given', 'a', 'premise,', 'focusing', 'on', 'the', 'COPA,', 'Swag,', 'HellaSwag', 'and', 'CommonsenseQA', 'datasets.']",
29,"By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g.",,,,,,able,are able,['we'],[],,,,,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,5,"By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g.","['By', 'exploiting', 'our', 'scoring', 'method', 'without', 'fine-tuning,', 'we', 'are', 'able', 'to', 'produce', 'strong', 'baselines', '(e.g.']",
30,80% test accuracy on COPA) that are comparable to supervised approaches.,,,,,,comparable,are comparable,['that'],[],,,,,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,6,80% test accuracy on COPA) that are comparable to supervised approaches.,"['80%', 'test', 'accuracy', 'on', 'COPA)', 'that', 'are', 'comparable', 'to', 'supervised', 'approaches.']",
31,"Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g $\times 10$ standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.",,,,,,show,show,"[',', 'we']",[],,,,,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Abstract,7,"Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g $\times 10$ standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.","['Moreover,', 'when', 'fine-tuning', 'directly', 'on', 'the', 'proposed', 'scoring', 'function,', 'we', 'show', 'that', 'our', 'method', 'provides', 'a', 'much', 'more', 'stable', 'training', 'phase', 'across', 'random', 'restarts', '(e.g', '$\\times', '10$', 'standard', 'deviation', 'reduction', 'on', 'COPA', 'test', 'accuracy)', 'and', 'requires', 'less', 'annotated', 'data', 'than', 'the', 'standard', 'classifier', 'approach', 'to', 'reach', 'equivalent', 'performances.']",
32,Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning.,,,,,,Pre,Pre,[],[],,,,,https://www.semanticscholar.org/paper/Pre-training-Is-(Almost)-All-You-Need%3A-An-to-Tamborrino-Pellicano/d6599d4dfaeb78bea1f975db683aa653e26b3987,3,Title,0,Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning.,"['Pre-training', 'Is', '(Almost)', 'All', 'You', 'Need:', 'An', 'Application', 'to', 'Commonsense', 'Reasoning.']",
33,Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP.,,,,,,successfully,have successfully,['Recent developments in unsupervised representation learning'],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,0,Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP.,"['Recent', 'developments', 'in', 'unsupervised', 'representation', 'learning', 'have', 'successfully', 'established', 'the', 'concept', 'of', 'transfer', 'learning', 'in', 'NLP.']",
34,Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information.,,,,,,driving,are driving,['the improvements in this area of research'],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,1,Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information.,"['Mainly', 'three', 'forces', 'are', 'driving', 'the', 'improvements', 'in', 'this', 'area', 'of', 'research:', 'More', 'elaborated', 'architectures', 'are', 'making', 'better', 'use', 'of', 'contextual', 'information.']",
35,"Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives.",,,,,,learned,are learned,"['Instead of simply plugging in static pre - trained representations ,', 'these']",[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,2,"Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives.","['Instead', 'of', 'simply', 'plugging', 'in', 'static', 'pre-trained', 'representations,', 'these', 'are', 'learned', 'based', 'on', 'surrounding', 'context', 'in', 'end-to-end', 'trainable', 'models', 'with', 'more', 'intelligently', 'designed', 'language', 'modelling', 'objectives.']",
36,"Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks.",,,,,,used,are used,[],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,3,"Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks.","['Along', 'with', 'this,', 'larger', 'corpora', 'are', 'used', 'as', 'resources', 'for', 'pre-training', 'large', 'language', 'models', 'in', 'a', 'self-supervised', 'fashion', 'which', 'are', 'afterwards', 'fine-tuned', 'on', 'supervised', 'tasks.']",
37,"Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models.",,,,,,made,made,[],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,4,"Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models.","['Advances', 'in', 'parallel', 'computing', 'as', 'well', 'as', 'in', 'cloud', 'computing,', 'made', 'it', 'possible', 'to', 'train', 'these', 'models', 'with', 'growing', 'capacities', 'in', 'the', 'same', 'or', 'even', 'in', 'shorter', 'time', 'than', 'previously', 'established', 'models.']",
38,These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency.,,,,,,These,These,[],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,5,These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency.,"['These', 'three', 'developments', 'agglomerate', 'in', 'new', 'state-of-the-art', '(SOTA)', 'results', 'being', 'revealed', 'in', 'a', 'higher', 'and', 'higher', 'frequency.']",
39,"It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces.",,,,,,obvious,obvious,['It'],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,6,"It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces.","['It', 'is', 'not', 'always', 'obvious', 'where', 'these', 'improvements', 'originate', 'from,', 'as', 'it', 'is', 'not', 'possible', 'to', 'completely', 'disentangle', 'the', 'contributions', 'of', 'the', 'three', 'driving', 'forces.']",
40,"We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources.",,,,,,set,set,[],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,7,"We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources.","['We', 'set', 'ourselves', 'to', 'providing', 'a', 'clear', 'and', 'concise', 'overview', 'on', 'several', 'large', 'pre-trained', 'language', 'models,', 'which', 'achieved', 'SOTA', 'results', 'in', 'the', 'last', 'two', 'years,', 'with', 'respect', 'to', 'their', 'use', 'of', 'new', 'architectures', 'and', 'resources.']",
41,We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes.,,,,,,want,want,['We'],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,8,We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes.,"['We', 'want', 'to', 'clarify', 'for', 'the', 'reader', 'where', 'the', 'differences', 'between', 'the', 'models', 'are', 'and', 'we', 'furthermore', 'attempt', 'to', 'gain', 'some', 'insight', 'into', 'the', 'single', 'contributions', 'of', 'lexical/computational', 'improvements', 'as', 'well', 'as', 'of', 'architectural', 'changes.']",
42,"We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons.",,,,,,intend,do intend,['We'],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,9,"We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons.","['We', 'explicitly', 'do', 'not', 'intend', 'to', 'quantify', 'these', 'contributions,', 'but', 'rather', 'see', 'our', 'work', 'as', 'an', 'overview', 'in', 'order', 'to', 'identify', 'potential', 'starting', 'points', 'for', 'benchmark', 'comparisons.']",
43,"Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.",,,,,,want,want,"['Furthermore', ',', 'we']",[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Abstract,10,"Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.","['Furthermore,', 'we', 'tentatively', 'want', 'to', 'point', 'at', 'potential', 'possibilities', 'for', 'improvement', 'in', 'the', 'field', 'of', 'open-sourcing', 'and', 'reproducible', 'research.']",
44,On the Comparability of Pre-trained Language Models.,,,,,,Language,Language,[],[],,,,,https://www.semanticscholar.org/paper/On-the-Comparability-of-Pre-trained-Language-Models-A%C3%9Fenmacher-Heumann/86bd570007c863c147eb9c13f00fc6908f6b3fc9,4,Title,0,On the Comparability of Pre-trained Language Models.,"['On', 'the', 'Comparability', 'of', 'Pre-trained', 'Language', 'Models.']",
45,"Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering.",,,,,,shown,has been shown,['Language model pre - training'],[],,,,,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,0,"Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering.","['Language', 'model', 'pre-training', 'has', 'been', 'shown', 'to', 'capture', 'a', 'surprising', 'amount', 'of', 'world', 'knowledge,', 'crucial', 'for', 'NLP', 'tasks', 'such', 'as', 'question', 'answering.']",
46,"However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.",,,,,,stored,is stored,"['However', ', this knowledge']",[],,,,,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,1,"However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.","['However,', 'this', 'knowledge', 'is', 'stored', 'implicitly', 'in', 'the', 'parameters', 'of', 'a', 'neural', 'network,', 'requiring', 'ever-larger', 'networks', 'to', 'cover', 'more', 'facts.']",
47,"To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference.",,,,,,model,model,[],[],,,,,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,2,"To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference.","['To', 'capture', 'knowledge', 'in', 'a', 'more', 'modular', 'and', 'interpretable', 'way,', 'we', 'augment', 'language', 'model', 'pre-training', 'with', 'a', 'latent', 'knowledge', 'retriever,', 'which', 'allows', 'the', 'model', 'to', 'retrieve', 'and', 'attend', 'over', 'documents', 'from', 'a', 'large', 'corpus', 'such', 'as', 'Wikipedia,', 'used', 'during', 'pre-training,', 'fine-tuning', 'and', 'inference.']",
48,"For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.",,,,,,show,show,"[',', 'we']",[],,,,,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,3,"For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.","['For', 'the', 'first', 'time,', 'we', 'show', 'how', 'to', 'pre-train', 'such', 'a', 'knowledge', 'retriever', 'in', 'an', 'unsupervised', 'manner,', 'using', 'masked', 'language', 'modeling', 'as', 'the', 'learning', 'signal', 'and', 'backpropagating', 'through', 'a', 'retrieval', 'step', 'that', 'considers', 'millions', 'of', 'documents.']",
49,We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA).,,,,,,Question,Question,[],[],,,,,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,4,We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA).,"['We', 'demonstrate', 'the', 'effectiveness', 'of', 'Retrieval-Augmented', 'Language', 'Model', 'pre-training', '(REALM)', 'by', 'fine-tuning', 'on', 'the', 'challenging', 'task', 'of', 'Open-domain', 'Question', 'Answering', '(Open-QA).']",
50,"We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",,,,,,providing,providing,[],"['qualitative', ', benefits such as interpretability and modularity .']",,,,,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Abstract,5,"We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.","['We', 'compare', 'against', 'state-of-the-art', 'models', 'for', 'both', 'explicit', 'and', 'implicit', 'knowledge', 'storage', 'on', 'three', 'popular', 'Open-QA', 'benchmarks,', 'and', 'find', 'that', 'we', 'outperform', 'all', 'previous', 'methods', 'by', 'a', 'significant', 'margin', '(4-16%', 'absolute', 'accuracy),', 'while', 'also', 'providing', 'qualitative', 'benefits', 'such', 'as', 'interpretability', 'and', 'modularity.']",
51,REALM: Retrieval-Augmented Language Model Pre-Training.,,,,,,REALM,REALM,[],[],,,,,https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56,5,Title,0,REALM: Retrieval-Augmented Language Model Pre-Training.,"['REALM:', 'Retrieval-Augmented', 'Language', 'Model', 'Pre-Training.']",
52,"There has been recent success in pre-training on monolingual data and fine-tuning on Machine Translation (MT), but it remains unclear how to best leverage a pre-trained model for a given MT task.",,,,,,recent,has been recent,['There'],[],,,,,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,0,"There has been recent success in pre-training on monolingual data and fine-tuning on Machine Translation (MT), but it remains unclear how to best leverage a pre-trained model for a given MT task.","['There', 'has', 'been', 'recent', 'success', 'in', 'pre-training', 'on', 'monolingual', 'data', 'and', 'fine-tuning', 'on', 'Machine', 'Translation', '(MT),', 'but', 'it', 'remains', 'unclear', 'how', 'to', 'best', 'leverage', 'a', 'pre-trained', 'model', 'for', 'a', 'given', 'MT', 'task.']",
53,"This paper investigates the benefits and drawbacks of freezing parameters, and adding new ones, when fine-tuning a pre-trained model on MT.",,,,,,investigates,investigates,"['This', 'paper']","['the benefits and drawbacks of freezing parameters , and adding new ones , when fine - tuning a pre - trained model on MT']",,,,,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,1,"This paper investigates the benefits and drawbacks of freezing parameters, and adding new ones, when fine-tuning a pre-trained model on MT.","['This', 'paper', 'investigates', 'the', 'benefits', 'and', 'drawbacks', 'of', 'freezing', 'parameters,', 'and', 'adding', 'new', 'ones,', 'when', 'fine-tuning', 'a', 'pre-trained', 'model', 'on', 'MT.']",
54,"We focus on 1) Fine-tuning a model trained only on English monolingual data, BART.",,,,,,focus,focus,['We'],[],,,,,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,2,"We focus on 1) Fine-tuning a model trained only on English monolingual data, BART.","['We', 'focus', 'on', '1)', 'Fine-tuning', 'a', 'model', 'trained', 'only', 'on', 'English', 'monolingual', 'data,', 'BART.']",
55,"2) Fine-tuning a model trained on monolingual data from 25 languages, mBART.",,,,,,),),[],[],,,,,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,3,"2) Fine-tuning a model trained on monolingual data from 25 languages, mBART.","['2)', 'Fine-tuning', 'a', 'model', 'trained', 'on', 'monolingual', 'data', 'from', '25', 'languages,', 'mBART.']",
56,"For BART we get the best performance by freezing most of the model parameters, and adding extra positional embeddings.",,,,,,get,get,['we'],[],,,,,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,4,"For BART we get the best performance by freezing most of the model parameters, and adding extra positional embeddings.","['For', 'BART', 'we', 'get', 'the', 'best', 'performance', 'by', 'freezing', 'most', 'of', 'the', 'model', 'parameters,', 'and', 'adding', 'extra', 'positional', 'embeddings.']",
57,"For mBART we match the performance of naive fine-tuning for most language pairs, and outperform it for Nepali to English (0.5 BLEU) and Czech to English (0.6 BLEU), all with a lower memory cost at training time.",,,,,,mBART,mBART,[],[],,,,,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,5,"For mBART we match the performance of naive fine-tuning for most language pairs, and outperform it for Nepali to English (0.5 BLEU) and Czech to English (0.6 BLEU), all with a lower memory cost at training time.","['For', 'mBART', 'we', 'match', 'the', 'performance', 'of', 'naive', 'fine-tuning', 'for', 'most', 'language', 'pairs,', 'and', 'outperform', 'it', 'for', 'Nepali', 'to', 'English', '(0.5', 'BLEU)', 'and', 'Czech', 'to', 'English', '(0.6', 'BLEU),', 'all', 'with', 'a', 'lower', 'memory', 'cost', 'at', 'training', 'time.']",
58,When constraining ourselves to an out-of-domain training set for Vietnamese to English we outperform the fine-tuning baseline by 0.9 BLEU.,,,,,,constraining,constraining,[],[],,,,,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Abstract,6,When constraining ourselves to an out-of-domain training set for Vietnamese to English we outperform the fine-tuning baseline by 0.9 BLEU.,"['When', 'constraining', 'ourselves', 'to', 'an', 'out-of-domain', 'training', 'set', 'for', 'Vietnamese', 'to', 'English', 'we', 'outperform', 'the', 'fine-tuning', 'baseline', 'by', '0.9', 'BLEU.']",
59,Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation.,,,,,,Recipes,Recipes,[],[],,,,,https://www.semanticscholar.org/paper/Recipes-for-Adapting-Pre-trained-Monolingual-and-to-Stickland-Li/1bed2b8a70715e95c419a627fc244a41f5501f7a,6,Title,0,Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation.,"['Recipes', 'for', 'Adapting', 'Pre-trained', 'Monolingual', 'and', 'Multilingual', 'Models', 'to', 'Machine', 'Translation.']",
60,"While transformer-based finetuning techniques have proven effective in tasks that involve low-resource, low-data environments, a lack of properly established baselines and benchmark datasets make it hard to compare different approaches that are aimed at tackling the low-resource setting.",,,,,,proven,proven,[],[],,,,,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,0,"While transformer-based finetuning techniques have proven effective in tasks that involve low-resource, low-data environments, a lack of properly established baselines and benchmark datasets make it hard to compare different approaches that are aimed at tackling the low-resource setting.","['While', 'transformer-based', 'finetuning', 'techniques', 'have', 'proven', 'effective', 'in', 'tasks', 'that', 'involve', 'low-resource,', 'low-data', 'environments,', 'a', 'lack', 'of', 'properly', 'established', 'baselines', 'and', 'benchmark', 'datasets', 'make', 'it', 'hard', 'to', 'compare', 'different', 'approaches', 'that', 'are', 'aimed', 'at', 'tackling', 'the', 'low-resource', 'setting.']",
61,"In this work, we provide three contributions.",,,,,,provide,provide,['we'],['three contributions'],,,,,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,1,"In this work, we provide three contributions.","['In', 'this', 'work,', 'we', 'provide', 'three', 'contributions.']",
62,"First, we introduce two previously unreleased datasets as benchmark datasets for text classification and low-resource multilabel text classification for the low-resource language Filipino.",,,,,,introduce,introduce,"[',', 'we']",[],,,,,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,2,"First, we introduce two previously unreleased datasets as benchmark datasets for text classification and low-resource multilabel text classification for the low-resource language Filipino.","['First,', 'we', 'introduce', 'two', 'previously', 'unreleased', 'datasets', 'as', 'benchmark', 'datasets', 'for', 'text', 'classification', 'and', 'low-resource', 'multilabel', 'text', 'classification', 'for', 'the', 'low-resource', 'language', 'Filipino.']",
63,"Second, we pretrain better",BERT,and DistilBERT models for use within the Filipino setting.,pretrain,['we'],[],pretrain,pretrain,['we'],[],,and,"['NOUN', 'VERB']","['BERT', 'pretrain']",https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,3,"Second, we pretrain better BERT and DistilBERT models for use within the Filipino setting.","['Second,', 'we', 'pretrain', 'better', 'BERT', 'and', 'DistilBERT', 'models', 'for', 'use', 'within', 'the', 'Filipino', 'setting.']","(4, 5)"
64,"Second, we pretrain better BERT and Distil",BERT,models for use within the Filipino setting.,pretrain,['we'],[],pretrain,pretrain,['we'],[],,models,"['PROPN', 'NOUN', 'PROPN']","['BERT', 'models', 'BERT']",https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,3,"Second, we pretrain better BERT and DistilBERT models for use within the Filipino setting.","['Second,', 'we', 'pretrain', 'better', 'BERT', 'and', 'Distil', 'BERT', 'models', 'for', 'use', 'within', 'the', 'Filipino', 'setting.']","(7, 8)"
65,"Third, we introduce a simple degradation test that benchmarks a model's resistance to performance degradation as the number of training samples are reduced.",,,,,,introduce,introduce,"['Third', ',']",[],,,,,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,4,"Third, we introduce a simple degradation test that benchmarks a model's resistance to performance degradation as the number of training samples are reduced.","['Third,', 'we', 'introduce', 'a', 'simple', 'degradation', 'test', 'that', 'benchmarks', 'a', ""model's"", 'resistance', 'to', 'performance', 'degradation', 'as', 'the', 'number', 'of', 'training', 'samples', 'are', 'reduced.']",
66,We analyze our pretrained model's degradation speeds and look towards the use of this method for comparing models aimed at operating within the low-resource setting.,,,,,,look,look,['and'],[],,,,,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,5,We analyze our pretrained model's degradation speeds and look towards the use of this method for comparing models aimed at operating within the low-resource setting.,"['We', 'analyze', 'our', 'pretrained', ""model's"", 'degradation', 'speeds', 'and', 'look', 'towards', 'the', 'use', 'of', 'this', 'method', 'for', 'comparing', 'models', 'aimed', 'at', 'operating', 'within', 'the', 'low-resource', 'setting.']",
67,We release all our models and datasets for the research community to use.,,,,,,release,release,[],[],,,,,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Abstract,6,We release all our models and datasets for the research community to use.,"['We', 'release', 'all', 'our', 'models', 'and', 'datasets', 'for', 'the', 'research', 'community', 'to', 'use.']",
68,Establishing Baselines for Text Classification in Low-Resource Languages.,,,,,,Baselines,Baselines,[],[],,,,,https://www.semanticscholar.org/paper/Establishing-Baselines-for-Text-Classification-in-Cruz-Cheng/c2dfc728abf4127fdb24195143ce3d2059418f0e,7,Title,0,Establishing Baselines for Text Classification in Low-Resource Languages.,"['Establishing', 'Baselines', 'for', 'Text', 'Classification', 'in', 'Low-Resource', 'Languages.']",
69,"Neural Network based models have been state-of-the-art models for various Natural Language Processing tasks, however, the input and output dimension problem in the networks has still not been fully resolved, especially in text generation tasks (e.g.",,,,,,state,have been state,['Neural Network based models'],[],,,,,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,0,"Neural Network based models have been state-of-the-art models for various Natural Language Processing tasks, however, the input and output dimension problem in the networks has still not been fully resolved, especially in text generation tasks (e.g.","['Neural', 'Network', 'based', 'models', 'have', 'been', 'state-of-the-art', 'models', 'for', 'various', 'Natural', 'Language', 'Processing', 'tasks,', 'however,', 'the', 'input', 'and', 'output', 'dimension', 'problem', 'in', 'the', 'networks', 'has', 'still', 'not', 'been', 'fully', 'resolved,', 'especially', 'in', 'text', 'generation', 'tasks', '(e.g.']",
70,"Machine Translation, Text Summarization), in which input and output both have huge sizes of vocabularies.",,,,,,have,have,[],['huge sizes of vocabularies'],,,,,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,1,"Machine Translation, Text Summarization), in which input and output both have huge sizes of vocabularies.","['Machine', 'Translation,', 'Text', 'Summarization),', 'in', 'which', 'input', 'and', 'output', 'both', 'have', 'huge', 'sizes', 'of', 'vocabularies.']",
71,"Therefore, input-output embedding weight sharing has been introduced and adopted widely, which remains to be improved.",,,,,,introduced,has been introduced,"['Therefore , input - output embedding weight sharing']",[],,,,,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,2,"Therefore, input-output embedding weight sharing has been introduced and adopted widely, which remains to be improved.","['Therefore,', 'input-output', 'embedding', 'weight', 'sharing', 'has', 'been', 'introduced', 'and', 'adopted', 'widely,', 'which', 'remains', 'to', 'be', 'improved.']",
72,"Based on linear algebra and statistical theories, this paper locates the shortcoming of existed input-output embedding weight sharing method, then raises methods for improving input-output weight shared embedding, among which methods of normalization of embedding weight matrices show best performance.",,,,,,normalization,normalization,[],[],,,,,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,3,"Based on linear algebra and statistical theories, this paper locates the shortcoming of existed input-output embedding weight sharing method, then raises methods for improving input-output weight shared embedding, among which methods of normalization of embedding weight matrices show best performance.","['Based', 'on', 'linear', 'algebra', 'and', 'statistical', 'theories,', 'this', 'paper', 'locates', 'the', 'shortcoming', 'of', 'existed', 'input-output', 'embedding', 'weight', 'sharing', 'method,', 'then', 'raises', 'methods', 'for', 'improving', 'input-output', 'weight', 'shared', 'embedding,', 'among', 'which', 'methods', 'of', 'normalization', 'of', 'embedding', 'weight', 'matrices', 'show', 'best', 'performance.']",
73,"These methods are nearly computational cost-free, can get combined with other embedding techniques, and show good effectiveness when applied on state-of-the-art Neural Network models.",,,,,,free,free,[],[],,,,,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,4,"These methods are nearly computational cost-free, can get combined with other embedding techniques, and show good effectiveness when applied on state-of-the-art Neural Network models.","['These', 'methods', 'are', 'nearly', 'computational', 'cost-free,', 'can', 'get', 'combined', 'with', 'other', 'embedding', 'techniques,', 'and', 'show', 'good', 'effectiveness', 'when', 'applied', 'on', 'state-of-the-art', 'Neural', 'Network', 'models.']",
74,"For Transformer-big models, the normalization techniques can get at best 0.6 BLEU improvement compared to the original version of model on WMT'16 En-De dataset, and similar BLEU improvements on IWSLT 14' datasets.",,,,,,at,at,[],[],,,,,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,5,"For Transformer-big models, the normalization techniques can get at best 0.6 BLEU improvement compared to the original version of model on WMT'16 En-De dataset, and similar BLEU improvements on IWSLT 14' datasets.","['For', 'Transformer-big', 'models,', 'the', 'normalization', 'techniques', 'can', 'get', 'at', 'best', '0.6', 'BLEU', 'improvement', 'compared', 'to', 'the', 'original', 'version', 'of', 'model', 'on', ""WMT'16"", 'En-De', 'dataset,', 'and', 'similar', 'BLEU', 'improvements', 'on', 'IWSLT', ""14'"", 'datasets.']",
75,"For DynamicConv models, 0.5 BLEU improvement can be attained on WMT'16 En-De dataset, and 0.41 BLEU improvement on IWSLT 14' De-En translation task is achieved.",,,,,,attained,can be attained,[],[],,,,,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Abstract,6,"For DynamicConv models, 0.5 BLEU improvement can be attained on WMT'16 En-De dataset, and 0.41 BLEU improvement on IWSLT 14' De-En translation task is achieved.","['For', 'DynamicConv', 'models,', '0.5', 'BLEU', 'improvement', 'can', 'be', 'attained', 'on', ""WMT'16"", 'En-De', 'dataset,', 'and', '0.41', 'BLEU', 'improvement', 'on', 'IWSLT', ""14'"", 'De-En', 'translation', 'task', 'is', 'achieved.']",
76,Normalization of Input-output Shared Embeddings in Text Generation Models.,,,,,,Normalization,Normalization,[],[],,,,,https://www.semanticscholar.org/paper/Normalization-of-Input-output-Shared-Embeddings-in-Liu-Zhai/6925eda7b26bfca99bfc3e1d1998845bbbd64acb,8,Title,0,Normalization of Input-output Shared Embeddings in Text Generation Models.,"['Normalization', 'of', 'Input-output', 'Shared', 'Embeddings', 'in', 'Text', 'Generation', 'Models.']",
77,"Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era.",,,,,,brought,brought,[],[],,,,,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,0,"Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era.","['Recently,', 'the', 'emergence', 'of', 'pre-trained', 'models', '(PTMs)', 'has', 'brought', 'natural', 'language', 'processing', '(NLP)', 'to', 'a', 'new', 'era.']",
78,"In this survey, we provide a comprehensive review of PTMs for NLP.",,,,,,provide,provide,['we'],['a comprehensive review of PTMs'],,,,,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,1,"In this survey, we provide a comprehensive review of PTMs for NLP.","['In', 'this', 'survey,', 'we', 'provide', 'a', 'comprehensive', 'review', 'of', 'PTMs', 'for', 'NLP.']",
79,We first briefly introduce language representation learning and its research progress.,,,,,,briefly,briefly,['We'],[],,,,,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,2,We first briefly introduce language representation learning and its research progress.,"['We', 'first', 'briefly', 'introduce', 'language', 'representation', 'learning', 'and', 'its', 'research', 'progress.']",
80,Then we systematically categorize existing PTMs based on a taxonomy with four perspectives.,,,,,,categorize,categorize,[],[],,,,,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,3,Then we systematically categorize existing PTMs based on a taxonomy with four perspectives.,"['Then', 'we', 'systematically', 'categorize', 'existing', 'PTMs', 'based', 'on', 'a', 'taxonomy', 'with', 'four', 'perspectives.']",
81,"Next, we describe how to adapt the knowledge of PTMs to the downstream tasks.",,,,,,describe,describe,"[',', 'we']",[],,,,,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,4,"Next, we describe how to adapt the knowledge of PTMs to the downstream tasks.","['Next,', 'we', 'describe', 'how', 'to', 'adapt', 'the', 'knowledge', 'of', 'PTMs', 'to', 'the', 'downstream', 'tasks.']",
82,"Finally, we outline some potential directions of PTMs for future research.",,,,,,outline,outline,"[',', 'we']",['some potential directions of PTMs for future research'],,,,,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,5,"Finally, we outline some potential directions of PTMs for future research.","['Finally,', 'we', 'outline', 'some', 'potential', 'directions', 'of', 'PTMs', 'for', 'future', 'research.']",
83,"This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.",,,,,,purposed,is purposed,"['This', 'survey']",[],,,,,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Abstract,6,"This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.","['This', 'survey', 'is', 'purposed', 'to', 'be', 'a', 'hands-on', 'guide', 'for', 'understanding,', 'using,', 'and', 'developing', 'PTMs', 'for', 'various', 'NLP', 'tasks.']",
84,Pre-trained Models for Natural Language Processing: A Survey.,,,,,,Pre,Pre,[],[],,,,,https://www.semanticscholar.org/paper/Pre-trained-Models-for-Natural-Language-Processing%3A-Qiu-Sun/e06b36f1076b3497992b558d7707f053161dc840,9,Title,0,Pre-trained Models for Natural Language Processing: A Survey.,"['Pre-trained', 'Models', 'for', 'Natural', 'Language', 'Processing:', 'A', 'Survey.']",
85,Large pretrained natural language representations such as,BERT,", ALBERT, and other variants (BERT-models) have achieved state of the art performance on a diverse set of NLP tasks after fine tuning.",have,[],[],achieved,achieved,[],['state of the art performance'],have,ALBERT,"['PROPN', 'SCONJ', 'NOUN']","['BERT', 'as', 'representations']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,0,"Large pretrained natural language representations such as BERT, ALBERT, and other variants (BERT-models) have achieved state of the art performance on a diverse set of NLP tasks after fine tuning.","['Large', 'pretrained', 'natural', 'language', 'representations', 'such', 'as', 'BERT', ',', 'ALBERT,', 'and', 'other', 'variants', '(BERT-models)', 'have', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'a', 'diverse', 'set', 'of', 'NLP', 'tasks', 'after', 'fine', 'tuning.']","(7, 8)"
86,"Large pretrained natural language representations such as BERT, AL",BERT,", and other variants (BERT-models) have achieved state of the art performance on a diverse set of NLP tasks after fine tuning.",have,[],[],achieved,achieved,[],[],have,and,"['PROPN', 'PROPN', 'SCONJ']","['BERT', 'BERT', 'as']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,0,"Large pretrained natural language representations such as BERT, ALBERT, and other variants (BERT-models) have achieved state of the art performance on a diverse set of NLP tasks after fine tuning.","['Large', 'pretrained', 'natural', 'language', 'representations', 'such', 'as', 'BERT,', 'AL', 'BERT', ',', 'and', 'other', 'variants', '(BERT-models)', 'have', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'a', 'diverse', 'set', 'of', 'NLP', 'tasks', 'after', 'fine', 'tuning.']","(9, 10)"
87,"Large pretrained natural language representations such as BERT, ALBERT, and other variants (",BERT,-models) have achieved state of the art performance on a diverse set of NLP tasks after fine tuning.,have,[],[],achieved,have achieved,[],[],have,models,"['PROPN', 'PUNCT', 'ADJ']","['BERT', ')', 'other']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,0,"Large pretrained natural language representations such as BERT, ALBERT, and other variants (BERT-models) have achieved state of the art performance on a diverse set of NLP tasks after fine tuning.","['Large', 'pretrained', 'natural', 'language', 'representations', 'such', 'as', 'BERT,', 'ALBERT,', 'and', 'other', 'variants', '(', 'BERT', '-models)', 'have', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'a', 'diverse', 'set', 'of', 'NLP', 'tasks', 'after', 'fine', 'tuning.']","(13, 14)"
88,This suggests these,BERT,"-models learn to extract signal-rich, transferable language features.",suggests,"['This', 'these BERT -models']",[],suggests,suggests,"['This', 'these BERT -models']",[],learn,models,"['PROPN', 'NOUN', 'VERB']","['BERT', '-models', 'suggests']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,1,"This suggests these BERT-models learn to extract signal-rich, transferable language features.","['This', 'suggests', 'these', 'BERT', '-models', 'learn', 'to', 'extract', 'signal-rich,', 'transferable', 'language', 'features.']","(3, 4)"
89,We investigate whether there is an appreciable difference in feature quality at various depths of pretrained and fine tuned,BERT,"-models by training softmax regression probes to perform a semantic level task, question answering (QA) on SQuAD 2.0.",is,"['there', 'an appreciable difference in feature quality at various depths of pretrained and fine tuned BERT -models by training softmax regression probes']",[],question,question,[],[],training,models,"['NOUN', 'NOUN', 'ADP']","['BERT', 'quality', 'in']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,2,"We investigate whether there is an appreciable difference in feature quality at various depths of pretrained and fine tuned BERT-models by training softmax regression probes to perform a semantic level task, question answering (QA) on SQuAD 2.0.","['We', 'investigate', 'whether', 'there', 'is', 'an', 'appreciable', 'difference', 'in', 'feature', 'quality', 'at', 'various', 'depths', 'of', 'pretrained', 'and', 'fine', 'tuned', 'BERT', '-models', 'by', 'training', 'softmax', 'regression', 'probes', 'to', 'perform', 'a', 'semantic', 'level', 'task,', 'question', 'answering', '(QA)', 'on', 'SQuAD', '2.0.']","(19, 20)"
90,We find that the feature quality of fine tuned,BERT,"-models improves with each successive layer, while the feature quality of pretrained BERT-models remains constant.",improves,['the feature quality of fine tuned BERT -models'],[],find,find,['We'],[],improves,models,"['PROPN', 'ADP', 'NOUN']","['BERT', 'of', 'quality']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,3,"We find that the feature quality of fine tuned BERT-models improves with each successive layer, while the feature quality of pretrained BERT-models remains constant.","['We', 'find', 'that', 'the', 'feature', 'quality', 'of', 'fine', 'tuned', 'BERT', '-models', 'improves', 'with', 'each', 'successive', 'layer,', 'while', 'the', 'feature', 'quality', 'of', 'pretrained', 'BERT-models', 'remains', 'constant.']","(9, 10)"
91,"We find that the feature quality of fine tuned BERT-models improves with each successive layer, while the feature quality of pretrained",BERT,-models remains constant.,improves,['the feature quality of fine tuned BERT - models'],[],find,find,['We'],[],remains,models,"['PROPN', 'NOUN', 'ADP']","['BERT', '-models', 'of']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,3,"We find that the feature quality of fine tuned BERT-models improves with each successive layer, while the feature quality of pretrained BERT-models remains constant.","['We', 'find', 'that', 'the', 'feature', 'quality', 'of', 'fine', 'tuned', 'BERT-models', 'improves', 'with', 'each', 'successive', 'layer,', 'while', 'the', 'feature', 'quality', 'of', 'pretrained', 'BERT', '-models', 'remains', 'constant.']","(21, 22)"
92,We also find that pretrained,BERT,"-models feature quality is relatively poor in comparison to high layers in the fine tuned models, suggesting that the fine tuning process is key for extracting high quality features.",is,['pretrained BERT -models feature quality'],[],find,find,['We'],[],is,models,"['PROPN', 'NOUN', 'AUX']","['BERT', 'quality', 'is']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,4,"We also find that pretrained BERT-models feature quality is relatively poor in comparison to high layers in the fine tuned models, suggesting that the fine tuning process is key for extracting high quality features.","['We', 'also', 'find', 'that', 'pretrained', 'BERT', '-models', 'feature', 'quality', 'is', 'relatively', 'poor', 'in', 'comparison', 'to', 'high', 'layers', 'in', 'the', 'fine', 'tuned', 'models,', 'suggesting', 'that', 'the', 'fine', 'tuning', 'process', 'is', 'key', 'for', 'extracting', 'high', 'quality', 'features.']","(5, 6)"
93,One particularly interesting finding is that the early to middle layers in fine tuned,BERT,"-models begin to perform well on questions with answers, at the cost of performance on questions with no answer.",begin,['the early to middle layers in fine tuned BERT -models'],[],is,is,['One particularly interesting'],[],begin,models,"['PROPN', 'NOUN', 'VERB']","['BERT', '-models', 'begin']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,5,"One particularly interesting finding is that the early to middle layers in fine tuned BERT-models begin to perform well on questions with answers, at the cost of performance on questions with no answer.","['One', 'particularly', 'interesting', 'finding', 'is', 'that', 'the', 'early', 'to', 'middle', 'layers', 'in', 'fine', 'tuned', 'BERT', '-models', 'begin', 'to', 'perform', 'well', 'on', 'questions', 'with', 'answers,', 'at', 'the', 'cost', 'of', 'performance', 'on', 'questions', 'with', 'no', 'answer.']","(14, 15)"
94,Higher layers in fine tuned,BERT,-models are able to perform well on both questions with and without answers.,,,,able,are able,"['Higher', 'layers in fine tuned BERT -models']",[],are,models,"['PROPN', 'NOUN', 'ADP']","['BERT', '-models', 'in']",https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,6,Higher layers in fine tuned BERT-models are able to perform well on both questions with and without answers.,"['Higher', 'layers', 'in', 'fine', 'tuned', 'BERT', '-models', 'are', 'able', 'to', 'perform', 'well', 'on', 'both', 'questions', 'with', 'and', 'without', 'answers.']","(5, 6)"
95,Code available at https://github.com/travismcguire/cs224nfinalproject,,,,,,Code,Code,[],[],,,,,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Abstract,7,Code available at https://github.com/travismcguire/cs224nfinalproject,"['Code', 'available', 'at', 'https://github.com/travismcguire/cs224nfinalproject']",
96,Transferability of Contextual Representations for Question Answering.,,,,,,Transferability,Transferability,[],[],,,,,https://www.semanticscholar.org/paper/Transferability-of-Contextual-Representations-for-McGuire-Bhardwaj/e15dc8c8ca010a12b8b4c2edf16b942d108d181e,10,Title,0,Transferability of Contextual Representations for Question Answering.,"['Transferability', 'of', 'Contextual', 'Representations', 'for', 'Question', 'Answering.']",
97,"Large pre-trained contextual word representations have transformed the field of natural language processing, obtaining impressive results on a wide range of tasks.",,,,,,Large,Large,[],[],,,,,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,0,"Large pre-trained contextual word representations have transformed the field of natural language processing, obtaining impressive results on a wide range of tasks.","['Large', 'pre-trained', 'contextual', 'word', 'representations', 'have', 'transformed', 'the', 'field', 'of', 'natural', 'language', 'processing,', 'obtaining', 'impressive', 'results', 'on', 'a', 'wide', 'range', 'of', 'tasks.']",
98,"However, as models increase in size, computational limitations make them impractical for researchers and practitioners alike.",,,,,,make,make,"['However', ',', ', computational', 'as models increase in size limitations']",[],,,,,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,1,"However, as models increase in size, computational limitations make them impractical for researchers and practitioners alike.","['However,', 'as', 'models', 'increase', 'in', 'size,', 'computational', 'limitations', 'make', 'them', 'impractical', 'for', 'researchers', 'and', 'practitioners', 'alike.']",
99,We hypothesize that contextual representations have both intrinsic and task-specific redundancies.,,,,,,We,We,[],[],,,,,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,2,We hypothesize that contextual representations have both intrinsic and task-specific redundancies.,"['We', 'hypothesize', 'that', 'contextual', 'representations', 'have', 'both', 'intrinsic', 'and', 'task-specific', 'redundancies.']",
100,"We propose a novel feature selection method, which takes advantage of these redundancies to reduce the size of the pre-trained features.",,,,,,propose,propose,['We'],[],,,,,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,3,"We propose a novel feature selection method, which takes advantage of these redundancies to reduce the size of the pre-trained features.","['We', 'propose', 'a', 'novel', 'feature', 'selection', 'method,', 'which', 'takes', 'advantage', 'of', 'these', 'redundancies', 'to', 'reduce', 'the', 'size', 'of', 'the', 'pre-trained', 'features.']",
101,"In a comprehensive evaluation on two pre-trained models,",BERT,"and XLNet, using a diverse suite of sequence labeling and sequence classification tasks, our method reduces the feature set down to 1--7% of the original size, while maintaining more than 97% of the performance.",reduces,"[', using a diverse suite of sequence labeling and sequence classification tasks , our method the']",[],-,-,[],[],using,and,"['PROPN', 'NOUN', 'ADP']","['BERT', 'models', 'on']",https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Abstract,4,"In a comprehensive evaluation on two pre-trained models, BERT and XLNet, using a diverse suite of sequence labeling and sequence classification tasks, our method reduces the feature set down to 1--7% of the original size, while maintaining more than 97% of the performance.","['In', 'a', 'comprehensive', 'evaluation', 'on', 'two', 'pre-trained', 'models,', 'BERT', 'and', 'XLNet,', 'using', 'a', 'diverse', 'suite', 'of', 'sequence', 'labeling', 'and', 'sequence', 'classification', 'tasks,', 'our', 'method', 'reduces', 'the', 'feature', 'set', 'down', 'to', '1--7%', 'of', 'the', 'original', 'size,', 'while', 'maintaining', 'more', 'than', '97%', 'of', 'the', 'performance.']","(8, 9)"
102,Exploiting Redundancy in Pre-trained Language Models for Efficient Transfer Learning.,,,,,,Redundancy,Redundancy,[],[],,,,,https://www.semanticscholar.org/paper/Exploiting-Redundancy-in-Pre-trained-Language-for-Dalvi-Sajjad/dbd4944ea3098501a4486a6e8d2a79286c625e0a,11,Title,0,Exploiting Redundancy in Pre-trained Language Models for Efficient Transfer Learning.,"['Exploiting', 'Redundancy', 'in', 'Pre-trained', 'Language', 'Models', 'for', 'Efficient', 'Transfer', 'Learning.']",
103,The success of pretrained transformer language models in natural language processing has led to a wide range of different pretraining setups.,,,,,,led,has led,['The success of pretrained transformer language models in natural language processing'],[],,,,,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,0,The success of pretrained transformer language models in natural language processing has led to a wide range of different pretraining setups.,"['The', 'success', 'of', 'pretrained', 'transformer', 'language', 'models', 'in', 'natural', 'language', 'processing', 'has', 'led', 'to', 'a', 'wide', 'range', 'of', 'different', 'pretraining', 'setups.']",
104,"These models employ a variety of subword tokenization methods, most notably byte pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text.",,,,,,),),[],[],,,,,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,1,"These models employ a variety of subword tokenization methods, most notably byte pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text.","['These', 'models', 'employ', 'a', 'variety', 'of', 'subword', 'tokenization', 'methods,', 'most', 'notably', 'byte', 'pair', 'encoding', '(BPE)', '(Sennrich', 'et', 'al.,', '2016;', 'Gage,', '1994),', 'the', 'WordPiece', 'method', '(Schuster', 'and', 'Nakajima,', '2012),', 'and', 'unigram', 'language', 'modeling', '(Kudo,', '2018),', 'to', 'segment', 'text.']",
105,"However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining.",,,,,,contain,does contain,"['However', ',', 'the literature']",['a direct evaluation of the impact of tokenization on language model pretraining .'],,,,,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,2,"However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining.","['However,', 'to', 'the', 'best', 'of', 'our', 'knowledge,', 'the', 'literature', 'does', 'not', 'contain', 'a', 'direct', 'evaluation', 'of', 'the', 'impact', 'of', 'tokenization', 'on', 'language', 'model', 'pretraining.']",
106,"First, we analyze differences between BPE and unigram LM tokenization, and find that the unigram LM method is able to recover subword units that more strongly align with underlying morphology, in addition to avoiding several shortcomings of BPE stemming from its greedy construction procedure.",,,,,,analyze,analyze,['we'],"['differences between BPE and unigram LM tokenization ,']",,,,,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,3,"First, we analyze differences between BPE and unigram LM tokenization, and find that the unigram LM method is able to recover subword units that more strongly align with underlying morphology, in addition to avoiding several shortcomings of BPE stemming from its greedy construction procedure.","['First,', 'we', 'analyze', 'differences', 'between', 'BPE', 'and', 'unigram', 'LM', 'tokenization,', 'and', 'find', 'that', 'the', 'unigram', 'LM', 'method', 'is', 'able', 'to', 'recover', 'subword', 'units', 'that', 'more', 'strongly', 'align', 'with', 'underlying', 'morphology,', 'in', 'addition', 'to', 'avoiding', 'several', 'shortcomings', 'of', 'BPE', 'stemming', 'from', 'its', 'greedy', 'construction', 'procedure.']",
107,We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations.,,,,,,compare,compare,"['We', 'then']",['the fine - tuned task performance of identical transformer masked language models pretrained with these tokenizations .'],,,,,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,4,We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations.,"['We', 'then', 'compare', 'the', 'fine-tuned', 'task', 'performance', 'of', 'identical', 'transformer', 'masked', 'language', 'models', 'pretrained', 'with', 'these', 'tokenizations.']",
108,"Across downstream tasks, we find that the unigram LM tokenization method consistently matches or outperforms BPE.",,,,,,find,find,['we'],[],,,,,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,5,"Across downstream tasks, we find that the unigram LM tokenization method consistently matches or outperforms BPE.","['Across', 'downstream', 'tasks,', 'we', 'find', 'that', 'the', 'unigram', 'LM', 'tokenization', 'method', 'consistently', 'matches', 'or', 'outperforms', 'BPE.']",
109,We hope that developers of future pretrained language models will consider adopting the unigram LM method over the more common BPE.,,,,,,hope,hope,['We'],[],,,,,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Abstract,6,We hope that developers of future pretrained language models will consider adopting the unigram LM method over the more common BPE.,"['We', 'hope', 'that', 'developers', 'of', 'future', 'pretrained', 'language', 'models', 'will', 'consider', 'adopting', 'the', 'unigram', 'LM', 'method', 'over', 'the', 'more', 'common', 'BPE.']",
110,Byte Pair Encoding is Suboptimal for Language Model Pretraining.,,,,,,Suboptimal,is Suboptimal,['Byte Pair Encoding'],[],,,,,https://www.semanticscholar.org/paper/Byte-Pair-Encoding-is-Suboptimal-for-Language-Model-Bostrom-Durrett/bd91623adf62a71252a96fa9487e5338d343599d,12,Title,0,Byte Pair Encoding is Suboptimal for Language Model Pretraining.,"['Byte', 'Pair', 'Encoding', 'is', 'Suboptimal', 'for', 'Language', 'Model', 'Pretraining.']",
111,A recent introduction of Transformer deep learning architecture made breakthroughs in various natural language processing tasks.,,,,,,made,made,[],[],,,,,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,0,A recent introduction of Transformer deep learning architecture made breakthroughs in various natural language processing tasks.,"['A', 'recent', 'introduction', 'of', 'Transformer', 'deep', 'learning', 'architecture', 'made', 'breakthroughs', 'in', 'various', 'natural', 'language', 'processing', 'tasks.']",
112,"However, non-English languages could not leverage such new opportunities with the English text pre-trained models.",,,,,,leverage,could leverage,"['However , non - English languages']",['such new opportunities'],,,,,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,1,"However, non-English languages could not leverage such new opportunities with the English text pre-trained models.","['However,', 'non-English', 'languages', 'could', 'not', 'leverage', 'such', 'new', 'opportunities', 'with', 'the', 'English', 'text', 'pre-trained', 'models.']",
113,"This changed with research focusing on multilingual models, where less-spoken languages are the main beneficiaries.",,,,,,beneficiaries,are beneficiaries,[],[],,,,,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,2,"This changed with research focusing on multilingual models, where less-spoken languages are the main beneficiaries.","['This', 'changed', 'with', 'research', 'focusing', 'on', 'multilingual', 'models,', 'where', 'less-spoken', 'languages', 'are', 'the', 'main', 'beneficiaries.']",
114,We compare pre-trained multilingual,BERT,", XLM-R, and older learned text representation methods as encodings for the task of Lithuanian news clustering.",pre,[],[],as,as,[],[],learned,XLM-R,"['NOUN', 'ADJ', 'VERB']","['BERT', 'trained', 'pre']",https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,3,"We compare pre-trained multilingual BERT, XLM-R, and older learned text representation methods as encodings for the task of Lithuanian news clustering.","['We', 'compare', 'pre-trained', 'multilingual', 'BERT', ',', 'XLM-R,', 'and', 'older', 'learned', 'text', 'representation', 'methods', 'as', 'encodings', 'for', 'the', 'task', 'of', 'Lithuanian', 'news', 'clustering.']","(4, 5)"
115,Our results indicate that publicly available pre-trained multilingual Transformer models can be fine-tuned to surpass word vectors but still score much lower than specially trained doc2vec embeddings.,,,,,,trained,trained,[],[],,,,,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Abstract,4,Our results indicate that publicly available pre-trained multilingual Transformer models can be fine-tuned to surpass word vectors but still score much lower than specially trained doc2vec embeddings.,"['Our', 'results', 'indicate', 'that', 'publicly', 'available', 'pre-trained', 'multilingual', 'Transformer', 'models', 'can', 'be', 'fine-tuned', 'to', 'surpass', 'word', 'vectors', 'but', 'still', 'score', 'much', 'lower', 'than', 'specially', 'trained', 'doc2vec', 'embeddings.']",
116,Testing pre-trained Transformer models for Lithuanian news clustering.,,,,,,models,models,[],[],,,,,https://www.semanticscholar.org/paper/Testing-pre-trained-Transformer-models-for-news-Stankevivcius-Lukovsevivcius/eb71979689ba655ba9f4d7077f6184c4a1b51c05,13,Title,0,Testing pre-trained Transformer models for Lithuanian news clustering.,"['Testing', 'pre-trained', 'Transformer', 'models', 'for', 'Lithuanian', 'news', 'clustering.']",
117,"Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence.",,,,,,achieved,have achieved,['Textual representation learners trained on large amounts of data'],['notable success'],,,,,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,0,"Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence.","['Textual', 'representation', 'learners', 'trained', 'on', 'large', 'amounts', 'of', 'data', 'have', 'achieved', 'notable', 'success', 'on', 'downstream', 'tasks;', 'intriguingly,', 'they', 'have', 'also', 'performed', 'well', 'on', 'challenging', 'tests', 'of', 'syntactic', 'competence.']",
118,"Given this success, it remains an open question whether scalable learners like",BERT,"can become fully proficient in the syntax of natural language by virtue of data scale alone, or whether they still benefit from more explicit syntactic biases.",can,['scalable learners like BERT'],[],Given,Given,[],[],become,can,"['PROPN', 'SCONJ', 'NOUN']","['BERT', 'like', 'learners']",https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,1,"Given this success, it remains an open question whether scalable learners like BERT can become fully proficient in the syntax of natural language by virtue of data scale alone, or whether they still benefit from more explicit syntactic biases.","['Given', 'this', 'success,', 'it', 'remains', 'an', 'open', 'question', 'whether', 'scalable', 'learners', 'like', 'BERT', 'can', 'become', 'fully', 'proficient', 'in', 'the', 'syntax', 'of', 'natural', 'language', 'by', 'virtue', 'of', 'data', 'scale', 'alone,', 'or', 'whether', 'they', 'still', 'benefit', 'from', 'more', 'explicit', 'syntactic', 'biases.']","(12, 13)"
119,"To answer this question, we introduce a knowledge distillation strategy for injecting syntactic biases into",BERT,"pretraining, by distilling the syntactically informative predictions of a hierarchical---albeit harder to scale---syntactic language model.",injecting,[],['syntactic biases'],question,question,['this'],[],distilling,pretraining,"['PROPN', 'NOUN', 'ADP']","['BERT', 'pretraining', 'into']",https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,2,"To answer this question, we introduce a knowledge distillation strategy for injecting syntactic biases into BERT pretraining, by distilling the syntactically informative predictions of a hierarchical---albeit harder to scale---syntactic language model.","['To', 'answer', 'this', 'question,', 'we', 'introduce', 'a', 'knowledge', 'distillation', 'strategy', 'for', 'injecting', 'syntactic', 'biases', 'into', 'BERT', 'pretraining,', 'by', 'distilling', 'the', 'syntactically', 'informative', 'predictions', 'of', 'a', 'hierarchical---albeit', 'harder', 'to', 'scale---syntactic', 'language', 'model.']","(15, 16)"
120,Since,BERT,"models masked words in bidirectional context, we propose to distill the approximate marginal distribution over words in context from the syntactic LM.",propose,"['Since BERT models masked words in bidirectional context', ',', 'we']",[],propose,propose,"['Since BERT models masked words in bidirectional context', ',', 'we']",[],masked,models,"['PROPN', 'NOUN', 'VERB']","['BERT', 'models', 'propose']",https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,3,"Since BERT models masked words in bidirectional context, we propose to distill the approximate marginal distribution over words in context from the syntactic LM.","['Since', 'BERT', 'models', 'masked', 'words', 'in', 'bidirectional', 'context,', 'we', 'propose', 'to', 'distill', 'the', 'approximate', 'marginal', 'distribution', 'over', 'words', 'in', 'context', 'from', 'the', 'syntactic', 'LM.']","(1, 2)"
121,"Our approach reduces relative error by 2-21% on a diverse set of structured prediction tasks, although we obtain mixed results on the GLUE benchmark.",,,,,,reduces,reduces,['Our approach'],[],,,,,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,4,"Our approach reduces relative error by 2-21% on a diverse set of structured prediction tasks, although we obtain mixed results on the GLUE benchmark.","['Our', 'approach', 'reduces', 'relative', 'error', 'by', '2-21%', 'on', 'a', 'diverse', 'set', 'of', 'structured', 'prediction', 'tasks,', 'although', 'we', 'obtain', 'mixed', 'results', 'on', 'the', 'GLUE', 'benchmark.']",
122,"Our findings demonstrate the benefits of syntactic biases, even in representation learners that exploit large amounts of data, and contribute to a better understanding of where syntactic biases are most helpful in benchmarks of natural language understanding.",,,,,,demonstrate,demonstrate,"['Our findings', 'the benefits of syntactic biases even in representation learners that exploit large amounts of data , and contribute to a better understanding of where syntactic biases']",[],,,,,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Abstract,5,"Our findings demonstrate the benefits of syntactic biases, even in representation learners that exploit large amounts of data, and contribute to a better understanding of where syntactic biases are most helpful in benchmarks of natural language understanding.","['Our', 'findings', 'demonstrate', 'the', 'benefits', 'of', 'syntactic', 'biases,', 'even', 'in', 'representation', 'learners', 'that', 'exploit', 'large', 'amounts', 'of', 'data,', 'and', 'contribute', 'to', 'a', 'better', 'understanding', 'of', 'where', 'syntactic', 'biases', 'are', 'most', 'helpful', 'in', 'benchmarks', 'of', 'natural', 'language', 'understanding.']",
123,Syntactic Structure Distillation Pretraining For Bidirectional Encoders.,,,,,,Structure,Structure,[],[],,,,,https://www.semanticscholar.org/paper/Syntactic-Structure-Distillation-Pretraining-For-Kuncoro-Kong/0b46915313f95a0112564fdfd2baaa2bcf36565a,14,Title,0,Syntactic Structure Distillation Pretraining For Bidirectional Encoders.,"['Syntactic', 'Structure', 'Distillation', 'Pretraining', 'For', 'Bidirectional', 'Encoders.']",
124,"The notion of ""in-domain data"" in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality.",,,,,,simplistic,simplistic,[],[],,,,,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,0,"The notion of ""in-domain data"" in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality.","['The', 'notion', 'of', '""in-domain', 'data""', 'in', 'NLP', 'is', 'often', 'over-simplistic', 'and', 'vague,', 'as', 'textual', 'data', 'varies', 'in', 'many', 'nuanced', 'linguistic', 'aspects', 'such', 'as', 'topic,', 'style', 'or', 'level', 'of', 'formality.']",
125,"In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems.",,,,,,unavailable,are unavailable,"[',']",[],,,,,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,1,"In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems.","['In', 'addition,', 'domain', 'labels', 'are', 'many', 'times', 'unavailable,', 'making', 'it', 'challenging', 'to', 'build', 'domain-specific', 'systems.']",
126,We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision -- suggesting a simple data-driven definition of domains in textual data.,,,,,,show,show,['We'],[],,,,,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,2,We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision -- suggesting a simple data-driven definition of domains in textual data.,"['We', 'show', 'that', 'massive', 'pre-trained', 'language', 'models', 'implicitly', 'learn', 'sentence', 'representations', 'that', 'cluster', 'by', 'domains', 'without', 'supervision', '--', 'suggesting', 'a', 'simple', 'data-driven', 'definition', 'of', 'domains', 'in', 'textual', 'data.']",
127,"We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data.",,,,,,harness,harness,[],[],,,,,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,3,"We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data.","['We', 'harness', 'this', 'property', 'and', 'propose', 'domain', 'data', 'selection', 'methods', 'based', 'on', 'such', 'models,', 'which', 'require', 'only', 'a', 'small', 'set', 'of', 'in-domain', 'monolingual', 'data.']",
128,"We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and by precision and recall of sentence selection with respect to an oracle.",,,,,,with,with,[],[],,,,,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Abstract,4,"We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and by precision and recall of sentence selection with respect to an oracle.","['We', 'evaluate', 'our', 'data', 'selection', 'methods', 'for', 'neural', 'machine', 'translation', 'across', 'five', 'diverse', 'domains,', 'where', 'they', 'outperform', 'an', 'established', 'approach', 'as', 'measured', 'by', 'both', 'BLEU', 'and', 'by', 'precision', 'and', 'recall', 'of', 'sentence', 'selection', 'with', 'respect', 'to', 'an', 'oracle.']",
129,Unsupervised Domain Clusters in Pretrained Language Models.,,,,,,Domain,Domain,[],[],,,,,https://www.semanticscholar.org/paper/Unsupervised-Domain-Clusters-in-Pretrained-Language-Aharoni-Goldberg/95856e0789481eedc2cedc413581a0a819ef8fc8,15,Title,0,Unsupervised Domain Clusters in Pretrained Language Models.,"['Unsupervised', 'Domain', 'Clusters', 'in', 'Pretrained', 'Language', 'Models.']",
130,"Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of a trained model to downstream natural language processing tasks, such as named entity recognition (NER) and question answering.",,,,,,named,named,[],[],,,,,https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,0,"Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of a trained model to downstream natural language processing tasks, such as named entity recognition (NER) and question answering.","['Recent', 'advances', 'in', 'language', 'representation', 'using', 'neural', 'networks', 'have', 'made', 'it', 'viable', 'to', 'transfer', 'the', 'learned', 'internal', 'states', 'of', 'a', 'trained', 'model', 'to', 'downstream', 'natural', 'language', 'processing', 'tasks,', 'such', 'as', 'named', 'entity', 'recognition', '(NER)', 'and', 'question', 'answering.']",
131,It has been shown that the leverage of pre-trained language models improves the overall performance on many tasks and is highly beneficial when labeled data is scarce.,,,,,,shown,has been shown,['It'],[],,,,,https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,1,It has been shown that the leverage of pre-trained language models improves the overall performance on many tasks and is highly beneficial when labeled data is scarce.,"['It', 'has', 'been', 'shown', 'that', 'the', 'leverage', 'of', 'pre-trained', 'language', 'models', 'improves', 'the', 'overall', 'performance', 'on', 'many', 'tasks', 'and', 'is', 'highly', 'beneficial', 'when', 'labeled', 'data', 'is', 'scarce.']",
132,"In this work, we employ a pre-trained",BERT,"with Conditional Random Fields (CRF) architecture to the NER task on the Portuguese language, combining the transfer capabilities of BERT with the structured predictions of CRF.",employ,['we'],['a pre - trained BERT with'],employ,employ,['we'],['a pre - trained BERT with'],combining,with,"['PROPN', 'VERB']","['BERT', 'employ']",https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,2,"In this work, we employ a pre-trained BERT with Conditional Random Fields (CRF) architecture to the NER task on the Portuguese language, combining the transfer capabilities of BERT with the structured predictions of CRF.","['In', 'this', 'work,', 'we', 'employ', 'a', 'pre-trained', 'BERT', 'with', 'Conditional', 'Random', 'Fields', '(CRF)', 'architecture', 'to', 'the', 'NER', 'task', 'on', 'the', 'Portuguese', 'language,', 'combining', 'the', 'transfer', 'capabilities', 'of', 'BERT', 'with', 'the', 'structured', 'predictions', 'of', 'CRF.']","(7, 8)"
133,"In this work, we employ a pre-trained BERT with Conditional Random Fields (CRF) architecture to the NER task on the Portuguese language, combining the transfer capabilities of",BERT,with the structured predictions of CRF.,combining,[],[],employ,employ,['we'],['a pre - trained BERT with'],,with,"['PROPN', 'ADP', 'NOUN']","['BERT', 'of', 'capabilities']",https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,2,"In this work, we employ a pre-trained BERT with Conditional Random Fields (CRF) architecture to the NER task on the Portuguese language, combining the transfer capabilities of BERT with the structured predictions of CRF.","['In', 'this', 'work,', 'we', 'employ', 'a', 'pre-trained', 'BERT', 'with', 'Conditional', 'Random', 'Fields', '(CRF)', 'architecture', 'to', 'the', 'NER', 'task', 'on', 'the', 'Portuguese', 'language,', 'combining', 'the', 'transfer', 'capabilities', 'of', 'BERT', 'with', 'the', 'structured', 'predictions', 'of', 'CRF.']","(27, 28)"
134,We explore feature-based and fine-tuning training strategies for the,BERT,model.,based,[],[],We,We,[],[],,model,"['PROPN', 'NOUN', 'ADP']","['BERT', 'model', 'for']",https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,3,We explore feature-based and fine-tuning training strategies for the BERT model.,"['We', 'explore', 'feature-based', 'and', 'fine-tuning', 'training', 'strategies', 'for', 'the', 'BERT', 'model.']","(9, 10)"
135,"Our fine-tuning approach obtains new state-of-the-art results on the HAREM I dataset, improving the F1-score by 3.2 points on the selective scenario (5 NE classes) and by 3.8 points on the total scenario (10 NE classes).",,,,,,NE,NE,[],[],,,,,https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Abstract,4,"Our fine-tuning approach obtains new state-of-the-art results on the HAREM I dataset, improving the F1-score by 3.2 points on the selective scenario (5 NE classes) and by 3.8 points on the total scenario (10 NE classes).","['Our', 'fine-tuning', 'approach', 'obtains', 'new', 'state-of-the-art', 'results', 'on', 'the', 'HAREM', 'I', 'dataset,', 'improving', 'the', 'F1-score', 'by', '3.2', 'points', 'on', 'the', 'selective', 'scenario', '(5', 'NE', 'classes)', 'and', 'by', '3.8', 'points', 'on', 'the', 'total', 'scenario', '(10', 'NE', 'classes).']",
136,Portuguese Named Entity Recognition using,BERT,-CRF.,using,[],[],Named,Named,[],[],,CRF,"['PROPN', 'PUNCT', 'VERB']","['BERT', '-CRF', 'using']",https://www.semanticscholar.org/paper/Portuguese-Named-Entity-Recognition-using-BERT-CRF-Souza-Nogueira/bd729e1e094ebab5a1b19aa2a4d331711ab1760f,16,Title,0,Portuguese Named Entity Recognition using BERT-CRF.,"['Portuguese', 'Named', 'Entity', 'Recognition', 'using', 'BERT', '-CRF.']","(5, 6)"
137,Transformer based Very Large Language Models (VLLMs) like,BERT,", XLNet and RoBERTa, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks.",based,[],[],shown,have shown,"['Transformer based Very Large Language Models ( VLLMs ) like BERT , XLNet and RoBERTa ,']",[],have,XLNet,"['PROPN', 'SCONJ', 'PUNCT']","['BERT', 'like', ')']",https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,0,"Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and RoBERTa, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks.","['Transformer', 'based', 'Very', 'Large', 'Language', 'Models', '(VLLMs)', 'like', 'BERT', ',', 'XLNet', 'and', 'RoBERTa,', 'have', 'recently', 'shown', 'tremendous', 'performance', 'on', 'a', 'large', 'variety', 'of', 'Natural', 'Language', 'Understanding', '(NLU)', 'tasks.']","(8, 9)"
138,"Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and Ro",BERT,"a, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks.",Transformer,[],[],shown,shown,"['Transformer based Very Large Language Models ( VLLMs ) like BERT , XLNet and Ro BERT a ,']",[],have,a,"['PROPN', 'DET', 'CCONJ']","['BERT', 'a', 'and']",https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,0,"Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and RoBERTa, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks.","['Transformer', 'based', 'Very', 'Large', 'Language', 'Models', '(VLLMs)', 'like', 'BERT,', 'XLNet', 'and', 'Ro', 'BERT', 'a,', 'have', 'recently', 'shown', 'tremendous', 'performance', 'on', 'a', 'large', 'variety', 'of', 'Natural', 'Language', 'Understanding', '(NLU)', 'tasks.']","(12, 13)"
139,"However, due to their size, these VLLMs are extremely resource intensive and cumbersome to deploy at production time.",,,,,,resource,are resource,"['However , due to their size , these VLLMs']",[],,,,,https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,1,"However, due to their size, these VLLMs are extremely resource intensive and cumbersome to deploy at production time.","['However,', 'due', 'to', 'their', 'size,', 'these', 'VLLMs', 'are', 'extremely', 'resource', 'intensive', 'and', 'cumbersome', 'to', 'deploy', 'at', 'production', 'time.']",
140,Several recent publications have looked into various ways to distil knowledge from a transformer based VLLM (most commonly,BERT,-Base) into a smaller model which can run much faster at inference time.,based,[],[],looked,have looked,['Several recent publications'],[],run,Base,"['PROPN', 'PUNCT', 'VERB']","['BERT', '(', 'based']",https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,2,Several recent publications have looked into various ways to distil knowledge from a transformer based VLLM (most commonly BERT-Base) into a smaller model which can run much faster at inference time.,"['Several', 'recent', 'publications', 'have', 'looked', 'into', 'various', 'ways', 'to', 'distil', 'knowledge', 'from', 'a', 'transformer', 'based', 'VLLM', '(most', 'commonly', 'BERT', '-Base)', 'into', 'a', 'smaller', 'model', 'which', 'can', 'run', 'much', 'faster', 'at', 'inference', 'time.']","(18, 19)"
141,"Here, we propose a novel set of techniques which together produce a task-specific hybrid convolutional and transformer model, WaLDORf, that achieves state-of-the-art inference speed while still being more accurate than previous distilled models.",,,,,,propose,propose,"[',', 'we']",['a novel set of techniques'],,,,,https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Abstract,3,"Here, we propose a novel set of techniques which together produce a task-specific hybrid convolutional and transformer model, WaLDORf, that achieves state-of-the-art inference speed while still being more accurate than previous distilled models.","['Here,', 'we', 'propose', 'a', 'novel', 'set', 'of', 'techniques', 'which', 'together', 'produce', 'a', 'task-specific', 'hybrid', 'convolutional', 'and', 'transformer', 'model,', 'WaLDORf,', 'that', 'achieves', 'state-of-the-art', 'inference', 'speed', 'while', 'still', 'being', 'more', 'accurate', 'than', 'previous', 'distilled', 'models.']",
142,WaLDORf: Wasteless Language-model Distillation On Reading-comprehension.,,,,,,WaLDORf,WaLDORf,[],[],,,,,https://www.semanticscholar.org/paper/WaLDORf%3A-Wasteless-Language-model-Distillation-On-Tian-Kreuzer/1e447e3e3584dbcccf7904bf17c7c0e6b1d2f976,17,Title,0,WaLDORf: Wasteless Language-model Distillation On Reading-comprehension.,"['WaLDORf:', 'Wasteless', 'Language-model', 'Distillation', 'On', 'Reading-comprehension.']",
143,"Many visual scenes contain text that carries crucial information, and it is thus essential to understand text in images for downstream reasoning tasks.",,,,,,text,text,[],[],,,,,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,0,"Many visual scenes contain text that carries crucial information, and it is thus essential to understand text in images for downstream reasoning tasks.","['Many', 'visual', 'scenes', 'contain', 'text', 'that', 'carries', 'crucial', 'information,', 'and', 'it', 'is', 'thus', 'essential', 'to', 'understand', 'text', 'in', 'images', 'for', 'downstream', 'reasoning', 'tasks.']",
144,"For example, a deep water label on a warning sign warns people about the danger in the scene.",,,,,,warns,warns,"[', a deep water label on a warning', 'sign', 'people about the danger in the scene']",[],,,,,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,1,"For example, a deep water label on a warning sign warns people about the danger in the scene.","['For', 'example,', 'a', 'deep', 'water', 'label', 'on', 'a', 'warning', 'sign', 'warns', 'people', 'about', 'the', 'danger', 'in', 'the', 'scene.']",
145,Recent work has explored the TextVQA task that requires reading and understanding text in images to answer a question.,,,,,,explored,has explored,"['Recent', 'work', 'the TextVQA task that requires reading and understanding text in images to answer a question']",[],,,,,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,2,Recent work has explored the TextVQA task that requires reading and understanding text in images to answer a question.,"['Recent', 'work', 'has', 'explored', 'the', 'TextVQA', 'task', 'that', 'requires', 'reading', 'and', 'understanding', 'text', 'in', 'images', 'to', 'answer', 'a', 'question.']",
146,"However, existing approaches for TextVQA are mostly based on custom pairwise fusion mechanisms between a pair of two modalities and are restricted to a single prediction step by casting TextVQA as a classification task.",,,,,,mostly,are mostly,"['However , existing approaches for TextVQA']",[],,,,,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,3,"However, existing approaches for TextVQA are mostly based on custom pairwise fusion mechanisms between a pair of two modalities and are restricted to a single prediction step by casting TextVQA as a classification task.","['However,', 'existing', 'approaches', 'for', 'TextVQA', 'are', 'mostly', 'based', 'on', 'custom', 'pairwise', 'fusion', 'mechanisms', 'between', 'a', 'pair', 'of', 'two', 'modalities', 'and', 'are', 'restricted', 'to', 'a', 'single', 'prediction', 'step', 'by', 'casting', 'TextVQA', 'as', 'a', 'classification', 'task.']",
147,"In this work, we propose a novel model for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images.",,,,,,propose,propose,['we'],[],,,,,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,4,"In this work, we propose a novel model for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images.","['In', 'this', 'work,', 'we', 'propose', 'a', 'novel', 'model', 'for', 'the', 'TextVQA', 'task', 'based', 'on', 'a', 'multimodal', 'transformer', 'architecture', 'accompanied', 'by', 'a', 'rich', 'representation', 'for', 'text', 'in', 'images.']",
148,Our model naturally fuses different modalities homogeneously by embedding them into a common semantic space where self-attention is applied to model inter- and intra- modality context.,,,,,,context,context,[],[],,,,,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,5,Our model naturally fuses different modalities homogeneously by embedding them into a common semantic space where self-attention is applied to model inter- and intra- modality context.,"['Our', 'model', 'naturally', 'fuses', 'different', 'modalities', 'homogeneously', 'by', 'embedding', 'them', 'into', 'a', 'common', 'semantic', 'space', 'where', 'self-attention', 'is', 'applied', 'to', 'model', 'inter-', 'and', 'intra-', 'modality', 'context.']",
149,"Furthermore, it enables iterative answer decoding with a dynamic pointer network, allowing the model to form an answer through multi-step prediction instead of one-step classification.",,,,,,enables,enables,"['it', 'iterative']",[],,,,,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,6,"Furthermore, it enables iterative answer decoding with a dynamic pointer network, allowing the model to form an answer through multi-step prediction instead of one-step classification.","['Furthermore,', 'it', 'enables', 'iterative', 'answer', 'decoding', 'with', 'a', 'dynamic', 'pointer', 'network,', 'allowing', 'the', 'model', 'to', 'form', 'an', 'answer', 'through', 'multi-step', 'prediction', 'instead', 'of', 'one-step', 'classification.']",
150,Our model outperforms existing approaches on three benchmark datasets for the TextVQA task by a large margin.,,,,,,outperforms,outperforms,['Our model'],[],,,,,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Abstract,7,Our model outperforms existing approaches on three benchmark datasets for the TextVQA task by a large margin.,"['Our', 'model', 'outperforms', 'existing', 'approaches', 'on', 'three', 'benchmark', 'datasets', 'for', 'the', 'TextVQA', 'task', 'by', 'a', 'large', 'margin.']",
151,Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA.,,,,,,Answer,Answer,[],[],,,,,https://www.semanticscholar.org/paper/Iterative-Answer-Prediction-with-Pointer-Augmented-Hu-Singh/4fc2eb1b494a29c7cb39da3b180a302521e459a0,18,Title,0,Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA.,"['Iterative', 'Answer', 'Prediction', 'with', 'Pointer-Augmented', 'Multimodal', 'Transformers', 'for', 'TextVQA.']",
152,Learning knowledge graph embeddings (KGEs) is an efficient approach to knowledge graph completion.,,,,,,approach,is approach,['Learning knowledge graph embeddings ( KGEs )'],[],,,,,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,0,Learning knowledge graph embeddings (KGEs) is an efficient approach to knowledge graph completion.,"['Learning', 'knowledge', 'graph', 'embeddings', '(KGEs)', 'is', 'an', 'efficient', 'approach', 'to', 'knowledge', 'graph', 'completion.']",
153,"Conventional KGEs often suffer from limited knowledge representation, which causes less accuracy especially when training on sparse knowledge graphs.",,,,,,KGEs,KGEs,[],[],,,,,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,1,"Conventional KGEs often suffer from limited knowledge representation, which causes less accuracy especially when training on sparse knowledge graphs.","['Conventional', 'KGEs', 'often', 'suffer', 'from', 'limited', 'knowledge', 'representation,', 'which', 'causes', 'less', 'accuracy', 'especially', 'when', 'training', 'on', 'sparse', 'knowledge', 'graphs.']",
154,"To remedy this, we present Pretrain-KGEs, a training framework for learning better knowledgeable entity and relation embeddings, leveraging the abundant linguistic knowledge from pretrained language models.",,,,,,present,present,"['To', 'we']",[],,,,,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,2,"To remedy this, we present Pretrain-KGEs, a training framework for learning better knowledgeable entity and relation embeddings, leveraging the abundant linguistic knowledge from pretrained language models.","['To', 'remedy', 'this,', 'we', 'present', 'Pretrain-KGEs,', 'a', 'training', 'framework', 'for', 'learning', 'better', 'knowledgeable', 'entity', 'and', 'relation', 'embeddings,', 'leveraging', 'the', 'abundant', 'linguistic', 'knowledge', 'from', 'pretrained', 'language', 'models.']",
155,"Specifically, we propose a unified approach in which we first learn entity and relation representations via pretrained language models and use the representations to initialize entity and relation embeddings for training KGE models.",,,,,,propose,propose,['we'],[],,,,,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,3,"Specifically, we propose a unified approach in which we first learn entity and relation representations via pretrained language models and use the representations to initialize entity and relation embeddings for training KGE models.","['Specifically,', 'we', 'propose', 'a', 'unified', 'approach', 'in', 'which', 'we', 'first', 'learn', 'entity', 'and', 'relation', 'representations', 'via', 'pretrained', 'language', 'models', 'and', 'use', 'the', 'representations', 'to', 'initialize', 'entity', 'and', 'relation', 'embeddings', 'for', 'training', 'KGE', 'models.']",
156,Our proposed method is model agnostic in the sense that it can be applied to any variant of KGE models.,,,,,,model,is model,['Our proposed method'],[],,,,,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,4,Our proposed method is model agnostic in the sense that it can be applied to any variant of KGE models.,"['Our', 'proposed', 'method', 'is', 'model', 'agnostic', 'in', 'the', 'sense', 'that', 'it', 'can', 'be', 'applied', 'to', 'any', 'variant', 'of', 'KGE', 'models.']",
157,"Experimental results show that our method can consistently improve results and achieve state-of-the-art performance using different KGE models such as TransE and QuatE, across four benchmark KG datasets in link prediction and triplet classification tasks.",,,,,,results,results,['Experimental'],[],,,,,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Abstract,5,"Experimental results show that our method can consistently improve results and achieve state-of-the-art performance using different KGE models such as TransE and QuatE, across four benchmark KG datasets in link prediction and triplet classification tasks.","['Experimental', 'results', 'show', 'that', 'our', 'method', 'can', 'consistently', 'improve', 'results', 'and', 'achieve', 'state-of-the-art', 'performance', 'using', 'different', 'KGE', 'models', 'such', 'as', 'TransE', 'and', 'QuatE,', 'across', 'four', 'benchmark', 'KG', 'datasets', 'in', 'link', 'prediction', 'and', 'triplet', 'classification', 'tasks.']",
158,Pretrain-KGEs: Learning Knowledge Representation from Pretrained Models for Knowledge Graph Embeddings.,,,,,,KGEs,KGEs,[],[],,,,,https://www.semanticscholar.org/paper/Pretrain-KGEs%3A-Learning-Knowledge-Representation/dbce057592a62a9954011c4c45edfeac31ebb9f5,19,Title,0,Pretrain-KGEs: Learning Knowledge Representation from Pretrained Models for Knowledge Graph Embeddings.,"['Pretrain-KGEs:', 'Learning', 'Knowledge', 'Representation', 'from', 'Pretrained', 'Models', 'for', 'Knowledge', 'Graph', 'Embeddings.']",
159,"As single-task accuracy on individual language and image tasks has improved substantially in the last few years, the long-term goal of a generally skilled agent that can both see and talk becomes more feasible to explore.",,,,,,task,task,['As'],[],,,,,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,0,"As single-task accuracy on individual language and image tasks has improved substantially in the last few years, the long-term goal of a generally skilled agent that can both see and talk becomes more feasible to explore.","['As', 'single-task', 'accuracy', 'on', 'individual', 'language', 'and', 'image', 'tasks', 'has', 'improved', 'substantially', 'in', 'the', 'last', 'few', 'years,', 'the', 'long-term', 'goal', 'of', 'a', 'generally', 'skilled', 'agent', 'that', 'can', 'both', 'see', 'and', 'talk', 'becomes', 'more', 'feasible', 'to', 'explore.']",
160,"In this work, we focus on leveraging individual language and image tasks, along with resources that incorporate both vision and language towards that objective.",,,,,,focus,focus,['we'],[],,,,,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,1,"In this work, we focus on leveraging individual language and image tasks, along with resources that incorporate both vision and language towards that objective.","['In', 'this', 'work,', 'we', 'focus', 'on', 'leveraging', 'individual', 'language', 'and', 'image', 'tasks,', 'along', 'with', 'resources', 'that', 'incorporate', 'both', 'vision', 'and', 'language', 'towards', 'that', 'objective.']",
161,We design an architecture that combines state-of-the-art Transformer and ResNeXt modules fed into a novel attentive multimodal module to produce a combined model trained on many tasks.,,,,,,architecture,architecture,[],[],,,,,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,2,We design an architecture that combines state-of-the-art Transformer and ResNeXt modules fed into a novel attentive multimodal module to produce a combined model trained on many tasks.,"['We', 'design', 'an', 'architecture', 'that', 'combines', 'state-of-the-art', 'Transformer', 'and', 'ResNeXt', 'modules', 'fed', 'into', 'a', 'novel', 'attentive', 'multimodal', 'module', 'to', 'produce', 'a', 'combined', 'model', 'trained', 'on', 'many', 'tasks.']",
162,"We provide a thorough analysis of the components of the model, and transfer performance when training on one, some, or all of the tasks.",,,,,,provide,provide,['We'],"['a thorough analysis of the components of the model , and transfer performance when training on one , some , or all of the tasks']",,,,,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,3,"We provide a thorough analysis of the components of the model, and transfer performance when training on one, some, or all of the tasks.","['We', 'provide', 'a', 'thorough', 'analysis', 'of', 'the', 'components', 'of', 'the', 'model,', 'and', 'transfer', 'performance', 'when', 'training', 'on', 'one,', 'some,', 'or', 'all', 'of', 'the', 'tasks.']",
163,"Our final models provide a single system that obtains good results on all vision and language tasks considered, and improves the state-of-the-art in image-grounded conversational applications.",,,,,,provide,provide,['Our final models'],"['a single system that obtains good results on all vision and language tasks considered ,']",,,,,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Abstract,4,"Our final models provide a single system that obtains good results on all vision and language tasks considered, and improves the state-of-the-art in image-grounded conversational applications.","['Our', 'final', 'models', 'provide', 'a', 'single', 'system', 'that', 'obtains', 'good', 'results', 'on', 'all', 'vision', 'and', 'language', 'tasks', 'considered,', 'and', 'improves', 'the', 'state-of-the-art', 'in', 'image-grounded', 'conversational', 'applications.']",
164,All-in-One Image-Grounded Conversational Agents.,,,,,,in,in,[],[],,,,,https://www.semanticscholar.org/paper/All-in-One-Image-Grounded-Conversational-Agents-Ju-Shuster/4aa5454addde1542e0d01cfc68e6f5129630964d,20,Title,0,All-in-One Image-Grounded Conversational Agents.,"['All-in-One', 'Image-Grounded', 'Conversational', 'Agents.']",
165,Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications.,,,,,,demonstrates,demonstrates,"['Recent', 'work in', 'language modeling']",[],,,,,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,0,Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications.,"['Recent', 'work', 'in', 'language', 'modeling', 'demonstrates', 'that', 'training', 'large', 'transformer', 'models', 'advances', 'the', 'state', 'of', 'the', 'art', 'in', 'Natural', 'Language', 'Processing', 'applications.']",
166,"However, very large models can be quite difficult to train due to memory constraints.",,,,,,difficult,can be difficult,"['However', ', very large models']",[],,,,,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,1,"However, very large models can be quite difficult to train due to memory constraints.","['However,', 'very', 'large', 'models', 'can', 'be', 'quite', 'difficult', 'to', 'train', 'due', 'to', 'memory', 'constraints.']",
167,"In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters.",,,,,,approach,approach,[],[],,,,,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,2,"In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters.","['In', 'this', 'work,', 'we', 'present', 'our', 'techniques', 'for', 'training', 'very', 'large', 'transformer', 'models', 'and', 'implement', 'a', 'simple,', 'efficient', 'intra-layer', 'model', 'parallel', 'approach', 'that', 'enables', 'training', 'transformer', 'models', 'with', 'billions', 'of', 'parameters.']",
168,"Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch.",,,,,,require,does not require,"['Our', 'approach']",[],,,,,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,3,"Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch.","['Our', 'approach', 'does', 'not', 'require', 'a', 'new', 'compiler', 'or', 'library', 'changes,', 'is', 'orthogonal', 'and', 'complimentary', 'to', 'pipeline', 'model', 'parallelism,', 'and', 'can', 'be', 'fully', 'implemented', 'with', 'the', 'insertion', 'of', 'a', 'few', 'communication', 'operations', 'in', 'native', 'PyTorch.']",
169,We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs.,,,,,,approach,approach,"['We', 'illustrate', 'this']",[],,,,,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,4,We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs.,"['We', 'illustrate', 'this', 'approach', 'by', 'converging', 'transformer', 'based', 'models', 'up', 'to', '8.3', 'billion', 'parameters', 'using', '512', 'GPUs.']",
170,"We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs.",,,,,,sustain,sustain,['We'],"['15.1', 'PetaFLOPs']",,,,,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,5,"We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs.","['We', 'sustain', '15.1', 'PetaFLOPs', 'across', 'the', 'entire', 'application', 'with', '76%', 'scaling', 'efficiency', 'when', 'compared', 'to', 'a', 'strong', 'single', 'GPU', 'baseline', 'that', 'sustains', '39', 'TeraFLOPs,', 'which', 'is', '30%', 'of', 'peak', 'FLOPs.']",
171,"To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to",BERT,.,train,[],[],train,train,[],[],,,"['PROPN', 'ADP', 'ADJ']","['BERT', 'to', 'similar']",https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,6,"To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT.","['To', 'demonstrate', 'that', 'large', 'language', 'models', 'can', 'further', 'advance', 'the', 'state', 'of', 'the', 'art', '(SOTA),', 'we', 'train', 'an', '8.3', 'billion', 'parameter', 'transformer', 'language', 'model', 'similar', 'to', 'GPT-2', 'and', 'a', '3.9', 'billion', 'parameter', 'model', 'similar', 'to', 'BERT', '.']","(35, 36)"
172,We show that careful attention to the placement of layer normalization in,BERT,-like models is critical to achieving increased performance as the model size grows.,show,['We'],[],show,show,['We'],[],is,like,"['PROPN', 'NOUN', 'ADP']","['BERT', 'models', 'in']",https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,7,We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows.,"['We', 'show', 'that', 'careful', 'attention', 'to', 'the', 'placement', 'of', 'layer', 'normalization', 'in', 'BERT', '-like', 'models', 'is', 'critical', 'to', 'achieving', 'increased', 'performance', 'as', 'the', 'model', 'size', 'grows.']","(12, 13)"
173,Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets.,,,,,,achieve,achieve,"['Using the GPT-2 model', 'we']",[],,,,,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,8,Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets.,"['Using', 'the', 'GPT-2', 'model', 'we', 'achieve', 'SOTA', 'results', 'on', 'the', 'WikiText103', '(10.8', 'compared', 'to', 'SOTA', 'perplexity', 'of', '15.8)', 'and', 'LAMBADA', '(66.5%', 'compared', 'to', 'SOTA', 'accuracy', 'of', '63.2%)', 'datasets.']",
174,Our,BERT,model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).,achieves,['Our BERT model'],[],results,results,[],[],compared,model,"['PROPN', 'NOUN', 'VERB']","['BERT', 'model', 'achieves']",https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Abstract,9,Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).,"['Our', 'BERT', 'model', 'achieves', 'SOTA', 'results', 'on', 'the', 'RACE', 'dataset', '(90.9%', 'compared', 'to', 'SOTA', 'accuracy', 'of', '89.4%).']","(1, 2)"
175,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.,,,,,,LM,LM,[],[],,,,,https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/2688c500c389feeebb1af3a3d8b3ffee680b6814,21,Title,0,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.,"['Megatron-LM:', 'Training', 'Multi-Billion', 'Parameter', 'Language', 'Models', 'Using', 'Model', 'Parallelism.']",
176,"Transfer learning has become the de facto standard in computer vision and natural language processing, especially where labeled data is scarce.",,,,,,Transfer,Transfer has,[],[],,,,,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,0,"Transfer learning has become the de facto standard in computer vision and natural language processing, especially where labeled data is scarce.","['Transfer', 'learning', 'has', 'become', 'the', 'de', 'facto', 'standard', 'in', 'computer', 'vision', 'and', 'natural', 'language', 'processing,', 'especially', 'where', 'labeled', 'data', 'is', 'scarce.']",
177,Accuracy can be significantly improved by using pre-trained models and subsequent fine-tuning.,,,,,,significantly,can be significantly,['Accuracy'],[],,,,,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,1,Accuracy can be significantly improved by using pre-trained models and subsequent fine-tuning.,"['Accuracy', 'can', 'be', 'significantly', 'improved', 'by', 'using', 'pre-trained', 'models', 'and', 'subsequent', 'fine-tuning.']",
178,"In visual reasoning tasks, such as image question answering, transfer learning is more complex.",,,,,,complex,is complex,[],[],,,,,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,2,"In visual reasoning tasks, such as image question answering, transfer learning is more complex.","['In', 'visual', 'reasoning', 'tasks,', 'such', 'as', 'image', 'question', 'answering,', 'transfer', 'learning', 'is', 'more', 'complex.']",
179,"In addition to transferring the capability to recognize visual features, we also expect to transfer the system's ability to reason.",,,,,,expect,expect,['we'],[],,,,,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,3,"In addition to transferring the capability to recognize visual features, we also expect to transfer the system's ability to reason.","['In', 'addition', 'to', 'transferring', 'the', 'capability', 'to', 'recognize', 'visual', 'features,', 'we', 'also', 'expect', 'to', 'transfer', 'the', ""system's"", 'ability', 'to', 'reason.']",
180,"Moreover, for video data, temporal reasoning adds another dimension.",,,,,,adds,adds,"['Moreover , for video data ,', 'temporal reasoning']",[],,,,,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,4,"Moreover, for video data, temporal reasoning adds another dimension.","['Moreover,', 'for', 'video', 'data,', 'temporal', 'reasoning', 'adds', 'another', 'dimension.']",
181,"In this work, we formalize these unique aspects of transfer learning and propose a theoretical framework for visual reasoning, exemplified by the well-established CLEVR and COG datasets.",,,,,,formalize,formalize,['we'],"['these unique aspects of transfer learning and a theoretical framework for visual reasoning , exemplified by the well - established CLEVR and COG datasets']",,,,,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,5,"In this work, we formalize these unique aspects of transfer learning and propose a theoretical framework for visual reasoning, exemplified by the well-established CLEVR and COG datasets.","['In', 'this', 'work,', 'we', 'formalize', 'these', 'unique', 'aspects', 'of', 'transfer', 'learning', 'and', 'propose', 'a', 'theoretical', 'framework', 'for', 'visual', 'reasoning,', 'exemplified', 'by', 'the', 'well-established', 'CLEVR', 'and', 'COG', 'datasets.']",
182,"Furthermore, we introduce a new, end-to-end differentiable recurrent model (SAMNet), which shows state-of-the-art accuracy and better performance in transfer learning on both datasets.",,,,,,introduce,introduce,"['Furthermore ,', 'we']",[],,,,,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,6,"Furthermore, we introduce a new, end-to-end differentiable recurrent model (SAMNet), which shows state-of-the-art accuracy and better performance in transfer learning on both datasets.","['Furthermore,', 'we', 'introduce', 'a', 'new,', 'end-to-end', 'differentiable', 'recurrent', 'model', '(SAMNet),', 'which', 'shows', 'state-of-the-art', 'accuracy', 'and', 'better', 'performance', 'in', 'transfer', 'learning', 'on', 'both', 'datasets.']",
183,The improved performance of SAMNet stems from its capability to decouple the abstract multi-step reasoning from the length of the sequence and its selective attention enabling to store only the question-relevant objects in the external memory.,,,,,,stems,stems,['The improved performance of SAMNet'],[],,,,,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Abstract,7,The improved performance of SAMNet stems from its capability to decouple the abstract multi-step reasoning from the length of the sequence and its selective attention enabling to store only the question-relevant objects in the external memory.,"['The', 'improved', 'performance', 'of', 'SAMNet', 'stems', 'from', 'its', 'capability', 'to', 'decouple', 'the', 'abstract', 'multi-step', 'reasoning', 'from', 'the', 'length', 'of', 'the', 'sequence', 'and', 'its', 'selective', 'attention', 'enabling', 'to', 'store', 'only', 'the', 'question-relevant', 'objects', 'in', 'the', 'external', 'memory.']",
184,Transfer Learning in Visual and Relational Reasoning.,,,,,,Transfer,Transfer,[],[],,,,,https://www.semanticscholar.org/paper/Transfer-Learning-in-Visual-and-Relational-Jayram-Marois/e32b914586ebf185e9c470a34d89a01f86a2c19e,22,Title,0,Transfer Learning in Visual and Relational Reasoning.,"['Transfer', 'Learning', 'in', 'Visual', 'and', 'Relational', 'Reasoning.']",
185,"Speculation is a naturally occurring phenomena in textual data, forming an integral component of many systems, especially in the biomedical information retrieval domain.",,,,,,occurring,is occurring,['Speculation'],[],,,,,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,0,"Speculation is a naturally occurring phenomena in textual data, forming an integral component of many systems, especially in the biomedical information retrieval domain.","['Speculation', 'is', 'a', 'naturally', 'occurring', 'phenomena', 'in', 'textual', 'data,', 'forming', 'an', 'integral', 'component', 'of', 'many', 'systems,', 'especially', 'in', 'the', 'biomedical', 'information', 'retrieval', 'domain.']",
186,Previous work addressing cue detection and scope resolution (the two subtasks of speculation detection) have ranged from rule-based systems to deep learning-based approaches.,,,,,,resolution,resolution,[],[],,,,,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,1,Previous work addressing cue detection and scope resolution (the two subtasks of speculation detection) have ranged from rule-based systems to deep learning-based approaches.,"['Previous', 'work', 'addressing', 'cue', 'detection', 'and', 'scope', 'resolution', '(the', 'two', 'subtasks', 'of', 'speculation', 'detection)', 'have', 'ranged', 'from', 'rule-based', 'systems', 'to', 'deep', 'learning-based', 'approaches.']",
187,"In this paper, we apply three popular transformer-based architectures,",BERT,", XLNet and RoBERTa to this task, on two publicly available datasets, BioScope Corpus and SFU Review Corpus, reporting substantial improvements over previously reported results (by at least 0.29 F1 points on cue detection and 4.27 F1 points on scope resolution).",based,[],[],popular,popular,[],[],reporting,XLNet,"['PROPN', 'VERB', 'ADJ']","['BERT', 'based', 'popular']",https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,2,"In this paper, we apply three popular transformer-based architectures, BERT, XLNet and RoBERTa to this task, on two publicly available datasets, BioScope Corpus and SFU Review Corpus, reporting substantial improvements over previously reported results (by at least 0.29 F1 points on cue detection and 4.27 F1 points on scope resolution).","['In', 'this', 'paper,', 'we', 'apply', 'three', 'popular', 'transformer-based', 'architectures,', 'BERT', ',', 'XLNet', 'and', 'RoBERTa', 'to', 'this', 'task,', 'on', 'two', 'publicly', 'available', 'datasets,', 'BioScope', 'Corpus', 'and', 'SFU', 'Review', 'Corpus,', 'reporting', 'substantial', 'improvements', 'over', 'previously', 'reported', 'results', '(by', 'at', 'least', '0.29', 'F1', 'points', 'on', 'cue', 'detection', 'and', '4.27', 'F1', 'points', 'on', 'scope', 'resolution).']","(9, 10)"
188,"In this paper, we apply three popular transformer-based architectures, BERT, XLNet and Ro",BERT,"a to this task, on two publicly available datasets, BioScope Corpus and SFU Review Corpus, reporting substantial improvements over previously reported results (by at least 0.29 F1 points on cue detection and 4.27 F1 points on scope resolution).",based,[],[],popular,popular,[],[],reporting,a,"['PROPN', 'PROPN', 'VERB']","['BERT', 'XLNet', 'based']",https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,2,"In this paper, we apply three popular transformer-based architectures, BERT, XLNet and RoBERTa to this task, on two publicly available datasets, BioScope Corpus and SFU Review Corpus, reporting substantial improvements over previously reported results (by at least 0.29 F1 points on cue detection and 4.27 F1 points on scope resolution).","['In', 'this', 'paper,', 'we', 'apply', 'three', 'popular', 'transformer-based', 'architectures,', 'BERT,', 'XLNet', 'and', 'Ro', 'BERT', 'a', 'to', 'this', 'task,', 'on', 'two', 'publicly', 'available', 'datasets,', 'BioScope', 'Corpus', 'and', 'SFU', 'Review', 'Corpus,', 'reporting', 'substantial', 'improvements', 'over', 'previously', 'reported', 'results', '(by', 'at', 'least', '0.29', 'F1', 'points', 'on', 'cue', 'detection', 'and', '4.27', 'F1', 'points', 'on', 'scope', 'resolution).']","(13, 14)"
189,"We also experiment with joint training of the model on multiple datasets, which outperforms the single dataset training approach by a good margin.",,,,,,experiment,experiment,['We'],[],,,,,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,3,"We also experiment with joint training of the model on multiple datasets, which outperforms the single dataset training approach by a good margin.","['We', 'also', 'experiment', 'with', 'joint', 'training', 'of', 'the', 'model', 'on', 'multiple', 'datasets,', 'which', 'outperforms', 'the', 'single', 'dataset', 'training', 'approach', 'by', 'a', 'good', 'margin.']",
190,We observe that XLNet consistently outperforms,BERT,"and RoBERTa, contrary to results on other benchmark datasets.",outperforms,['XLNet'],['BERT and RoBERTa'],contrary,contrary,[],[],,and,"['PROPN', 'VERB', 'VERB']","['BERT', 'outperforms', 'observe']",https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,4,"We observe that XLNet consistently outperforms BERT and RoBERTa, contrary to results on other benchmark datasets.","['We', 'observe', 'that', 'XLNet', 'consistently', 'outperforms', 'BERT', 'and', 'RoBERTa,', 'contrary', 'to', 'results', 'on', 'other', 'benchmark', 'datasets.']","(6, 7)"
191,We observe that XLNet consistently outperforms BERT and Ro,BERT,"a, contrary to results on other benchmark datasets.",,,,BERT,BERT,[],[],,a,"['PROPN', 'PROPN']","['BERT', 'BERT']",https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,4,"We observe that XLNet consistently outperforms BERT and RoBERTa, contrary to results on other benchmark datasets.","['We', 'observe', 'that', 'XLNet', 'consistently', 'outperforms', 'BERT', 'and', 'Ro', 'BERT', 'a,', 'contrary', 'to', 'results', 'on', 'other', 'benchmark', 'datasets.']","(9, 10)"
192,"To confirm this observation, we apply XLNet and",RoBERTa,"to negation detection and scope resolution, reporting state-of-the-art results on negation scope resolution for the BioScope Corpus (increase of 3.16 F1 points on the BioScope Full Papers, 0.06 F1 points on the BioScope Abstracts) and the SFU Review Corpus (increase of 0.3 F1 points).",apply,"['XLNet and RoBERTa', 'scope resolution , reporting state - of - the - art negation']",[],confirm,To confirm,[],['this observation'],negation,to,"['PROPN', 'PROPN', 'VERB']","['RoBERTa', 'XLNet', 'apply']",https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Abstract,5,"To confirm this observation, we apply XLNet and RoBERTa to negation detection and scope resolution, reporting state-of-the-art results on negation scope resolution for the BioScope Corpus (increase of 3.16 F1 points on the BioScope Full Papers, 0.06 F1 points on the BioScope Abstracts) and the SFU Review Corpus (increase of 0.3 F1 points).","['To', 'confirm', 'this', 'observation,', 'we', 'apply', 'XLNet', 'and', 'RoBERTa', 'to', 'negation', 'detection', 'and', 'scope', 'resolution,', 'reporting', 'state-of-the-art', 'results', 'on', 'negation', 'scope', 'resolution', 'for', 'the', 'BioScope', 'Corpus', '(increase', 'of', '3.16', 'F1', 'points', 'on', 'the', 'BioScope', 'Full', 'Papers,', '0.06', 'F1', 'points', 'on', 'the', 'BioScope', 'Abstracts)', 'and', 'the', 'SFU', 'Review', 'Corpus', '(increase', 'of', '0.3', 'F1', 'points).']","(8, 9)"
193,Resolving the Scope of Speculation and Negation using Transformer-Based Architectures.,,,,,,Negation,Negation,[],[],,,,,https://www.semanticscholar.org/paper/Resolving-the-Scope-of-Speculation-and-Negation-Britto-Khandelwal/d3bcf04ea083d0e71955822df73b9c00d2499db6,23,Title,0,Resolving the Scope of Speculation and Negation using Transformer-Based Architectures.,"['Resolving', 'the', 'Scope', 'of', 'Speculation', 'and', 'Negation', 'using', 'Transformer-Based', 'Architectures.']",
194,Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data.,,,,,,witnessed,have witnessed,['Recent'],['the emerging success of graph neural networks ( GNNs ) for modeling structured data'],,,,,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,0,Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data.,"['Recent', 'years', 'have', 'witnessed', 'the', 'emerging', 'success', 'of', 'graph', 'neural', 'networks', '(GNNs)', 'for', 'modeling', 'structured', 'data.']",
195,"However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making it infeasible to represent heterogeneous structures.",,,,,,are,are,"['However', ', most GNNs']",[],,,,,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,1,"However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making it infeasible to represent heterogeneous structures.","['However,', 'most', 'GNNs', 'are', 'designed', 'for', 'homogeneous', 'graphs,', 'in', 'which', 'all', 'nodes', 'and', 'edges', 'belong', 'to', 'the', 'same', 'types,', 'making', 'it', 'infeasible', 'to', 'represent', 'heterogeneous', 'structures.']",
196,"In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs.",,,,,,present,present,['we'],[],,,,,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,2,"In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs.","['In', 'this', 'paper,', 'we', 'present', 'the', 'Heterogeneous', 'Graph', 'Transformer', '(HGT)', 'architecture', 'for', 'modeling', 'Web-scale', 'heterogeneous', 'graphs.']",
197,"To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges.",,,,,,model,model,[],[],,,,,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,3,"To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges.","['To', 'model', 'heterogeneity,', 'we', 'design', 'node-', 'and', 'edge-type', 'dependent', 'parameters', 'to', 'characterize', 'the', 'heterogeneous', 'attention', 'over', 'each', 'edge,', 'empowering', 'HGT', 'to', 'maintain', 'dedicated', 'representations', 'for', 'different', 'types', 'of', 'nodes', 'and', 'edges.']",
198,"To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithmHGSamplingfor efficient and scalable training.",,,,,,graph,graph,[],[],,,,,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,4,"To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithmHGSamplingfor efficient and scalable training.","['To', 'handle', 'Web-scale', 'graph', 'data,', 'we', 'design', 'the', 'heterogeneous', 'mini-batch', 'graph', 'sampling', 'algorithmHGSamplingfor', 'efficient', 'and', 'scalable', 'training.']",
199,Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 921 on various downstream tasks.,,,,,,show,show,['Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges'],[],,,,,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,5,Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 921 on various downstream tasks.,"['Extensive', 'experiments', 'on', 'the', 'Open', 'Academic', 'Graph', 'of', '179', 'million', 'nodes', 'and', '2', 'billion', 'edges', 'show', 'that', 'the', 'proposed', 'HGT', 'model', 'consistently', 'outperforms', 'all', 'the', 'state-of-the-art', 'GNN', 'baselines', 'by', '921', 'on', 'various', 'downstream', 'tasks.']",
200,The dataset and source code of HGT are publicly available at https://github.com/acbull/pyHGT.,,,,,,publicly,are publicly,['The dataset and source code of HGT'],[],,,,,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Abstract,6,The dataset and source code of HGT are publicly available at https://github.com/acbull/pyHGT.,"['The', 'dataset', 'and', 'source', 'code', 'of', 'HGT', 'are', 'publicly', 'available', 'at', 'https://github.com/acbull/pyHGT.']",
201,Heterogeneous Graph Transformer.,,,,,,Heterogeneous,Heterogeneous,[],[],,,,,https://www.semanticscholar.org/paper/Heterogeneous-Graph-Transformer-Hu-Dong/0ca7d8c3250d43d14fdde46bf6fc299654d861ef,24,Title,0,Heterogeneous Graph Transformer.,"['Heterogeneous', 'Graph', 'Transformer.']",
202,A growing body of knowledge about biological mechanisms and interaction of biological components is contained in the peer-reviewed scientific literature.,,,,,,contained,is contained,['A growing body of knowledge about biological mechanisms and interaction of biological components'],[],,,,,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,0,A growing body of knowledge about biological mechanisms and interaction of biological components is contained in the peer-reviewed scientific literature.,"['A', 'growing', 'body', 'of', 'knowledge', 'about', 'biological', 'mechanisms', 'and', 'interaction', 'of', 'biological', 'components', 'is', 'contained', 'in', 'the', 'peer-reviewed', 'scientific', 'literature.']",
203,"In order to leverage this knowledge towards the development of predictive models, one must first extract these relationships from the text.",,,,,,extract,extract,['one'],"[', must first these relationships from the text']",,,,,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,1,"In order to leverage this knowledge towards the development of predictive models, one must first extract these relationships from the text.","['In', 'order', 'to', 'leverage', 'this', 'knowledge', 'towards', 'the', 'development', 'of', 'predictive', 'models,', 'one', 'must', 'first', 'extract', 'these', 'relationships', 'from', 'the', 'text.']",
204,"However, the context in which the interaction was reported is critical in ensuring that it is used in a manner consistent with the model's intended application.",,,,,,critical,is critical,"['However', ',', 'the context in which the interaction was reported']",[],,,,,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,2,"However, the context in which the interaction was reported is critical in ensuring that it is used in a manner consistent with the model's intended application.","['However,', 'the', 'context', 'in', 'which', 'the', 'interaction', 'was', 'reported', 'is', 'critical', 'in', 'ensuring', 'that', 'it', 'is', 'used', 'in', 'a', 'manner', 'consistent', 'with', 'the', ""model's"", 'intended', 'application.']",
205,Here we assess the applicability of two generic automated methods for leveraging a broader contextual structure in the more specific domain of a biological experiment using only the paper's title and abstract.,,,,,,assess,assess,['we'],['the applicability of two generic automated methods'],,,,,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,3,Here we assess the applicability of two generic automated methods for leveraging a broader contextual structure in the more specific domain of a biological experiment using only the paper's title and abstract.,"['Here', 'we', 'assess', 'the', 'applicability', 'of', 'two', 'generic', 'automated', 'methods', 'for', 'leveraging', 'a', 'broader', 'contextual', 'structure', 'in', 'the', 'more', 'specific', 'domain', 'of', 'a', 'biological', 'experiment', 'using', 'only', 'the', ""paper's"", 'title', 'and', 'abstract.']",
206,"In an example use case, a Support Vector Machine (SVM) and two variants of the broadly-used Bidirectional Encoder Representations from Transformers (",BERT,") neural network model, serve to distinguish mouse from human subject experiments in a corpus of over 12,000 papers documenting mechanistic interactions in a regulatory model of of mucosal immune signaling.",,,,",",",",[],[],serve,neural,"['PROPN', 'NOUN', 'ADP']","['BERT', 'model', 'of']",https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,4,"In an example use case, a Support Vector Machine (SVM) and two variants of the broadly-used Bidirectional Encoder Representations from Transformers (BERT) neural network model, serve to distinguish mouse from human subject experiments in a corpus of over 12,000 papers documenting mechanistic interactions in a regulatory model of of mucosal immune signaling.","['In', 'an', 'example', 'use', 'case,', 'a', 'Support', 'Vector', 'Machine', '(SVM)', 'and', 'two', 'variants', 'of', 'the', 'broadly-used', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '(', 'BERT', ')', 'neural', 'network', 'model,', 'serve', 'to', 'distinguish', 'mouse', 'from', 'human', 'subject', 'experiments', 'in', 'a', 'corpus', 'of', 'over', '12,000', 'papers', 'documenting', 'mechanistic', 'interactions', 'in', 'a', 'regulatory', 'model', 'of', 'of', 'mucosal', 'immune', 'signaling.']","(22, 23)"
207,The,BERT,and domain-specific BioBERT yielded essentially equivalent classification accuracy with both neural network models performing only marginally better than the SVM.,,,,specific,specific,[],[],yielded,and,"['PROPN', 'ADJ']","['BERT', 'specific']",https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,5,The BERT and domain-specific BioBERT yielded essentially equivalent classification accuracy with both neural network models performing only marginally better than the SVM.,"['The', 'BERT', 'and', 'domain-specific', 'BioBERT', 'yielded', 'essentially', 'equivalent', 'classification', 'accuracy', 'with', 'both', 'neural', 'network', 'models', 'performing', 'only', 'marginally', 'better', 'than', 'the', 'SVM.']","(1, 2)"
208,The BERT and domain-specific Bio,BERT,yielded essentially equivalent classification accuracy with both neural network models performing only marginally better than the SVM.,,,,specific,specific,[],[],yielded,yielded,"['PROPN', 'ADJ']","['BERT', 'specific']",https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,5,The BERT and domain-specific BioBERT yielded essentially equivalent classification accuracy with both neural network models performing only marginally better than the SVM.,"['The', 'BERT', 'and', 'domain-specific', 'Bio', 'BERT', 'yielded', 'essentially', 'equivalent', 'classification', 'accuracy', 'with', 'both', 'neural', 'network', 'models', 'performing', 'only', 'marginally', 'better', 'than', 'the', 'SVM.']","(5, 6)"
209,"Words occurring frequently in abstracts were largely non-specific, whereas words unique to each class were used in 4% or less of the abstracts.",,,,,,largely,were largely,[],[],,,,,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,6,"Words occurring frequently in abstracts were largely non-specific, whereas words unique to each class were used in 4% or less of the abstracts.","['Words', 'occurring', 'frequently', 'in', 'abstracts', 'were', 'largely', 'non-specific,', 'whereas', 'words', 'unique', 'to', 'each', 'class', 'were', 'used', 'in', '4%', 'or', 'less', 'of', 'the', 'abstracts.']",
210,These high-specificity words were used in very similar contexts that separated mouse and human study abstracts on the basis of study design and experimental procedure rather than species or basic biological markers.,,,,,,used,were used,['These high - specificity words'],[],,,,,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Abstract,7,These high-specificity words were used in very similar contexts that separated mouse and human study abstracts on the basis of study design and experimental procedure rather than species or basic biological markers.,"['These', 'high-specificity', 'words', 'were', 'used', 'in', 'very', 'similar', 'contexts', 'that', 'separated', 'mouse', 'and', 'human', 'study', 'abstracts', 'on', 'the', 'basis', 'of', 'study', 'design', 'and', 'experimental', 'procedure', 'rather', 'than', 'species', 'or', 'basic', 'biological', 'markers.']",
211,When the How Outweighs the What: the Pivotal Importance of Context.,,,,,,Outweighs,Outweighs,[],[],,,,,https://www.semanticscholar.org/paper/When-the-How-Outweighs-the-What%3A-the-Pivotal-of-Lyman-Anderson/49c21f5c8773932ac8bf096a86e2f13648a3e2e0,25,Title,0,When the How Outweighs the What: the Pivotal Importance of Context.,"['When', 'the', 'How', 'Outweighs', 'the', 'What:', 'the', 'Pivotal', 'Importance', 'of', 'Context.']",
212,"We present our approach for the identification of cited text spans in scientific literature, using pre-trained encoders (",BERT,) in combination with different neural networks.,trained,[],[],present,present,[],[],,in,"['PROPN', 'PUNCT', 'NOUN']","['BERT', ')', 'encoders']",https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,0,"We present our approach for the identification of cited text spans in scientific literature, using pre-trained encoders (BERT) in combination with different neural networks.","['We', 'present', 'our', 'approach', 'for', 'the', 'identification', 'of', 'cited', 'text', 'spans', 'in', 'scientific', 'literature,', 'using', 'pre-trained', 'encoders', '(', 'BERT', ')', 'in', 'combination', 'with', 'different', 'neural', 'networks.']","(18, 19)"
213,We further experiment to assess the impact of using these cited text spans as input in,BERT,-based extractive summarisation methods.,using,[],['these cited text spans as input in BERT -based extractive summarisation methods .'],experiment,experiment,[],[],-based,based,"['PROPN', 'ADJ', 'NOUN']","['BERT', '-based', 'methods']",https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,1,We further experiment to assess the impact of using these cited text spans as input in BERT-based extractive summarisation methods.,"['We', 'further', 'experiment', 'to', 'assess', 'the', 'impact', 'of', 'using', 'these', 'cited', 'text', 'spans', 'as', 'input', 'in', 'BERT', '-based', 'extractive', 'summarisation', 'methods.']","(16, 17)"
214,"Inspired and motivated by the CL-SciSumm shared tasks, we explore different methods to adapt pre-trained models which are tuned for generic domain to scientific literature.",,,,,,explore,explore,"['Inspired and motivated by the CL - SciSumm shared tasks', 'we']",['different methods to adapt pre - trained models which are tuned for generic domain to scientific literature'],,,,,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,2,"Inspired and motivated by the CL-SciSumm shared tasks, we explore different methods to adapt pre-trained models which are tuned for generic domain to scientific literature.","['Inspired', 'and', 'motivated', 'by', 'the', 'CL-SciSumm', 'shared', 'tasks,', 'we', 'explore', 'different', 'methods', 'to', 'adapt', 'pre-trained', 'models', 'which', 'are', 'tuned', 'for', 'generic', 'domain', 'to', 'scientific', 'literature.']",
215,"For the identification of cited text spans, we assess the impact of different configurations in terms of learning from augmented data and using different features and network architectures (",BERT,", XLNET, CNN, and BiMPM) for training.",learning,[],[],assess,assess,"[',', 'we']","['the impact of different configurations in terms of learning from augmented data and using different features and network architectures ( BERT , XLNET , CNN , and BiMPM ) for training .']",,XLNET,"['PROPN', 'NOUN', 'NOUN']","['BERT', 'network', 'features']",https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,3,"For the identification of cited text spans, we assess the impact of different configurations in terms of learning from augmented data and using different features and network architectures (BERT, XLNET, CNN, and BiMPM) for training.","['For', 'the', 'identification', 'of', 'cited', 'text', 'spans,', 'we', 'assess', 'the', 'impact', 'of', 'different', 'configurations', 'in', 'terms', 'of', 'learning', 'from', 'augmented', 'data', 'and', 'using', 'different', 'features', 'and', 'network', 'architectures', '(', 'BERT', ',', 'XLNET,', 'CNN,', 'and', 'BiMPM)', 'for', 'training.']","(29, 30)"
216,We show that identifying and fine-tuning the language models on unlabelled or augmented domain specific data can improve the performance of cited text span identification models.,,,,,,show,show,['We'],[],,,,,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,4,We show that identifying and fine-tuning the language models on unlabelled or augmented domain specific data can improve the performance of cited text span identification models.,"['We', 'show', 'that', 'identifying', 'and', 'fine-tuning', 'the', 'language', 'models', 'on', 'unlabelled', 'or', 'augmented', 'domain', 'specific', 'data', 'can', 'improve', 'the', 'performance', 'of', 'cited', 'text', 'span', 'identification', 'models.']",
217,For the scientific summarisation we implement an extractive summarisation model adapted from,BERT,.,adapted,[],[],implement,implement,['the scientific summarisation'],['an extractive summarisation model'],,,"['PROPN', 'ADP', 'VERB']","['BERT', 'from', 'adapted']",https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,5,For the scientific summarisation we implement an extractive summarisation model adapted from BERT.,"['For', 'the', 'scientific', 'summarisation', 'we', 'implement', 'an', 'extractive', 'summarisation', 'model', 'adapted', 'from', 'BERT', '.']","(12, 13)"
218,"With respect to the input sentences taken from the cited paper, we explore two different scenarios: (1) consider all the sentences (full-text) of the referenced article as input and (2) consider only the text spans that have been identified to be cited by other publications.",,,,,,explore,explore,[],['two different scenarios : ( 1 ) the'],,,,,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,6,"With respect to the input sentences taken from the cited paper, we explore two different scenarios: (1) consider all the sentences (full-text) of the referenced article as input and (2) consider only the text spans that have been identified to be cited by other publications.","['With', 'respect', 'to', 'the', 'input', 'sentences', 'taken', 'from', 'the', 'cited', 'paper,', 'we', 'explore', 'two', 'different', 'scenarios:', '(1)', 'consider', 'all', 'the', 'sentences', '(full-text)', 'of', 'the', 'referenced', 'article', 'as', 'input', 'and', '(2)', 'consider', 'only', 'the', 'text', 'spans', 'that', 'have', 'been', 'identified', 'to', 'be', 'cited', 'by', 'other', 'publications.']",
219,"We observe that in certain experiments, by using only the cited text-spans we can achieve better performance, while minimising the input size needed.",,,,,,achieve,can achieve,"['We observe', 'we']",[],,,,,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Abstract,7,"We observe that in certain experiments, by using only the cited text-spans we can achieve better performance, while minimising the input size needed.","['We', 'observe', 'that', 'in', 'certain', 'experiments,', 'by', 'using', 'only', 'the', 'cited', 'text-spans', 'we', 'can', 'achieve', 'better', 'performance,', 'while', 'minimising', 'the', 'input', 'size', 'needed.']",
220,Cited text span identification for scientific summarisation using pre-trained encoders.,,,,,,identification,identification,['Cited text'],[],,,,,https://www.semanticscholar.org/paper/Cited-text-span-identification-for-scientific-using-Zerva-Nghiem/5b928047a634f26d7a1871d173058a6fe578fa60,26,Title,0,Cited text span identification for scientific summarisation using pre-trained encoders.,"['Cited', 'text', 'span', 'identification', 'for', 'scientific', 'summarisation', 'using', 'pre-trained', 'encoders.']",
221,The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters.,,,,,,involves,involves,['The current modus operandi in NLP'],[],,,,,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,0,The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters.,"['The', 'current', 'modus', 'operandi', 'in', 'NLP', 'involves', 'downloading', 'and', 'fine-tuning', 'pre-trained', 'models', 'consisting', 'of', 'millions', 'or', 'billions', 'of', 'parameters.']",
222,"Storing and sharing such large trained models is expensive, slow, and timeconsuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks.",,,,,,expensive,is expensive,['Storing and sharing such large trained models'],[],,,,,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,1,"Storing and sharing such large trained models is expensive, slow, and timeconsuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks.","['Storing', 'and', 'sharing', 'such', 'large', 'trained', 'models', 'is', 'expensive,', 'slow,', 'and', 'timeconsuming,', 'which', 'impedes', 'progress', 'towards', 'more', 'general', 'and', 'versatile', 'NLP', 'methods', 'that', 'learn', 'from', 'and', 'for', 'many', 'tasks.']",
223,Adapters small learnt bottleneck layers inserted within each layer of a pre-trained model ameliorate this issue by avoiding full fine-tuning of the entire model.,,,,,,,,[],[],,,,,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,2,Adapters small learnt bottleneck layers inserted within each layer of a pre-trained model ameliorate this issue by avoiding full fine-tuning of the entire model.,"['Adapters', 'small', 'learnt', 'bottleneck', 'layers', 'inserted', 'within', 'each', 'layer', 'of', 'a', 'pre-trained', 'model', 'ameliorate', 'this', 'issue', 'by', 'avoiding', 'full', 'fine-tuning', 'of', 'the', 'entire', 'model.']",
224,"However, sharing and integrating adapter layers is not straightforward.",,,,,,straightforward,is straightforward,"['However', ', sharing and integrating adapter layers']",[],,,,,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,3,"However, sharing and integrating adapter layers is not straightforward.","['However,', 'sharing', 'and', 'integrating', 'adapter', 'layers', 'is', 'not', 'straightforward.']",
225,"We propose AdapterHub, a framework that allows dynamic stiching-in of pre-trained adapters for different tasks and languages.",,,,,,propose,propose,[],[],,,,,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,4,"We propose AdapterHub, a framework that allows dynamic stiching-in of pre-trained adapters for different tasks and languages.","['We', 'propose', 'AdapterHub,', 'a', 'framework', 'that', 'allows', 'dynamic', 'stiching-in', 'of', 'pre-trained', 'adapters', 'for', 'different', 'tasks', 'and', 'languages.']",
226,"The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pretrained models (e.g.,",BERT,", RoBERTa, XLMR) across tasks and languages.",enables,[],[],built,built,"['The framework ,']",[],,RoBERTa,"['PROPN', 'ADV', 'NOUN']","['BERT', 'e.g.', 'models']",https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,5,"The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pretrained models (e.g., BERT, RoBERTa, XLMR) across tasks and languages.","['The', 'framework,', 'built', 'on', 'top', 'of', 'the', 'popular', 'HuggingFace', 'Transformers', 'library,', 'enables', 'extremely', 'easy', 'and', 'quick', 'adaptations', 'of', 'state-of-the-art', 'pretrained', 'models', '(e.g.,', 'BERT', ',', 'RoBERTa,', 'XLMR)', 'across', 'tasks', 'and', 'languages.']","(22, 23)"
227,"The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pretrained models (e.g., BERT, Ro",BERT,"a, XLMR) across tasks and languages.",,,,Ro,Ro,[],[],,a,"['PROPN', 'PROPN']","['BERT', 'Ro']",https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,5,"The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pretrained models (e.g., BERT, RoBERTa, XLMR) across tasks and languages.","['The', 'framework,', 'built', 'on', 'top', 'of', 'the', 'popular', 'HuggingFace', 'Transformers', 'library,', 'enables', 'extremely', 'easy', 'and', 'quick', 'adaptations', 'of', 'state-of-the-art', 'pretrained', 'models', '(e.g.,', 'BERT,', 'Ro', 'BERT', 'a,', 'XLMR)', 'across', 'tasks', 'and', 'languages.']","(24, 25)"
228,"Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure.",,,,,,as,is as,[],[],,,,,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,6,"Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure.","['Downloading,', 'sharing,', 'and', 'training', 'adapters', 'is', 'as', 'seamless', 'as', 'possible', 'using', 'minimal', 'changes', 'to', 'the', 'training', 'scripts', 'and', 'a', 'specialized', 'infrastructure.']",
229,"Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios.",,,,,,enables,enables,"['Our', 'framework']",[],,,,,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,7,"Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios.","['Our', 'framework', 'enables', 'scalable', 'and', 'easy', 'access', 'to', 'sharing', 'of', 'task-specific', 'models,', 'particularly', 'in', 'low-resource', 'scenarios.']",
230,AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml.,,,,,,includes,includes,['AdapterHub'],['all recent adapter architectures and'],,,,,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Abstract,8,AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml.,"['AdapterHub', 'includes', 'all', 'recent', 'adapter', 'architectures', 'and', 'can', 'be', 'found', 'at', 'AdapterHub.ml.']",
231,AdapterHub: A Framework for Adapting Transformers.,,,,,,AdapterHub,AdapterHub,[],[],,,,,https://www.semanticscholar.org/paper/AdapterHub%3A-A-Framework-for-Adapting-Transformers-Pfeiffer-Ruckl'e/816e3f40914ddc693c549ea95a2f0ed063989f6e,27,Title,0,AdapterHub: A Framework for Adapting Transformers.,"['AdapterHub:', 'A', 'Framework', 'for', 'Adapting', 'Transformers.']",
232,Anonymization of clinical data is a crucial prerequisite for its many uses for both scientific analysis and extrapolation using modern research methods such as deep learning.,,,,,,is,is,['Anonymization of clinical data'],[],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,0,Anonymization of clinical data is a crucial prerequisite for its many uses for both scientific analysis and extrapolation using modern research methods such as deep learning.,"['Anonymization', 'of', 'clinical', 'data', 'is', 'a', 'crucial', 'prerequisite', 'for', 'its', 'many', 'uses', 'for', 'both', 'scientific', 'analysis', 'and', 'extrapolation', 'using', 'modern', 'research', 'methods', 'such', 'as', 'deep', 'learning.']",
233,"State-of-the-art Natural Language Processing (NLP) techniques have been found useful for anonymization of patient notes prescribed by doctors, which aims to remove Personal Health Identifiers (PHIs) from medical datasets.",,,,,,found,have been found,['State - of - the - art Natural'],[],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,1,"State-of-the-art Natural Language Processing (NLP) techniques have been found useful for anonymization of patient notes prescribed by doctors, which aims to remove Personal Health Identifiers (PHIs) from medical datasets.","['State-of-the-art', 'Natural', 'Language', 'Processing', '(NLP)', 'techniques', 'have', 'been', 'found', 'useful', 'for', 'anonymization', 'of', 'patient', 'notes', 'prescribed', 'by', 'doctors,', 'which', 'aims', 'to', 'remove', 'Personal', 'Health', 'Identifiers', '(PHIs)', 'from', 'medical', 'datasets.']",
234,Our goal is to identify and extract named entities present as PHIs within discharge summaries with NLP-based techniques.,,,,,,is,is,['Our goal'],[],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,2,Our goal is to identify and extract named entities present as PHIs within discharge summaries with NLP-based techniques.,"['Our', 'goal', 'is', 'to', 'identify', 'and', 'extract', 'named', 'entities', 'present', 'as', 'PHIs', 'within', 'discharge', 'summaries', 'with', 'NLP-based', 'techniques.']",
235,"We experiment with an existing Artificial Neural Network (ANN) model, for which we implement a random search to tune hyperparameters.",,,,,,experiment,experiment,[],[],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,3,"We experiment with an existing Artificial Neural Network (ANN) model, for which we implement a random search to tune hyperparameters.","['We', 'experiment', 'with', 'an', 'existing', 'Artificial', 'Neural', 'Network', '(ANN)', 'model,', 'for', 'which', 'we', 'implement', 'a', 'random', 'search', 'to', 'tune', 'hyperparameters.']",
236,We find that models with higher learning rates yield better results on average and that increasing dropout rate increases accuracy as well.,,,,,,find,find,['We'],[],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,4,We find that models with higher learning rates yield better results on average and that increasing dropout rate increases accuracy as well.,"['We', 'find', 'that', 'models', 'with', 'higher', 'learning', 'rates', 'yield', 'better', 'results', 'on', 'average', 'and', 'that', 'increasing', 'dropout', 'rate', 'increases', 'accuracy', 'as', 'well.']",
237,"Increasing number of character embedding dimensions for the bi-directional LSTM increased the accuracy as well, albeit insignificantly.",,,,,,insignificantly,insignificantly,[],[],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,5,"Increasing number of character embedding dimensions for the bi-directional LSTM increased the accuracy as well, albeit insignificantly.","['Increasing', 'number', 'of', 'character', 'embedding', 'dimensions', 'for', 'the', 'bi-directional', 'LSTM', 'increased', 'the', 'accuracy', 'as', 'well,', 'albeit', 'insignificantly.']",
238,"Notably, we implement a Transformer model and train on a large corpus of patient notes, for which we modify an existing model built on top of HuggingFaces transformers package to adapt it to a medical context.",,,,,,implement,implement,"[',', 'we']",['a Transformer model and'],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,6,"Notably, we implement a Transformer model and train on a large corpus of patient notes, for which we modify an existing model built on top of HuggingFaces transformers package to adapt it to a medical context.","['Notably,', 'we', 'implement', 'a', 'Transformer', 'model', 'and', 'train', 'on', 'a', 'large', 'corpus', 'of', 'patient', 'notes,', 'for', 'which', 'we', 'modify', 'an', 'existing', 'model', 'built', 'on', 'top', 'of', 'HuggingFaces', 'transformers', 'package', 'to', 'adapt', 'it', 'to', 'a', 'medical', 'context.']",
239,"With just a few training iterations, we are able to achieve near state-of-theart results.",,,,,,able,are able,['we'],[],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,7,"With just a few training iterations, we are able to achieve near state-of-theart results.","['With', 'just', 'a', 'few', 'training', 'iterations,', 'we', 'are', 'able', 'to', 'achieve', 'near', 'state-of-theart', 'results.']",
240,"Though there is precedent for disease predictions from EHR data, such utilization of a Transformer for de-identification of personal health indicators, with near-state-of-the-art accuracy results, is, to the best of our knowledge, the first of its kind.",,,,,,",",",",[],[],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Abstract,8,"Though there is precedent for disease predictions from EHR data, such utilization of a Transformer for de-identification of personal health indicators, with near-state-of-the-art accuracy results, is, to the best of our knowledge, the first of its kind.","['Though', 'there', 'is', 'precedent', 'for', 'disease', 'predictions', 'from', 'EHR', 'data,', 'such', 'utilization', 'of', 'a', 'Transformer', 'for', 'de-identification', 'of', 'personal', 'health', 'indicators,', 'with', 'near-state-of-the-art', 'accuracy', 'results,', 'is,', 'to', 'the', 'best', 'of', 'our', 'knowledge,', 'the', 'first', 'of', 'its', 'kind.']",
241,Simple Transformers for PHI De-identification.,,,,,,Transformers,Transformers,[],[],,,,,https://www.semanticscholar.org/paper/Simple-Transformers-for-PHI-De-identification-Khandelwal-Soin/f701901e46884613f3fc02f0917126e71e7ce37e,28,Title,0,Simple Transformers for PHI De-identification.,"['Simple', 'Transformers', 'for', 'PHI', 'De-identification.']",
242,"Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length.",,,,,,models,are models,['Transformers'],[],,,,,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,0,"Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length.","['Transformers', 'are', 'powerful', 'sequence', 'models,', 'but', 'require', 'time', 'and', 'memory', 'that', 'grows', 'quadratically', 'with', 'the', 'sequence', 'length.']",
243,In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$.,,,,,,introduce,introduce,['we'],['sparse factorizations of the attention matrix'],,,,,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,1,In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$.,"['In', 'this', 'paper', 'we', 'introduce', 'sparse', 'factorizations', 'of', 'the', 'attention', 'matrix', 'which', 'reduce', 'this', 'to', '$O(n', '\\sqrt{n})$.']",
244,"We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training.",,,,,,attention,attention,[],[],,,,,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,2,"We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training.","['We', 'also', 'introduce', 'a)', 'a', 'variation', 'on', 'architecture', 'and', 'initialization', 'to', 'train', 'deeper', 'networks,', 'b)', 'the', 'recomputation', 'of', 'attention', 'matrices', 'to', 'save', 'memory,', 'and', 'c)', 'fast', 'attention', 'kernels', 'for', 'training.']",
245,"We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers.",,,,,,We,We,[],[],,,,,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,3,"We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers.","['We', 'call', 'networks', 'with', 'these', 'changes', 'Sparse', 'Transformers,', 'and', 'show', 'they', 'can', 'model', 'sequences', 'tens', 'of', 'thousands', 'of', 'timesteps', 'long', 'using', 'hundreds', 'of', 'layers.']",
246,"We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64.",,,,,,use,use,[],[],,,,,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,4,"We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64.","['We', 'use', 'the', 'same', 'architecture', 'to', 'model', 'images,', 'audio,', 'and', 'text', 'from', 'raw', 'bytes,', 'setting', 'a', 'new', 'state', 'of', 'the', 'art', 'for', 'density', 'modeling', 'of', 'Enwik8,', 'CIFAR-10,', 'and', 'ImageNet-64.']",
247,"We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",,,,,,samples,samples,[],[],,,,,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Abstract,5,"We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.","['We', 'generate', 'unconditional', 'samples', 'that', 'demonstrate', 'global', 'coherence', 'and', 'great', 'diversity,', 'and', 'show', 'it', 'is', 'possible', 'in', 'principle', 'to', 'use', 'self-attention', 'to', 'model', 'sequences', 'of', 'length', 'one', 'million', 'or', 'more.']",
248,Generating Long Sequences with Sparse Transformers.,,,,,,Sequences,Sequences,[],[],,,,,https://www.semanticscholar.org/paper/Generating-Long-Sequences-with-Sparse-Transformers-Child-Gray/21da617a0f79aabf94272107184606cefe90ab75,29,Title,0,Generating Long Sequences with Sparse Transformers.,"['Generating', 'Long', 'Sequences', 'with', 'Sparse', 'Transformers.']",
249,"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.",,,,,,choice,choice,"['Recurrent neural networks ( RNNs ) sequentially process data by updating their state with each new data point , and']",[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,0,"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.","['Recurrent', 'neural', 'networks', '(RNNs)', 'sequentially', 'process', 'data', 'by', 'updating', 'their', 'state', 'with', 'each', 'new', 'data', 'point,', 'and', 'have', 'long', 'been', 'the', 'de', 'facto', 'choice', 'for', 'sequence', 'modeling', 'tasks.']",
250,"However, their inherently sequential computation makes them slow to train.",,,,,,makes,makes,"['However', 'their inherently sequential computation']",[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,1,"However, their inherently sequential computation makes them slow to train.","['However,', 'their', 'inherently', 'sequential', 'computation', 'makes', 'them', 'slow', 'to', 'train.']",
251,"Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.",,,,,,training,training,[],[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,2,"Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.","['Feed-forward', 'and', 'convolutional', 'architectures', 'have', 'recently', 'been', 'shown', 'to', 'achieve', 'superior', 'results', 'on', 'some', 'sequence', 'modeling', 'tasks', 'such', 'as', 'machine', 'translation,', 'with', 'the', 'added', 'advantage', 'that', 'they', 'concurrently', 'process', 'all', 'inputs', 'in', 'the', 'sequence,', 'leading', 'to', 'easy', 'parallelization', 'and', 'faster', 'training', 'times.']",
252,"Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g.",,,,,,generalize,to generalize,[],[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,3,"Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g.","['Despite', 'these', 'successes,', 'however,', 'popular', 'feed-forward', 'sequence', 'models', 'like', 'the', 'Transformer', 'fail', 'to', 'generalize', 'in', 'many', 'simple', 'tasks', 'that', 'recurrent', 'models', 'handle', 'with', 'ease,', 'e.g.']",
253,copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.,,,,,,logical,logical,['copying strings or'],[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,4,copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.,"['copying', 'strings', 'or', 'even', 'simple', 'logical', 'inference', 'when', 'the', 'string', 'or', 'formula', 'lengths', 'exceed', 'those', 'observed', 'at', 'training', 'time.']",
254,"We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.",,,,,,propose,propose,['We'],[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,5,"We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.","['We', 'propose', 'the', 'Universal', 'Transformer', '(UT),', 'a', 'parallel-in-time', 'self-attentive', 'recurrent', 'sequence', 'model', 'which', 'can', 'be', 'cast', 'as', 'a', 'generalization', 'of', 'the', 'Transformer', 'model', 'and', 'which', 'addresses', 'these', 'issues.']",
255,UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs.,,,,,,like,like,[],[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,6,UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs.,"['UTs', 'combine', 'the', 'parallelizability', 'and', 'global', 'receptive', 'field', 'of', 'feed-forward', 'sequence', 'models', 'like', 'the', 'Transformer', 'with', 'the', 'recurrent', 'inductive', 'bias', 'of', 'RNNs.']",
256,We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.,,,,,,add,add,['We'],[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,7,We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.,"['We', 'also', 'add', 'a', 'dynamic', 'per-position', 'halting', 'mechanism', 'and', 'find', 'that', 'it', 'improves', 'accuracy', 'on', 'several', 'tasks.']",
257,"In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete.",,,,,,shown,can be shown,"[', UTs']",[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,8,"In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete.","['In', 'contrast', 'to', 'the', 'standard', 'Transformer,', 'under', 'certain', 'assumptions,', 'UTs', 'can', 'be', 'shown', 'to', 'be', 'Turing-complete.']",
258,"Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",,,,,,show,show,['Our experiments'],[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Abstract,9,"Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.","['Our', 'experiments', 'show', 'that', 'UTs', 'outperform', 'standard', 'Transformers', 'on', 'a', 'wide', 'range', 'of', 'algorithmic', 'and', 'language', 'understanding', 'tasks,', 'including', 'the', 'challenging', 'LAMBADA', 'language', 'modeling', 'task', 'where', 'UTs', 'achieve', 'a', 'new', 'state', 'of', 'the', 'art,', 'and', 'machine', 'translation', 'where', 'UTs', 'achieve', 'a', '0.9', 'BLEU', 'improvement', 'over', 'Transformers', 'on', 'the', 'WMT14', 'En-De', 'dataset.']",
259,Universal Transformers.,,,,,,Universal,Universal,[],[],,,,,https://www.semanticscholar.org/paper/Universal-Transformers-Dehghani-Gouws/ac4dafdef1d2b685b7f28a11837414573d39ff4e,30,Title,0,Universal Transformers.,"['Universal', 'Transformers.']",
260,The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition.,,,,,,led,has led,['The recent success of transformer networks for neural machine translation and other NLP tasks'],[],,,,,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,0,The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition.,"['The', 'recent', 'success', 'of', 'transformer', 'networks', 'for', 'neural', 'machine', 'translation', 'and', 'other', 'NLP', 'tasks', 'has', 'led', 'to', 'a', 'surge', 'in', 'research', 'work', 'trying', 'to', 'apply', 'it', 'for', 'speech', 'recognition.']",
261,"Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks.",,,,,,studied,studied,[],[],,,,,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,1,"Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks.","['Recent', 'efforts', 'studied', 'key', 'research', 'questions', 'around', 'ways', 'of', 'combining', 'positional', 'embedding', 'with', 'speech', 'features,', 'and', 'stability', 'of', 'optimization', 'for', 'large', 'scale', 'learning', 'of', 'transformer', 'networks.']",
262,"In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations.",,,,,,propose,propose,"[',', 'we']",[],,,,,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,2,"In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations.","['In', 'this', 'paper,', 'we', 'propose', 'replacing', 'the', 'sinusoidal', 'positional', 'embedding', 'for', 'transformers', 'with', 'convolutionally', 'learned', 'input', 'representations.']",
263,These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts.,,,,,,provide,provide,['These contextual representations'],[],,,,,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,3,These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts.,"['These', 'contextual', 'representations', 'provide', 'subsequent', 'transformer', 'blocks', 'with', 'relative', 'positional', 'information', 'needed', 'for', 'discovering', 'long-range', 'relationships', 'between', 'local', 'concepts.']",
264,The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps.,,,,,,favorable,has favorable,['The proposed system'],[],,,,,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,4,The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps.,"['The', 'proposed', 'system', 'has', 'favorable', 'optimization', 'characteristics', 'where', 'our', 'reported', 'results', 'are', 'produced', 'with', 'fixed', 'learning', 'rate', 'of', '1.0', 'and', 'no', 'warmup', 'steps.']",
265,The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.,,,,,,achieves,achieves,['The proposed model'],[],,,,,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Abstract,5,The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.,"['The', 'proposed', 'model', 'achieves', 'a', 'competitive', '4.7%', 'and', '12.9%', 'WER', 'on', 'the', 'Librispeech', '``test', ""clean''"", 'and', '``test', ""other''"", 'subsets', 'when', 'no', 'extra', 'LM', 'text', 'is', 'provided.']",
266,Transformers with convolutional context for ASR.,,,,,,Transformers,Transformers,[],[],,,,,https://www.semanticscholar.org/paper/Transformers-with-convolutional-context-for-ASR-Mohamed-Okhonko/606c03a5bb36b77bfc99e33fd7cb6731b13b8eab,31,Title,0,Transformers with convolutional context for ASR.,"['Transformers', 'with', 'convolutional', 'context', 'for', 'ASR.']",
267,We propose a novel self-attention mechanism that can learn its optimal attention span.,,,,,,propose,propose,[],[],,,,,https://www.semanticscholar.org/paper/Adaptive-Attention-Span-in-Transformers-Sukhbaatar-Grave/f4238bd2385a52413ccbacfd9e409a650235bd13,32,Abstract,0,We propose a novel self-attention mechanism that can learn its optimal attention span.,"['We', 'propose', 'a', 'novel', 'self-attention', 'mechanism', 'that', 'can', 'learn', 'its', 'optimal', 'attention', 'span.']",
268,"This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time.",,,,,,allows,allows,['This'],[],,,,,https://www.semanticscholar.org/paper/Adaptive-Attention-Span-in-Transformers-Sukhbaatar-Grave/f4238bd2385a52413ccbacfd9e409a650235bd13,32,Abstract,1,"This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time.","['This', 'allows', 'us', 'to', 'extend', 'significantly', 'the', 'maximum', 'context', 'size', 'used', 'in', 'Transformer,', 'while', 'maintaining', 'control', 'over', 'their', 'memory', 'footprint', 'and', 'computational', 'time.']",
269,"We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",,,,,,show,show,['We'],"['the effectiveness of our approach on the task of character level language modeling , where we achieve state - of - the - art performances on text8 and enwiki8 by using a maximum context of 8k characters']",,,,,https://www.semanticscholar.org/paper/Adaptive-Attention-Span-in-Transformers-Sukhbaatar-Grave/f4238bd2385a52413ccbacfd9e409a650235bd13,32,Abstract,2,"We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.","['We', 'show', 'the', 'effectiveness', 'of', 'our', 'approach', 'on', 'the', 'task', 'of', 'character', 'level', 'language', 'modeling,', 'where', 'we', 'achieve', 'state-of-the-art', 'performances', 'on', 'text8', 'and', 'enwiki8', 'by', 'using', 'a', 'maximum', 'context', 'of', '8k', 'characters.']",
270,Adaptive Attention Span in Transformers.,,,,,,Adaptive,Adaptive,[],[],,,,,https://www.semanticscholar.org/paper/Adaptive-Attention-Span-in-Transformers-Sukhbaatar-Grave/f4238bd2385a52413ccbacfd9e409a650235bd13,32,Title,0,Adaptive Attention Span in Transformers.,"['Adaptive', 'Attention', 'Span', 'in', 'Transformers.']",
271,We learn models to generate the immediate future in video.,,,,,,models,models,[],[],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,0,We learn models to generate the immediate future in video.,"['We', 'learn', 'models', 'to', 'generate', 'the', 'immediate', 'future', 'in', 'video.']",
272,This problem has two main challenges.,,,,,,has,has,"['This', 'problem']",['two main challenges'],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,1,This problem has two main challenges.,"['This', 'problem', 'has', 'two', 'main', 'challenges.']",
273,"Firstly, since the future is uncertain, models should be multi-modal, which can be difficult to learn.",,,,,,since,since,[],[],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,2,"Firstly, since the future is uncertain, models should be multi-modal, which can be difficult to learn.","['Firstly,', 'since', 'the', 'future', 'is', 'uncertain,', 'models', 'should', 'be', 'multi-modal,', 'which', 'can', 'be', 'difficult', 'to', 'learn.']",
274,"Secondly, since the future is similar to the past, models store low-level details, which complicates learning of high-level semantics.",,,,,,similar,is similar,"['Secondly', ',', 'since the future']",[],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,3,"Secondly, since the future is similar to the past, models store low-level details, which complicates learning of high-level semantics.","['Secondly,', 'since', 'the', 'future', 'is', 'similar', 'to', 'the', 'past,', 'models', 'store', 'low-level', 'details,', 'which', 'complicates', 'learning', 'of', 'high-level', 'semantics.']",
275,We propose a framework to tackle both of these challenges.,,,,,,propose,propose,['We'],['a framework to tackle both of these challenges'],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,4,We propose a framework to tackle both of these challenges.,"['We', 'propose', 'a', 'framework', 'to', 'tackle', 'both', 'of', 'these', 'challenges.']",
276,We present a model that generates the future by transforming pixels in the past.,,,,,,We,We,[],[],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,5,We present a model that generates the future by transforming pixels in the past.,"['We', 'present', 'a', 'model', 'that', 'generates', 'the', 'future', 'by', 'transforming', 'pixels', 'in', 'the', 'past.']",
277,"Our approach explicitly disentangles the models memory from the prediction, which helps the model learn desirable invariances.",,,,,,models,models,[],[],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,6,"Our approach explicitly disentangles the models memory from the prediction, which helps the model learn desirable invariances.","['Our', 'approach', 'explicitly', 'disentangles', 'the', 'models', 'memory', 'from', 'the', 'prediction,', 'which', 'helps', 'the', 'model', 'learn', 'desirable', 'invariances.']",
278,Experiments suggest that this model can generate short videos of plausible futures.,,,,,,suggest,suggest,['Experiments'],[],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,7,Experiments suggest that this model can generate short videos of plausible futures.,"['Experiments', 'suggest', 'that', 'this', 'model', 'can', 'generate', 'short', 'videos', 'of', 'plausible', 'futures.']",
279,"We believe predictive models have many applications in robotics, health-care, and video understanding.",,,,,,believe,believe,['We'],[],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Abstract,8,"We believe predictive models have many applications in robotics, health-care, and video understanding.","['We', 'believe', 'predictive', 'models', 'have', 'many', 'applications', 'in', 'robotics,', 'health-care,', 'and', 'video', 'understanding.']",
280,Generating the Future with Adversarial Transformers.,,,,,,Future,Future,[],[],,,,,https://www.semanticscholar.org/paper/Generating-the-Future-with-Adversarial-Transformers-Vondrick-Torralba/6d2892f82a89bfc81f9924adb8bd070fe007adf7,33,Title,0,Generating the Future with Adversarial Transformers.,"['Generating', 'the', 'Future', 'with', 'Adversarial', 'Transformers.']",
