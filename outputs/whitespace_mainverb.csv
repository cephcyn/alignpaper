,text_split_0,text_split_1,text_split_2,group
0,,BERT:,Pre-training of Deep Bidirectional Transformers for Language Understanding,BERT
1,We introduce a new language representation model called,"BERT,",which stands for Bidirectional Encoder Representations from Transformers. ,stands
2,"Unlike recent language representation models,",BERT,is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. ,designed
3,"As a result, the pre-trained",BERT,"model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.",architecture
4,,BERT,is conceptually simple and empirically powerful. ,conceptually
5,"It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)",,,point
6,,RoBERTa:,A Robustly Optimized BERT Pretraining Approach,RoBERTa
7,Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. ,,,led
8,"Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. ",,,is
9,We present a replication study of,BERT,"pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. ",measures
10,We find that,BERT,"was significantly undertrained, and can match or exceed the performance of every model published after it. ",We
11,"Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. ",,,of
12,"These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. ",,,These
13,We release our models and code.,,,release
14,,"DistilBERT,","a distilled version of BERT: smaller, faster, cheaper and lighter",DistilBERT
15,"As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. ",,,prevalent
16,"In this work, we propose a method to pre-train a smaller general-purpose language representation model, called","DistilBERT,",which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. ,propose
17,"While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a",BERT,"model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. ",show
18,"To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. ",,,introduce
19,"Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",,,cheaper
20,,BERT,Rediscovers the Classical NLP Pipeline,Rediscovers
21,Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. ,,,advanced
22,"We focus on one such model,","BERT,",and aim to quantify where linguistic information is captured within the network. ,captured
23,"We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. ",,,roles
24,"Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",,,analysis
25,What Does,BERT,Look At? An Analysis of BERT's Attention,BERT
26,Large pre-trained neural networks such as,BERT,"have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. ",success
27,"Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). ",,,outputs
28,"Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to",BERT.,,propose
29,,BERT's,"attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. ",BERT
30,We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. ,,,show
31,"For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. ",,,find
32,"Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in",BERT's,attention.,Lastly
33,Passage Re-ranking with,BERT,,Re
34,"Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and",BERT,"(Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. ",Recently
35,"In this paper, we describe a simple re-implementation of",BERT,for query-based passage re-ranking. ,describe
36,"Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10. ",,,task
37,The code to reproduce our results is available at this https URL,,,available
38,Assessing,BERT's,Syntactic Abilities,BERT
39,I assess the extent to which the recently introduced,BERT,"model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. ",)
40,The,BERT,model performs remarkably well on all cases.,performs
41,"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of",BERT,,Beto
42,"Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. ",,,models
43,A new release of,BERT,"(Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. ",includes
44,This paper explores the broader cross-lingual potential of,mBERT,"(multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. ",NER
45,We compare,mBERT,with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. ,find
46,"Additionally, we investigate the most effective strategy for utilizing",mBERT,"in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.",investigate
47,,TinyBERT:,Distilling BERT for Natural Language Understanding,TinyBERT
48,"Language model pre-training, such as","BERT,",has significantly improved the performances of many natural language processing tasks. ,improved
49,"However, pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on some resource-restricted devices. ",,,usually
50,"To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel transformer distillation method that is a specially designed knowledge distillation (KD) method for transformer-based models. ",,,accelerate
51,"By leveraging this new KD method, the plenty of knowledge encoded in a large teacher",BERT,can be well transferred to a small student TinyBERT. ,transferred
52,"Moreover, we introduce a new two-stage learning framework for","TinyBERT,",which performs transformer distillation at both the pre-training and task-specific learning stages. ,introduce
53,This framework ensures that,TinyBERT,can capture both the general-domain and task-specific knowledge of the teacher BERT. ,ensures
54,,TinyBERT,"is empirically effective and achieves comparable results with BERT in GLUE datasets, while being 7.5x smaller and 9.4x faster on inference. ",empirically
55,,TinyBERT,"is also significantly better than state-of-the-art baselines, even with only about 28% parameters and 31% inference time of baselines",is
56,Fine-tune,BERT,for Extractive Summarization,BERT
57,,"BERT,","a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks. ",ground
58,"In this paper, we describe","BERTSUM,","a simple variant of BERT, for extractive summarization. ",describe
59,"Our system is the state of the art on the CNN/Dailymail dataset, outperforming the previous best-performed system by 1.65 on ROUGE-L. ",,,state
60,The codes to reproduce our results are available at this https URL,,,available
61,,BERT,Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis,Post
62,Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. ,,,Question
63,"Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions.~We call this problem Review Reading Comprehension (RRC). ",,,)
64,"To the best of our knowledge, no existing work has been done on RRC. ",,,done
65,"In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis. ",,,build
66,"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model",BERT,to enhance the performance of fine-tuning of BERT for RRC. ,has
67,"To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis. ",,,applied
68,Experimental results demonstrate that the proposed post-training is highly effective. ,,,results
69,The datasets and code are available at this https URL,,,available
70,How to Fine-Tune,BERT,for Text Classification?,How
71,Language model pre-training has proven to be useful in learning universal language representations. ,,,proven
72,"As a state-of-the-art language model pre-training model,",BERT,(Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. ,achieved
73,"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of",BERT,on text classification task and provide a general solution for BERT fine-tuning. ,conduct
74,"Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets",,,state
75,,BERT,"has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model",has
76,We show that,BERT,"(Devlin et al., 2018) is a Markov random field language model. ",show
77,This formulation gives way to a natural procedure to sample sentences from,BERT.,,gives
78,We generate from,BERT,"and find that it can produce high-quality, fluent generations. ",generate
79,"Compared to the generations of a traditional left-to-right language model,",BERT,generates sentences that are more diverse but of slightly worse quality.,diverse
80,Utilizing,BERT,for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence,BERT
81,"Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA). ",,,analysis
82,"In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI). ",,,construct
83,We fine-tune the pre-trained model from,BERT,and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets,tune
84,Simple Applications of,BERT,for Ad Hoc Document Retrieval,Simple
85,Following recent successes in applying,BERT,"to question answering, we explore simple applications to ad hoc document retrieval. ",explore
86,This required confronting the challenge posed by documents that are typically longer than the length of input,BERT,was designed to handle. ,required
87,"We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores. ",,,scores
88,"Experiments on TREC microblog and newswire test collections show that our approach is simple yet effective, as we report the highest average precision on these datasets by neural approaches that we are aware of",,,effective
89,Revealing the Dark Secrets of,BERT,,Revealing
90,,BERT-based,"architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. ",currently
91,"In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of",BERT.,,focus
92,"Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual",BERT's,heads. ,propose
93,"Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. ",,,findings
94,"While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. ",,,have
95,We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned,BERT,models,We
96,Pre-Training with Whole Word Masking for Chinese,BERT,,Pre
97,Bidirectional Encoder Representations from Transformers,(BERT),has shown marvelous improvements across various NLP tasks. ,Encoder
98,"Recently, an upgraded version of",BERT,"has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT. ",released
99,"In this technical report, we adapt whole word masking in Chinese text, that masking the whole word instead of masking Chinese characters, which could bring another challenge in Masked Language Model (MLM) pre-training task. ",,,word
100,The model was trained on the latest Chinese Wikipedia dump. ,,,trained
101,We aim to provide easy extensibility and better performance for Chinese,BERT,without changing any neural architecture or even hyper-parameters. ,aim
102,"The model is verified on various NLP tasks, across sentence-level to document-level, including sentiment classification (ChnSentiCorp, Sina Weibo), named entity recognition (People Daily, MSRA-NER), natural language inference (XNLI), sentence pair matching (LCQMC, BQ Corpus), and machine reading comprehension (CMRC 2018, DRCD, CAIL RC). ",,,","
103,Experimental results on these datasets show that the whole word masking could bring another significant gain. ,,,show
104,"Moreover, we also examine the effectiveness of Chinese pre-trained models:","BERT,","ERNIE, BERT-wwm. ",examine
105,We release the pre-trained model (both TensorFlow and PyTorch) on GitHub: this https URL,,,release
106,Multi-passage,BERT:,A Globally Normalized BERT Model for Open-domain Question Answering,passage
107,,BERT,model has been successfully applied to open-domain QA tasks. ,successfully
108,"However, previous work trains",BERT,"by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. ",cause
109,"To tackle this issue, we propose a multi-passage",BERT,"model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. ",propose
110,"In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. ",,,find
111,"By leveraging a passage ranker to select high-quality passages, multi-passage",BERT,gains additional 2%. ,By
112,Experiments on four standard benchmarks showed that our multi-passage,BERT,outperforms all state-of-the-art models on all benchmarks. ,showed
113,"In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% $F_1$ over all",non-BERT,"models, and 5.8% EM and 6.5% $F_1$ over BERT-based models",and
114,,BERT,for Joint Intent Classification and Slot Filling,BERT
115,Intent classification and slot filling are two essential tasks for natural language understanding. ,,,are
116,"They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. ",,,suffer
117,"Recently a new language representation model,",BERT,"(Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. ",state
118,"However, there has not been much effort on exploring",BERT,for natural language understanding. ,much
119,"In this work, we propose a joint intent classification and slot filling model based on",BERT.,,propose
120,"Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models",,,demonstrate
121,,BERT,with History Answer Embedding for Conversational Question Answering,BERT
122,Conversational search is an emerging topic in the information retrieval community. ,,,is
123,One of the major challenges to multi-turn conversational search is to model the conversation history to answer the current question. ,,,is
124,Existing methods either prepend history turns to the current question or use complicated attention mechanisms to model the history. ,,,turns
125,We propose a conceptually simple yet highly effective approach referred to as history answer embedding. ,,,propose
126,It enables seamless integration of conversation history into a conversational question answering (ConvQA) model built on,BERT,(Bidirectional Encoder Representations from Transformers). ,history
127,"We first explain our view that ConvQA is a simplified but concrete setting of conversational search, and then we provide a general framework to solve ConvQA. ",,,explain
128,We further demonstrate the effectiveness of our approach under this framework. ,,,demonstrate
129,"Finally, we analyze the impact of different numbers of history turns under different settings to provide new insights into conversation history modeling in ConvQA",,,analyze
130,Understanding the Behaviors of,BERT,in Ranking,Understanding
131,This paper studies the performances and behaviors of,BERT,in ranking tasks. ,studies
132,We explore several different ways to leverage the pre-trained,BERT,and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking. ,ranking
133,Experimental results on MS MARCO demonstrate the strong effectiveness of,BERT,"in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model. ",demonstrate
134,Experimental results on TREC show the gaps between the,BERT,pre-trained on surrounding contexts and the needs of ad hoc document ranking. ,results
135,Analyses illustrate how,BERT,"allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker",illustrate
136,Simple,BERT,Models for Relation Extraction and Semantic Role Labeling,Models
137,We present simple,BERT-based,models for relation extraction and semantic role labeling. ,role
138,"In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. ",,,achieved
139,"In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple",BERT-based,model can achieve state-of-the-art performance. ,show
140,"To our knowledge, we are the first to successfully apply",BERT,in this manner. ,first
141,Our models provide strong baselines for future research,,,provide
142,,BERT,and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning,BERT
143,Multi-task learning allows the sharing of useful information between multiple related tasks. ,,,allows
144,"In natural language processing several recent approaches have successfully leveraged unsupervised pre-training on large amounts of data to perform well on various tasks, such as those in the GLUE benchmark. ",,,successfully
145,These results are based on fine-tuning on each task separately. ,,,are
146,We explore the multi-task learning setting for the recent,BERT,"model on the GLUE benchmark, and how to best add task-specific parameters to a pre-trained BERT network, with a high degree of parameter sharing between tasks. ",add
147,"We introduce new adaptation modules, PALs or `projected attention layers', which use a low-dimensional multi-head attention mechanism, based on the idea that it is important to include layers with inductive biases useful for the input domain. ",,,introduce
148,By using PALs in parallel with,BERT,"layers, we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset",match
149,What,BERT,Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models,Is
150,"Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. ",,,approach
151,"In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. ",,,introduce
152,"As a case study, we apply these diagnostics to the popular",BERT,"model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation",insensitivity
153,Visualizing and Understanding the Effectiveness of,BERT,,Visualizing
154,"Language model pre-training, such as","BERT,",has achieved remarkable results in many NLP tasks. ,achieved
155,"However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. ",,,unclear
156,"In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning",BERT,on specific datasets. ,propose
157,"First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. ",,,First
158,"We also demonstrate that the fine-tuning procedure is robust to overfitting, even though",BERT,is highly over-parameterized for downstream tasks. ,demonstrate
159,"Second, the visualization results indicate that fine-tuning",BERT,"tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. ",indicate
160,"Third, the lower layers of",BERT,"are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language",invariant
161,Conditional,BERT,Contextual Augmentation,BERT
162,Data augmentation methods are often applied to prevent overfitting and improve generalization of deep neural network models. ,,,are
163,Recently proposed contextual augmentation augments labeled sentences by randomly replacing words with more varied substitutions predicted by language model. ,,,Recently
164,Bidirectional Encoder Representations from Transformers,(BERT),demonstrates that a deep bidirectional language model is more powerful than either an unidirectional language model or the shallow concatenation of a forward and backward model. ,Representations
165,We propose a novel data augmentation method for labeled sentences called conditional,BERT,contextual augmentation. ,propose
166,We retrofit,BERT,"to conditional BERT by introducing a new conditional masked language model (The term “conditional masked language model” appeared once in original BERT paper, which indicates context-conditional, is equivalent to term “masked language model”. ",model
167,"In our paper, “conditional masked language model” indicates we apply extra label-conditional constraint to the “masked language model”.) task. ",,,label
168,The well trained conditional,BERT,can be applied to enhance contextual augmentation. ,applied
169,Experiments on six various different text classification tasks show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement,,,tasks
170,Small and Practical,BERT,Models for Sequence Labeling,Small
171,We propose a practical scheme to train a single multilingual sequence labeling model that yields state of the art results and is small and fast enough to run on a single CPU. ,,,propose
172,Starting from a public multilingual,BERT,"checkpoint, our final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-the-art multilingual baseline. ",6x
173,"We show that our model especially outperforms on low-resource languages, and works on codemixed input text without being explicitly trained on codemixed examples. ",,,show
174,We showcase the effectiveness of our method by reporting on part-of-speech tagging and morphological prediction on 70 treebanks and 48 languages,,,showcase
175,Data Augmentation for,BERT,Fine-Tuning in Open-Domain Question Answering,Augmentation
176,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a",BERT,"reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. ",found
177,"In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. ",,,present
178,We apply a stage-wise approach to fine tuning,BERT,"on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". ",approach
179,"Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets",,,show
180,,BERT,is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA,is
181,The,BERT,"language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. ",good
182,Petroni et al. (2019) take this as evidence that,BERT,memorizes factual knowledge during pre-training. ,et
183,We take issue with this interpretation and argue that the performance of,BERT,"is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian. ",issue
184,"More specifically, we show that",BERT's,precision drops dramatically when we filter certain easy-to-guess facts. ,guess
185,"As a remedy, we propose","E-BERT,",an extension of BERT that replaces entity mentions with symbolic entity embeddings. ,propose
186,,E-BERT,"outperforms both BERT and ERNIE (Zhang et al., 2019) on hard-to-guess queries. ",BERT
187,We take this as evidence that,E-BERT,"is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT",take
188,How Contextual are Contextualized Word Representations? Comparing the Geometry of,"BERT,","ELMo, and GPT-2 Embeddings",Contextualized
189,Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. ,,,yielded
190,"However, just how contextual are the contextualized representations produced by models such as ELMo and",BERT?,,representations
191,"Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? ",,,words
192,"For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. ",,,find
193,"While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. ",,,have
194,"This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. ",,,suggests
195,"In all layers of ELMo,","BERT,","and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations",explained
