,URL,ID,Type,Index,Text,split_0,split_1,split_2,split_tokens,split_anchor_span,split_anchor_indices,within_anchor_index,group,group2,group3,group4,group5,group6,group7,group8,group9,group10,group11,group12,group13,group14,group15,group16
0,https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992,0,Abstract,0,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.",We introduce a new language representation model called,BERT,", which stands for Bidirectional Encoder Representations from Transformers .","['We', 'introduce', 'a', 'new', 'language', 'representation', 'model', 'called', 'BERT', ',', 'which', 'stands', 'for', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '.']","(8, 9)","(55, 59)",0,We,0,0,2,0,2,"[(0, 2)]",introduce,0,3,12,3,12,"[(3, 12)]",['We'],['introduce']
1,https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992,0,Abstract,1,"Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.","Unlike recent language representation models ,",BERT,is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .,"['Unlike', 'recent', 'language', 'representation', 'models', ',', 'BERT', 'is', 'designed', 'to', 'pre-train', 'deep', 'bidirectional', 'representations', 'from', 'unlabeled', 'text', 'by', 'jointly', 'conditioning', 'on', 'both', 'left', 'and', 'right', 'context', 'in', 'all', 'layers', '.']","(6, 7)","(46, 50)",0,BERT,1,47,51,0,4,"[(47, 51)]",is designed,2,52,63,0,11,"[(52, 54), (55, 63)]",['BERT'],"['is', 'designed', 'pre', '-', 'train']"
2,https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992,0,Abstract,2,"As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.","As a result , the pre-trained",BERT,"model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks , such as question answering and language inference , without substantial task-specific architecture modifications .","['As', 'a', 'result', ',', 'the', 'pre-trained', 'BERT', 'model', 'can', 'be', 'fine-tuned', 'with', 'just', 'one', 'additional', 'output', 'layer', 'to', 'create', 'state-of-the-art', 'models', 'for', 'a', 'wide', 'range', 'of', 'tasks', ',', 'such', 'as', 'question', 'answering', 'and', 'language', 'inference', ',', 'without', 'substantial', 'task-specific', 'architecture', 'modifications', '.']","(6, 7)","(29, 33)",0,the pre - trained BERT model,0,14,42,14,42,"[(14, 17), (18, 21), (22, 23), (24, 31), (32, 36), (37, 42)]",can be tuned,2,43,62,8,27,"[(43, 46), (47, 49), (57, 62)]","['the', 'pre', '-', 'trained', 'BERT', 'model']","['can', 'be', 'tuned']"
3,https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992,0,Abstract,3,BERT is conceptually simple and empirically powerful.,,BERT,is conceptually simple and empirically powerful .,"['BERT', 'is', 'conceptually', 'simple', 'and', 'empirically', 'powerful', '.']","(0, 1)","(0, 4)",0,BERT,1,0,4,0,4,"[(0, 4)]",is,2,5,7,0,2,"[(5, 7)]",['BERT'],['is']
4,https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992,0,Abstract,4,"It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)","It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)",,,"['It', 'obtains', 'new', 'state-of-the-art', 'results', 'on', 'eleven', 'natural', 'language', 'processing', 'tasks,', 'including', 'pushing', 'the', 'GLUE', 'score', 'to', '80.5%', '(7.7%', 'point', 'absolute', 'improvement),', 'MultiNLI', 'accuracy', 'to', '86.7%', '(4.6%', 'absolute', 'improvement),', 'SQuAD', 'v1.1', 'question', 'answering', 'Test', 'F1', 'to', '93.2', '(1.5', 'point', 'absolute', 'improvement)', 'and', 'SQuAD', 'v2.0', 'Test', 'F1', 'to', '83.1', '(5.1', 'point', 'absolute', 'improvement)']",,,,It,0,0,2,0,2,"[(0, 2)]",obtains,0,3,10,3,10,"[(3, 10)]",['It'],['obtains']
5,https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992,0,Title,0,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.,,BERT,: Pre-training of Deep Bidirectional Transformers for Language Understanding .,"['BERT', ':', 'Pre-training', 'of', 'Deep', 'Bidirectional', 'Transformers', 'for', 'Language', 'Understanding', '.']","(0, 1)","(0, 4)",0,BERT Pre - training,1,0,21,0,21,"[(0, 4), (7, 10), (11, 12), (13, 21)]",,,,,,,[],"['BERT', 'Pre', '-', 'training']",[]
6,https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,1,Abstract,0,Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging.,Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging.,,,"['Language', 'model', 'pretraining', 'has', 'led', 'to', 'significant', 'performance', 'gains', 'but', 'careful', 'comparison', 'between', 'different', 'approaches', 'is', 'challenging.']",,,,,,,,,,[],but,0,68,71,68,71,"[(68, 71)]","['Language', 'model', 'pretraining', 'careful', 'comparison']","['has', 'led', 'is']"
7,https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,1,Abstract,1,"Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results.","Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results.",,,"['Training', 'is', 'computationally', 'expensive,', 'often', 'done', 'on', 'private', 'datasets', 'of', 'different', 'sizes,', 'and,', 'as', 'we', 'will', 'show,', 'hyperparameter', 'choices', 'have', 'significant', 'impact', 'on', 'the', 'final', 'results.']",,,,,,,,,,[],and,0,92,95,92,95,"[(92, 95)]","['Training', 'hyperparameter', 'choices']","['is', 'done', 'have']"
8,https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,1,Abstract,2,"We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size.",We present a replication study of,BERT,"pretraining ( Devlin et al. , 2019 ) that carefully measures the impact of many key hyperparameters and training data size .","['We', 'present', 'a', 'replication', 'study', 'of', 'BERT', 'pretraining', '(', 'Devlin', 'et', 'al.', ',', '2019', ')', 'that', 'carefully', 'measures', 'the', 'impact', 'of', 'many', 'key', 'hyperparameters', 'and', 'training', 'data', 'size', '.']","(6, 7)","(33, 37)",0,We,0,0,2,0,2,"[(0, 2)]",present,0,3,10,3,10,"[(3, 10)]",['We'],['present']
9,https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,1,Abstract,3,"We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.",We find that,BERT,"was significantly undertrained , and can match or exceed the performance of every model published after it .","['We', 'find', 'that', 'BERT', 'was', 'significantly', 'undertrained', ',', 'and', 'can', 'match', 'or', 'exceed', 'the', 'performance', 'of', 'every', 'model', 'published', 'after', 'it', '.']","(3, 4)","(12, 16)",0,We,0,0,2,0,2,"[(0, 2)]",find,0,3,7,3,7,"[(3, 7)]",['We'],['find']
10,https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,1,Abstract,4,"Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD.","Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD.",,,"['Our', 'best', 'model', 'achieves', 'state-of-the-art', 'results', 'on', 'GLUE,', 'RACE', 'and', 'SQuAD.']",,,,best model,0,4,14,4,14,"[(4, 8), (9, 14)]",achieves,0,15,23,15,23,"[(15, 23)]","['best', 'model']",['achieves']
11,https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,1,Abstract,5,"These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements.","These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements.",,,"['These', 'results', 'highlight', 'the', 'importance', 'of', 'previously', 'overlooked', 'design', 'choices,', 'and', 'raise', 'questions', 'about', 'the', 'source', 'of', 'recently', 'reported', 'improvements.']",,,,These results,0,0,13,0,13,"[(0, 5), (6, 13)]",highlight and raise,0,14,90,14,90,"[(14, 23), (81, 84), (85, 90)]","['These', 'results']","['highlight', 'raise']"
12,https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,1,Abstract,6,We release our models and code.,We release our models and code.,,,"['We', 'release', 'our', 'models', 'and', 'code.']",,,,We,0,0,2,0,2,"[(0, 2)]",release,0,3,10,3,10,"[(3, 10)]",['We'],['release']
13,https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,1,Title,0,RoBERTa: A Robustly Optimized BERT Pretraining Approach.,,RoBERTa,: A Robustly Optimized BERT Pretraining Approach .,"['RoBERTa', ':', 'A', 'Robustly', 'Optimized', 'BERT', 'Pretraining', 'Approach', '.']","(0, 1)","(0, 7)",2,RoBERTa A BERT Approach,1,0,56,0,56,"[(0, 7), (10, 11), (31, 35), (48, 56)]",,,,,,,[],"['RoBERTa', 'A', 'BERT', 'Approach']",[]
14,https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38,2,Abstract,0,"As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging.","As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging.",,,"['As', 'Transfer', 'Learning', 'from', 'large-scale', 'pre-trained', 'models', 'becomes', 'more', 'prevalent', 'in', 'Natural', 'Language', 'Processing', '(NLP),', 'operating', 'these', 'large', 'models', 'in', 'on-the-edge', 'and/or', 'under', 'constrained', 'computational', 'training', 'or', 'inference', 'budgets', 'remains', 'challenging.']",,,,,,,,,,[],remains,0,242,249,242,249,"[(242, 249)]",[],"['operating', 'remains']"
15,https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38,2,Abstract,1,"In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts.","In this work , we propose a method to pre-train a smaller general-purpose language representation model , called",DistilBERT,", which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts .","['In', 'this', 'work', ',', 'we', 'propose', 'a', 'method', 'to', 'pre-train', 'a', 'smaller', 'general-purpose', 'language', 'representation', 'model', ',', 'called', 'DistilBERT', ',', 'which', 'can', 'then', 'be', 'fine-tuned', 'with', 'good', 'performances', 'on', 'a', 'wide', 'range', 'of', 'tasks', 'like', 'its', 'larger', 'counterparts', '.']","(18, 19)","(112, 122)",6,we,0,15,17,15,17,"[(15, 17)]",propose,0,18,25,18,25,"[(18, 25)]",['we'],['propose']
16,https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38,2,Abstract,2,"While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.","While most prior work investigated the use of distillation for building task-specific models , we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a",BERT,"model by 40 % , while retaining 97 % of its language understanding capabilities and being 60 % faster .","['While', 'most', 'prior', 'work', 'investigated', 'the', 'use', 'of', 'distillation', 'for', 'building', 'task-specific', 'models', ',', 'we', 'leverage', 'knowledge', 'distillation', 'during', 'the', 'pre-training', 'phase', 'and', 'show', 'that', 'it', 'is', 'possible', 'to', 'reduce', 'the', 'size', 'of', 'a', 'BERT', 'model', 'by', '40', '%', ',', 'while', 'retaining', '97', '%', 'of', 'its', 'language', 'understanding', 'capabilities', 'and', 'being', '60', '%', 'faster', '.']","(34, 35)","(212, 216)",0,,,,,,,[],and show,0,164,172,164,172,"[(164, 167), (168, 172)]","['most', 'prior', 'work', 'we']","['investigated', 'building', 'leverage', 'show']"
17,https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38,2,Abstract,3,"To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses.","To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses.",,,"['To', 'leverage', 'the', 'inductive', 'biases', 'learned', 'by', 'larger', 'models', 'during', 'pre-training,', 'we', 'introduce', 'a', 'triple', 'loss', 'combining', 'language', 'modeling,', 'distillation', 'and', 'cosine-distance', 'losses.']",,,,we,0,82,84,82,84,"[(82, 84)]",introduce,0,85,94,85,94,"[(85, 94)]",['we'],"['leverage', 'introduce']"
18,https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38,2,Abstract,4,"Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.","Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",,,"['Our', 'smaller,', 'faster', 'and', 'lighter', 'model', 'is', 'cheaper', 'to', 'pre-train', 'and', 'we', 'demonstrate', 'its', 'capabilities', 'for', 'on-device', 'computations', 'in', 'a', 'proof-of-concept', 'experiment', 'and', 'a', 'comparative', 'on-device', 'study.']",,,,,,,,,,[],and,0,65,68,65,68,"[(65, 68)]","['model', 'we']","['is', 'train', 'demonstrate']"
19,https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38,2,Title,0,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.",,DistilBERT,", a distilled version of BERT : smaller , faster , cheaper and lighter .","['DistilBERT', ',', 'a', 'distilled', 'version', 'of', 'BERT', ':', 'smaller', ',', 'faster', ',', 'cheaper', 'and', 'lighter', '.']","(0, 1)","(0, 10)",6,a distilled version,2,13,32,2,21,"[(13, 14), (15, 24), (25, 32)]",,,,,,,[],"['a', 'distilled', 'version']",[]
20,https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c,3,Abstract,0,Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks.,Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks.,,,"['Pre-trained', 'text', 'encoders', 'have', 'rapidly', 'advanced', 'the', 'state', 'of', 'the', 'art', 'on', 'many', 'NLP', 'tasks.']",,,,Pre - trained text encoders,0,0,27,0,27,"[(0, 3), (4, 5), (6, 13), (14, 18), (19, 27)]",have advanced,0,28,49,28,49,"[(28, 32), (41, 49)]","['Pre', '-', 'trained', 'text', 'encoders']","['have', 'advanced']"
21,https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c,3,Abstract,1,"We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network.","We focus on one such model ,",BERT,", and aim to quantify where linguistic information is captured within the network .","['We', 'focus', 'on', 'one', 'such', 'model', ',', 'BERT', ',', 'and', 'aim', 'to', 'quantify', 'where', 'linguistic', 'information', 'is', 'captured', 'within', 'the', 'network', '.']","(7, 8)","(28, 32)",0,We,0,0,2,0,2,"[(0, 2)]",focus and aim,0,3,43,3,43,"[(3, 8), (36, 39), (40, 43)]",['We'],"['focus', 'aim', 'quantify']"
22,https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c,3,Abstract,2,"We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference.","We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference.",,,"['We', 'find', 'that', 'the', 'model', 'represents', 'the', 'steps', 'of', 'the', 'traditional', 'NLP', 'pipeline', 'in', 'an', 'interpretable', 'and', 'localizable', 'way,', 'and', 'that', 'the', 'regions', 'responsible', 'for', 'each', 'step', 'appear', 'in', 'the', 'expected', 'sequence:', 'POS', 'tagging,', 'parsing,', 'NER,', 'semantic', 'roles,', 'then', 'coreference.']",,,,We,0,0,2,0,2,"[(0, 2)]",find,0,3,7,3,7,"[(3, 7)]",['We'],['find']
23,https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c,3,Abstract,3,"Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.","Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",,,"['Qualitative', 'analysis', 'reveals', 'that', 'the', 'model', 'can', 'and', 'often', 'does', 'adjust', 'this', 'pipeline', 'dynamically,', 'revising', 'lower-level', 'decisions', 'on', 'the', 'basis', 'of', 'disambiguating', 'information', 'from', 'higher-level', 'representations.']",,,,Qualitative analysis,0,0,20,0,20,"[(0, 11), (12, 20)]",reveals,0,21,28,21,28,"[(21, 28)]","['Qualitative', 'analysis']",['reveals']
24,https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c,3,Title,0,BERT Rediscovers the Classical NLP Pipeline.,,BERT,Rediscovers the Classical NLP Pipeline .,"['BERT', 'Rediscovers', 'the', 'Classical', 'NLP', 'Pipeline', '.']","(0, 1)","(0, 4)",0,BERT,1,0,4,0,4,"[(0, 4)]",,,,,,,[],['BERT'],[]
25,https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986,4,Abstract,0,"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.",Large pre-trained neural networks such as,BERT,"have had great recent success in NLP , motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data .","['Large', 'pre-trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']","(6, 7)","(41, 45)",0,Large pre - trained neural networks,0,0,35,0,35,"[(0, 5), (6, 9), (10, 11), (12, 19), (20, 26), (27, 35)]",have had,2,49,57,2,10,"[(49, 53), (54, 57)]","['Large', 'pre', '-', 'trained', 'neural', 'networks']","['have', 'had', 'motivating']"
26,https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986,4,Abstract,1,"Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers).","Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers).",,,"['Most', 'recent', 'analysis', 'has', 'focused', 'on', 'model', 'outputs', '(e.g.,', 'language', 'model', 'surprisal)', 'or', 'internal', 'vector', 'representations', '(e.g.,', 'probing', 'classifiers).']",,,,Most recent analysis,0,0,20,0,20,"[(0, 4), (5, 11), (12, 20)]",has focused,0,21,32,21,32,"[(21, 24), (25, 32)]","['Most', 'recent', 'analysis']","['has', 'focused']"
27,https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986,4,Abstract,2,"Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT.","Complementary to these works , we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to",BERT,.,"['Complementary', 'to', 'these', 'works', ',', 'we', 'propose', 'methods', 'for', 'analyzing', 'the', 'attention', 'mechanisms', 'of', 'pre-trained', 'models', 'and', 'apply', 'them', 'to', 'BERT', '.']","(20, 21)","(128, 132)",0,we,0,31,33,31,33,"[(31, 33)]",propose,0,34,41,34,41,"[(34, 41)]",['we'],['propose']
28,https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986,4,Abstract,3,"BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors.",,BERT,"'s attention heads exhibit patterns such as attending to delimiter tokens , specific positional offsets , or broadly attending over the whole sentence , with heads in the same layer often exhibiting similar behaviors .","['BERT', ""'s"", 'attention', 'heads', 'exhibit', 'patterns', 'such', 'as', 'attending', 'to', 'delimiter', 'tokens', ',', 'specific', 'positional', 'offsets', ',', 'or', 'broadly', 'attending', 'over', 'the', 'whole', 'sentence', ',', 'with', 'heads', 'in', 'the', 'same', 'layer', 'often', 'exhibiting', 'similar', 'behaviors', '.']","(0, 1)","(0, 4)",0,BERT 's attention,1,0,17,0,17,"[(0, 4), (5, 7), (8, 17)]",,,,,,,[],"['BERT', 'attention']",[]
29,https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986,4,Abstract,4,We further show that certain attention heads correspond well to linguistic notions of syntax and coreference.,We further show that certain attention heads correspond well to linguistic notions of syntax and coreference.,,,"['We', 'further', 'show', 'that', 'certain', 'attention', 'heads', 'correspond', 'well', 'to', 'linguistic', 'notions', 'of', 'syntax', 'and', 'coreference.']",,,,We,0,0,2,0,2,"[(0, 2)]",show,0,11,15,11,15,"[(11, 15)]",['We'],['show']
30,https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986,4,Abstract,5,"For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy.","For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy.",,,"['For', 'example,', 'we', 'find', 'heads', 'that', 'attend', 'to', 'the', 'direct', 'objects', 'of', 'verbs,', 'determiners', 'of', 'nouns,', 'objects', 'of', 'prepositions,', 'and', 'coreferent', 'mentions', 'with', 'remarkably', 'high', 'accuracy.']",,,,we,0,14,16,14,16,"[(14, 16)]",find,0,17,21,17,21,"[(17, 21)]",['we'],['find']
31,https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986,4,Abstract,6,"Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.","Lastly , we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in",BERT,'s attention .,"['Lastly', ',', 'we', 'propose', 'an', 'attention-based', 'probing', 'classifier', 'and', 'use', 'it', 'to', 'further', 'demonstrate', 'that', 'substantial', 'syntactic', 'information', 'is', 'captured', 'in', 'BERT', ""'s"", 'attention', '.']","(21, 22)","(145, 149)",0,we,0,9,11,9,11,"[(9, 11)]",propose and use,0,12,67,12,67,"[(12, 19), (60, 63), (64, 67)]",['we'],"['propose', 'use', 'demonstrate']"
32,https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986,4,Title,0,What Does BERT Look At?,What Does,BERT,Look At ?,"['What', 'Does', 'BERT', 'Look', 'At', '?']","(2, 3)","(9, 13)",0,,,,,,,[],,,,,,,[],[],[]
33,https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986,4,Title,1,An Analysis of BERT's Attention.,An Analysis of,BERT,'s Attention .,"['An', 'Analysis', 'of', 'BERT', ""'s"", 'Attention', '.']","(3, 4)","(14, 18)",0,An Analysis,0,0,11,0,11,"[(0, 2), (3, 11)]",,,,,,,[],"['An', 'Analysis']",[]
34,https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2,5,Abstract,0,"Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference.","Recently , neural models pretrained on a language modeling task , such as ELMo ( Peters et al. , 2017 ) , OpenAI GPT ( Radford et al. , 2018 ) , and",BERT,"( Devlin et al. , 2018 ) , have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference .","['Recently', ',', 'neural', 'models', 'pretrained', 'on', 'a', 'language', 'modeling', 'task', ',', 'such', 'as', 'ELMo', '(', 'Peters', 'et', 'al.', ',', '2017', ')', ',', 'OpenAI', 'GPT', '(', 'Radford', 'et', 'al.', ',', '2018', ')', ',', 'and', 'BERT', '(', 'Devlin', 'et', 'al.', ',', '2018', ')', ',', 'have', 'achieved', 'impressive', 'results', 'on', 'various', 'natural', 'language', 'processing', 'tasks', 'such', 'as', 'question-answering', 'and', 'natural', 'language', 'inference', '.']","(33, 34)","(148, 152)",0,neural models,0,11,24,11,24,"[(11, 17), (18, 24)]",have achieved,2,184,197,30,43,"[(184, 188), (189, 197)]","['neural', 'models']","['have', 'achieved']"
35,https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2,5,Abstract,1,"In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking.","In this paper , we describe a simple re-implementation of",BERT,for query-based passage re-ranking .,"['In', 'this', 'paper', ',', 'we', 'describe', 'a', 'simple', 're-implementation', 'of', 'BERT', 'for', 'query-based', 'passage', 're-ranking', '.']","(10, 11)","(57, 61)",0,we,0,16,18,16,18,"[(16, 18)]",describe,0,19,27,19,27,"[(19, 27)]",['we'],['describe']
36,https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2,5,Abstract,2,"Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10.","Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10.",,,"['Our', 'system', 'is', 'the', 'state', 'of', 'the', 'art', 'on', 'the', 'TREC-CAR', 'dataset', 'and', 'the', 'top', 'entry', 'in', 'the', 'leaderboard', 'of', 'the', 'MS', 'MARCO', 'passage', 'retrieval', 'task,', 'outperforming', 'the', 'previous', 'state', 'of', 'the', 'art', 'by', '27%', '(relative)', 'in', 'MRR@10.']",,,,system,0,4,10,4,10,"[(4, 10)]",is,0,11,13,11,13,"[(11, 13)]",['system'],['is']
37,https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2,5,Abstract,3,The code to reproduce our results is available at this https URL,The code to reproduce our results is available at this https URL,,,"['The', 'code', 'to', 'reproduce', 'our', 'results', 'is', 'available', 'at', 'this', 'https', 'URL']",,,,The code,0,0,8,0,8,"[(0, 3), (4, 8)]",is,0,34,36,34,36,"[(34, 36)]","['The', 'code']",['is']
38,https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2,5,Title,0,Passage Re-ranking with BERT.,Passage Re-ranking with,BERT,.,"['Passage', 'Re-ranking', 'with', 'BERT', '.']","(3, 4)","(23, 27)",0,Passage Re - ranking,0,0,20,0,20,"[(0, 7), (8, 10), (11, 12), (13, 20)]",,,,,,,[],"['Passage', 'Re', '-', 'ranking']",[]
39,https://www.semanticscholar.org/paper/Assessing-BERT's-Syntactic-Abilities-Goldberg/efeab0dcdb4c1cce5e537e57745d84774be99b9a,6,Abstract,0,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.",I assess the extent to which the recently introduced,BERT,"model captures English syntactic phenomena , using ( 1 ) naturally-occurring subject-verb agreement stimuli ; ( 2 ) `` coloreless green ideas '' subject-verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection ; and ( 3 ) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena .","['I', 'assess', 'the', 'extent', 'to', 'which', 'the', 'recently', 'introduced', 'BERT', 'model', 'captures', 'English', 'syntactic', 'phenomena', ',', 'using', '(', '1', ')', 'naturally-occurring', 'subject-verb', 'agreement', 'stimuli', ';', '(', '2', ')', '``', 'coloreless', 'green', 'ideas', ""''"", 'subject-verb', 'agreement', 'stimuli', ',', 'in', 'which', 'content', 'words', 'in', 'natural', 'sentences', 'are', 'randomly', 'replaced', 'with', 'words', 'sharing', 'the', 'same', 'part-of-speech', 'and', 'inflection', ';', 'and', '(', '3', ')', 'manually', 'crafted', 'stimuli', 'for', 'subject-verb', 'agreement', 'and', 'reflexive', 'anaphora', 'phenomena', '.']","(9, 10)","(52, 56)",0,I,0,0,1,0,1,"[(0, 1)]",assess,0,2,8,2,8,"[(2, 8)]",['I'],['assess']
40,https://www.semanticscholar.org/paper/Assessing-BERT's-Syntactic-Abilities-Goldberg/efeab0dcdb4c1cce5e537e57745d84774be99b9a,6,Abstract,1,The BERT model performs remarkably well on all cases.,The,BERT,model performs remarkably well on all cases .,"['The', 'BERT', 'model', 'performs', 'remarkably', 'well', 'on', 'all', 'cases', '.']","(1, 2)","(3, 7)",0,The BERT model,0,0,14,0,14,"[(0, 3), (4, 8), (9, 14)]",performs,2,15,23,6,14,"[(15, 23)]","['The', 'BERT', 'model']",['performs']
41,https://www.semanticscholar.org/paper/Assessing-BERT's-Syntactic-Abilities-Goldberg/efeab0dcdb4c1cce5e537e57745d84774be99b9a,6,Title,0,Assessing BERT's Syntactic Abilities.,Assessing,BERT,'s Syntactic Abilities .,"['Assessing', 'BERT', ""'s"", 'Syntactic', 'Abilities', '.']","(1, 2)","(9, 13)",0,,,,,,,[],Assessing,0,0,9,0,9,"[(0, 9)]",[],['Assessing']
42,https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3,7,Abstract,0,"Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks.","Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks.",,,"['Pretrained', 'contextual', 'representation', 'models', '(Peters', 'et', 'al.,', '2018;', 'Devlin', 'et', 'al.,', '2018)', 'have', 'pushed', 'forward', 'the', 'state-of-the-art', 'on', 'many', 'NLP', 'tasks.']",,,,contextual representation models Peters et al . Devlin et al .,0,11,84,11,84,"[(11, 21), (22, 36), (37, 43), (46, 52), (53, 55), (56, 58), (59, 60), (70, 76), (77, 79), (80, 82), (83, 84)]",have pushed,0,94,105,94,105,"[(94, 98), (99, 105)]","['contextual', 'representation', 'models', 'Peters', 'et', 'al', '.', 'Devlin', 'et', 'al', '.']","['have', 'pushed']"
43,https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3,7,Abstract,1,"A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task.",A new release of,BERT,"( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task .","['A', 'new', 'release', 'of', 'BERT', '(', 'Devlin', ',', '2018', ')', 'includes', 'a', 'model', 'simultaneously', 'pretrained', 'on', '104', 'languages', 'with', 'impressive', 'performance', 'for', 'zero-shot', 'cross-lingual', 'transfer', 'on', 'a', 'natural', 'language', 'inference', 'task', '.']","(4, 5)","(16, 20)",0,A new release,0,0,13,0,13,"[(0, 1), (2, 5), (6, 13)]",includes,2,40,48,18,26,"[(40, 48)]","['A', 'new', 'release']",['includes']
44,https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3,7,Abstract,2,"This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing.",This paper explores the broader cross-lingual potential of,mBERT,"( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families : NLI , document classification , NER , POS tagging , and dependency parsing .","['This', 'paper', 'explores', 'the', 'broader', 'cross-lingual', 'potential', 'of', 'mBERT', '(', 'multilingual', ')', 'as', 'a', 'zero', 'shot', 'language', 'transfer', 'model', 'on', '5', 'NLP', 'tasks', 'covering', 'a', 'total', 'of', '39', 'languages', 'from', 'various', 'language', 'families', ':', 'NLI', ',', 'document', 'classification', ',', 'NER', ',', 'POS', 'tagging', ',', 'and', 'dependency', 'parsing', '.']","(8, 9)","(58, 63)",1,This paper,0,0,10,0,10,"[(0, 4), (5, 10)]",explores,0,11,19,11,19,"[(11, 19)]","['This', 'paper']",['explores']
45,https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3,7,Abstract,3,We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task.,We compare,mBERT,with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task .,"['We', 'compare', 'mBERT', 'with', 'the', 'best-published', 'methods', 'for', 'zero-shot', 'cross-lingual', 'transfer', 'and', 'find', 'mBERT', 'competitive', 'on', 'each', 'task', '.']","(2, 3)","(10, 15)",1,We,0,0,2,0,2,"[(0, 2)]",compare shot and find,0,3,100,3,100,"[(3, 10), (62, 66), (92, 95), (96, 100)]",['We'],"['compare', 'shot', 'find']"
46,https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3,7,Abstract,3,We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task.,We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find,mBERT,competitive on each task .,"['We', 'compare', 'mBERT', 'with', 'the', 'best-published', 'methods', 'for', 'zero-shot', 'cross-lingual', 'transfer', 'and', 'find', 'mBERT', 'competitive', 'on', 'each', 'task', '.']","(13, 14)","(94, 99)",1,We,0,0,2,0,2,"[(0, 2)]",compare shot and find,0,3,100,3,100,"[(3, 10), (62, 66), (92, 95), (96, 100)]",['We'],"['compare', 'shot', 'find']"
47,https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3,7,Abstract,4,"Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.","Additionally , we investigate the most effective strategy for utilizing",mBERT,"in this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross-lingual transfer .","['Additionally', ',', 'we', 'investigate', 'the', 'most', 'effective', 'strategy', 'for', 'utilizing', 'mBERT', 'in', 'this', 'manner', ',', 'determine', 'to', 'what', 'extent', 'mBERT', 'generalizes', 'away', 'from', 'language', 'specific', 'features', ',', 'and', 'measure', 'factors', 'that', 'influence', 'cross-lingual', 'transfer', '.']","(10, 11)","(71, 76)",1,we,0,15,17,15,17,"[(15, 17)]",investigate determine,0,18,104,18,104,"[(18, 29), (95, 104)]",['we'],"['investigate', 'determine']"
48,https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3,7,Abstract,4,"Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.","Additionally , we investigate the most effective strategy for utilizing mBERT in this manner , determine to what extent",mBERT,"generalizes away from language specific features , and measure factors that influence cross-lingual transfer .","['Additionally', ',', 'we', 'investigate', 'the', 'most', 'effective', 'strategy', 'for', 'utilizing', 'mBERT', 'in', 'this', 'manner', ',', 'determine', 'to', 'what', 'extent', 'mBERT', 'generalizes', 'away', 'from', 'language', 'specific', 'features', ',', 'and', 'measure', 'factors', 'that', 'influence', 'cross-lingual', 'transfer', '.']","(19, 20)","(119, 124)",1,we,0,15,17,15,17,"[(15, 17)]",investigate determine,0,18,104,18,104,"[(18, 29), (95, 104)]",['we'],"['investigate', 'determine']"
49,https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3,7,Title,0,"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT.","Beto , Bentz , Becas : The Surprising Cross-Lingual Effectiveness of",BERT,.,"['Beto', ',', 'Bentz', ',', 'Becas', ':', 'The', 'Surprising', 'Cross-Lingual', 'Effectiveness', 'of', 'BERT', '.']","(11, 12)","(68, 72)",0,Beto Bentz Becas The Surprising Cross Lingual Effectiveness,0,0,67,0,67,"[(0, 4), (7, 12), (15, 20), (23, 26), (27, 37), (38, 43), (46, 53), (54, 67)]",,,,,,,[],"['Beto', 'Bentz', 'Becas', 'The', 'Surprising', 'Cross', 'Lingual', 'Effectiveness']",[]
50,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Abstract,0,"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks.","Language model pre-training , such as",BERT,", has significantly improved the performances of many natural language processing tasks .","['Language', 'model', 'pre-training', ',', 'such', 'as', 'BERT', ',', 'has', 'significantly', 'improved', 'the', 'performances', 'of', 'many', 'natural', 'language', 'processing', 'tasks', '.']","(6, 7)","(37, 41)",0,Language model - training,0,0,29,0,29,"[(0, 8), (9, 14), (19, 20), (21, 29)]",has improved,2,47,73,4,30,"[(47, 50), (65, 73)]","['Language', 'model', '-', 'training']","['has', 'improved']"
51,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Abstract,1,"However, pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on some resource-restricted devices.","However, pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on some resource-restricted devices.",,,"['However,', 'pre-trained', 'language', 'models', 'are', 'usually', 'computationally', 'expensive', 'and', 'memory', 'intensive,', 'so', 'it', 'is', 'difficult', 'to', 'effectively', 'execute', 'them', 'on', 'some', 'resource-restricted', 'devices.']",,,,,,,,,,[],so,0,101,103,101,103,"[(101, 103)]","['-', 'trained', 'language', 'models', 'it']","['are', 'is', 'execute']"
52,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Abstract,2,"To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel transformer distillation method that is a specially designed knowledge distillation (KD) method for transformer-based models.","To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel transformer distillation method that is a specially designed knowledge distillation (KD) method for transformer-based models.",,,"['To', 'accelerate', 'inference', 'and', 'reduce', 'model', 'size', 'while', 'maintaining', 'accuracy,', 'we', 'firstly', 'propose', 'a', 'novel', 'transformer', 'distillation', 'method', 'that', 'is', 'a', 'specially', 'designed', 'knowledge', 'distillation', '(KD)', 'method', 'for', 'transformer-based', 'models.']",,,,we,0,75,77,75,77,"[(75, 77)]",propose,0,86,93,86,93,"[(86, 93)]",['we'],"['accelerate', 'reduce', 'propose']"
53,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Abstract,3,"By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT.","By leveraging this new KD method , the plenty of knowledge encoded in a large teacher",BERT,can be well transferred to a small student TinyBERT .,"['By', 'leveraging', 'this', 'new', 'KD', 'method', ',', 'the', 'plenty', 'of', 'knowledge', 'encoded', 'in', 'a', 'large', 'teacher', 'BERT', 'can', 'be', 'well', 'transferred', 'to', 'a', 'small', 'student', 'TinyBERT', '.']","(16, 17)","(85, 89)",0,the plenty,0,35,45,35,45,"[(35, 38), (39, 45)]",can be transferred,2,91,114,0,23,"[(91, 94), (95, 97), (103, 114)]","['the', 'plenty']","['can', 'be', 'transferred']"
54,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Abstract,3,"By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT.","By leveraging this new KD method , the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student Tiny",BERT,.,"['By', 'leveraging', 'this', 'new', 'KD', 'method', ',', 'the', 'plenty', 'of', 'knowledge', 'encoded', 'in', 'a', 'large', 'teacher', 'BERT', 'can', 'be', 'well', 'transferred', 'to', 'a', 'small', 'student', 'Tiny', 'BERT', '.']","(26, 27)","(138, 142)",0,the plenty knowledge,0,35,58,35,58,"[(35, 38), (39, 45), (49, 58)]",can be transferred,0,91,114,91,114,"[(91, 94), (95, 97), (103, 114)]","['the', 'plenty', 'knowledge']","['can', 'be', 'transferred']"
55,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Abstract,4,"Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages.","Moreover , we introduce a new two-stage learning framework for",TinyBERT,", which performs transformer distillation at both the pre-training and task-specific learning stages .","['Moreover', ',', 'we', 'introduce', 'a', 'new', 'two-stage', 'learning', 'framework', 'for', 'TinyBERT', ',', 'which', 'performs', 'transformer', 'distillation', 'at', 'both', 'the', 'pre-training', 'and', 'task-specific', 'learning', 'stages', '.']","(10, 11)","(62, 70)",4,we,0,11,13,11,13,"[(11, 13)]",introduce,0,14,23,14,23,"[(14, 23)]",['we'],['introduce']
56,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Abstract,5,This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.,This framework ensures that,TinyBERT,can capture both the general-domain and task-specific knowledge of the teacher BERT .,"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general-domain', 'and', 'task-specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']","(4, 5)","(27, 35)",4,This framework,0,0,14,0,14,"[(0, 4), (5, 14)]",ensures,0,15,22,15,22,"[(15, 22)]","['This', 'framework']",['ensures']
57,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Abstract,6,"TinyBERT is empirically effective and achieves comparable results with BERT in GLUE datasets, while being 7.5x smaller and 9.4x faster on inference.",,TinyBERT,"is empirically effective and achieves comparable results with BERT in GLUE datasets , while being 7.5x smaller and 9.4x faster on inference .","['TinyBERT', 'is', 'empirically', 'effective', 'and', 'achieves', 'comparable', 'results', 'with', 'BERT', 'in', 'GLUE', 'datasets', ',', 'while', 'being', '7.5x', 'smaller', 'and', '9.4x', 'faster', 'on', 'inference', '.']","(0, 1)","(0, 8)",4,TinyBERT,1,0,8,0,8,"[(0, 8)]",is and achieves,2,9,46,0,37,"[(9, 11), (34, 37), (38, 46)]",['TinyBERT'],"['is', 'achieves']"
58,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Abstract,7,"TinyBERT is also significantly better than state-of-the-art baselines, even with only about 28% parameters and 31% inference time of baselines",,TinyBERT,"is also significantly better than state-of-the-art baselines , even with only about 28 % parameters and 31 % inference time of baselines","['TinyBERT', 'is', 'also', 'significantly', 'better', 'than', 'state-of-the-art', 'baselines', ',', 'even', 'with', 'only', 'about', '28', '%', 'parameters', 'and', '31', '%', 'inference', 'time', 'of', 'baselines']","(0, 1)","(0, 8)",4,TinyBERT,1,0,8,0,8,"[(0, 8)]",is,2,9,11,0,2,"[(9, 11)]",['TinyBERT'],['is']
59,https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7,8,Title,0,TinyBERT: Distilling BERT for Natural Language Understanding.,,TinyBERT,: Distilling BERT for Natural Language Understanding .,"['TinyBERT', ':', 'Distilling', 'BERT', 'for', 'Natural', 'Language', 'Understanding', '.']","(0, 1)","(0, 8)",4,,,,,,,[],TinyBERT,1,0,8,0,8,"[(0, 8)]",[],['Distilling']
60,https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3,9,Abstract,0,"BERT, a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks.",,BERT,", a pre-trained Transformer model , has achieved ground-breaking performance on multiple NLP tasks .","['BERT', ',', 'a', 'pre-trained', 'Transformer', 'model', ',', 'has', 'achieved', 'ground-breaking', 'performance', 'on', 'multiple', 'NLP', 'tasks', '.']","(0, 1)","(0, 4)",0,BERT a pre - Transformer model,1,0,40,0,40,"[(0, 4), (7, 8), (9, 12), (13, 14), (23, 34), (35, 40)]",has achieved,2,43,55,38,50,"[(43, 46), (47, 55)]","['BERT', 'a', 'pre', '-', 'Transformer', 'model']","['has', 'achieved', 'breaking']"
61,https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3,9,Abstract,1,"In this paper, we describe BERTSUM, a simple variant of BERT, for extractive summarization.","In this paper , we describe",BERTSUM,", a simple variant of BERT , for extractive summarization .","['In', 'this', 'paper', ',', 'we', 'describe', 'BERTSUM', ',', 'a', 'simple', 'variant', 'of', 'BERT', ',', 'for', 'extractive', 'summarization', '.']","(6, 7)","(27, 34)",0,we,0,16,18,16,18,"[(16, 18)]",describe,0,19,27,19,27,"[(19, 27)]",['we'],['describe']
62,https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3,9,Abstract,2,"Our system is the state of the art on the CNN/Dailymail dataset, outperforming the previous best-performed system by 1.65 on ROUGE-L.","Our system is the state of the art on the CNN/Dailymail dataset, outperforming the previous best-performed system by 1.65 on ROUGE-L.",,,"['Our', 'system', 'is', 'the', 'state', 'of', 'the', 'art', 'on', 'the', 'CNN/Dailymail', 'dataset,', 'outperforming', 'the', 'previous', 'best-performed', 'system', 'by', '1.65', 'on', 'ROUGE-L.']",,,,system,0,4,10,4,10,"[(4, 10)]",is outperforming,0,11,81,11,81,"[(11, 13), (68, 81)]",['system'],"['is', 'outperforming']"
63,https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3,9,Abstract,3,The codes to reproduce our results are available at this https URL,The codes to reproduce our results are available at this https URL,,,"['The', 'codes', 'to', 'reproduce', 'our', 'results', 'are', 'available', 'at', 'this', 'https', 'URL']",,,,The codes,0,0,9,0,9,"[(0, 3), (4, 9)]",are,0,35,38,35,38,"[(35, 38)]","['The', 'codes']",['are']
64,https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3,9,Title,0,Fine-tune BERT for Extractive Summarization.,Fine-tune,BERT,for Extractive Summarization .,"['Fine-tune', 'BERT', 'for', 'Extractive', 'Summarization', '.']","(1, 2)","(9, 13)",0,tune BERT,0,7,16,7,16,"[(7, 11), (12, 16)]",,,,,,,[],"['tune', 'BERT']",[]
65,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,0,Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making.,Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making.,,,"['Question-answering', 'plays', 'an', 'important', 'role', 'in', 'e-commerce', 'as', 'it', 'allows', 'potential', 'customers', 'to', 'actively', 'seek', 'crucial', 'information', 'about', 'products', 'or', 'services', 'to', 'help', 'their', 'purchase', 'decision', 'making.']",,,,Question answering,0,0,20,0,20,"[(0, 8), (11, 20)]",plays,0,21,26,21,26,"[(21, 26)]","['Question', 'answering']",['plays']
66,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,1,"Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions.","Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions.",,,"['Inspired', 'by', 'the', 'recent', 'success', 'of', 'machine', 'reading', 'comprehension', '(MRC)', 'on', 'formal', 'documents,', 'this', 'paper', 'explores', 'the', 'potential', 'of', 'turning', 'customer', 'reviews', 'into', 'a', 'large', 'source', 'of', 'knowledge', 'that', 'can', 'be', 'exploited', 'to', 'answer', 'user', 'questions.']",,,,this paper,0,94,104,94,104,"[(94, 98), (99, 104)]",explores,0,105,113,105,113,"[(105, 113)]","['this', 'paper']","['Inspired', 'explores']"
67,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,2,We call this problem Review Reading Comprehension (RRC).,We call this problem Review Reading Comprehension (RRC).,,,"['We', 'call', 'this', 'problem', 'Review', 'Reading', 'Comprehension', '(RRC).']",,,,We,0,0,2,0,2,"[(0, 2)]",call,0,3,7,3,7,"[(3, 7)]",['We'],['call']
68,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,3,"To the best of our knowledge, no existing work has been done on RRC.","To the best of our knowledge, no existing work has been done on RRC.",,,"['To', 'the', 'best', 'of', 'our', 'knowledge,', 'no', 'existing', 'work', 'has', 'been', 'done', 'on', 'RRC.']",,,,no work,0,31,47,31,47,"[(31, 33), (43, 47)]",has been done,0,48,61,48,61,"[(48, 51), (52, 56), (57, 61)]","['no', 'work']","['has', 'been', 'done']"
69,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,4,"In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis.","In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis.",,,"['In', 'this', 'work,', 'we', 'first', 'build', 'an', 'RRC', 'dataset', 'called', 'ReviewRC', 'based', 'on', 'a', 'popular', 'benchmark', 'for', 'aspect-based', 'sentiment', 'analysis.']",,,,we,0,15,17,15,17,"[(15, 17)]",build,0,24,29,24,29,"[(24, 29)]",['we'],['build']
70,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,5,"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.","Since ReviewRC has limited training examples for RRC ( and also for aspect-based sentiment analysis ) , we then explore a novel post-training approach on the popular language model",BERT,to enhance the performance of fine-tuning of BERT for RRC .,"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect-based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post-training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine-tuning', 'of', 'BERT', 'for', 'RRC', '.']","(29, 30)","(180, 184)",0,we,0,106,108,106,108,"[(106, 108)]",explore,0,114,121,114,121,"[(114, 121)]",['we'],['explore']
71,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,5,"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.","Since ReviewRC has limited training examples for RRC ( and also for aspect-based sentiment analysis ) , we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of",BERT,for RRC .,"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect-based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post-training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine-tuning', 'of', 'BERT', 'for', 'RRC', '.']","(37, 38)","(230, 234)",0,we,0,106,108,106,108,"[(106, 108)]",explore,0,114,121,114,121,"[(114, 121)]",['we'],['explore']
72,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,6,"To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis.","To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis.",,,"['To', 'show', 'the', 'generality', 'of', 'the', 'approach,', 'the', 'proposed', 'post-training', 'is', 'also', 'applied', 'to', 'some', 'other', 'review-based', 'tasks', 'such', 'as', 'aspect', 'extraction', 'and', 'aspect', 'sentiment', 'classification', 'in', 'aspect-based', 'sentiment', 'analysis.']",,,,the generality the post - training,0,8,69,8,69,"[(8, 11), (12, 22), (41, 44), (54, 58), (59, 60), (61, 69)]",show is applied,0,3,85,3,85,"[(3, 7), (70, 72), (78, 85)]","['the', 'generality', 'the', 'post', '-', 'training']","['show', 'is', 'applied']"
73,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,7,Experimental results demonstrate that the proposed post-training is highly effective.,Experimental results demonstrate that the proposed post-training is highly effective.,,,"['Experimental', 'results', 'demonstrate', 'that', 'the', 'proposed', 'post-training', 'is', 'highly', 'effective.']",,,,Experimental results,0,0,20,0,20,"[(0, 12), (13, 20)]",demonstrate,0,21,32,21,32,"[(21, 32)]","['Experimental', 'results']",['demonstrate']
74,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Abstract,8,The datasets and code are available at this https URL,The datasets and code are available at this https URL,,,"['The', 'datasets', 'and', 'code', 'are', 'available', 'at', 'this', 'https', 'URL']",,,,The datasets code,0,0,21,0,21,"[(0, 3), (4, 12), (17, 21)]",are,0,22,25,22,25,"[(22, 25)]","['The', 'datasets', 'code']",['are']
75,https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5,10,Title,0,BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis.,,BERT,Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis .,"['BERT', 'Post-Training', 'for', 'Review', 'Reading', 'Comprehension', 'and', 'Aspect-based', 'Sentiment', 'Analysis', '.']","(0, 1)","(0, 4)",0,BERT Post - Training Sentiment Analysis,1,0,91,0,91,"[(0, 4), (5, 9), (10, 11), (12, 20), (73, 82), (83, 91)]",,,,,,,[],"['BERT', 'Post', '-', 'Training', 'Sentiment', 'Analysis']",[]
76,https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9,11,Abstract,0,Language model pre-training has proven to be useful in learning universal language representations.,Language model pre-training has proven to be useful in learning universal language representations.,,,"['Language', 'model', 'pre-training', 'has', 'proven', 'to', 'be', 'useful', 'in', 'learning', 'universal', 'language', 'representations.']",,,,Language model - training,0,0,29,0,29,"[(0, 8), (9, 14), (19, 20), (21, 29)]",has proven,0,30,40,30,40,"[(30, 33), (34, 40)]","['Language', 'model', '-', 'training']","['has', 'proven', 'be']"
77,https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9,11,Abstract,1,"As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks.","As a state-of-the-art language model pre-training model ,",BERT,( Bidirectional Encoder Representations from Transformers ) has achieved amazing results in many language understanding tasks .,"['As', 'a', 'state-of-the-art', 'language', 'model', 'pre-training', 'model', ',', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', 'has', 'achieved', 'amazing', 'results', 'in', 'many', 'language', 'understanding', 'tasks', '.']","(8, 9)","(57, 61)",0,BERT Bidirectional Encoder Representations,2,66,110,3,47,"[(66, 70), (73, 86), (87, 94), (95, 110)]",has achieved,2,131,143,68,80,"[(131, 134), (135, 143)]","['BERT', 'Bidirectional', 'Encoder', 'Representations']","['has', 'achieved']"
78,https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9,11,Abstract,2,"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.","In this paper , we conduct exhaustive experiments to investigate different fine-tuning methods of",BERT,on text classification task and provide a general solution for BERT fine-tuning .,"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine-tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine-tuning', '.']","(14, 15)","(97, 101)",0,we,0,16,18,16,18,"[(16, 18)]",conduct,0,19,26,19,26,"[(19, 26)]",['we'],['conduct']
79,https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9,11,Abstract,2,"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.","In this paper , we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for",BERT,fine-tuning .,"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine-tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine-tuning', '.']","(25, 26)","(165, 169)",0,we,0,16,18,16,18,"[(16, 18)]",conduct,0,19,26,19,26,"[(19, 26)]",['we'],['conduct']
80,https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9,11,Abstract,3,"Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets","Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets",,,"['Finally,', 'the', 'proposed', 'solution', 'obtains', 'new', 'state-of-the-art', 'results', 'on', 'eight', 'widely-studied', 'text', 'classification', 'datasets']",,,,the solution,0,10,31,10,31,"[(10, 13), (23, 31)]",,,,,,,[],"['the', 'solution']",[]
81,https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9,11,Title,0,How to Fine-Tune BERT for Text Classification?.,How to Fine-Tune,BERT,for Text Classification ? .,"['How', 'to', 'Fine-Tune', 'BERT', 'for', 'Text', 'Classification', '?', '.']","(3, 4)","(16, 20)",0,,,,,,,[],,,,,,,[],[],[]
82,https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18,12,Abstract,0,"We show that BERT (Devlin et al., 2018) is a Markov random field language model.",We show that,BERT,"( Devlin et al. , 2018 ) is a Markov random field language model .","['We', 'show', 'that', 'BERT', '(', 'Devlin', 'et', 'al.', ',', '2018', ')', 'is', 'a', 'Markov', 'random', 'field', 'language', 'model', '.']","(3, 4)","(12, 16)",0,We,0,0,2,0,2,"[(0, 2)]",show,0,3,7,3,7,"[(3, 7)]",['We'],['show']
83,https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18,12,Abstract,1,This formulation gives way to a natural procedure to sample sentences from BERT.,This formulation gives way to a natural procedure to sample sentences from,BERT,.,"['This', 'formulation', 'gives', 'way', 'to', 'a', 'natural', 'procedure', 'to', 'sample', 'sentences', 'from', 'BERT', '.']","(12, 13)","(74, 78)",0,This formulation,0,0,16,0,16,"[(0, 4), (5, 16)]",gives,0,17,22,17,22,"[(17, 22)]","['This', 'formulation']",['gives']
84,https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18,12,Abstract,2,"We generate from BERT and find that it can produce high-quality, fluent generations.",We generate from,BERT,"and find that it can produce high-quality , fluent generations .","['We', 'generate', 'from', 'BERT', 'and', 'find', 'that', 'it', 'can', 'produce', 'high-quality', ',', 'fluent', 'generations', '.']","(3, 4)","(16, 20)",0,We,0,0,2,0,2,"[(0, 2)]",generate and find,0,3,30,3,30,"[(3, 11), (22, 25), (26, 30)]",['We'],"['generate', 'find']"
85,https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18,12,Abstract,3,"Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.","Compared to the generations of a traditional left-to-right language model ,",BERT,generates sentences that are more diverse but of slightly worse quality .,"['Compared', 'to', 'the', 'generations', 'of', 'a', 'traditional', 'left-to-right', 'language', 'model', ',', 'BERT', 'generates', 'sentences', 'that', 'are', 'more', 'diverse', 'but', 'of', 'slightly', 'worse', 'quality', '.']","(11, 12)","(75, 79)",0,BERT,2,80,84,-1,3,"[(80, 84)]",generates,2,85,94,4,13,"[(85, 94)]",['BERT'],['generates']
86,https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18,12,Title,0,"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.",,BERT,"has a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .","['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It', 'Must', 'Speak', ':', 'BERT', 'as', 'a', 'Markov', 'Random', 'Field', 'Language', 'Model', '.']","(0, 1)","(0, 4)",0,,,,,,,[],and,2,19,22,14,17,"[(19, 22)]","['BERT', 'It']","['has', 'Must', 'Speak']"
87,https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18,12,Title,0,"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.","BERT has a Mouth , and It Must Speak :",BERT,as a Markov Random Field Language Model .,"['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It', 'Must', 'Speak', ':', 'BERT', 'as', 'a', 'Markov', 'Random', 'Field', 'Language', 'Model', '.']","(10, 11)","(38, 42)",0,,,,,,,[],and,0,19,22,19,22,"[(19, 22)]","['BERT', 'It']","['has', 'Must', 'Speak']"
88,https://www.semanticscholar.org/paper/Utilizing-BERT-for-Aspect-Based-Sentiment-Analysis-Sun-Huang/0de47f354468283efc7765ec0b3588b2ae483c77,13,Abstract,0,"Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA).","Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA).",,,"['Aspect-based', 'sentiment', 'analysis', '(ABSA),', 'which', 'aims', 'to', 'identify', 'fine-grained', 'opinion', 'polarity', 'towards', 'a', 'specific', 'aspect,', 'is', 'a', 'challenging', 'subtask', 'of', 'sentiment', 'analysis', '(SA).']",,,,Aspect sentiment analysis ABSA,0,0,40,0,40,"[(0, 6), (15, 24), (25, 33), (36, 40)]",is,0,128,130,128,130,"[(128, 130)]","['Aspect', 'sentiment', 'analysis', 'ABSA']",['is']
89,https://www.semanticscholar.org/paper/Utilizing-BERT-for-Aspect-Based-Sentiment-Analysis-Sun-Huang/0de47f354468283efc7765ec0b3588b2ae483c77,13,Abstract,1,"In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI).","In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI).",,,"['In', 'this', 'paper,', 'we', 'construct', 'an', 'auxiliary', 'sentence', 'from', 'the', 'aspect', 'and', 'convert', 'ABSA', 'to', 'a', 'sentence-pair', 'classification', 'task,', 'such', 'as', 'question', 'answering', '(QA)', 'and', 'natural', 'language', 'inference', '(NLI).']",,,,we,0,16,18,16,18,"[(16, 18)]",construct and convert,0,19,78,19,78,"[(19, 28), (67, 70), (71, 78)]",['we'],"['construct', 'convert']"
90,https://www.semanticscholar.org/paper/Utilizing-BERT-for-Aspect-Based-Sentiment-Analysis-Sun-Huang/0de47f354468283efc7765ec0b3588b2ae483c77,13,Abstract,2,We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets,We fine-tune the pre-trained model from,BERT,and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets,"['We', 'fine-tune', 'the', 'pre-trained', 'model', 'from', 'BERT', 'and', 'achieve', 'new', 'state-of-the-art', 'results', 'on', 'SentiHood', 'and', 'SemEval-2014', 'Task', '4', 'datasets']","(6, 7)","(39, 43)",0,We tune the model new state the art results,0,0,95,0,95,"[(0, 2), (10, 14), (15, 18), (33, 38), (61, 64), (65, 70), (78, 81), (84, 87), (88, 95)]",,,,,,,[],"['We', 'tune', 'the', 'model', 'new', 'state', 'the', 'art', 'results']",[]
91,https://www.semanticscholar.org/paper/Utilizing-BERT-for-Aspect-Based-Sentiment-Analysis-Sun-Huang/0de47f354468283efc7765ec0b3588b2ae483c77,13,Title,0,Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence.,Utilizing,BERT,for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence .,"['Utilizing', 'BERT', 'for', 'Aspect-Based', 'Sentiment', 'Analysis', 'via', 'Constructing', 'Auxiliary', 'Sentence', '.']","(1, 2)","(9, 13)",0,BERT,1,10,14,0,4,"[(10, 14)]",Utilizing,0,0,9,0,9,"[(0, 9)]",['BERT'],['Utilizing']
92,https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543,14,Abstract,0,"Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval.",Following recent successes in applying,BERT,"to question answering , we explore simple applications to ad hoc document retrieval .","['Following', 'recent', 'successes', 'in', 'applying', 'BERT', 'to', 'question', 'answering', ',', 'we', 'explore', 'simple', 'applications', 'to', 'ad', 'hoc', 'document', 'retrieval', '.']","(5, 6)","(38, 42)",0,we,2,68,70,24,26,"[(68, 70)]",explore,2,71,78,27,34,"[(71, 78)]",['we'],['explore']
93,https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543,14,Abstract,1,This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle.,This required confronting the challenge posed by documents that are typically longer than the length of input,BERT,was designed to handle .,"['This', 'required', 'confronting', 'the', 'challenge', 'posed', 'by', 'documents', 'that', 'are', 'typically', 'longer', 'than', 'the', 'length', 'of', 'input', 'BERT', 'was', 'designed', 'to', 'handle', '.']","(17, 18)","(109, 113)",0,This,0,0,4,0,4,"[(0, 4)]",required,0,5,13,5,13,"[(5, 13)]",['This'],"['required', 'confronting']"
94,https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543,14,Abstract,2,"We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores.","We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores.",,,"['We', 'address', 'this', 'issue', 'by', 'applying', 'inference', 'on', 'sentences', 'individually,', 'and', 'then', 'aggregating', 'sentence', 'scores', 'to', 'produce', 'document', 'scores.']",,,,We,0,0,2,0,2,"[(0, 2)]",address,0,3,10,3,10,"[(3, 10)]",['We'],['address']
95,https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543,14,Abstract,3,"Experiments on TREC microblog and newswire test collections show that our approach is simple yet effective, as we report the highest average precision on these datasets by neural approaches that we are aware of","Experiments on TREC microblog and newswire test collections show that our approach is simple yet effective, as we report the highest average precision on these datasets by neural approaches that we are aware of",,,"['Experiments', 'on', 'TREC', 'microblog', 'and', 'newswire', 'test', 'collections', 'show', 'that', 'our', 'approach', 'is', 'simple', 'yet', 'effective,', 'as', 'we', 'report', 'the', 'highest', 'average', 'precision', 'on', 'these', 'datasets', 'by', 'neural', 'approaches', 'that', 'we', 'are', 'aware', 'of']",,,,Experiments,0,0,11,0,11,"[(0, 11)]",show,0,60,64,60,64,"[(60, 64)]",['Experiments'],['show']
96,https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543,14,Title,0,Simple Applications of BERT for Ad Hoc Document Retrieval.,Simple Applications of,BERT,for Ad Hoc Document Retrieval .,"['Simple', 'Applications', 'of', 'BERT', 'for', 'Ad', 'Hoc', 'Document', 'Retrieval', '.']","(3, 4)","(22, 26)",0,Simple Applications,0,0,19,0,19,"[(0, 6), (7, 19)]",,,,,,,[],"['Simple', 'Applications']",[]
97,https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103,15,Abstract,0,"BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success.",,BERT,"-based architectures currently give state-of-the-art performance on many NLP tasks , but little is known about the exact mechanisms that contribute to its success .","['BERT', '-based', 'architectures', 'currently', 'give', 'state-of-the-art', 'performance', 'on', 'many', 'NLP', 'tasks', ',', 'but', 'little', 'is', 'known', 'about', 'the', 'exact', 'mechanisms', 'that', 'contribute', 'to', 'its', 'success', '.']","(0, 1)","(0, 4)",0,,,,,,,[],but,2,96,99,91,94,"[(96, 99)]","['BERT', '-based', 'architectures', 'little']","['give', 'is', 'known']"
98,https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103,15,Abstract,1,"In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT.","In the current work , we focus on the interpretation of self-attention , which is one of the fundamental underlying components of",BERT,.,"['In', 'the', 'current', 'work', ',', 'we', 'focus', 'on', 'the', 'interpretation', 'of', 'self-attention', ',', 'which', 'is', 'one', 'of', 'the', 'fundamental', 'underlying', 'components', 'of', 'BERT', '.']","(22, 23)","(129, 133)",0,we,0,22,24,22,24,"[(22, 24)]",focus,0,25,30,25,30,"[(25, 30)]",['we'],['focus']
99,https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103,15,Abstract,2,"Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT's heads.","Using a subset of GLUE tasks and a set of handcrafted features-of-interest , we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual",BERT,'s heads .,"['Using', 'a', 'subset', 'of', 'GLUE', 'tasks', 'and', 'a', 'set', 'of', 'handcrafted', 'features-of-interest', ',', 'we', 'propose', 'the', 'methodology', 'and', 'carry', 'out', 'a', 'qualitative', 'and', 'quantitative', 'analysis', 'of', 'the', 'information', 'encoded', 'by', 'the', 'individual', 'BERT', ""'s"", 'heads', '.']","(32, 33)","(202, 206)",0,we,0,81,83,81,83,"[(81, 83)]",propose and carry,0,84,117,84,117,"[(84, 91), (108, 111), (112, 117)]",['we'],"['Using', 'propose', 'carry']"
100,https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103,15,Abstract,3,"Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization.","Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization.",,,"['Our', 'findings', 'suggest', 'that', 'there', 'is', 'a', 'limited', 'set', 'of', 'attention', 'patterns', 'that', 'are', 'repeated', 'across', 'different', 'heads,', 'indicating', 'the', 'overall', 'model', 'overparametrization.']",,,,findings,0,4,12,4,12,"[(4, 12)]",suggest,0,13,20,13,20,"[(13, 20)]",['findings'],['suggest']
101,https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103,15,Abstract,4,"While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks.","While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks.",,,"['While', 'different', 'heads', 'consistently', 'use', 'the', 'same', 'attention', 'patterns,', 'they', 'have', 'varying', 'impact', 'on', 'performance', 'across', 'different', 'tasks.']",,,,they,0,69,73,69,73,"[(69, 73)]",have,0,74,78,74,78,"[(74, 78)]",['they'],['have']
102,https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103,15,Abstract,5,We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models,We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned,BERT,models,"['We', 'show', 'that', 'manually', 'disabling', 'attention', 'in', 'certain', 'heads', 'leads', 'to', 'a', 'performance', 'improvement', 'over', 'the', 'regular', 'fine-tuned', 'BERT', 'models']","(18, 19)","(121, 125)",0,We,0,0,2,0,2,"[(0, 2)]",show,0,3,7,3,7,"[(3, 7)]",['We'],['show']
103,https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103,15,Title,0,Revealing the Dark Secrets of BERT.,Revealing the Dark Secrets of,BERT,.,"['Revealing', 'the', 'Dark', 'Secrets', 'of', 'BERT', '.']","(5, 6)","(29, 33)",0,,,,,,,[],Revealing,0,0,9,0,9,"[(0, 9)]",[],['Revealing']
104,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,0,Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks.,Bidirectional Encoder Representations from Transformers (,BERT,) has shown marvelous improvements across various NLP tasks .,"['Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '(', 'BERT', ')', 'has', 'shown', 'marvelous', 'improvements', 'across', 'various', 'NLP', 'tasks', '.']","(6, 7)","(57, 61)",0,Bidirectional Encoder Representations,0,0,37,0,37,"[(0, 13), (14, 21), (22, 37)]",has shown,2,65,74,2,11,"[(65, 68), (69, 74)]","['Bidirectional', 'Encoder', 'Representations']","['has', 'shown']"
105,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,1,"Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT.","Recently , an upgraded version of",BERT,"has been released with Whole Word Masking ( WWM ) , which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT .","['Recently', ',', 'an', 'upgraded', 'version', 'of', 'BERT', 'has', 'been', 'released', 'with', 'Whole', 'Word', 'Masking', '(', 'WWM', ')', ',', 'which', 'mitigate', 'the', 'drawbacks', 'of', 'masking', 'partial', 'WordPiece', 'tokens', 'in', 'pre-training', 'BERT', '.']","(6, 7)","(33, 37)",0,an upgraded version,0,11,30,11,30,"[(11, 13), (14, 22), (23, 30)]",has been released,2,39,56,0,17,"[(39, 42), (43, 47), (48, 56)]","['an', 'upgraded', 'version']","['has', 'been', 'released']"
106,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,1,"Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT.","Recently , an upgraded version of BERT has been released with Whole Word Masking ( WWM ) , which mitigate the drawbacks of masking partial WordPiece tokens in pre-training",BERT,.,"['Recently', ',', 'an', 'upgraded', 'version', 'of', 'BERT', 'has', 'been', 'released', 'with', 'Whole', 'Word', 'Masking', '(', 'WWM', ')', ',', 'which', 'mitigate', 'the', 'drawbacks', 'of', 'masking', 'partial', 'WordPiece', 'tokens', 'in', 'pre-training', 'BERT', '.']","(29, 30)","(171, 175)",0,an upgraded version,0,11,30,11,30,"[(11, 13), (14, 22), (23, 30)]",has been released,0,39,56,39,56,"[(39, 42), (43, 47), (48, 56)]","['an', 'upgraded', 'version']","['has', 'been', 'released']"
107,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,2,"In this technical report, we adapt whole word masking in Chinese text, that masking the whole word instead of masking Chinese characters, which could bring another challenge in Masked Language Model (MLM) pre-training task.","In this technical report, we adapt whole word masking in Chinese text, that masking the whole word instead of masking Chinese characters, which could bring another challenge in Masked Language Model (MLM) pre-training task.",,,"['In', 'this', 'technical', 'report,', 'we', 'adapt', 'whole', 'word', 'masking', 'in', 'Chinese', 'text,', 'that', 'masking', 'the', 'whole', 'word', 'instead', 'of', 'masking', 'Chinese', 'characters,', 'which', 'could', 'bring', 'another', 'challenge', 'in', 'Masked', 'Language', 'Model', '(MLM)', 'pre-training', 'task.']",,,,we,0,27,29,27,29,"[(27, 29)]",adapt,0,30,35,30,35,"[(30, 35)]",['we'],['adapt']
108,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,3,The model was trained on the latest Chinese Wikipedia dump.,The model was trained on the latest Chinese Wikipedia dump.,,,"['The', 'model', 'was', 'trained', 'on', 'the', 'latest', 'Chinese', 'Wikipedia', 'dump.']",,,,The model,0,0,9,0,9,"[(0, 3), (4, 9)]",was trained,0,10,21,10,21,"[(10, 13), (14, 21)]","['The', 'model']","['was', 'trained']"
109,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,4,We aim to provide easy extensibility and better performance for Chinese BERT without changing any neural architecture or even hyper-parameters.,We aim to provide easy extensibility and better performance for Chinese,BERT,without changing any neural architecture or even hyper-parameters .,"['We', 'aim', 'to', 'provide', 'easy', 'extensibility', 'and', 'better', 'performance', 'for', 'Chinese', 'BERT', 'without', 'changing', 'any', 'neural', 'architecture', 'or', 'even', 'hyper-parameters', '.']","(11, 12)","(71, 75)",0,We,0,0,2,0,2,"[(0, 2)]",aim,0,3,6,3,6,"[(3, 6)]",['We'],"['aim', 'provide']"
110,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,5,"The model is verified on various NLP tasks, across sentence-level to document-level, including sentiment classification (ChnSentiCorp, Sina Weibo), named entity recognition (People Daily, MSRA-NER), natural language inference (XNLI), sentence pair matching (LCQMC, BQ Corpus), and machine reading comprehension (CMRC 2018, DRCD, CAIL RC).","The model is verified on various NLP tasks, across sentence-level to document-level, including sentiment classification (ChnSentiCorp, Sina Weibo), named entity recognition (People Daily, MSRA-NER), natural language inference (XNLI), sentence pair matching (LCQMC, BQ Corpus), and machine reading comprehension (CMRC 2018, DRCD, CAIL RC).",,,"['The', 'model', 'is', 'verified', 'on', 'various', 'NLP', 'tasks,', 'across', 'sentence-level', 'to', 'document-level,', 'including', 'sentiment', 'classification', '(ChnSentiCorp,', 'Sina', 'Weibo),', 'named', 'entity', 'recognition', '(People', 'Daily,', 'MSRA-NER),', 'natural', 'language', 'inference', '(XNLI),', 'sentence', 'pair', 'matching', '(LCQMC,', 'BQ', 'Corpus),', 'and', 'machine', 'reading', 'comprehension', '(CMRC', '2018,', 'DRCD,', 'CAIL', 'RC).']",,,,The model,0,0,9,0,9,"[(0, 3), (4, 9)]",is verified,0,10,21,10,21,"[(10, 12), (13, 21)]","['The', 'model']","['is', 'verified']"
111,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,6,Experimental results on these datasets show that the whole word masking could bring another significant gain.,Experimental results on these datasets show that the whole word masking could bring another significant gain.,,,"['Experimental', 'results', 'on', 'these', 'datasets', 'show', 'that', 'the', 'whole', 'word', 'masking', 'could', 'bring', 'another', 'significant', 'gain.']",,,,Experimental results,0,0,20,0,20,"[(0, 12), (13, 20)]",show,0,39,43,39,43,"[(39, 43)]","['Experimental', 'results']",['show']
112,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,7,"Moreover, we also examine the effectiveness of Chinese pre-trained models: BERT, ERNIE, BERT-wwm.","Moreover , we also examine the effectiveness of Chinese pre-trained models :",BERT,", ERNIE , BERT-wwm .","['Moreover', ',', 'we', 'also', 'examine', 'the', 'effectiveness', 'of', 'Chinese', 'pre-trained', 'models', ':', 'BERT', ',', 'ERNIE', ',', 'BERT-wwm', '.']","(12, 13)","(76, 80)",0,we,0,11,13,11,13,"[(11, 13)]",examine,0,19,26,19,26,"[(19, 26)]",['we'],['examine']
113,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,7,"Moreover, we also examine the effectiveness of Chinese pre-trained models: BERT, ERNIE, BERT-wwm.","Moreover , we also examine the effectiveness of Chinese pre-trained models : BERT , ERNIE ,",BERT,-wwm .,"['Moreover', ',', 'we', 'also', 'examine', 'the', 'effectiveness', 'of', 'Chinese', 'pre-trained', 'models', ':', 'BERT', ',', 'ERNIE', ',', 'BERT', '-wwm', '.']","(16, 17)","(91, 95)",0,we,0,11,13,11,13,"[(11, 13)]",examine,0,19,26,19,26,"[(19, 26)]",['we'],['examine']
114,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Abstract,8,We release the pre-trained model (both TensorFlow and PyTorch) on GitHub: this https URL,We release the pre-trained model (both TensorFlow and PyTorch) on GitHub: this https URL,,,"['We', 'release', 'the', 'pre-trained', 'model', '(both', 'TensorFlow', 'and', 'PyTorch)', 'on', 'GitHub:', 'this', 'https', 'URL']",,,,We,0,0,2,0,2,"[(0, 2)]",release both and,0,3,56,3,56,"[(3, 10), (37, 41), (53, 56)]",['We'],['release']
115,https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20,16,Title,0,Pre-Training with Whole Word Masking for Chinese BERT.,Pre-Training with Whole Word Masking for Chinese,BERT,.,"['Pre-Training', 'with', 'Whole', 'Word', 'Masking', 'for', 'Chinese', 'BERT', '.']","(7, 8)","(48, 52)",0,Pre - Training,0,0,14,0,14,"[(0, 3), (4, 5), (6, 14)]",,,,,,,[],"['Pre', '-', 'Training']",[]
116,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Abstract,0,BERT model has been successfully applied to open-domain QA tasks.,,BERT,model has been successfully applied to open-domain QA tasks .,"['BERT', 'model', 'has', 'been', 'successfully', 'applied', 'to', 'open-domain', 'QA', 'tasks', '.']","(0, 1)","(0, 4)",0,BERT model,1,0,10,0,10,"[(0, 4), (5, 10)]",has been applied,2,11,40,6,35,"[(11, 14), (15, 19), (33, 40)]","['BERT', 'model']","['has', 'been', 'applied']"
117,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Abstract,1,"However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages.","However , previous work trains",BERT,"by viewing passages corresponding to the same question as independent training instances , which may cause incomparable scores for answers from different passages .","['However', ',', 'previous', 'work', 'trains', 'BERT', 'by', 'viewing', 'passages', 'corresponding', 'to', 'the', 'same', 'question', 'as', 'independent', 'training', 'instances', ',', 'which', 'may', 'cause', 'incomparable', 'scores', 'for', 'answers', 'from', 'different', 'passages', '.']","(5, 6)","(30, 34)",0,previous work,0,10,23,10,23,"[(10, 18), (19, 23)]",trains BERT,0,24,35,24,35,"[(24, 30), (31, 35)]","['previous', 'work']","['trains', 'BERT']"
118,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Abstract,2,"To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages.","To tackle this issue , we propose a multi-passage",BERT,"model to globally normalize answer scores across all passages of the same question , and this change enables our QA model find better answers by utilizing more passages .","['To', 'tackle', 'this', 'issue', ',', 'we', 'propose', 'a', 'multi-passage', 'BERT', 'model', 'to', 'globally', 'normalize', 'answer', 'scores', 'across', 'all', 'passages', 'of', 'the', 'same', 'question', ',', 'and', 'this', 'change', 'enables', 'our', 'QA', 'model', 'find', 'better', 'answers', 'by', 'utilizing', 'more', 'passages', '.']","(9, 10)","(49, 53)",0,,,,,,,[],and,2,142,145,87,90,"[(142, 145)]","['we', 'this', 'change']","['tackle', 'propose', 'enables', 'find']"
119,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Abstract,3,"In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%.","In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%.",,,"['In', 'addition,', 'we', 'find', 'that', 'splitting', 'articles', 'into', 'passages', 'with', 'the', 'length', 'of', '100', 'words', 'by', 'sliding', 'window', 'improves', 'performance', 'by', '4%.']",,,,we,0,14,16,14,16,"[(14, 16)]",find,0,17,21,17,21,"[(17, 21)]",['we'],['find']
120,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Abstract,4,"By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%.","By leveraging a passage ranker to select high-quality passages , multi-passage",BERT,gains additional 2 % .,"['By', 'leveraging', 'a', 'passage', 'ranker', 'to', 'select', 'high-quality', 'passages', ',', 'multi-passage', 'BERT', 'gains', 'additional', '2', '%', '.']","(11, 12)","(78, 82)",0,a passage ranker quality passages multi - passage BERT,0,14,87,14,87,"[(14, 15), (16, 23), (24, 30), (48, 55), (56, 64), (67, 72), (73, 74), (75, 82), (83, 87)]",leveraging select gains,0,3,93,3,93,"[(3, 13), (34, 40), (88, 93)]","['a', 'passage', 'ranker', 'quality', 'passages', 'multi', '-', 'passage', 'BERT']","['leveraging', 'select', 'gains']"
121,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Abstract,5,Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks.,Experiments on four standard benchmarks showed that our multi-passage,BERT,outperforms all state-of-the-art models on all benchmarks .,"['Experiments', 'on', 'four', 'standard', 'benchmarks', 'showed', 'that', 'our', 'multi-passage', 'BERT', 'outperforms', 'all', 'state-of-the-art', 'models', 'on', 'all', 'benchmarks', '.']","(9, 10)","(69, 73)",0,Experiments,0,0,11,0,11,"[(0, 11)]",showed,0,40,46,40,46,"[(40, 46)]",['Experiments'],['showed']
122,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Abstract,6,"In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% $F_1$ over all non-BERT models, and 5.8% EM and 6.5% $F_1$ over BERT-based models","In particular , on the OpenSQuAD dataset , our model gains 21.4 % EM and 21.5 % $ F_1 $ over all non-",BERT,"models , and 5.8 % EM and 6.5 % $ F_1 $ over BERT-based models","['In', 'particular', ',', 'on', 'the', 'OpenSQuAD', 'dataset', ',', 'our', 'model', 'gains', '21.4', '%', 'EM', 'and', '21.5', '%', '$', 'F_1', '$', 'over', 'all', 'non-', 'BERT', 'models', ',', 'and', '5.8', '%', 'EM', 'and', '6.5', '%', '$', 'F_1', '$', 'over', 'BERT-based', 'models']","(23, 24)","(101, 105)",0,model,0,47,52,47,52,"[(47, 52)]",gains,0,53,58,53,58,"[(53, 58)]",['model'],['gains']
123,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Abstract,6,"In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% $F_1$ over all non-BERT models, and 5.8% EM and 6.5% $F_1$ over BERT-based models","In particular , on the OpenSQuAD dataset , our model gains 21.4 % EM and 21.5 % $ F_1 $ over all non-BERT models , and 5.8 % EM and 6.5 % $ F_1 $ over",BERT,-based models,"['In', 'particular', ',', 'on', 'the', 'OpenSQuAD', 'dataset', ',', 'our', 'model', 'gains', '21.4', '%', 'EM', 'and', '21.5', '%', '$', 'F_1', '$', 'over', 'all', 'non-BERT', 'models', ',', 'and', '5.8', '%', 'EM', 'and', '6.5', '%', '$', 'F_1', '$', 'over', 'BERT', '-based', 'models']","(36, 37)","(150, 154)",0,model,0,47,52,47,52,"[(47, 52)]",gains,0,53,58,53,58,"[(53, 58)]",['model'],['gains']
124,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Title,0,Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering.,Multi-passage,BERT,: A Globally Normalized BERT Model for Open-domain Question Answering .,"['Multi-passage', 'BERT', ':', 'A', 'Globally', 'Normalized', 'BERT', 'Model', 'for', 'Open-domain', 'Question', 'Answering', '.']","(1, 2)","(13, 17)",0,Multi - passage BERT A BERT Model,0,0,55,0,55,"[(0, 5), (6, 7), (8, 15), (16, 20), (23, 24), (45, 49), (50, 55)]",,,,,,,[],"['Multi', '-', 'passage', 'BERT', 'A', 'BERT', 'Model']",[]
125,https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d,17,Title,0,Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering.,Multi-passage BERT : A Globally Normalized,BERT,Model for Open-domain Question Answering .,"['Multi-passage', 'BERT', ':', 'A', 'Globally', 'Normalized', 'BERT', 'Model', 'for', 'Open-domain', 'Question', 'Answering', '.']","(6, 7)","(42, 46)",0,Multi - passage BERT A BERT Model,0,0,55,0,55,"[(0, 5), (6, 7), (8, 15), (16, 20), (23, 24), (45, 49), (50, 55)]",,,,,,,[],"['Multi', '-', 'passage', 'BERT', 'A', 'BERT', 'Model']",[]
126,https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52,18,Abstract,0,Intent classification and slot filling are two essential tasks for natural language understanding.,Intent classification and slot filling are two essential tasks for natural language understanding.,,,"['Intent', 'classification', 'and', 'slot', 'filling', 'are', 'two', 'essential', 'tasks', 'for', 'natural', 'language', 'understanding.']",,,,Intent classification slot filling,0,0,38,0,38,"[(0, 6), (7, 21), (26, 30), (31, 38)]",are,0,39,42,39,42,"[(39, 42)]","['Intent', 'classification', 'slot', 'filling']",['are']
127,https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52,18,Abstract,1,"They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words.","They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words.",,,"['They', 'often', 'suffer', 'from', 'small-scale', 'human-labeled', 'training', 'data,', 'resulting', 'in', 'poor', 'generalization', 'capability,', 'especially', 'for', 'rare', 'words.']",,,,They,0,0,4,0,4,"[(0, 4)]",suffer,0,11,17,11,17,"[(11, 17)]",['They'],"['suffer', 'resulting']"
128,https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52,18,Abstract,2,"Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning.","Recently a new language representation model ,",BERT,"( Bidirectional Encoder Representations from Transformers ) , facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora , and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning .","['Recently', 'a', 'new', 'language', 'representation', 'model', ',', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', ',', 'facilitates', 'pre-training', 'deep', 'bidirectional', 'representations', 'on', 'large-scale', 'unlabeled', 'corpora', ',', 'and', 'has', 'created', 'state-of-the-art', 'models', 'for', 'a', 'wide', 'variety', 'of', 'natural', 'language', 'processing', 'tasks', 'after', 'simple', 'fine-tuning', '.']","(7, 8)","(46, 50)",0,a new language representation model BERT Bidirectional Encoder Representations,0,9,91,9,91,"[(9, 10), (11, 14), (15, 23), (24, 38), (39, 44), (47, 51), (54, 67), (68, 75), (76, 91)]",facilitates and has created,2,114,228,62,176,"[(114, 125), (213, 216), (217, 220), (221, 228)]","['a', 'new', 'language', 'representation', 'model', 'BERT', 'Bidirectional', 'Encoder', 'Representations']","['facilitates', 'has', 'created']"
129,https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52,18,Abstract,3,"However, there has not been much effort on exploring BERT for natural language understanding.","However , there has not been much effort on exploring",BERT,for natural language understanding .,"['However', ',', 'there', 'has', 'not', 'been', 'much', 'effort', 'on', 'exploring', 'BERT', 'for', 'natural', 'language', 'understanding', '.']","(10, 11)","(53, 57)",0,,,,,,,[],has been,0,16,28,16,28,"[(16, 19), (24, 28)]",[],"['has', 'been']"
130,https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52,18,Abstract,4,"In this work, we propose a joint intent classification and slot filling model based on BERT.","In this work , we propose a joint intent classification and slot filling model based on",BERT,.,"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'joint', 'intent', 'classification', 'and', 'slot', 'filling', 'model', 'based', 'on', 'BERT', '.']","(16, 17)","(87, 91)",0,we,0,15,17,15,17,"[(15, 17)]",propose,0,18,25,18,25,"[(18, 25)]",['we'],['propose']
131,https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52,18,Abstract,5,"Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models","Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models",,,"['Experimental', 'results', 'demonstrate', 'that', 'our', 'proposed', 'model', 'achieves', 'significant', 'improvement', 'on', 'intent', 'classification', 'accuracy,', 'slot', 'filling', 'F1,', 'and', 'sentence-level', 'semantic', 'frame', 'accuracy', 'on', 'several', 'public', 'benchmark', 'datasets,', 'compared', 'to', 'the', 'attention-based', 'recurrent', 'neural', 'network', 'models', 'and', 'slot-gated', 'models']",,,,Experimental results,0,0,20,0,20,"[(0, 12), (13, 20)]",demonstrate,0,21,32,21,32,"[(21, 32)]","['Experimental', 'results']",['demonstrate']
132,https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52,18,Title,0,BERT for Joint Intent Classification and Slot Filling.,,BERT,for Joint Intent Classification and Slot Filling .,"['BERT', 'for', 'Joint', 'Intent', 'Classification', 'and', 'Slot', 'Filling', '.']","(0, 1)","(0, 4)",0,BERT,1,0,4,0,4,"[(0, 4)]",,,,,,,[],['BERT'],[]
133,https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921,19,Abstract,0,Conversational search is an emerging topic in the information retrieval community.,Conversational search is an emerging topic in the information retrieval community.,,,"['Conversational', 'search', 'is', 'an', 'emerging', 'topic', 'in', 'the', 'information', 'retrieval', 'community.']",,,,Conversational search,0,0,21,0,21,"[(0, 14), (15, 21)]",is,0,22,24,22,24,"[(22, 24)]","['Conversational', 'search']",['is']
134,https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921,19,Abstract,1,One of the major challenges to multi-turn conversational search is to model the conversation history to answer the current question.,One of the major challenges to multi-turn conversational search is to model the conversation history to answer the current question.,,,"['One', 'of', 'the', 'major', 'challenges', 'to', 'multi-turn', 'conversational', 'search', 'is', 'to', 'model', 'the', 'conversation', 'history', 'to', 'answer', 'the', 'current', 'question.']",,,,,,,,,,[],is,0,66,68,66,68,"[(66, 68)]",[],"['is', 'model', 'answer']"
135,https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921,19,Abstract,2,Existing methods either prepend history turns to the current question or use complicated attention mechanisms to model the history.,Existing methods either prepend history turns to the current question or use complicated attention mechanisms to model the history.,,,"['Existing', 'methods', 'either', 'prepend', 'history', 'turns', 'to', 'the', 'current', 'question', 'or', 'use', 'complicated', 'attention', 'mechanisms', 'to', 'model', 'the', 'history.']",,,,methods,0,9,16,9,16,"[(9, 16)]",either turns or use,0,17,76,17,76,"[(17, 23), (40, 45), (70, 72), (73, 76)]",['methods'],"['turns', 'use', 'model']"
136,https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921,19,Abstract,3,We propose a conceptually simple yet highly effective approach referred to as history answer embedding.,We propose a conceptually simple yet highly effective approach referred to as history answer embedding.,,,"['We', 'propose', 'a', 'conceptually', 'simple', 'yet', 'highly', 'effective', 'approach', 'referred', 'to', 'as', 'history', 'answer', 'embedding.']",,,,We,0,0,2,0,2,"[(0, 2)]",propose,0,3,10,3,10,"[(3, 10)]",['We'],['propose']
137,https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921,19,Abstract,4,It enables seamless integration of conversation history into a conversational question answering (ConvQA) model built on BERT (Bidirectional Encoder Representations from Transformers).,It enables seamless integration of conversation history into a conversational question answering ( ConvQA ) model built on,BERT,( Bidirectional Encoder Representations from Transformers ) .,"['It', 'enables', 'seamless', 'integration', 'of', 'conversation', 'history', 'into', 'a', 'conversational', 'question', 'answering', '(', 'ConvQA', ')', 'model', 'built', 'on', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', '.']","(18, 19)","(122, 126)",0,It,0,0,2,0,2,"[(0, 2)]",enables,0,3,10,3,10,"[(3, 10)]",['It'],['enables']
138,https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921,19,Abstract,5,"We first explain our view that ConvQA is a simplified but concrete setting of conversational search, and then we provide a general framework to solve ConvQA.","We first explain our view that ConvQA is a simplified but concrete setting of conversational search, and then we provide a general framework to solve ConvQA.",,,"['We', 'first', 'explain', 'our', 'view', 'that', 'ConvQA', 'is', 'a', 'simplified', 'but', 'concrete', 'setting', 'of', 'conversational', 'search,', 'and', 'then', 'we', 'provide', 'a', 'general', 'framework', 'to', 'solve', 'ConvQA.']",,,,we,0,111,113,111,113,"[(111, 113)]",and provide,0,102,121,102,121,"[(102, 105), (114, 121)]","['We', 'we']","['explain', 'provide']"
139,https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921,19,Abstract,6,We further demonstrate the effectiveness of our approach under this framework.,We further demonstrate the effectiveness of our approach under this framework.,,,"['We', 'further', 'demonstrate', 'the', 'effectiveness', 'of', 'our', 'approach', 'under', 'this', 'framework.']",,,,We the effectiveness,0,0,40,0,40,"[(0, 2), (23, 26), (27, 40)]",demonstrate,0,11,22,11,22,"[(11, 22)]","['We', 'the', 'effectiveness']",['demonstrate']
140,https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921,19,Abstract,7,"Finally, we analyze the impact of different numbers of history turns under different settings to provide new insights into conversation history modeling in ConvQA","Finally, we analyze the impact of different numbers of history turns under different settings to provide new insights into conversation history modeling in ConvQA",,,"['Finally,', 'we', 'analyze', 'the', 'impact', 'of', 'different', 'numbers', 'of', 'history', 'turns', 'under', 'different', 'settings', 'to', 'provide', 'new', 'insights', 'into', 'conversation', 'history', 'modeling', 'in', 'ConvQA']",,,,we,0,10,12,10,12,"[(10, 12)]",analyze turns,0,13,69,13,69,"[(13, 20), (64, 69)]",['we'],"['analyze', 'turns', 'provide']"
141,https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921,19,Title,0,BERT with History Answer Embedding for Conversational Question Answering.,,BERT,with History Answer Embedding for Conversational Question Answering .,"['BERT', 'with', 'History', 'Answer', 'Embedding', 'for', 'Conversational', 'Question', 'Answering', '.']","(0, 1)","(0, 4)",0,,,,,,,[],BERT,1,0,4,0,4,"[(0, 4)]",[],['BERT']
142,https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da,20,Abstract,0,This paper studies the performances and behaviors of BERT in ranking tasks.,This paper studies the performances and behaviors of,BERT,in ranking tasks .,"['This', 'paper', 'studies', 'the', 'performances', 'and', 'behaviors', 'of', 'BERT', 'in', 'ranking', 'tasks', '.']","(8, 9)","(52, 56)",0,This paper,0,0,10,0,10,"[(0, 4), (5, 10)]",studies,0,11,18,11,18,"[(11, 18)]","['This', 'paper']",['studies']
143,https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da,20,Abstract,1,We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking.,We explore several different ways to leverage the pre-trained,BERT,and fine-tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document ranking .,"['We', 'explore', 'several', 'different', 'ways', 'to', 'leverage', 'the', 'pre-trained', 'BERT', 'and', 'fine-tune', 'it', 'on', 'two', 'ranking', 'tasks', ':', 'MS', 'MARCO', 'passage', 'reranking', 'and', 'TREC', 'Web', 'Track', 'ad', 'hoc', 'document', 'ranking', '.']","(9, 10)","(61, 65)",0,We,0,0,2,0,2,"[(0, 2)]",explore,0,3,10,3,10,"[(3, 10)]",['We'],['explore']
144,https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da,20,Abstract,2,"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model.",Experimental results on MS MARCO demonstrate the strong effectiveness of,BERT,"in question-answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction-based seq2seq matching model .","['Experimental', 'results', 'on', 'MS', 'MARCO', 'demonstrate', 'the', 'strong', 'effectiveness', 'of', 'BERT', 'in', 'question-answering', 'focused', 'passage', 'ranking', 'tasks', ',', 'as', 'well', 'as', 'the', 'fact', 'that', 'BERT', 'is', 'a', 'strong', 'interaction-based', 'seq2seq', 'matching', 'model', '.']","(10, 11)","(72, 76)",0,Experimental results,0,0,20,0,20,"[(0, 12), (13, 20)]",demonstrate,0,33,44,33,44,"[(33, 44)]","['Experimental', 'results']",['demonstrate']
145,https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da,20,Abstract,2,"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model.","Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks , as well as the fact that",BERT,is a strong interaction-based seq2seq matching model .,"['Experimental', 'results', 'on', 'MS', 'MARCO', 'demonstrate', 'the', 'strong', 'effectiveness', 'of', 'BERT', 'in', 'question-answering', 'focused', 'passage', 'ranking', 'tasks', ',', 'as', 'well', 'as', 'the', 'fact', 'that', 'BERT', 'is', 'a', 'strong', 'interaction-based', 'seq2seq', 'matching', 'model', '.']","(24, 25)","(156, 160)",0,Experimental results,0,0,20,0,20,"[(0, 12), (13, 20)]",demonstrate,0,33,44,33,44,"[(33, 44)]","['Experimental', 'results']",['demonstrate']
146,https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da,20,Abstract,3,Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking.,Experimental results on TREC show the gaps between the,BERT,pre-trained on surrounding contexts and the needs of ad hoc document ranking .,"['Experimental', 'results', 'on', 'TREC', 'show', 'the', 'gaps', 'between', 'the', 'BERT', 'pre-trained', 'on', 'surrounding', 'contexts', 'and', 'the', 'needs', 'of', 'ad', 'hoc', 'document', 'ranking', '.']","(9, 10)","(54, 58)",0,Experimental results,0,0,20,0,20,"[(0, 12), (13, 20)]",show,0,29,33,29,33,"[(29, 33)]","['Experimental', 'results']",['show']
147,https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da,20,Abstract,4,"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker",Analyses illustrate how,BERT,"allocates its attentions between query-document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click-trained neural ranker","['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query-document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click-trained', 'neural', 'ranker']","(3, 4)","(23, 27)",0,Analyses,0,0,8,0,8,"[(0, 8)]",illustrate and,0,9,177,9,177,"[(9, 19), (174, 177)]",['Analyses'],['illustrate']
148,https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da,20,Title,0,Understanding the Behaviors of BERT in Ranking.,Understanding the Behaviors of,BERT,in Ranking .,"['Understanding', 'the', 'Behaviors', 'of', 'BERT', 'in', 'Ranking', '.']","(4, 5)","(30, 34)",0,,,,,,,[],Understanding,0,0,13,0,13,"[(0, 13)]",[],['Understanding']
149,https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b,21,Abstract,0,We present simple BERT-based models for relation extraction and semantic role labeling.,We present simple,BERT,-based models for relation extraction and semantic role labeling .,"['We', 'present', 'simple', 'BERT', '-based', 'models', 'for', 'relation', 'extraction', 'and', 'semantic', 'role', 'labeling', '.']","(3, 4)","(17, 21)",0,We,0,0,2,0,2,"[(0, 2)]",present,0,3,10,3,10,"[(3, 10)]",['We'],['present']
150,https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b,21,Abstract,1,"In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees.","In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees.",,,"['In', 'recent', 'years,', 'state-of-the-art', 'performance', 'has', 'been', 'achieved', 'using', 'neural', 'models', 'by', 'incorporating', 'lexical', 'and', 'syntactic', 'features', 'such', 'as', 'part-of-speech', 'tags', 'and', 'dependency', 'trees.']",,,,state the art performance,0,18,52,18,52,"[(18, 23), (31, 34), (37, 40), (41, 52)]",has been achieved,0,53,70,53,70,"[(53, 56), (57, 61), (62, 70)]","['state', 'the', 'art', 'performance']","['has', 'been', 'achieved', 'using']"
151,https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b,21,Abstract,2,"In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance.","In this paper , extensive experiments on datasets for these two tasks show that without using any external features , a simple",BERT,-based model can achieve state-of-the-art performance .,"['In', 'this', 'paper', ',', 'extensive', 'experiments', 'on', 'datasets', 'for', 'these', 'two', 'tasks', 'show', 'that', 'without', 'using', 'any', 'external', 'features', ',', 'a', 'simple', 'BERT', '-based', 'model', 'can', 'achieve', 'state-of-the-art', 'performance', '.']","(22, 23)","(126, 130)",0,extensive experiments,0,16,37,16,37,"[(16, 25), (26, 37)]",show,0,70,74,70,74,"[(70, 74)]","['extensive', 'experiments']",['show']
152,https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b,21,Abstract,3,"To our knowledge, we are the first to successfully apply BERT in this manner.","To our knowledge , we are the first to successfully apply",BERT,in this manner .,"['To', 'our', 'knowledge', ',', 'we', 'are', 'the', 'first', 'to', 'successfully', 'apply', 'BERT', 'in', 'this', 'manner', '.']","(11, 12)","(57, 61)",0,we,0,19,21,19,21,"[(19, 21)]",are,0,22,25,22,25,"[(22, 25)]",['we'],['are']
153,https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b,21,Abstract,4,Our models provide strong baselines for future research,Our models provide strong baselines for future research,,,"['Our', 'models', 'provide', 'strong', 'baselines', 'for', 'future', 'research']",,,,models,0,4,10,4,10,"[(4, 10)]",provide,0,11,18,11,18,"[(11, 18)]",['models'],['provide']
154,https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b,21,Title,0,Simple BERT Models for Relation Extraction and Semantic Role Labeling.,Simple,BERT,Models for Relation Extraction and Semantic Role Labeling .,"['Simple', 'BERT', 'Models', 'for', 'Relation', 'Extraction', 'and', 'Semantic', 'Role', 'Labeling', '.']","(1, 2)","(6, 10)",0,Simple BERT Models,0,0,18,0,18,"[(0, 6), (7, 11), (12, 18)]",,,,,,,[],"['Simple', 'BERT', 'Models']",[]
155,https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5,22,Abstract,0,Multi-task learning allows the sharing of useful information between multiple related tasks.,Multi-task learning allows the sharing of useful information between multiple related tasks.,,,"['Multi-task', 'learning', 'allows', 'the', 'sharing', 'of', 'useful', 'information', 'between', 'multiple', 'related', 'tasks.']",,,,task learning,0,8,21,8,21,"[(8, 12), (13, 21)]",allows,0,22,28,22,28,"[(22, 28)]","['task', 'learning']",['allows']
156,https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5,22,Abstract,1,"In natural language processing several recent approaches have successfully leveraged unsupervised pre-training on large amounts of data to perform well on various tasks, such as those in the GLUE benchmark.","In natural language processing several recent approaches have successfully leveraged unsupervised pre-training on large amounts of data to perform well on various tasks, such as those in the GLUE benchmark.",,,"['In', 'natural', 'language', 'processing', 'several', 'recent', 'approaches', 'have', 'successfully', 'leveraged', 'unsupervised', 'pre-training', 'on', 'large', 'amounts', 'of', 'data', 'to', 'perform', 'well', 'on', 'various', 'tasks,', 'such', 'as', 'those', 'in', 'the', 'GLUE', 'benchmark.']",,,,several recent approaches,0,31,56,31,56,"[(31, 38), (39, 45), (46, 56)]",have leveraged,0,57,84,57,84,"[(57, 61), (75, 84)]","['several', 'recent', 'approaches']","['have', 'leveraged', 'perform']"
157,https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5,22,Abstract,2,These results are based on fine-tuning on each task separately.,These results are based on fine-tuning on each task separately.,,,"['These', 'results', 'are', 'based', 'on', 'fine-tuning', 'on', 'each', 'task', 'separately.']",,,,These results,0,0,13,0,13,"[(0, 5), (6, 13)]",are based,0,14,23,14,23,"[(14, 17), (18, 23)]","['These', 'results']","['are', 'based']"
158,https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5,22,Abstract,3,"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark, and how to best add task-specific parameters to a pre-trained BERT network, with a high degree of parameter sharing between tasks.",We explore the multi-task learning setting for the recent,BERT,"model on the GLUE benchmark , and how to best add task-specific parameters to a pre-trained BERT network , with a high degree of parameter sharing between tasks .","['We', 'explore', 'the', 'multi-task', 'learning', 'setting', 'for', 'the', 'recent', 'BERT', 'model', 'on', 'the', 'GLUE', 'benchmark', ',', 'and', 'how', 'to', 'best', 'add', 'task-specific', 'parameters', 'to', 'a', 'pre-trained', 'BERT', 'network', ',', 'with', 'a', 'high', 'degree', 'of', 'parameter', 'sharing', 'between', 'tasks', '.']","(9, 10)","(57, 61)",0,We,0,0,2,0,2,"[(0, 2)]",explore,0,3,10,3,10,"[(3, 10)]",['We'],['explore']
159,https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5,22,Abstract,3,"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark, and how to best add task-specific parameters to a pre-trained BERT network, with a high degree of parameter sharing between tasks.","We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark , and how to best add task-specific parameters to a pre-trained",BERT,"network , with a high degree of parameter sharing between tasks .","['We', 'explore', 'the', 'multi-task', 'learning', 'setting', 'for', 'the', 'recent', 'BERT', 'model', 'on', 'the', 'GLUE', 'benchmark', ',', 'and', 'how', 'to', 'best', 'add', 'task-specific', 'parameters', 'to', 'a', 'pre-trained', 'BERT', 'network', ',', 'with', 'a', 'high', 'degree', 'of', 'parameter', 'sharing', 'between', 'tasks', '.']","(26, 27)","(154, 158)",0,We,0,0,2,0,2,"[(0, 2)]",explore,0,3,10,3,10,"[(3, 10)]",['We'],['explore']
160,https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5,22,Abstract,4,"We introduce new adaptation modules, PALs or `projected attention layers', which use a low-dimensional multi-head attention mechanism, based on the idea that it is important to include layers with inductive biases useful for the input domain.","We introduce new adaptation modules, PALs or `projected attention layers', which use a low-dimensional multi-head attention mechanism, based on the idea that it is important to include layers with inductive biases useful for the input domain.",,,"['We', 'introduce', 'new', 'adaptation', 'modules,', 'PALs', 'or', '`projected', 'attention', ""layers',"", 'which', 'use', 'a', 'low-dimensional', 'multi-head', 'attention', 'mechanism,', 'based', 'on', 'the', 'idea', 'that', 'it', 'is', 'important', 'to', 'include', 'layers', 'with', 'inductive', 'biases', 'useful', 'for', 'the', 'input', 'domain.']",,,,We,0,0,2,0,2,"[(0, 2)]",introduce,0,3,12,3,12,"[(3, 12)]",['We'],['introduce']
161,https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5,22,Abstract,5,"By using PALs in parallel with BERT layers, we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset",By using PALs in parallel with,BERT,"layers , we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state-of-the-art results on the Recognizing Textual Entailment dataset","['By', 'using', 'PALs', 'in', 'parallel', 'with', 'BERT', 'layers', ',', 'we', 'match', 'the', 'performance', 'of', 'fine-tuned', 'BERT', 'on', 'the', 'GLUE', 'benchmark', 'with', 'roughly', '7', 'times', 'fewer', 'parameters', ',', 'and', 'obtain', 'state-of-the-art', 'results', 'on', 'the', 'Recognizing', 'Textual', 'Entailment', 'dataset']","(6, 7)","(30, 34)",0,we,2,45,47,9,11,"[(45, 47)]",match,2,48,53,12,17,"[(48, 53)]",['we'],['match']
162,https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5,22,Abstract,5,"By using PALs in parallel with BERT layers, we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset","By using PALs in parallel with BERT layers , we match the performance of fine-tuned",BERT,"on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state-of-the-art results on the Recognizing Textual Entailment dataset","['By', 'using', 'PALs', 'in', 'parallel', 'with', 'BERT', 'layers', ',', 'we', 'match', 'the', 'performance', 'of', 'fine-tuned', 'BERT', 'on', 'the', 'GLUE', 'benchmark', 'with', 'roughly', '7', 'times', 'fewer', 'parameters', ',', 'and', 'obtain', 'state-of-the-art', 'results', 'on', 'the', 'Recognizing', 'Textual', 'Entailment', 'dataset']","(15, 16)","(83, 87)",0,we,0,45,47,45,47,"[(45, 47)]",match,0,48,53,48,53,"[(48, 53)]",['we'],['match']
163,https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5,22,Title,0,BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning.,,BERT,and PALs : Projected Attention Layers for Efficient Adaptation in Multi-Task Learning .,"['BERT', 'and', 'PALs', ':', 'Projected', 'Attention', 'Layers', 'for', 'Efficient', 'Adaptation', 'in', 'Multi-Task', 'Learning', '.']","(0, 1)","(0, 4)",0,BERT PALs Projected Attention Layers,1,0,42,0,42,"[(0, 4), (9, 13), (16, 25), (26, 35), (36, 42)]",,,,,,,[],"['BERT', 'PALs', 'Projected', 'Attention', 'Layers']",[]
164,https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75,23,Abstract,0,"Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models.","Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models.",,,"['Pre-training', 'by', 'language', 'modeling', 'has', 'become', 'a', 'popular', 'and', 'successful', 'approach', 'to', 'NLP', 'tasks,', 'but', 'we', 'have', 'yet', 'to', 'understand', 'exactly', 'what', 'linguistic', 'capacities', 'these', 'pre-training', 'processes', 'confer', 'upon', 'models.']",,,,,,,,,,[],but,0,96,99,96,99,"[(96, 99)]","['Pre', '-', 'training', 'we']","['has', 'become', 'have', 'understand']"
165,https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75,23,Abstract,1,"In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context.","In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context.",,,"['In', 'this', 'paper', 'we', 'introduce', 'a', 'suite', 'of', 'diagnostics', 'drawn', 'from', 'human', 'language', 'experiments,', 'which', 'allow', 'us', 'to', 'ask', 'targeted', 'questions', 'about', 'information', 'used', 'by', 'language', 'models', 'for', 'generating', 'predictions', 'in', 'context.']",,,,we,0,14,16,14,16,"[(14, 16)]",introduce,0,17,26,17,26,"[(17, 26)]",['we'],['introduce']
166,https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75,23,Abstract,2,"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation","As a case study , we apply these diagnostics to the popular",BERT,"model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role-based event prediction— and , in particular , it shows clear insensitivity to the contextual impacts of negation","['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role-based', 'event', 'prediction—', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']","(12, 13)","(59, 63)",0,we,0,18,20,18,20,"[(18, 20)]",apply finding and but,0,21,275,21,275,"[(21, 26), (73, 80), (229, 232), (272, 275)]",['we'],"['apply', 'finding', 'retrieves', 'struggles']"
167,https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75,23,Title,0,What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.,What,BERT,Is Not : Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .,"['What', 'BERT', 'Is', 'Not', ':', 'Lessons', 'from', 'a', 'New', 'Suite', 'of', 'Psycholinguistic', 'Diagnostics', 'for', 'Language', 'Models', '.']","(1, 2)","(4, 8)",0,Lessons,2,19,26,9,16,"[(19, 26)]",,,,,,,[],['Lessons'],[]
168,https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822,24,Abstract,0,"Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks.","Language model pre-training , such as",BERT,", has achieved remarkable results in many NLP tasks .","['Language', 'model', 'pre-training', ',', 'such', 'as', 'BERT', ',', 'has', 'achieved', 'remarkable', 'results', 'in', 'many', 'NLP', 'tasks', '.']","(6, 7)","(37, 41)",0,Language model - training,0,0,29,0,29,"[(0, 8), (9, 14), (19, 20), (21, 29)]",has achieved,2,47,59,4,16,"[(47, 50), (51, 59)]","['Language', 'model', '-', 'training']","['has', 'achieved']"
169,https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822,24,Abstract,1,"However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks.","However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks.",,,"['However,', 'it', 'is', 'unclear', 'why', 'the', 'pre-training-then-fine-tuning', 'paradigm', 'can', 'improve', 'performance', 'and', 'generalization', 'capability', 'across', 'different', 'tasks.']",,,,it,0,10,12,10,12,"[(10, 12)]",is,0,13,15,13,15,"[(13, 15)]",['it'],['is']
170,https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822,24,Abstract,2,"In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets.","In this paper , we propose to visualize loss landscapes and optimization trajectories of fine-tuning",BERT,on specific datasets .,"['In', 'this', 'paper', ',', 'we', 'propose', 'to', 'visualize', 'loss', 'landscapes', 'and', 'optimization', 'trajectories', 'of', 'fine-tuning', 'BERT', 'on', 'specific', 'datasets', '.']","(15, 16)","(100, 104)",0,we,0,16,18,16,18,"[(16, 18)]",propose,0,19,26,19,26,"[(19, 26)]",['we'],"['propose', 'visualize']"
171,https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822,24,Abstract,3,"First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch.","First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch.",,,"['First,', 'we', 'find', 'that', 'pre-training', 'reaches', 'a', 'good', 'initial', 'point', 'across', 'downstream', 'tasks,', 'which', 'leads', 'to', 'wider', 'optima', 'and', 'easier', 'optimization', 'compared', 'with', 'training', 'from', 'scratch.']",,,,we,0,8,10,8,10,"[(8, 10)]",find,0,11,15,11,15,"[(11, 15)]",['we'],['find']
172,https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822,24,Abstract,4,"We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks.","We also demonstrate that the fine-tuning procedure is robust to overfitting , even though",BERT,is highly over-parameterized for downstream tasks .,"['We', 'also', 'demonstrate', 'that', 'the', 'fine-tuning', 'procedure', 'is', 'robust', 'to', 'overfitting', ',', 'even', 'though', 'BERT', 'is', 'highly', 'over-parameterized', 'for', 'downstream', 'tasks', '.']","(14, 15)","(89, 93)",0,We,0,0,2,0,2,"[(0, 2)]",demonstrate,0,8,19,8,19,"[(8, 19)]",['We'],['demonstrate']
173,https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822,24,Abstract,5,"Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface.","Second , the visualization results indicate that fine-tuning",BERT,"tends to generalize better because of the flat and wide optima , and the consistency between the training loss surface and the generalization error surface .","['Second', ',', 'the', 'visualization', 'results', 'indicate', 'that', 'fine-tuning', 'BERT', 'tends', 'to', 'generalize', 'better', 'because', 'of', 'the', 'flat', 'and', 'wide', 'optima', ',', 'and', 'the', 'consistency', 'between', 'the', 'training', 'loss', 'surface', 'and', 'the', 'generalization', 'error', 'surface', '.']","(8, 9)","(60, 64)",0,the visualization results,0,9,34,9,34,"[(9, 12), (13, 26), (27, 34)]",indicate,0,35,43,35,43,"[(35, 43)]","['the', 'visualization', 'results']",['indicate']
174,https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822,24,Abstract,6,"Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language","Third , the lower layers of",BERT,"are more invariant during fine-tuning , which suggests that the layers that are close to input learn more transferable representations of language","['Third', ',', 'the', 'lower', 'layers', 'of', 'BERT', 'are', 'more', 'invariant', 'during', 'fine-tuning', ',', 'which', 'suggests', 'that', 'the', 'layers', 'that', 'are', 'close', 'to', 'input', 'learn', 'more', 'transferable', 'representations', 'of', 'language']","(6, 7)","(27, 31)",0,the lower layers,0,8,24,8,24,"[(8, 11), (12, 17), (18, 24)]",are,2,33,36,0,3,"[(33, 36)]","['the', 'lower', 'layers']",['are']
175,https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822,24,Title,0,Visualizing and Understanding the Effectiveness of BERT.,Visualizing and Understanding the Effectiveness of,BERT,.,"['Visualizing', 'and', 'Understanding', 'the', 'Effectiveness', 'of', 'BERT', '.']","(6, 7)","(50, 54)",0,,,,,,,[],Visualizing and Understanding,0,0,29,0,29,"[(0, 11), (12, 15), (16, 29)]",[],"['Visualizing', 'Understanding']"
176,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,0,Data augmentation methods are often applied to prevent overfitting and improve generalization of deep neural network models.,Data augmentation methods are often applied to prevent overfitting and improve generalization of deep neural network models.,,,"['Data', 'augmentation', 'methods', 'are', 'often', 'applied', 'to', 'prevent', 'overfitting', 'and', 'improve', 'generalization', 'of', 'deep', 'neural', 'network', 'models.']",,,,Data augmentation methods,0,0,25,0,25,"[(0, 4), (5, 17), (18, 25)]",are applied,0,26,43,26,43,"[(26, 29), (36, 43)]","['Data', 'augmentation', 'methods']","['are', 'applied', 'prevent', 'improve']"
177,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,1,Recently proposed contextual augmentation augments labeled sentences by randomly replacing words with more varied substitutions predicted by language model.,Recently proposed contextual augmentation augments labeled sentences by randomly replacing words with more varied substitutions predicted by language model.,,,"['Recently', 'proposed', 'contextual', 'augmentation', 'augments', 'labeled', 'sentences', 'by', 'randomly', 'replacing', 'words', 'with', 'more', 'varied', 'substitutions', 'predicted', 'by', 'language', 'model.']",,,,contextual augmentation,0,18,41,18,41,"[(18, 28), (29, 41)]",proposed labeled,0,9,58,9,58,"[(9, 17), (51, 58)]","['contextual', 'augmentation']","['proposed', 'labeled']"
178,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,2,Bidirectional Encoder Representations from Transformers (BERT) demonstrates that a deep bidirectional language model is more powerful than either an unidirectional language model or the shallow concatenation of a forward and backward model.,Bidirectional Encoder Representations from Transformers (,BERT,) demonstrates that a deep bidirectional language model is more powerful than either an unidirectional language model or the shallow concatenation of a forward and backward model .,"['Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '(', 'BERT', ')', 'demonstrates', 'that', 'a', 'deep', 'bidirectional', 'language', 'model', 'is', 'more', 'powerful', 'than', 'either', 'an', 'unidirectional', 'language', 'model', 'or', 'the', 'shallow', 'concatenation', 'of', 'a', 'forward', 'and', 'backward', 'model', '.']","(6, 7)","(57, 61)",0,Bidirectional Encoder Representations,0,0,37,0,37,"[(0, 13), (14, 21), (22, 37)]",demonstrates,2,65,77,2,14,"[(65, 77)]","['Bidirectional', 'Encoder', 'Representations']",['demonstrates']
179,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,3,We propose a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation.,We propose a novel data augmentation method for labeled sentences called conditional,BERT,contextual augmentation .,"['We', 'propose', 'a', 'novel', 'data', 'augmentation', 'method', 'for', 'labeled', 'sentences', 'called', 'conditional', 'BERT', 'contextual', 'augmentation', '.']","(12, 13)","(84, 88)",0,We,0,0,2,0,2,"[(0, 2)]",propose,0,3,10,3,10,"[(3, 10)]",['We'],['propose']
180,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,4,"We retrofit BERT to conditional BERT by introducing a new conditional masked language model (The term “conditional masked language model” appeared once in original BERT paper, which indicates context-conditional, is equivalent to term “masked language model”.",We retrofit,BERT,"to conditional BERT by introducing a new conditional masked language model ( The term “ conditional masked language model ” appeared once in original BERT paper , which indicates context-conditional , is equivalent to term “ masked language model ” .","['We', 'retrofit', 'BERT', 'to', 'conditional', 'BERT', 'by', 'introducing', 'a', 'new', 'conditional', 'masked', 'language', 'model', '(', 'The', 'term', '“', 'conditional', 'masked', 'language', 'model', '”', 'appeared', 'once', 'in', 'original', 'BERT', 'paper', ',', 'which', 'indicates', 'context-conditional', ',', 'is', 'equivalent', 'to', 'term', '“', 'masked', 'language', 'model', '”', '.']","(2, 3)","(11, 15)",0,We,0,0,2,0,2,"[(0, 2)]",retrofit,0,3,11,3,11,"[(3, 11)]",['We'],"['retrofit', 'introducing', 'appeared', 'indicates', 'is']"
181,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,4,"We retrofit BERT to conditional BERT by introducing a new conditional masked language model (The term “conditional masked language model” appeared once in original BERT paper, which indicates context-conditional, is equivalent to term “masked language model”.",We retrofit BERT to conditional,BERT,"by introducing a new conditional masked language model ( The term “ conditional masked language model ” appeared once in original BERT paper , which indicates context-conditional , is equivalent to term “ masked language model ” .","['We', 'retrofit', 'BERT', 'to', 'conditional', 'BERT', 'by', 'introducing', 'a', 'new', 'conditional', 'masked', 'language', 'model', '(', 'The', 'term', '“', 'conditional', 'masked', 'language', 'model', '”', 'appeared', 'once', 'in', 'original', 'BERT', 'paper', ',', 'which', 'indicates', 'context-conditional', ',', 'is', 'equivalent', 'to', 'term', '“', 'masked', 'language', 'model', '”', '.']","(5, 6)","(31, 35)",0,We,0,0,2,0,2,"[(0, 2)]",retrofit,0,3,11,3,11,"[(3, 11)]",['We'],"['retrofit', 'introducing', 'appeared', 'indicates', 'is']"
182,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,4,"We retrofit BERT to conditional BERT by introducing a new conditional masked language model (The term “conditional masked language model” appeared once in original BERT paper, which indicates context-conditional, is equivalent to term “masked language model”.",We retrofit BERT to conditional BERT by introducing a new conditional masked language model ( The term “ conditional masked language model ” appeared once in original,BERT,"paper , which indicates context-conditional , is equivalent to term “ masked language model ” .","['We', 'retrofit', 'BERT', 'to', 'conditional', 'BERT', 'by', 'introducing', 'a', 'new', 'conditional', 'masked', 'language', 'model', '(', 'The', 'term', '“', 'conditional', 'masked', 'language', 'model', '”', 'appeared', 'once', 'in', 'original', 'BERT', 'paper', ',', 'which', 'indicates', 'context-conditional', ',', 'is', 'equivalent', 'to', 'term', '“', 'masked', 'language', 'model', '”', '.']","(27, 28)","(166, 170)",0,We,0,0,2,0,2,"[(0, 2)]",retrofit,0,3,11,3,11,"[(3, 11)]",['We'],"['retrofit', 'introducing', 'appeared', 'indicates', 'is']"
183,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,5,"In our paper, “conditional masked language model” indicates we apply extra label-conditional constraint to the “masked language model”.)","In our paper, “conditional masked language model” indicates we apply extra label-conditional constraint to the “masked language model”.)",,,"['In', 'our', 'paper,', '“conditional', 'masked', 'language', 'model”', 'indicates', 'we', 'apply', 'extra', 'label-conditional', 'constraint', 'to', 'the', '“masked', 'language', 'model”.)']",,,,conditional masked language model,0,17,50,17,50,"[(17, 28), (29, 35), (36, 44), (45, 50)]",indicates,0,53,62,53,62,"[(53, 62)]","['conditional', 'masked', 'language', 'model']",['indicates']
184,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,6,task.,task.,,,['task.'],,,,task,0,0,4,0,4,"[(0, 4)]",,,,,,,[],['task'],[]
185,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,7,The well trained conditional BERT can be applied to enhance contextual augmentation.,The well trained conditional,BERT,can be applied to enhance contextual augmentation .,"['The', 'well', 'trained', 'conditional', 'BERT', 'can', 'be', 'applied', 'to', 'enhance', 'contextual', 'augmentation', '.']","(4, 5)","(28, 32)",0,The conditional BERT,0,0,33,0,33,"[(0, 3), (17, 28), (29, 33)]",can be applied,2,34,48,0,14,"[(34, 37), (38, 40), (41, 48)]","['The', 'conditional', 'BERT']","['can', 'be', 'applied', 'enhance']"
186,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Abstract,8,Experiments on six various different text classification tasks show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement,Experiments on six various different text classification tasks show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement,,,"['Experiments', 'on', 'six', 'various', 'different', 'text', 'classification', 'tasks', 'show', 'that', 'our', 'method', 'can', 'be', 'easily', 'applied', 'to', 'both', 'convolutional', 'or', 'recurrent', 'neural', 'networks', 'classifier', 'to', 'obtain', 'improvement']",,,,Experiments,0,0,11,0,11,"[(0, 11)]",show,0,63,67,63,67,"[(63, 67)]",['Experiments'],['show']
187,https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68,25,Title,0,Conditional BERT Contextual Augmentation.,Conditional,BERT,Contextual Augmentation .,"['Conditional', 'BERT', 'Contextual', 'Augmentation', '.']","(1, 2)","(11, 15)",0,Conditional BERT Contextual Augmentation,0,0,40,0,40,"[(0, 11), (12, 16), (17, 27), (28, 40)]",,,,,,,[],"['Conditional', 'BERT', 'Contextual', 'Augmentation']",[]
188,https://www.semanticscholar.org/paper/Small-and-Practical-BERT-Models-for-Sequence-Tsai-Riesa/2f9d4887d0022400fc40c774c4c78350c3bc5390,26,Abstract,0,We propose a practical scheme to train a single multilingual sequence labeling model that yields state of the art results and is small and fast enough to run on a single CPU.,We propose a practical scheme to train a single multilingual sequence labeling model that yields state of the art results and is small and fast enough to run on a single CPU.,,,"['We', 'propose', 'a', 'practical', 'scheme', 'to', 'train', 'a', 'single', 'multilingual', 'sequence', 'labeling', 'model', 'that', 'yields', 'state', 'of', 'the', 'art', 'results', 'and', 'is', 'small', 'and', 'fast', 'enough', 'to', 'run', 'on', 'a', 'single', 'CPU.']",,,,We,0,0,2,0,2,"[(0, 2)]",propose,0,3,10,3,10,"[(3, 10)]",['We'],['propose']
189,https://www.semanticscholar.org/paper/Small-and-Practical-BERT-Models-for-Sequence-Tsai-Riesa/2f9d4887d0022400fc40c774c4c78350c3bc5390,26,Abstract,1,"Starting from a public multilingual BERT checkpoint, our final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-the-art multilingual baseline.",Starting from a public multilingual,BERT,"checkpoint , our final model is 6x smaller and 27x faster , and has higher accuracy than a state-of-the-art multilingual baseline .","['Starting', 'from', 'a', 'public', 'multilingual', 'BERT', 'checkpoint', ',', 'our', 'final', 'model', 'is', '6x', 'smaller', 'and', '27x', 'faster', ',', 'and', 'has', 'higher', 'accuracy', 'than', 'a', 'state-of-the-art', 'multilingual', 'baseline', '.']","(5, 6)","(35, 39)",0,final model,2,58,69,17,28,"[(58, 63), (64, 69)]",is and has,2,70,108,29,67,"[(70, 72), (101, 104), (105, 108)]","['final', 'model']","['Starting', 'is', 'has']"
190,https://www.semanticscholar.org/paper/Small-and-Practical-BERT-Models-for-Sequence-Tsai-Riesa/2f9d4887d0022400fc40c774c4c78350c3bc5390,26,Abstract,2,"We show that our model especially outperforms on low-resource languages, and works on codemixed input text without being explicitly trained on codemixed examples.","We show that our model especially outperforms on low-resource languages, and works on codemixed input text without being explicitly trained on codemixed examples.",,,"['We', 'show', 'that', 'our', 'model', 'especially', 'outperforms', 'on', 'low-resource', 'languages,', 'and', 'works', 'on', 'codemixed', 'input', 'text', 'without', 'being', 'explicitly', 'trained', 'on', 'codemixed', 'examples.']",,,,We,0,0,2,0,2,"[(0, 2)]",show,0,3,7,3,7,"[(3, 7)]",['We'],['show']
191,https://www.semanticscholar.org/paper/Small-and-Practical-BERT-Models-for-Sequence-Tsai-Riesa/2f9d4887d0022400fc40c774c4c78350c3bc5390,26,Abstract,3,We showcase the effectiveness of our method by reporting on part-of-speech tagging and morphological prediction on 70 treebanks and 48 languages,We showcase the effectiveness of our method by reporting on part-of-speech tagging and morphological prediction on 70 treebanks and 48 languages,,,"['We', 'showcase', 'the', 'effectiveness', 'of', 'our', 'method', 'by', 'reporting', 'on', 'part-of-speech', 'tagging', 'and', 'morphological', 'prediction', 'on', '70', 'treebanks', 'and', '48', 'languages']",,,,We,0,0,2,0,2,"[(0, 2)]",showcase,0,3,11,3,11,"[(3, 11)]",['We'],['showcase']
192,https://www.semanticscholar.org/paper/Small-and-Practical-BERT-Models-for-Sequence-Tsai-Riesa/2f9d4887d0022400fc40c774c4c78350c3bc5390,26,Title,0,Small and Practical BERT Models for Sequence Labeling.,Small and Practical,BERT,Models for Sequence Labeling .,"['Small', 'and', 'Practical', 'BERT', 'Models', 'for', 'Sequence', 'Labeling', '.']","(3, 4)","(19, 23)",0,BERT Models,1,20,31,0,11,"[(20, 24), (25, 31)]",,,,,,,[],"['BERT', 'Models']",[]
193,https://www.semanticscholar.org/paper/Data-Augmentation-for-BERT-Fine-Tuning-in-Question-Yang-Xie/f5eaf727b80240a13e9f631211c9ecec7e3b9feb,27,Abstract,0,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset.","Recently , a simple combination of passage retrieval using off-the-shelf IR techniques and a",BERT,"reader was found to be very effective for question answering directly on Wikipedia , yielding a large improvement over the previous state of the art on a standard benchmark dataset .","['Recently', ',', 'a', 'simple', 'combination', 'of', 'passage', 'retrieval', 'using', 'off-the-shelf', 'IR', 'techniques', 'and', 'a', 'BERT', 'reader', 'was', 'found', 'to', 'be', 'very', 'effective', 'for', 'question', 'answering', 'directly', 'on', 'Wikipedia', ',', 'yielding', 'a', 'large', 'improvement', 'over', 'the', 'previous', 'state', 'of', 'the', 'art', 'on', 'a', 'standard', 'benchmark', 'dataset', '.']","(14, 15)","(92, 96)",0,a simple combination,0,11,31,11,31,"[(11, 12), (13, 19), (20, 31)]",was found,2,109,118,11,20,"[(109, 112), (113, 118)]","['a', 'simple', 'combination']","['was', 'found', 'be', 'yielding']"
194,https://www.semanticscholar.org/paper/Data-Augmentation-for-BERT-Fine-Tuning-in-Question-Yang-Xie/f5eaf727b80240a13e9f631211c9ecec7e3b9feb,27,Abstract,1,"In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples.","In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples.",,,"['In', 'this', 'paper,', 'we', 'present', 'a', 'data', 'augmentation', 'technique', 'using', 'distant', 'supervision', 'that', 'exploits', 'positive', 'as', 'well', 'as', 'negative', 'examples.']",,,,we,0,16,18,16,18,"[(16, 18)]",present,0,19,26,19,26,"[(19, 26)]",['we'],['present']
195,https://www.semanticscholar.org/paper/Data-Augmentation-for-BERT-Fine-Tuning-in-Question-Yang-Xie/f5eaf727b80240a13e9f631211c9ecec7e3b9feb,27,Abstract,2,"We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"".",We apply a stage-wise approach to fine tuning,BERT,"on multiple datasets , starting with data that is `` furthest '' from the test data and ending with the `` closest '' .","['We', 'apply', 'a', 'stage-wise', 'approach', 'to', 'fine', 'tuning', 'BERT', 'on', 'multiple', 'datasets', ',', 'starting', 'with', 'data', 'that', 'is', '``', 'furthest', ""''"", 'from', 'the', 'test', 'data', 'and', 'ending', 'with', 'the', '``', 'closest', ""''"", '.']","(8, 9)","(45, 49)",0,We,0,0,2,0,2,"[(0, 2)]",apply starting is and ending,0,3,148,3,148,"[(3, 8), (76, 84), (100, 102), (138, 141), (142, 148)]",['We'],"['apply', 'starting', 'is', 'ending']"
196,https://www.semanticscholar.org/paper/Data-Augmentation-for-BERT-Fine-Tuning-in-Question-Yang-Xie/f5eaf727b80240a13e9f631211c9ecec7e3b9feb,27,Abstract,3,"Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets","Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets",,,"['Experimental', 'results', 'show', 'large', 'gains', 'in', 'effectiveness', 'over', 'previous', 'approaches', 'on', 'English', 'QA', 'datasets,', 'and', 'we', 'establish', 'new', 'baselines', 'on', 'two', 'recent', 'Chinese', 'QA', 'datasets']",,,,,,,,,,[],and,0,105,108,105,108,"[(105, 108)]","['Experimental', 'results', 'we']","['show', 'establish']"
197,https://www.semanticscholar.org/paper/Data-Augmentation-for-BERT-Fine-Tuning-in-Question-Yang-Xie/f5eaf727b80240a13e9f631211c9ecec7e3b9feb,27,Title,0,Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering.,Data Augmentation for,BERT,Fine-Tuning in Open-Domain Question Answering .,"['Data', 'Augmentation', 'for', 'BERT', 'Fine-Tuning', 'in', 'Open-Domain', 'Question', 'Answering', '.']","(3, 4)","(21, 25)",0,Data Augmentation Domain Question Answering,0,0,76,0,76,"[(0, 4), (5, 17), (51, 57), (58, 66), (67, 76)]",,,,,,,[],"['Data', 'Augmentation', 'Domain', 'Question', 'Answering']",['Tuning']
198,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,0,"The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts.",The,BERT,"language model ( LM ) ( Devlin et al. , 2019 ) is surprisingly good at answering cloze-style questions about relational facts .","['The', 'BERT', 'language', 'model', '(', 'LM', ')', '(', 'Devlin', 'et', 'al.', ',', '2019', ')', 'is', 'surprisingly', 'good', 'at', 'answering', 'cloze-style', 'questions', 'about', 'relational', 'facts', '.']","(1, 2)","(3, 7)",0,The BERT language model LM Devlin et al .,0,0,47,0,47,"[(0, 3), (4, 8), (9, 17), (18, 23), (26, 28), (33, 39), (40, 42), (43, 45), (46, 47)]",is,2,57,59,48,50,"[(57, 59)]","['The', 'BERT', 'language', 'model', 'LM', 'Devlin', 'et', 'al', '.']",['is']
199,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,1,Petroni et al.,Petroni et al.,,,"['Petroni', 'et', 'al.']",,,,Petroni et al,0,0,13,0,13,"[(0, 7), (8, 10), (11, 13)]",,,,,,,[],"['Petroni', 'et', 'al']",[]
200,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,2,(2019) take this as evidence that BERT memorizes factual knowledge during pre-training.,( 2019 ) take this as evidence that,BERT,memorizes factual knowledge during pre-training .,"['(', '2019', ')', 'take', 'this', 'as', 'evidence', 'that', 'BERT', 'memorizes', 'factual', 'knowledge', 'during', 'pre-training', '.']","(8, 9)","(35, 39)",0,,,,,,,[],take,0,9,13,9,13,"[(9, 13)]",[],['take']
201,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,3,"We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.",We take issue with this interpretation and argue that the performance of,BERT,"is partly due to reasoning about ( the surface form of ) entity names , e.g. , guessing that a person with an Italian-sounding name speaks Italian .","['We', 'take', 'issue', 'with', 'this', 'interpretation', 'and', 'argue', 'that', 'the', 'performance', 'of', 'BERT', 'is', 'partly', 'due', 'to', 'reasoning', 'about', '(', 'the', 'surface', 'form', 'of', ')', 'entity', 'names', ',', 'e.g.', ',', 'guessing', 'that', 'a', 'person', 'with', 'an', 'Italian-sounding', 'name', 'speaks', 'Italian', '.']","(12, 13)","(72, 76)",0,We,0,0,2,0,2,"[(0, 2)]",take and argue,0,3,48,3,48,"[(3, 7), (39, 42), (43, 48)]",['We'],"['take', 'argue']"
202,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,4,"More specifically, we show that BERT's precision drops dramatically when we filter certain easy-to-guess facts.","More specifically , we show that",BERT,'s precision drops dramatically when we filter certain easy-to-guess facts .,"['More', 'specifically', ',', 'we', 'show', 'that', 'BERT', ""'s"", 'precision', 'drops', 'dramatically', 'when', 'we', 'filter', 'certain', 'easy-to-guess', 'facts', '.']","(6, 7)","(32, 36)",0,we,0,20,22,20,22,"[(20, 22)]",show,0,23,27,23,27,"[(23, 27)]",['we'],['show']
203,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,5,"As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings.","As a remedy , we propose E-",BERT,", an extension of BERT that replaces entity mentions with symbolic entity embeddings .","['As', 'a', 'remedy', ',', 'we', 'propose', 'E-', 'BERT', ',', 'an', 'extension', 'of', 'BERT', 'that', 'replaces', 'entity', 'mentions', 'with', 'symbolic', 'entity', 'embeddings', '.']","(7, 8)","(27, 31)",0,we,0,14,16,14,16,"[(14, 16)]",propose,0,17,24,17,24,"[(17, 24)]",['we'],['propose']
204,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,5,"As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings.","As a remedy , we propose E-BERT , an extension of",BERT,that replaces entity mentions with symbolic entity embeddings .,"['As', 'a', 'remedy', ',', 'we', 'propose', 'E-BERT', ',', 'an', 'extension', 'of', 'BERT', 'that', 'replaces', 'entity', 'mentions', 'with', 'symbolic', 'entity', 'embeddings', '.']","(11, 12)","(49, 53)",0,we,0,14,16,14,16,"[(14, 16)]",propose,0,17,24,17,24,"[(17, 24)]",['we'],['propose']
205,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,6,"E-BERT outperforms both BERT and ERNIE (Zhang et al., 2019) on hard-to-guess queries.",E-,BERT,"outperforms both BERT and ERNIE ( Zhang et al. , 2019 ) on hard-to-guess queries .","['E-', 'BERT', 'outperforms', 'both', 'BERT', 'and', 'ERNIE', '(', 'Zhang', 'et', 'al.', ',', '2019', ')', 'on', 'hard-to-guess', 'queries', '.']","(1, 2)","(2, 6)",0,E- BERT,0,0,7,0,7,"[(0, 2), (3, 7)]",outperforms both,2,8,24,0,16,"[(8, 19), (20, 24)]","['E-', 'BERT']",['outperforms']
206,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,6,"E-BERT outperforms both BERT and ERNIE (Zhang et al., 2019) on hard-to-guess queries.",E-BERT outperforms both,BERT,"and ERNIE ( Zhang et al. , 2019 ) on hard-to-guess queries .","['E-BERT', 'outperforms', 'both', 'BERT', 'and', 'ERNIE', '(', 'Zhang', 'et', 'al.', ',', '2019', ')', 'on', 'hard-to-guess', 'queries', '.']","(3, 4)","(23, 27)",0,- BERT outperforms BERT ERNIE Zhang et al . hard guess queries,0,2,94,2,94,"[(2, 3), (4, 8), (9, 20), (26, 30), (35, 40), (43, 48), (49, 51), (52, 54), (55, 56), (69, 73), (81, 86), (87, 94)]",both,0,21,25,21,25,"[(21, 25)]","['-', 'BERT', 'outperforms', 'BERT', 'ERNIE', 'Zhang', 'et', 'al', '.', 'hard', 'guess', 'queries']",[]
207,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,7,"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT",We take this as evidence that E-,BERT,"is richer in factual knowledge , and we show two ways of ensembling BERT and E-BERT","['We', 'take', 'this', 'as', 'evidence', 'that', 'E-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E-BERT']","(7, 8)","(32, 36)",0,We,0,0,2,0,2,"[(0, 2)]",take and,0,3,74,3,74,"[(3, 7), (71, 74)]",['We'],"['take', 'show']"
208,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,7,"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT","We take this as evidence that E-BERT is richer in factual knowledge , and we show two ways of ensembling",BERT,and E-BERT,"['We', 'take', 'this', 'as', 'evidence', 'that', 'E-BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E-BERT']","(20, 21)","(104, 108)",0,We,0,0,2,0,2,"[(0, 2)]",take and,0,3,75,3,75,"[(3, 7), (72, 75)]",['We'],"['take', 'show']"
209,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Abstract,7,"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT","We take this as evidence that E-BERT is richer in factual knowledge , and we show two ways of ensembling BERT and E-",BERT,,"['We', 'take', 'this', 'as', 'evidence', 'that', 'E-BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E-', 'BERT']","(23, 24)","(116, 120)",0,We,0,0,2,0,2,"[(0, 2)]",take and,0,3,75,3,75,"[(3, 7), (72, 75)]",['We'],"['take', 'show']"
210,https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080,28,Title,0,BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA.,,BERT,is Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA .,"['BERT', 'is', 'Not', 'a', 'Knowledge', 'Base', '(', 'Yet', ')', ':', 'Factual', 'Knowledge', 'vs.', 'Name-Based', 'Reasoning', 'in', 'Unsupervised', 'QA', '.']","(0, 1)","(0, 4)",0,BERT,1,0,4,0,4,"[(0, 4)]",is,2,5,7,0,2,"[(5, 7)]",['BERT'],['is']
211,https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39,29,Abstract,0,Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks.,Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks.,,,"['Replacing', 'static', 'word', 'embeddings', 'with', 'contextualized', 'word', 'representations', 'has', 'yielded', 'significant', 'improvements', 'on', 'many', 'NLP', 'tasks.']",,,,,,,,,,[],has yielded,0,74,85,74,85,"[(74, 77), (78, 85)]",[],"['Replacing', 'has', 'yielded']"
212,https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39,29,Abstract,1,"However, just how contextual are the contextualized representations produced by models such as ELMo and BERT?","However , just how contextual are the contextualized representations produced by models such as ELMo and",BERT,?,"['However', ',', 'just', 'how', 'contextual', 'are', 'the', 'contextualized', 'representations', 'produced', 'by', 'models', 'such', 'as', 'ELMo', 'and', 'BERT', '?']","(16, 17)","(104, 108)",0,,,,,,,[],,,,,,,[],[],[]
213,https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39,29,Abstract,2,"Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations?","Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations?",,,"['Are', 'there', 'infinitely', 'many', 'context-specific', 'representations', 'for', 'each', 'word,', 'or', 'are', 'words', 'essentially', 'assigned', 'one', 'of', 'a', 'finite', 'number', 'of', 'word-sense', 'representations?']",,,,context specific representations words a finite number sense representations,0,26,165,26,165,"[(26, 33), (36, 44), (45, 60), (84, 89), (118, 119), (120, 126), (127, 133), (144, 149), (150, 165)]",Are,0,0,3,0,3,"[(0, 3)]","['context', 'specific', 'representations', 'words', 'a', 'finite', 'number', 'sense', 'representations']",['Are']
214,https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39,29,Abstract,3,"For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model.","For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model.",,,"['For', 'one,', 'we', 'find', 'that', 'the', 'contextualized', 'representations', 'of', 'all', 'words', 'are', 'not', 'isotropic', 'in', 'any', 'layer', 'of', 'the', 'contextualizing', 'model.']",,,,we,0,10,12,10,12,"[(10, 12)]",find,0,13,17,13,17,"[(13, 17)]",['we'],['find']
215,https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39,29,Abstract,4,"While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers.","While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers.",,,"['While', 'representations', 'of', 'the', 'same', 'word', 'in', 'different', 'contexts', 'still', 'have', 'a', 'greater', 'cosine', 'similarity', 'than', 'those', 'of', 'two', 'different', 'words,', 'this', 'self-similarity', 'is', 'much', 'lower', 'in', 'upper', 'layers.']",,,,similarity,0,148,158,148,158,"[(148, 158)]",is,0,159,161,159,161,"[(159, 161)]","['representations', 'a', 'greater', 'cosine', 'similarity', 'this', 'self', 'similarity']","['have', 'is']"
216,https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39,29,Abstract,5,"This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations.","This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations.",,,"['This', 'suggests', 'that', 'upper', 'layers', 'of', 'contextualizing', 'models', 'produce', 'more', 'context-specific', 'representations,', 'much', 'like', 'how', 'upper', 'layers', 'of', 'LSTMs', 'produce', 'more', 'task-specific', 'representations.']",,,,This,0,0,4,0,4,"[(0, 4)]",suggests,0,5,13,5,13,"[(5, 13)]",['This'],['suggests']
217,https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39,29,Abstract,6,"In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations","In all layers of ELMo ,",BERT,", and GPT-2 , on average , less than 5 % of the variance in a word 's contextualized representations can be explained by a static embedding for that word , providing some justification for the success of contextualized representations","['In', 'all', 'layers', 'of', 'ELMo', ',', 'BERT', ',', 'and', 'GPT-2', ',', 'on', 'average', ',', 'less', 'than', '5', '%', 'of', 'the', 'variance', 'in', 'a', 'word', ""'s"", 'contextualized', 'representations', 'can', 'be', 'explained', 'by', 'a', 'static', 'embedding', 'for', 'that', 'word', ',', 'providing', 'some', 'justification', 'for', 'the', 'success', 'of', 'contextualized', 'representations']","(6, 7)","(23, 27)",0,%,2,68,69,39,40,"[(68, 69)]",can be explained,2,130,146,101,117,"[(130, 133), (134, 136), (137, 146)]",['%'],"['can', 'be', 'explained', 'providing']"
218,https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39,29,Title,0,How Contextual are Contextualized Word Representations?,How Contextual are Contextualized Word Representations?,,,"['How', 'Contextual', 'are', 'Contextualized', 'Word', 'Representations?']",,,,Contextual,0,4,14,4,14,"[(4, 14)]",,,,,,,[],['Contextual'],[]
219,https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39,29,Title,1,"Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.",Comparing the Geometry of,BERT,", ELMo , and GPT-2 Embeddings .","['Comparing', 'the', 'Geometry', 'of', 'BERT', ',', 'ELMo', ',', 'and', 'GPT-2', 'Embeddings', '.']","(4, 5)","(25, 29)",0,,,,,,,[],Comparing,0,0,9,0,9,"[(0, 9)]",[],['Comparing']
