,URL,ID,Type,Index,Text,split_0,split_1,split_2,split_tokens,split_anchor_span,split_anchor_indices,within_anchor_index,group,group2,group3,group4,group5,group6,group7,group8,group9,group10,group11,group12,group13,group14
0,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,0,"Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task.","Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task.",,,"['Reading', 'comprehension', 'has', 'recently', 'seen', 'rapid', 'progress,', 'with', 'systems', 'matching', 'humans', 'on', 'the', 'most', 'popular', 'datasets', 'for', 'the', 'task.']",,,,comprehension,0,8,21,8,21,has seen,0,22,39,22,39,['comprehension'],"['has', 'seen']"
1,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,1,"However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done.","However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done.",,,"['However,', 'a', 'large', 'body', 'of', 'work', 'has', 'highlighted', 'the', 'brittleness', 'of', 'these', 'systems,', 'showing', 'that', 'there', 'is', 'much', 'work', 'left', 'to', 'be', 'done.']",,,,a large body,0,10,22,10,22,has highlighted,0,31,46,31,46,"['a', 'large', 'body']","['has', 'highlighted', 'showing']"
2,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,2,"We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs.",We introduce,"a new English reading comprehension benchmark , DROP , which requires Discrete Reasoning Over the content of Paragraphs",.,"['We', 'introduce', 'a', 'new', 'English', 'reading', 'comprehension', 'benchmark', ',', 'DROP', ',', 'which', 'requires', 'Discrete', 'Reasoning', 'Over', 'the', 'content', 'of', 'Paragraphs', '.']","(2, 20)","(12, 131)",48,We,0,0,2,0,2,introduce,0,3,12,3,12,['We'],['introduce']
3,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,3,"In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting).",In,"this crowdsourced , adversarially - created , 96k - question benchmark",", a system must resolve references in a question , perhaps to multiple input positions , and perform discrete operations over them ( such as addition , counting , or sorting ) .","['In', 'this', 'crowdsourced', ',', 'adversarially', '-', 'created', ',', '96k', '-', 'question', 'benchmark', ',', 'a', 'system', 'must', 'resolve', 'references', 'in', 'a', 'question', ',', 'perhaps', 'to', 'multiple', 'input', 'positions', ',', 'and', 'perform', 'discrete', 'operations', 'over', 'them', '(', 'such', 'as', 'addition', ',', 'counting', ',', 'or', 'sorting', ')', '.']","(1, 12)","(2, 72)",-1,a system,2,76,84,2,10,must resolve and perform,2,85,174,11,100,"['a', 'system']","['must', 'resolve', 'perform']"
4,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,4,These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.,These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.,,,"['These', 'operations', 'require', 'a', 'much', 'more', 'comprehensive', 'understanding', 'of', 'the', 'content', 'of', 'paragraphs', 'than', 'what', 'was', 'necessary', 'for', 'prior', 'datasets.']",,,,These operations,0,0,16,0,16,require,0,17,24,17,24,"['These', 'operations']",['require']
5,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,5,"We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.0%.",We apply state - of - the - art methods from both the reading comprehension and semantic parsing literature on,this dataset,"and show that the best systems only achieve 32.7 % F1 on our generalized accuracy metric , while expert human performance is 96.0 % .","['We', 'apply', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', 'from', 'both', 'the', 'reading', 'comprehension', 'and', 'semantic', 'parsing', 'literature', 'on', 'this', 'dataset', 'and', 'show', 'that', 'the', 'best', 'systems', 'only', 'achieve', '32.7', '%', 'F1', 'on', 'our', 'generalized', 'accuracy', 'metric', ',', 'while', 'expert', 'human', 'performance', 'is', '96.0', '%', '.']","(20, 22)","(110, 122)",-1,We,0,0,2,0,2,apply and show,0,3,132,3,132,['We'],"['apply', 'show']"
6,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,6,We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.,We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.,,,"['We', 'additionally', 'present', 'a', 'new', 'model', 'that', 'combines', 'reading', 'comprehension', 'methods', 'with', 'simple', 'numerical', 'reasoning', 'to', 'achieve', '47.0%', 'F1.']",,,,We,0,0,2,0,2,present,0,16,23,16,23,['We'],['present']
7,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Title,0,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs.,,DROP,: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs .,"['DROP', ':', 'A', 'Reading', 'Comprehension', 'Benchmark', 'Requiring', 'Discrete', 'Reasoning', 'Over', 'Paragraphs', '.']","(0, 1)","(0, 4)",0,DROP A Reading Comprehension Benchmark,1,0,40,0,40,,,,,,,"['DROP', 'A', 'Reading', 'Comprehension', 'Benchmark']",[]
8,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,0,"Machine reading comprehension, the task of evaluating a machine’s ability to comprehend a passage of text, has seen a surge in popularity in recent years.","Machine reading comprehension, the task of evaluating a machine’s ability to comprehend a passage of text, has seen a surge in popularity in recent years.",,,"['Machine', 'reading', 'comprehension,', 'the', 'task', 'of', 'evaluating', 'a', 'machine’s', 'ability', 'to', 'comprehend', 'a', 'passage', 'of', 'text,', 'has', 'seen', 'a', 'surge', 'in', 'popularity', 'in', 'recent', 'years.']",,,,Machine reading comprehension the task,0,0,40,0,40,has seen,0,110,118,110,118,"['Machine', 'reading', 'comprehension', 'the', 'task']","['has', 'seen']"
9,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,1,"There are many datasets that are targeted at reading comprehension, and many systems that perform as well as humans on some of these datasets.","There are many datasets that are targeted at reading comprehension, and many systems that perform as well as humans on some of these datasets.",,,"['There', 'are', 'many', 'datasets', 'that', 'are', 'targeted', 'at', 'reading', 'comprehension,', 'and', 'many', 'systems', 'that', 'perform', 'as', 'well', 'as', 'humans', 'on', 'some', 'of', 'these', 'datasets.']",,,,,,,,,,are,0,6,9,6,9,[],['are']
10,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,2,"Despite all of this interest, there is no work that systematically defines what reading comprehension is.","Despite all of this interest, there is no work that systematically defines what reading comprehension is.",,,"['Despite', 'all', 'of', 'this', 'interest,', 'there', 'is', 'no', 'work', 'that', 'systematically', 'defines', 'what', 'reading', 'comprehension', 'is.']",,,,,,,,,,is,0,37,39,37,39,[],['is']
11,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,3,"In this work, we justify a question answering approach to reading comprehension and describe the various kinds of questions one might use to more fully test a system’s comprehension of a passage, moving beyond questions that only probe local predicate-argument structures.","In this work, we justify a question answering approach to reading comprehension and describe the various kinds of questions one might use to more fully test a system’s comprehension of a passage, moving beyond questions that only probe local predicate-argument structures.",,,"['In', 'this', 'work,', 'we', 'justify', 'a', 'question', 'answering', 'approach', 'to', 'reading', 'comprehension', 'and', 'describe', 'the', 'various', 'kinds', 'of', 'questions', 'one', 'might', 'use', 'to', 'more', 'fully', 'test', 'a', 'system’s', 'comprehension', 'of', 'a', 'passage,', 'moving', 'beyond', 'questions', 'that', 'only', 'probe', 'local', 'predicate-argument', 'structures.']",,,,we,0,15,17,15,17,justify and describe,0,18,93,18,93,['we'],"['justify', 'describe']"
12,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,4,The main pitfall of this approach is that questions can easily have surface cues or other biases that allow a model to shortcut the intended reasoning process.,The main pitfall of this approach is that questions can easily have surface cues or other biases that allow a model to shortcut the intended reasoning process.,,,"['The', 'main', 'pitfall', 'of', 'this', 'approach', 'is', 'that', 'questions', 'can', 'easily', 'have', 'surface', 'cues', 'or', 'other', 'biases', 'that', 'allow', 'a', 'model', 'to', 'shortcut', 'the', 'intended', 'reasoning', 'process.']",,,,The main pitfall,0,0,16,0,16,is,0,34,36,34,36,"['The', 'main', 'pitfall']",['is']
13,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,5,"We discuss ways proposed in current literature to mitigate these shortcuts, and we conclude with recommendations for future dataset collection efforts.","We discuss ways proposed in current literature to mitigate these shortcuts, and we conclude with recommendations for future dataset collection efforts.",,,"['We', 'discuss', 'ways', 'proposed', 'in', 'current', 'literature', 'to', 'mitigate', 'these', 'shortcuts,', 'and', 'we', 'conclude', 'with', 'recommendations', 'for', 'future', 'dataset', 'collection', 'efforts.']",,,,,,,,,,and,0,77,80,77,80,"['We', 'we']","['discuss', 'conclude']"
14,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Title,0,On Making Reading Comprehension More Comprehensive.,On Making Reading Comprehension More Comprehensive.,,,"['On', 'Making', 'Reading', 'Comprehension', 'More', 'Comprehensive.']",,,,,,,,,,,,,,,,[],[]
15,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,0,Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension.,Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension.,,,"['Recent', 'powerful', 'pre-trained', 'language', 'models', 'have', 'achieved', 'remarkable', 'performance', 'on', 'most', 'of', 'the', 'popular', 'datasets', 'for', 'reading', 'comprehension.']",,,,Recent powerful pre - trained language models,0,0,45,0,45,have achieved,0,46,59,46,59,"['Recent', 'powerful', 'pre', '-', 'trained', 'language', 'models']","['have', 'achieved']"
16,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,1,It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text.,It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text.,,,"['It', 'is', 'time', 'to', 'introduce', 'more', 'challenging', 'datasets', 'to', 'push', 'the', 'development', 'of', 'this', 'field', 'towards', 'more', 'comprehensive', 'reasoning', 'of', 'text.']",,,,It,0,0,2,0,2,is,0,3,5,3,5,['It'],['is']
17,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,2,"In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations.","In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations.",,,"['In', 'this', 'paper,', 'we', 'introduce', 'a', 'new', 'Reading', 'Comprehension', 'dataset', 'requiring', 'logical', 'reasoning', '(ReClor)', 'extracted', 'from', 'standardized', 'graduate', 'admission', 'examinations.']",,,,we,0,16,18,16,18,introduce,0,19,28,19,28,['we'],['introduce']
18,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,3,"As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text.","As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text.",,,"['As', 'earlier', 'studies', 'suggest,', 'human-annotated', 'datasets', 'usually', 'contain', 'biases,', 'which', 'are', 'often', 'exploited', 'by', 'models', 'to', 'achieve', 'high', 'accuracy', 'without', 'truly', 'understanding', 'the', 'text.']",,,,human datasets,0,29,55,29,55,contain,0,64,71,64,71,"['human', 'datasets']",['contain']
19,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,4,"In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points and separate them into EASY set while the rest as HARD set.","In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points and separate them into EASY set while the rest as HARD set.",,,"['In', 'order', 'to', 'comprehensively', 'evaluate', 'the', 'logical', 'reasoning', 'ability', 'of', 'models', 'on', 'ReClor,', 'we', 'propose', 'to', 'identify', 'biased', 'data', 'points', 'and', 'separate', 'them', 'into', 'EASY', 'set', 'while', 'the', 'rest', 'as', 'HARD', 'set.']",,,,we,0,89,91,89,91,propose,0,92,99,92,99,['we'],"['propose', 'identify', 'separate']"
20,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,5,Empirical results show that the state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set.,Empirical results show that the state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set.,,,"['Empirical', 'results', 'show', 'that', 'the', 'state-of-the-art', 'models', 'have', 'an', 'outstanding', 'ability', 'to', 'capture', 'biases', 'contained', 'in', 'the', 'dataset', 'with', 'high', 'accuracy', 'on', 'EASY', 'set.']",,,,Empirical results,0,0,17,0,17,show,0,18,22,18,22,"['Empirical', 'results']",['show']
21,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,6,"However, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models.","However, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models.",,,"['However,', 'they', 'struggle', 'on', 'HARD', 'set', 'with', 'poor', 'performance', 'near', 'that', 'of', 'random', 'guess,', 'indicating', 'more', 'research', 'is', 'needed', 'to', 'essentially', 'enhance', 'the', 'logical', 'reasoning', 'ability', 'of', 'current', 'models.']",,,,they,0,10,14,10,14,struggle,0,15,23,15,23,['they'],"['struggle', 'indicating']"
22,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Title,0,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning.,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning.,,,"['ReClor:', 'A', 'Reading', 'Comprehension', 'Dataset', 'Requiring', 'Logical', 'Reasoning.']",,,,ReClor A Reading Comprehension Dataset,0,0,40,0,40,,,,,,,"['ReClor', 'A', 'Reading', 'Comprehension', 'Dataset']",[]
23,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,0,"With models reaching human performance on many popular reading comprehension datasets in recent years, a new dataset, DROP, introduced questions that were expected to present a harder challenge for reading comprehension models.","With models reaching human performance on many popular reading comprehension datasets in recent years ,","a new dataset , DROP ,",introduced questions that were expected to present a harder challenge for reading comprehension models .,"['With', 'models', 'reaching', 'human', 'performance', 'on', 'many', 'popular', 'reading', 'comprehension', 'datasets', 'in', 'recent', 'years', ',', 'a', 'new', 'dataset', ',', 'DROP', ',', 'introduced', 'questions', 'that', 'were', 'expected', 'to', 'present', 'a', 'harder', 'challenge', 'for', 'reading', 'comprehension', 'models', '.']","(15, 21)","(103, 125)",16,a new dataset DROP,1,104,124,0,20,introduced,2,127,137,0,10,"['a', 'new', 'dataset', 'DROP']",['introduced']
24,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,1,"Among these new types of questions were ""multi-span questions"", questions whose answers consist of several spans from either the paragraph or the question itself.","Among these new types of questions were ""multi-span questions"", questions whose answers consist of several spans from either the paragraph or the question itself.",,,"['Among', 'these', 'new', 'types', 'of', 'questions', 'were', '""multi-span', 'questions"",', 'questions', 'whose', 'answers', 'consist', 'of', 'several', 'spans', 'from', 'either', 'the', 'paragraph', 'or', 'the', 'question', 'itself.']",,,,,,,,,,were,0,35,39,35,39,[],['were']
25,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,2,"Until now, only one model attempted to tackle multi-span questions as a part of its design.","Until now, only one model attempted to tackle multi-span questions as a part of its design.",,,"['Until', 'now,', 'only', 'one', 'model', 'attempted', 'to', 'tackle', 'multi-span', 'questions', 'as', 'a', 'part', 'of', 'its', 'design.']",,,,model,0,21,26,21,26,attempted,0,27,36,27,36,['model'],"['attempted', 'tackle']"
26,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,3,"In this work, we suggest a new approach for tackling multi-span questions, based on sequence tagging, which differs from previous approaches for answering span questions.","In this work, we suggest a new approach for tackling multi-span questions, based on sequence tagging, which differs from previous approaches for answering span questions.",,,"['In', 'this', 'work,', 'we', 'suggest', 'a', 'new', 'approach', 'for', 'tackling', 'multi-span', 'questions,', 'based', 'on', 'sequence', 'tagging,', 'which', 'differs', 'from', 'previous', 'approaches', 'for', 'answering', 'span', 'questions.']",,,,we,0,15,17,15,17,suggest,0,18,25,18,25,['we'],['suggest']
27,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,4,"We show that our approach leads to an absolute improvement of 29.7 EM and 15.1 F1 compared to existing state-of-the-art results, while not hurting performance on other question types.","We show that our approach leads to an absolute improvement of 29.7 EM and 15.1 F1 compared to existing state-of-the-art results, while not hurting performance on other question types.",,,"['We', 'show', 'that', 'our', 'approach', 'leads', 'to', 'an', 'absolute', 'improvement', 'of', '29.7', 'EM', 'and', '15.1', 'F1', 'compared', 'to', 'existing', 'state-of-the-art', 'results,', 'while', 'not', 'hurting', 'performance', 'on', 'other', 'question', 'types.']",,,,We,0,0,2,0,2,show,0,3,7,3,7,['We'],['show']
28,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,5,"Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataset.","Furthermore , we show that our model slightly eclipses the current state - of - the - art results on",the entire DROP dataset,.,"['Furthermore', ',', 'we', 'show', 'that', 'our', 'model', 'slightly', 'eclipses', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'the', 'entire', 'DROP', 'dataset', '.']","(20, 24)","(100, 123)",11,we,0,14,16,14,16,show,0,17,21,17,21,['we'],['show']
29,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Title,0,Tag-based Multi-Span Extraction in Reading Comprehension.,Tag-based Multi-Span Extraction in Reading Comprehension.,,,"['Tag-based', 'Multi-Span', 'Extraction', 'in', 'Reading', 'Comprehension.']",,,,Tag Multi Span Extraction,0,0,35,0,35,based,0,6,11,6,11,"['Tag', 'Multi', 'Span', 'Extraction']",['based']
30,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,0,Machine comprehension of texts longer than a single sentence often requires coreference resolution.,Machine comprehension of texts longer than a single sentence often requires coreference resolution.,,,"['Machine', 'comprehension', 'of', 'texts', 'longer', 'than', 'a', 'single', 'sentence', 'often', 'requires', 'coreference', 'resolution.']",,,,Machine comprehension,0,0,21,0,21,requires,0,67,75,67,75,"['Machine', 'comprehension']",['requires']
31,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,1,"However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference.","However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference.",,,"['However,', 'most', 'current', 'reading', 'comprehension', 'benchmarks', 'do', 'not', 'contain', 'complex', 'coreferential', 'phenomena', 'and', 'hence', 'fail', 'to', 'evaluate', 'the', 'ability', 'of', 'models', 'to', 'resolve', 'coreference.']",,,,most current reading comprehension benchmarks,0,10,55,10,55,do contain and fail,0,56,117,56,117,"['most', 'current', 'reading', 'comprehension', 'benchmarks']","['do', 'contain', 'fail', 'evaluate']"
32,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,2,We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia.,We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia.,,,"['We', 'present', 'a', 'new', 'crowdsourced', 'dataset', 'containing', 'more', 'than', '24K', 'span-selection', 'questions', 'that', 'require', 'resolving', 'coreference', 'among', 'entities', 'in', 'over', '4.7K', 'English', 'paragraphs', 'from', 'Wikipedia.']",,,,We,0,0,2,0,2,present,0,3,10,3,10,['We'],['present']
33,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,3,"Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning.","Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning.",,,"['Obtaining', 'questions', 'focused', 'on', 'such', 'phenomena', 'is', 'challenging,', 'because', 'it', 'is', 'hard', 'to', 'avoid', 'lexical', 'cues', 'that', 'shortcut', 'complex', 'reasoning.']",,,,,,,,,,is,0,46,48,46,48,[],"['Obtaining', 'focused', 'is']"
34,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,4,"We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues.","We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues.",,,"['We', 'deal', 'with', 'this', 'issue', 'by', 'using', 'a', 'strong', 'baseline', 'model', 'as', 'an', 'adversary', 'in', 'the', 'crowdsourcing', 'loop,', 'which', 'helps', 'crowdworkers', 'avoid', 'writing', 'questions', 'with', 'exploitable', 'surface', 'cues.']",,,,We,0,0,2,0,2,deal,0,3,7,3,7,['We'],['deal']
35,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,5,"We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark—the best model performance is 70.5 F1, while the estimated human performance is 93.4 F1.","We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark—the best model performance is 70.5 F1, while the estimated human performance is 93.4 F1.",,,"['We', 'show', 'that', 'state-of-the-art', 'reading', 'comprehension', 'models', 'perform', 'significantly', 'worse', 'than', 'humans', 'on', 'this', 'benchmark—the', 'best', 'model', 'performance', 'is', '70.5', 'F1,', 'while', 'the', 'estimated', 'human', 'performance', 'is', '93.4', 'F1.']",,,,,,,,,,,,,,,,"['We', 'the', 'best', 'model', 'performance']","['show', 'perform', 'is']"
36,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Title,0,Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning.,Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning.,,,"['Quoref:', 'A', 'Reading', 'Comprehension', 'Dataset', 'with', 'Questions', 'Requiring', 'Coreferential', 'Reasoning.']",,,,Quoref A Reading Comprehension,0,0,32,0,32,,,,,,,"['Quoref', 'A', 'Reading', 'Comprehension']",[]
37,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,0,"Rapid progress has been made in the field of reading comprehension and question answering, where several systems have achieved human parity in some simplified settings.","Rapid progress has been made in the field of reading comprehension and question answering, where several systems have achieved human parity in some simplified settings.",,,"['Rapid', 'progress', 'has', 'been', 'made', 'in', 'the', 'field', 'of', 'reading', 'comprehension', 'and', 'question', 'answering,', 'where', 'several', 'systems', 'have', 'achieved', 'human', 'parity', 'in', 'some', 'simplified', 'settings.']",,,,Rapid progress,0,0,14,0,14,has been made,0,15,28,15,28,"['Rapid', 'progress']","['has', 'been', 'made']"
38,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,1,"However, the performance of these models degrades significantly when they are applied to more realistic scenarios, such as answers involve various types, multiple text strings are correct answers, or discrete reasoning abilities are required.","However, the performance of these models degrades significantly when they are applied to more realistic scenarios, such as answers involve various types, multiple text strings are correct answers, or discrete reasoning abilities are required.",,,"['However,', 'the', 'performance', 'of', 'these', 'models', 'degrades', 'significantly', 'when', 'they', 'are', 'applied', 'to', 'more', 'realistic', 'scenarios,', 'such', 'as', 'answers', 'involve', 'various', 'types,', 'multiple', 'text', 'strings', 'are', 'correct', 'answers,', 'or', 'discrete', 'reasoning', 'abilities', 'are', 'required.']",,,,the performance,0,10,25,10,25,are applied,0,75,86,75,86,"['the', 'performance']","['are', 'applied']"
39,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,2,"In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types (e.g., span, count, negation, and arithmetic expression) with a multi-span extraction method for dynamically producing one or multiple text spans.","In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types (e.g., span, count, negation, and arithmetic expression) with a multi-span extraction method for dynamically producing one or multiple text spans.",,,"['In', 'this', 'paper,', 'we', 'introduce', 'the', 'Multi-Type', 'Multi-Span', 'Network', '(MTMSN),', 'a', 'neural', 'reading', 'comprehension', 'model', 'that', 'combines', 'a', 'multi-type', 'answer', 'predictor', 'designed', 'to', 'support', 'various', 'answer', 'types', '(e.g.,', 'span,', 'count,', 'negation,', 'and', 'arithmetic', 'expression)', 'with', 'a', 'multi-span', 'extraction', 'method', 'for', 'dynamically', 'producing', 'one', 'or', 'multiple', 'text', 'spans.']",,,,we,0,16,18,16,18,introduce,0,19,28,19,28,['we'],['introduce']
40,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,3,"In addition, an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction.","In addition, an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction.",,,"['In', 'addition,', 'an', 'arithmetic', 'expression', 'reranking', 'mechanism', 'is', 'proposed', 'to', 'rank', 'expression', 'candidates', 'for', 'further', 'confirming', 'the', 'prediction.']",,,,an arithmetic expression mechanism,0,14,58,14,58,is proposed,0,59,70,59,70,"['an', 'arithmetic', 'expression', 'mechanism']","['is', 'proposed', 'rank']"
41,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,4,"Experiments show that our model achieves 79.9 F1 on the DROP hidden test set, creating new state-of-the-art results.",Experiments show that our model achieves 79.9 F1 on the,DROP,"hidden test set , creating new state-of-the-art results .","['Experiments', 'show', 'that', 'our', 'model', 'achieves', '79.9', 'F1', 'on', 'the', 'DROP', 'hidden', 'test', 'set', ',', 'creating', 'new', 'state-of-the-art', 'results', '.']","(10, 11)","(55, 59)",0,Experiments,0,0,11,0,11,show,0,12,16,12,16,['Experiments'],['show']
42,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,5,Source code\footnote{\url{https://github.com/huminghao16/MTMSN}} is released to facilitate future work.,Source code\footnote{\url{https://github.com/huminghao16/MTMSN}} is released to facilitate future work.,,,"['Source', 'code\\footnote{\\url{https://github.com/huminghao16/MTMSN}}', 'is', 'released', 'to', 'facilitate', 'future', 'work.']",,,,Source huminghao16 MTMSN,0,0,66,0,66,is released,0,71,82,71,82,"['Source', 'huminghao16', 'MTMSN']","['is', 'released', 'facilitate']"
43,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Title,0,A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning.,A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning.,,,"['A', 'Multi-Type', 'Multi-Span', 'Network', 'for', 'Reading', 'Comprehension', 'that', 'Requires', 'Discrete', 'Reasoning.']",,,,A Multi Type Multi Span Network,0,0,35,0,35,,,,,,,"['A', 'Multi', 'Type', 'Multi', 'Span', 'Network']",[]
44,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,0,Machine Reading Comprehension (MRC) is the task of answering a question over a paragraph of text.,Machine Reading Comprehension (MRC) is the task of answering a question over a paragraph of text.,,,"['Machine', 'Reading', 'Comprehension', '(MRC)', 'is', 'the', 'task', 'of', 'answering', 'a', 'question', 'over', 'a', 'paragraph', 'of', 'text.']",,,,Machine Reading Comprehension MRC,0,0,35,0,35,is,0,38,40,38,40,"['Machine', 'Reading', 'Comprehension', 'MRC']",['is']
45,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,1,"While neural MRC systems gain popularity and achieve noticeable performance, issues are being raised with the methodology used to establish their performance, particularly concerning the data design of gold standards that are used to evaluate them.","While neural MRC systems gain popularity and achieve noticeable performance, issues are being raised with the methodology used to establish their performance, particularly concerning the data design of gold standards that are used to evaluate them.",,,"['While', 'neural', 'MRC', 'systems', 'gain', 'popularity', 'and', 'achieve', 'noticeable', 'performance,', 'issues', 'are', 'being', 'raised', 'with', 'the', 'methodology', 'used', 'to', 'establish', 'their', 'performance,', 'particularly', 'concerning', 'the', 'data', 'design', 'of', 'gold', 'standards', 'that', 'are', 'used', 'to', 'evaluate', 'them.']",,,,issues,0,78,84,78,84,are being raised concerning,0,85,184,85,184,['issues'],"['are', 'being', 'raised', 'concerning']"
46,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,2,"There is but a limited understanding of the challenges present in this data, which makes it hard to draw comparisons and formulate reliable hypotheses.","There is but a limited understanding of the challenges present in this data, which makes it hard to draw comparisons and formulate reliable hypotheses.",,,"['There', 'is', 'but', 'a', 'limited', 'understanding', 'of', 'the', 'challenges', 'present', 'in', 'this', 'data,', 'which', 'makes', 'it', 'hard', 'to', 'draw', 'comparisons', 'and', 'formulate', 'reliable', 'hypotheses.']",,,,,,,,,,is but,0,6,12,6,12,[],['is']
47,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,3,"As a first step towards alleviating the problem, this paper proposes a unifying framework to systematically investigate the present linguistic features, required reasoning and background knowledge and factual correctness on one hand, and the presence of lexical cues as a lower bound for the requirement of understanding on the other hand.","As a first step towards alleviating the problem, this paper proposes a unifying framework to systematically investigate the present linguistic features, required reasoning and background knowledge and factual correctness on one hand, and the presence of lexical cues as a lower bound for the requirement of understanding on the other hand.",,,"['As', 'a', 'first', 'step', 'towards', 'alleviating', 'the', 'problem,', 'this', 'paper', 'proposes', 'a', 'unifying', 'framework', 'to', 'systematically', 'investigate', 'the', 'present', 'linguistic', 'features,', 'required', 'reasoning', 'and', 'background', 'knowledge', 'and', 'factual', 'correctness', 'on', 'one', 'hand,', 'and', 'the', 'presence', 'of', 'lexical', 'cues', 'as', 'a', 'lower', 'bound', 'for', 'the', 'requirement', 'of', 'understanding', 'on', 'the', 'other', 'hand.']",,,,this paper,0,50,60,50,60,proposes,0,61,69,61,69,"['this', 'paper']",['proposes']
48,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,4,We propose a qualitative annotation schema for the first and a set of approximative metrics for the latter.,We propose a qualitative annotation schema for the first and a set of approximative metrics for the latter.,,,"['We', 'propose', 'a', 'qualitative', 'annotation', 'schema', 'for', 'the', 'first', 'and', 'a', 'set', 'of', 'approximative', 'metrics', 'for', 'the', 'latter.']",,,,We,0,0,2,0,2,propose,0,3,10,3,10,['We'],['propose']
49,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,5,"In a first application of the framework, we analyse modern MRC gold standards and present our findings: the absence of features that contribute towards lexical ambiguity, the varying factual correctness of the expected answers and the presence of lexical cues, all of which potentially lower the reading comprehension complexity and quality of the evaluation data.","In a first application of the framework, we analyse modern MRC gold standards and present our findings: the absence of features that contribute towards lexical ambiguity, the varying factual correctness of the expected answers and the presence of lexical cues, all of which potentially lower the reading comprehension complexity and quality of the evaluation data.",,,"['In', 'a', 'first', 'application', 'of', 'the', 'framework,', 'we', 'analyse', 'modern', 'MRC', 'gold', 'standards', 'and', 'present', 'our', 'findings:', 'the', 'absence', 'of', 'features', 'that', 'contribute', 'towards', 'lexical', 'ambiguity,', 'the', 'varying', 'factual', 'correctness', 'of', 'the', 'expected', 'answers', 'and', 'the', 'presence', 'of', 'lexical', 'cues,', 'all', 'of', 'which', 'potentially', 'lower', 'the', 'reading', 'comprehension', 'complexity', 'and', 'quality', 'of', 'the', 'evaluation', 'data.']",,,,we,0,42,44,42,44,analyse and present,0,45,90,45,90,['we'],"['analyse', 'present']"
50,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Title,0,A Framework for Evaluation of Machine Reading Comprehension Gold Standards.,A Framework for Evaluation of Machine Reading Comprehension Gold Standards.,,,"['A', 'Framework', 'for', 'Evaluation', 'of', 'Machine', 'Reading', 'Comprehension', 'Gold', 'Standards.']",,,,A Framework,0,0,11,0,11,,,,,,,"['A', 'Framework']",[]
51,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,0,"Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations.","Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations.",,,"['Answering', 'compositional', 'questions', 'that', 'require', 'multiple', 'steps', 'of', 'reasoning', 'against', 'text', 'is', 'challenging,', 'especially', 'when', 'they', 'involve', 'discrete,', 'symbolic', 'operations.']",,,,,,,,,,is,0,88,90,88,90,[],"['Answering', 'is']"
52,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,1,"Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains.","Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains.",,,"['Neural', 'module', 'networks', '(NMNs)', 'learn', 'to', 'parse', 'such', 'questions', 'as', 'executable', 'programs', 'composed', 'of', 'learnable', 'modules,', 'performing', 'well', 'on', 'synthetic', 'visual', 'QA', 'domains.']",,,,Neural module networks NMNs,0,0,29,0,29,learn,0,32,37,32,37,"['Neural', 'module', 'networks', 'NMNs']","['learn', 'parse']"
53,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,2,"However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning.","However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning.",,,"['However,', 'we', 'find', 'that', 'it', 'is', 'challenging', 'to', 'learn', 'these', 'models', 'for', 'non-synthetic', 'questions', 'on', 'open-domain', 'text,', 'where', 'a', 'model', 'needs', 'to', 'deal', 'with', 'the', 'diversity', 'of', 'natural', 'language', 'and', 'perform', 'a', 'broader', 'range', 'of', 'reasoning.']",,,,we,0,10,12,10,12,find,0,13,17,13,17,['we'],['find']
54,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,3,"We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text.","We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text.",,,"['We', 'extend', 'NMNs', 'by:', '(a)', 'introducing', 'modules', 'that', 'reason', 'over', 'a', 'paragraph', 'of', 'text,', 'performing', 'symbolic', 'reasoning', '(such', 'as', 'arithmetic,', 'sorting,', 'counting)', 'over', 'numbers', 'and', 'dates', 'in', 'a', 'probabilistic', 'and', 'differentiable', 'manner;', 'and', '(b)', 'proposing', 'an', 'unsupervised', 'auxiliary', 'loss', 'to', 'help', 'extract', 'arguments', 'associated', 'with', 'the', 'events', 'in', 'text.']",,,,We,0,0,2,0,2,extend,0,3,9,3,9,['We'],['extend']
55,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,4,"Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning.","Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning.",,,"['Additionally,', 'we', 'show', 'that', 'a', 'limited', 'amount', 'of', 'heuristically-obtained', 'question', 'program', 'and', 'intermediate', 'module', 'output', 'supervision', 'provides', 'sufficient', 'inductive', 'bias', 'for', 'accurate', 'learning.']",,,,we,0,15,17,15,17,show,0,18,22,18,22,['we'],['show']
56,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,5,Our proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.,Our proposed model significantly outperforms state-of-the-art models on a subset of the,DROP,dataset that poses a variety of reasoning challenges that are covered by our modules .,"['Our', 'proposed', 'model', 'significantly', 'outperforms', 'state-of-the-art', 'models', 'on', 'a', 'subset', 'of', 'the', 'DROP', 'dataset', 'that', 'poses', 'a', 'variety', 'of', 'reasoning', 'challenges', 'that', 'are', 'covered', 'by', 'our', 'modules', '.']","(12, 13)","(87, 91)",0,model,0,13,18,13,18,outperforms,0,33,44,33,44,['model'],['outperforms']
57,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Title,0,Neural Module Networks for Reasoning over Text.,Neural Module Networks for Reasoning over Text.,,,"['Neural', 'Module', 'Networks', 'for', 'Reasoning', 'over', 'Text.']",,,,Neural Module Networks,0,0,22,0,22,,,,,,,"['Neural', 'Module', 'Networks']",[]
58,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,0,Numerical reasoning is often important to accurately understand the world.,Numerical reasoning is often important to accurately understand the world.,,,"['Numerical', 'reasoning', 'is', 'often', 'important', 'to', 'accurately', 'understand', 'the', 'world.']",,,,Numerical reasoning,0,0,19,0,19,is,0,20,22,20,22,"['Numerical', 'reasoning']","['is', 'understand']"
59,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,1,"Recently, several format-specific datasets have been proposed, such as numerical reasoning in the settings of Natural Language Inference (NLI), Reading Comprehension (RC), and Question Answering (QA).","Recently, several format-specific datasets have been proposed, such as numerical reasoning in the settings of Natural Language Inference (NLI), Reading Comprehension (RC), and Question Answering (QA).",,,"['Recently,', 'several', 'format-specific', 'datasets', 'have', 'been', 'proposed,', 'such', 'as', 'numerical', 'reasoning', 'in', 'the', 'settings', 'of', 'Natural', 'Language', 'Inference', '(NLI),', 'Reading', 'Comprehension', '(RC),', 'and', 'Question', 'Answering', '(QA).']",,,,several format specific datasets,0,11,45,11,45,have been proposed,0,46,64,46,64,"['several', 'format', 'specific', 'datasets']","['have', 'been', 'proposed']"
60,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,2,Several format-specific models and architectures in response to those datasets have also been proposed.,Several format-specific models and architectures in response to those datasets have also been proposed.,,,"['Several', 'format-specific', 'models', 'and', 'architectures', 'in', 'response', 'to', 'those', 'datasets', 'have', 'also', 'been', 'proposed.']",,,,Several format specific models architectures,0,0,50,0,50,have been proposed,0,81,104,81,104,"['Several', 'format', 'specific', 'models', 'architectures']","['have', 'been', 'proposed']"
61,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,3,"However, there exists a strong need for a benchmark which can evaluate the abilities of models, in performing question format independent numerical reasoning, as (i) the numerical reasoning capabilities we want to teach are not controlled by question formats, (ii) for numerical reasoning technology to have the best possible application, it must be able to process language and reason in a way that is not exclusive to a single format, task, dataset or domain.","However, there exists a strong need for a benchmark which can evaluate the abilities of models, in performing question format independent numerical reasoning, as (i) the numerical reasoning capabilities we want to teach are not controlled by question formats, (ii) for numerical reasoning technology to have the best possible application, it must be able to process language and reason in a way that is not exclusive to a single format, task, dataset or domain.",,,"['However,', 'there', 'exists', 'a', 'strong', 'need', 'for', 'a', 'benchmark', 'which', 'can', 'evaluate', 'the', 'abilities', 'of', 'models,', 'in', 'performing', 'question', 'format', 'independent', 'numerical', 'reasoning,', 'as', '(i)', 'the', 'numerical', 'reasoning', 'capabilities', 'we', 'want', 'to', 'teach', 'are', 'not', 'controlled', 'by', 'question', 'formats,', '(ii)', 'for', 'numerical', 'reasoning', 'technology', 'to', 'have', 'the', 'best', 'possible', 'application,', 'it', 'must', 'be', 'able', 'to', 'process', 'language', 'and', 'reason', 'in', 'a', 'way', 'that', 'is', 'not', 'exclusive', 'to', 'a', 'single', 'format,', 'task,', 'dataset', 'or', 'domain.']",,,,,,,,,,exists,0,16,22,16,22,[],['exists']
62,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,4,"In pursuit of this goal, we introduce NUMBERGAME, a multifaceted benchmark to evaluate model performance across numerical reasoning tasks of eight diverse formats.","In pursuit of this goal, we introduce NUMBERGAME, a multifaceted benchmark to evaluate model performance across numerical reasoning tasks of eight diverse formats.",,,"['In', 'pursuit', 'of', 'this', 'goal,', 'we', 'introduce', 'NUMBERGAME,', 'a', 'multifaceted', 'benchmark', 'to', 'evaluate', 'model', 'performance', 'across', 'numerical', 'reasoning', 'tasks', 'of', 'eight', 'diverse', 'formats.']",,,,we,0,26,28,26,28,introduce,0,29,38,29,38,['we'],['introduce']
63,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,5,We add four existing question types in our compilation.,We add four existing question types in our compilation.,,,"['We', 'add', 'four', 'existing', 'question', 'types', 'in', 'our', 'compilation.']",,,,We,0,0,2,0,2,add,0,3,6,3,6,['We'],['add']
64,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,6,"Two of the new types we add are about questions that require external numerical knowledge, commonsense knowledge and domain knowledge.","Two of the new types we add are about questions that require external numerical knowledge, commonsense knowledge and domain knowledge.",,,"['Two', 'of', 'the', 'new', 'types', 'we', 'add', 'are', 'about', 'questions', 'that', 'require', 'external', 'numerical', 'knowledge,', 'commonsense', 'knowledge', 'and', 'domain', 'knowledge.']",,,,,,,,,,are,0,28,31,28,31,[],['are']
65,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,7,"While recently many QA datasets involving external knowledge have been proposed, ours incorporates them in a numerical reasoning setting.","While recently many QA datasets involving external knowledge have been proposed, ours incorporates them in a numerical reasoning setting.",,,"['While', 'recently', 'many', 'QA', 'datasets', 'involving', 'external', 'knowledge', 'have', 'been', 'proposed,', 'ours', 'incorporates', 'them', 'in', 'a', 'numerical', 'reasoning', 'setting.']",,,,,,,,,,incorporates,0,87,99,87,99,[],['incorporates']
66,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,8,Other types in our compilation build upon existing data sets.,Other types in our compilation build upon existing data sets.,,,"['Other', 'types', 'in', 'our', 'compilation', 'build', 'upon', 'existing', 'data', 'sets.']",,,,Other types,0,0,11,0,11,,,,,,,"['Other', 'types']",[]
67,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,9,"For building a more practical numerical reasoning system, NUMBERGAME demands four capabilities beyond numerical reasoning: (i) detecting question format directly from data (ii) finding intermediate common format to which every format can be converted (iii) incorporating commonsense knowledge (iv) handling data imbalance across formats.","For building a more practical numerical reasoning system, NUMBERGAME demands four capabilities beyond numerical reasoning: (i) detecting question format directly from data (ii) finding intermediate common format to which every format can be converted (iii) incorporating commonsense knowledge (iv) handling data imbalance across formats.",,,"['For', 'building', 'a', 'more', 'practical', 'numerical', 'reasoning', 'system,', 'NUMBERGAME', 'demands', 'four', 'capabilities', 'beyond', 'numerical', 'reasoning:', '(i)', 'detecting', 'question', 'format', 'directly', 'from', 'data', '(ii)', 'finding', 'intermediate', 'common', 'format', 'to', 'which', 'every', 'format', 'can', 'be', 'converted', '(iii)', 'incorporating', 'commonsense', 'knowledge', '(iv)', 'handling', 'data', 'imbalance', 'across', 'formats.']",,,,NUMBERGAME,0,59,69,59,69,demands,0,70,77,70,77,['NUMBERGAME'],['demands']
68,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,10,"We build several baselines, including a new model based on knowledge hunting using a cheatsheet.","We build several baselines, including a new model based on knowledge hunting using a cheatsheet.",,,"['We', 'build', 'several', 'baselines,', 'including', 'a', 'new', 'model', 'based', 'on', 'knowledge', 'hunting', 'using', 'a', 'cheatsheet.']",,,,We,0,0,2,0,2,build,0,3,8,3,8,['We'],['build']
69,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,11,"However, all baselines perform poorly in contrast to the human baselines, indicating the hardness of our benchmark.","However, all baselines perform poorly in contrast to the human baselines, indicating the hardness of our benchmark.",,,"['However,', 'all', 'baselines', 'perform', 'poorly', 'in', 'contrast', 'to', 'the', 'human', 'baselines,', 'indicating', 'the', 'hardness', 'of', 'our', 'benchmark.']",,,,all baselines,0,10,23,10,23,perform,0,24,31,24,31,"['all', 'baselines']","['perform', 'indicating']"
70,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,12,"Our work takes forward the recent progress in generic system development, demonstrating the scope of these under-explored tasks.","Our work takes forward the recent progress in generic system development, demonstrating the scope of these under-explored tasks.",,,"['Our', 'work', 'takes', 'forward', 'the', 'recent', 'progress', 'in', 'generic', 'system', 'development,', 'demonstrating', 'the', 'scope', 'of', 'these', 'under-explored', 'tasks.']",,,,work,0,4,8,4,8,takes,0,9,14,9,14,['work'],['takes']
71,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Title,0,Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks.,Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks.,,,"['Towards', 'Question', 'Format', 'Independent', 'Numerical', 'Reasoning:', 'A', 'Set', 'of', 'Prerequisite', 'Tasks.']",,,,Question Format Independent Numerical Reasoning A Set,0,8,63,8,63,,,,,,,"['Question', 'Format', 'Independent', 'Numerical', 'Reasoning', 'A', 'Set']",[]
72,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,0,"Integrating distributed representations with symbolic operations is essential for reading comprehension requiring complex reasoning, such as counting, sorting and arithmetics, but most existing approaches are hard to scale to more domains or more complex reasoning.","Integrating distributed representations with symbolic operations is essential for reading comprehension requiring complex reasoning, such as counting, sorting and arithmetics, but most existing approaches are hard to scale to more domains or more complex reasoning.",,,"['Integrating', 'distributed', 'representations', 'with', 'symbolic', 'operations', 'is', 'essential', 'for', 'reading', 'comprehension', 'requiring', 'complex', 'reasoning,', 'such', 'as', 'counting,', 'sorting', 'and', 'arithmetics,', 'but', 'most', 'existing', 'approaches', 'are', 'hard', 'to', 'scale', 'to', 'more', 'domains', 'or', 'more', 'complex', 'reasoning.']",,,,,,,,,,but,0,179,182,179,182,"['existing', 'approaches']","['Integrating', 'is', 'are', 'scale']"
73,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,1,"In this work, we propose the Neural Symbolic Reader (NeRd), which includes a reader, e.g., BERT, to encode the passage and question, and a programmer, e.g., LSTM, to generate a program that is executed to produce the answer.","In this work, we propose the Neural Symbolic Reader (NeRd), which includes a reader, e.g., BERT, to encode the passage and question, and a programmer, e.g., LSTM, to generate a program that is executed to produce the answer.",,,"['In', 'this', 'work,', 'we', 'propose', 'the', 'Neural', 'Symbolic', 'Reader', '(NeRd),', 'which', 'includes', 'a', 'reader,', 'e.g.,', 'BERT,', 'to', 'encode', 'the', 'passage', 'and', 'question,', 'and', 'a', 'programmer,', 'e.g.,', 'LSTM,', 'to', 'generate', 'a', 'program', 'that', 'is', 'executed', 'to', 'produce', 'the', 'answer.']",,,,we,0,15,17,15,17,propose,0,18,25,18,25,['we'],['propose']
74,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,2,"Compared to previous works, NeRd is more scalable in two aspects: (1) domain-agnostic, i.e., the same neural architecture works for different domains; (2) compositional, i.e., when needed, complex programs can be generated by recursively applying the predefined operators, which become executable and interpretable representations for more complex reasoning.","Compared to previous works, NeRd is more scalable in two aspects: (1) domain-agnostic, i.e., the same neural architecture works for different domains; (2) compositional, i.e., when needed, complex programs can be generated by recursively applying the predefined operators, which become executable and interpretable representations for more complex reasoning.",,,"['Compared', 'to', 'previous', 'works,', 'NeRd', 'is', 'more', 'scalable', 'in', 'two', 'aspects:', '(1)', 'domain-agnostic,', 'i.e.,', 'the', 'same', 'neural', 'architecture', 'works', 'for', 'different', 'domains;', '(2)', 'compositional,', 'i.e.,', 'when', 'needed,', 'complex', 'programs', 'can', 'be', 'generated', 'by', 'recursively', 'applying', 'the', 'predefined', 'operators,', 'which', 'become', 'executable', 'and', 'interpretable', 'representations', 'for', 'more', 'complex', 'reasoning.']",,,,NeRd,0,29,33,29,33,is,0,34,36,34,36,['NeRd'],['is']
75,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,3,"Furthermore, to overcome the challenge of training NeRd with weak supervision, we apply data augmentation techniques and hard Expectation-Maximization (EM) with thresholding.","Furthermore, to overcome the challenge of training NeRd with weak supervision, we apply data augmentation techniques and hard Expectation-Maximization (EM) with thresholding.",,,"['Furthermore,', 'to', 'overcome', 'the', 'challenge', 'of', 'training', 'NeRd', 'with', 'weak', 'supervision,', 'we', 'apply', 'data', 'augmentation', 'techniques', 'and', 'hard', 'Expectation-Maximization', '(EM)', 'with', 'thresholding.']",,,,we,0,81,83,81,83,apply and,0,84,122,84,122,['we'],"['overcome', 'apply']"
76,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,4,"On DROP, a challenging reading comprehension dataset that requires discrete reasoning, NeRd achieves 2.5%/1.8% absolute improvement over the state-of-the-art on EM/F1 metrics.",On,DROP,", a challenging reading comprehension dataset that requires discrete reasoning , NeRd achieves 2.5 % /1.8 % absolute improvement over the state-of-the-art on EM/F1 metrics .","['On', 'DROP', ',', 'a', 'challenging', 'reading', 'comprehension', 'dataset', 'that', 'requires', 'discrete', 'reasoning', ',', 'NeRd', 'achieves', '2.5', '%', '/1.8', '%', 'absolute', 'improvement', 'over', 'the', 'state-of-the-art', 'on', 'EM/F1', 'metrics', '.']","(1, 2)","(2, 6)",0,NeRd,2,89,93,81,85,achieves,2,94,102,86,94,['NeRd'],['achieves']
77,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,5,"With the same architecture, NeRd significantly outperforms the baselines on MathQA, a math problem benchmark that requires multiple steps of reasoning, by 25.5% absolute increment on accuracy when trained on all the annotated programs.","With the same architecture, NeRd significantly outperforms the baselines on MathQA, a math problem benchmark that requires multiple steps of reasoning, by 25.5% absolute increment on accuracy when trained on all the annotated programs.",,,"['With', 'the', 'same', 'architecture,', 'NeRd', 'significantly', 'outperforms', 'the', 'baselines', 'on', 'MathQA,', 'a', 'math', 'problem', 'benchmark', 'that', 'requires', 'multiple', 'steps', 'of', 'reasoning,', 'by', '25.5%', 'absolute', 'increment', 'on', 'accuracy', 'when', 'trained', 'on', 'all', 'the', 'annotated', 'programs.']",,,,NeRd,0,29,33,29,33,outperforms,0,48,59,48,59,['NeRd'],['outperforms']
78,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,6,"More importantly, NeRd still beats the baselines even when only 20% of the program annotations are given.","More importantly, NeRd still beats the baselines even when only 20% of the program annotations are given.",,,"['More', 'importantly,', 'NeRd', 'still', 'beats', 'the', 'baselines', 'even', 'when', 'only', '20%', 'of', 'the', 'program', 'annotations', 'are', 'given.']",,,,NeRd,0,19,23,19,23,beats,0,30,35,30,35,['NeRd'],['beats']
79,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Title,0,Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension.,Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension.,,,"['Neural', 'Symbolic', 'Reader:', 'Scalable', 'Integration', 'of', 'Distributed', 'and', 'Symbolic', 'Representations', 'for', 'Reading', 'Comprehension.']",,,,Neural Symbolic Reader Scalable Integration,0,0,45,0,45,,,,,,,"['Neural', 'Symbolic', 'Reader', 'Scalable', 'Integration']",[]
80,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,0,A key component of successfully reading a passage of text is the ability to apply knowledge gained from the passage to a new situation.,A key component of successfully reading a passage of text is the ability to apply knowledge gained from the passage to a new situation.,,,"['A', 'key', 'component', 'of', 'successfully', 'reading', 'a', 'passage', 'of', 'text', 'is', 'the', 'ability', 'to', 'apply', 'knowledge', 'gained', 'from', 'the', 'passage', 'to', 'a', 'new', 'situation.']",,,,A key component,0,0,15,0,15,is,0,58,60,58,60,"['A', 'key', 'component']",['is']
81,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,1,"In order to facilitate progress on this kind of reading, we present ROPES, a challenging benchmark for reading comprehension targeting Reasoning Over Paragraph Effects in Situations.","In order to facilitate progress on this kind of reading, we present ROPES, a challenging benchmark for reading comprehension targeting Reasoning Over Paragraph Effects in Situations.",,,"['In', 'order', 'to', 'facilitate', 'progress', 'on', 'this', 'kind', 'of', 'reading,', 'we', 'present', 'ROPES,', 'a', 'challenging', 'benchmark', 'for', 'reading', 'comprehension', 'targeting', 'Reasoning', 'Over', 'Paragraph', 'Effects', 'in', 'Situations.']",,,,we,0,58,60,58,60,present,0,61,68,61,68,['we'],['present']
82,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,2,"We target expository language describing causes and effects (e.g., ""animal pollinators increase efficiency of fertilization in flowers""), as they have clear implications for new situations.","We target expository language describing causes and effects (e.g., ""animal pollinators increase efficiency of fertilization in flowers""), as they have clear implications for new situations.",,,"['We', 'target', 'expository', 'language', 'describing', 'causes', 'and', 'effects', '(e.g.,', '""animal', 'pollinators', 'increase', 'efficiency', 'of', 'fertilization', 'in', 'flowers""),', 'as', 'they', 'have', 'clear', 'implications', 'for', 'new', 'situations.']",,,,We,0,0,2,0,2,target,0,3,9,3,9,['We'],['target']
83,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,3,"A system is presented a background passage containing at least one of these relations, a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the background passage in the context of the situation.","A system is presented a background passage containing at least one of these relations, a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the background passage in the context of the situation.",,,"['A', 'system', 'is', 'presented', 'a', 'background', 'passage', 'containing', 'at', 'least', 'one', 'of', 'these', 'relations,', 'a', 'novel', 'situation', 'that', 'uses', 'this', 'background,', 'and', 'questions', 'that', 'require', 'reasoning', 'about', 'effects', 'of', 'the', 'relationships', 'in', 'the', 'background', 'passage', 'in', 'the', 'context', 'of', 'the', 'situation.']",,,,A system,0,0,8,0,8,is presented,0,9,21,9,21,"['A', 'system']","['is', 'presented']"
84,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,4,"We collect background passages from science textbooks and Wikipedia that contain such phenomena, and ask crowd workers to author situations, questions, and answers, resulting in a 14,102 question dataset.","We collect background passages from science textbooks and Wikipedia that contain such phenomena, and ask crowd workers to author situations, questions, and answers, resulting in a 14,102 question dataset.",,,"['We', 'collect', 'background', 'passages', 'from', 'science', 'textbooks', 'and', 'Wikipedia', 'that', 'contain', 'such', 'phenomena,', 'and', 'ask', 'crowd', 'workers', 'to', 'author', 'situations,', 'questions,', 'and', 'answers,', 'resulting', 'in', 'a', '14,102', 'question', 'dataset.']",,,,We,0,0,2,0,2,collect and ask resulting,0,3,178,3,178,['We'],"['collect', 'ask', 'resulting']"
85,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,5,We analyze the challenges of this task and evaluate the performance of state-of-the-art reading comprehension models.,We analyze the challenges of this task and evaluate the performance of state-of-the-art reading comprehension models.,,,"['We', 'analyze', 'the', 'challenges', 'of', 'this', 'task', 'and', 'evaluate', 'the', 'performance', 'of', 'state-of-the-art', 'reading', 'comprehension', 'models.']",,,,We,0,0,2,0,2,analyze and evaluate,0,3,51,3,51,['We'],"['analyze', 'evaluate']"
86,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,6,"The best model performs only slightly better than randomly guessing an answer of the correct type, at 51.9% F1, well below the human performance of 89.0%.","The best model performs only slightly better than randomly guessing an answer of the correct type, at 51.9% F1, well below the human performance of 89.0%.",,,"['The', 'best', 'model', 'performs', 'only', 'slightly', 'better', 'than', 'randomly', 'guessing', 'an', 'answer', 'of', 'the', 'correct', 'type,', 'at', '51.9%', 'F1,', 'well', 'below', 'the', 'human', 'performance', 'of', '89.0%.']",,,,The best model,0,0,14,0,14,performs,0,15,23,15,23,"['The', 'best', 'model']",['performs']
87,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Title,0,Reasoning Over Paragraph Effects in Situations.,Reasoning Over Paragraph Effects in Situations.,,,"['Reasoning', 'Over', 'Paragraph', 'Effects', 'in', 'Situations.']",,,,,,,,,,Reasoning,0,0,9,0,9,[],['Reasoning']
88,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,0,"Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers.","Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers.",,,"['Reading', 'comprehension', 'models', 'have', 'been', 'successfully', 'applied', 'to', 'extractive', 'text', 'answers,', 'but', 'it', 'is', 'unclear', 'how', 'best', 'to', 'generalize', 'these', 'models', 'to', 'abstractive', 'numerical', 'answers.']",,,,,,,,,,but,0,89,92,89,92,"['comprehension', 'models', 'it']","['have', 'been', 'applied', 'is']"
89,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,1,We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning.,We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning.,,,"['We', 'enable', 'a', 'BERT-based', 'reading', 'comprehension', 'model', 'to', 'perform', 'lightweight', 'numerical', 'reasoning.']",,,,We,0,0,2,0,2,enable,0,3,9,3,9,['We'],['enable']
90,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,2,We augment the model with a predefined set of executable 'programs' which encompass simple arithmetic as well as extraction.,We augment the model with a predefined set of executable 'programs' which encompass simple arithmetic as well as extraction.,,,"['We', 'augment', 'the', 'model', 'with', 'a', 'predefined', 'set', 'of', 'executable', ""'programs'"", 'which', 'encompass', 'simple', 'arithmetic', 'as', 'well', 'as', 'extraction.']",,,,We,0,0,2,0,2,augment,0,3,10,3,10,['We'],['augment']
91,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,3,"Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it.","Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it.",,,"['Rather', 'than', 'having', 'to', 'learn', 'to', 'manipulate', 'numbers', 'directly,', 'the', 'model', 'can', 'pick', 'a', 'program', 'and', 'execute', 'it.']",,,,the model,0,61,70,61,70,can pick and execute,0,71,101,71,101,"['the', 'model']","['can', 'pick', 'execute']"
92,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,4,"On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a 33% absolute improvement by adding shallow programs.",On the recent Discrete Reasoning Over Passages (,DROP,") dataset , designed to challenge reading comprehension models , we show a 33 % absolute improvement by adding shallow programs .","['On', 'the', 'recent', 'Discrete', 'Reasoning', 'Over', 'Passages', '(', 'DROP', ')', 'dataset', ',', 'designed', 'to', 'challenge', 'reading', 'comprehension', 'models', ',', 'we', 'show', 'a', '33', '%', 'absolute', 'improvement', 'by', 'adding', 'shallow', 'programs', '.']","(8, 9)","(48, 52)",0,we,2,119,121,65,67,show,2,122,126,68,72,['we'],['show']
93,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,5,"The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.","The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.",,,"['The', 'model', 'can', 'learn', 'to', 'predict', 'new', 'operations', 'when', 'appropriate', 'in', 'a', 'math', 'word', 'problem', 'setting', '(Roy', 'and', 'Roth,', '2015)', 'with', 'very', 'few', 'training', 'examples.']",,,,The model,0,0,9,0,9,can learn,0,10,19,10,19,"['The', 'model']","['can', 'learn', 'predict']"
94,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Title,0,Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension.,Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension.,,,"['Giving', 'BERT', 'a', 'Calculator:', 'Finding', 'Operations', 'and', 'Arguments', 'with', 'Reading', 'Comprehension.']",,,,BERT a Calculator,0,7,24,7,24,Giving,0,0,6,0,6,"['BERT', 'a', 'Calculator']",['Giving']
95,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,0,"Machine reading comprehension (MRC) aims to teach machines to read and comprehend human languages, which is a long-standing goal of natural language processing (NLP).","Machine reading comprehension (MRC) aims to teach machines to read and comprehend human languages, which is a long-standing goal of natural language processing (NLP).",,,"['Machine', 'reading', 'comprehension', '(MRC)', 'aims', 'to', 'teach', 'machines', 'to', 'read', 'and', 'comprehend', 'human', 'languages,', 'which', 'is', 'a', 'long-standing', 'goal', 'of', 'natural', 'language', 'processing', '(NLP).']",,,,Machine reading comprehension MRC,0,0,35,0,35,aims,0,38,42,38,42,"['Machine', 'reading', 'comprehension', 'MRC']","['aims', 'teach', 'read', 'comprehend']"
96,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,1,"With the burst of deep neural networks and the evolution of contextualized language models (CLMs), the research of MRC has experienced two significant breakthroughs.","With the burst of deep neural networks and the evolution of contextualized language models (CLMs), the research of MRC has experienced two significant breakthroughs.",,,"['With', 'the', 'burst', 'of', 'deep', 'neural', 'networks', 'and', 'the', 'evolution', 'of', 'contextualized', 'language', 'models', '(CLMs),', 'the', 'research', 'of', 'MRC', 'has', 'experienced', 'two', 'significant', 'breakthroughs.']",,,,the research,0,102,114,102,114,has experienced,0,122,137,122,137,"['the', 'research']","['has', 'experienced']"
97,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,2,"MRC and CLM, as a phenomenon, have a great impact on the NLP community.","MRC and CLM, as a phenomenon, have a great impact on the NLP community.",,,"['MRC', 'and', 'CLM,', 'as', 'a', 'phenomenon,', 'have', 'a', 'great', 'impact', 'on', 'the', 'NLP', 'community.']",,,,MRC CLM,0,0,11,0,11,have,0,32,36,32,36,"['MRC', 'CLM']",['have']
98,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,3,"In this survey, we provide a comprehensive and comparative review on MRC covering overall research topics about 1) the origin and development of MRC and CLM, with a particular focus on the role of CLMs; 2) the impact of MRC and CLM to the NLP community; 3) the definition, datasets, and evaluation of MRC; 4) general MRC architecture and technical methods in the view of two-stage Encoder-Decoder solving architecture from the insights of the cognitive process of humans; 5) previous highlights, emerging topics, and our empirical analysis, among which we especially focus on what works in different periods of MRC researches.","In this survey, we provide a comprehensive and comparative review on MRC covering overall research topics about 1) the origin and development of MRC and CLM, with a particular focus on the role of CLMs; 2) the impact of MRC and CLM to the NLP community; 3) the definition, datasets, and evaluation of MRC; 4) general MRC architecture and technical methods in the view of two-stage Encoder-Decoder solving architecture from the insights of the cognitive process of humans; 5) previous highlights, emerging topics, and our empirical analysis, among which we especially focus on what works in different periods of MRC researches.",,,"['In', 'this', 'survey,', 'we', 'provide', 'a', 'comprehensive', 'and', 'comparative', 'review', 'on', 'MRC', 'covering', 'overall', 'research', 'topics', 'about', '1)', 'the', 'origin', 'and', 'development', 'of', 'MRC', 'and', 'CLM,', 'with', 'a', 'particular', 'focus', 'on', 'the', 'role', 'of', 'CLMs;', '2)', 'the', 'impact', 'of', 'MRC', 'and', 'CLM', 'to', 'the', 'NLP', 'community;', '3)', 'the', 'definition,', 'datasets,', 'and', 'evaluation', 'of', 'MRC;', '4)', 'general', 'MRC', 'architecture', 'and', 'technical', 'methods', 'in', 'the', 'view', 'of', 'two-stage', 'Encoder-Decoder', 'solving', 'architecture', 'from', 'the', 'insights', 'of', 'the', 'cognitive', 'process', 'of', 'humans;', '5)', 'previous', 'highlights,', 'emerging', 'topics,', 'and', 'our', 'empirical', 'analysis,', 'among', 'which', 'we', 'especially', 'focus', 'on', 'what', 'works', 'in', 'different', 'periods', 'of', 'MRC', 'researches.']",,,,we,0,17,19,17,19,provide covering,0,20,82,20,82,['we'],"['provide', 'covering']"
99,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,4,We propose a full-view categorization and new taxonomies on these topics.,We propose a full-view categorization and new taxonomies on these topics.,,,"['We', 'propose', 'a', 'full-view', 'categorization', 'and', 'new', 'taxonomies', 'on', 'these', 'topics.']",,,,We,0,0,2,0,2,propose,0,3,10,3,10,['We'],['propose']
100,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,5,The primary views we have arrived at are that 1) MRC boosts the progress from language processing to understanding; 2) the rapid improvement of MRC systems greatly benefits from the development of CLMs; 3) the theme of MRC is gradually moving from shallow text matching to cognitive reasoning.,The primary views we have arrived at are that 1) MRC boosts the progress from language processing to understanding; 2) the rapid improvement of MRC systems greatly benefits from the development of CLMs; 3) the theme of MRC is gradually moving from shallow text matching to cognitive reasoning.,,,"['The', 'primary', 'views', 'we', 'have', 'arrived', 'at', 'are', 'that', '1)', 'MRC', 'boosts', 'the', 'progress', 'from', 'language', 'processing', 'to', 'understanding;', '2)', 'the', 'rapid', 'improvement', 'of', 'MRC', 'systems', 'greatly', 'benefits', 'from', 'the', 'development', 'of', 'CLMs;', '3)', 'the', 'theme', 'of', 'MRC', 'is', 'gradually', 'moving', 'from', 'shallow', 'text', 'matching', 'to', 'cognitive', 'reasoning.']",,,,The primary views,0,0,17,0,17,are,0,37,40,37,40,"['The', 'primary', 'views']",['are']
101,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Title,0,Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond.,Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond.,,,"['Machine', 'Reading', 'Comprehension:', 'The', 'Role', 'of', 'Contextualized', 'Language', 'Models', 'and', 'Beyond.']",,,,Machine Reading Comprehension The Role,0,0,40,0,40,,,,,,,"['Machine', 'Reading', 'Comprehension', 'The', 'Role']",[]
102,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,0,Many question answering (QA) tasks only provide weak supervision for how the answer should be computed.,Many question answering (QA) tasks only provide weak supervision for how the answer should be computed.,,,"['Many', 'question', 'answering', '(QA)', 'tasks', 'only', 'provide', 'weak', 'supervision', 'for', 'how', 'the', 'answer', 'should', 'be', 'computed.']",,,,Many question QA tasks,0,0,36,0,36,provide,0,42,49,42,49,"['Many', 'question', 'QA', 'tasks']",['provide']
103,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,1,"For example, TriviaQA answers are entities that can be mentioned multiple times in supporting documents, while DROP answers can be computed by deriving many different equations from numbers in the reference text.","For example , TriviaQA answers are entities that can be mentioned multiple times in supporting documents , while",DROP,answers can be computed by deriving many different equations from numbers in the reference text .,"['For', 'example', ',', 'TriviaQA', 'answers', 'are', 'entities', 'that', 'can', 'be', 'mentioned', 'multiple', 'times', 'in', 'supporting', 'documents', ',', 'while', 'DROP', 'answers', 'can', 'be', 'computed', 'by', 'deriving', 'many', 'different', 'equations', 'from', 'numbers', 'in', 'the', 'reference', 'text', '.']","(18, 19)","(112, 116)",0,TriviaQA answers,0,14,30,14,30,are,0,31,34,31,34,"['TriviaQA', 'answers']",['are']
104,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,2,"In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible ""solutions"" (e.g.","In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible ""solutions"" (e.g.",,,"['In', 'this', 'paper,', 'we', 'show', 'it', 'is', 'possible', 'to', 'convert', 'such', 'tasks', 'into', 'discrete', 'latent', 'variable', 'learning', 'problems', 'with', 'a', 'precomputed,', 'task-specific', 'set', 'of', 'possible', '""solutions""', '(e.g.']",,,,we,0,16,18,16,18,show,0,19,23,19,23,['we'],['show']
105,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,3,different mentions or equations) that contains one correct option.,different mentions or equations) that contains one correct option.,,,"['different', 'mentions', 'or', 'equations)', 'that', 'contains', 'one', 'correct', 'option.']",,,,different mentions equations,0,0,31,0,31,,,,,,,"['different', 'mentions', 'equations']",[]
106,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,4,We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update.,We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update.,,,"['We', 'then', 'develop', 'a', 'hard', 'EM', 'learning', 'scheme', 'that', 'computes', 'gradients', 'relative', 'to', 'the', 'most', 'likely', 'solution', 'at', 'each', 'update.']",,,,We,0,0,2,0,2,develop,0,8,15,8,15,['We'],['develop']
107,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,5,"Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2--10%, and achieves the state-of-the-art on five of them.","Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2--10%, and achieves the state-of-the-art on five of them.",,,"['Despite', 'its', 'simplicity,', 'we', 'show', 'that', 'this', 'approach', 'significantly', 'outperforms', 'previous', 'methods', 'on', 'six', 'QA', 'tasks,', 'including', 'absolute', 'gains', 'of', '2--10%,', 'and', 'achieves', 'the', 'state-of-the-art', 'on', 'five', 'of', 'them.']",,,,we,0,25,27,25,27,show,0,28,32,28,32,['we'],['show']
108,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,6,"Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.","Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.",,,"['Using', 'hard', 'updates', 'instead', 'of', 'maximizing', 'marginal', 'likelihood', 'is', 'key', 'to', 'these', 'results', 'as', 'it', 'encourages', 'the', 'model', 'to', 'find', 'the', 'one', 'correct', 'answer,', 'which', 'we', 'show', 'through', 'detailed', 'qualitative', 'analysis.']",,,,,,,,,,is,0,61,63,61,63,[],"['Using', 'is']"
109,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Title,0,A Discrete Hard EM Approach for Weakly Supervised Question Answering.,A Discrete Hard EM Approach for Weakly Supervised Question Answering.,,,"['A', 'Discrete', 'Hard', 'EM', 'Approach', 'for', 'Weakly', 'Supervised', 'Question', 'Answering.']",,,,A Discrete Hard EM Approach,0,0,27,0,27,,,,,,,"['A', 'Discrete', 'Hard', 'EM', 'Approach']",[]
110,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,0,Innovations in annotation methodology have been a propellant for Reading Comprehension (RC) datasets and models.,Innovations in annotation methodology have been a propellant for Reading Comprehension (RC) datasets and models.,,,"['Innovations', 'in', 'annotation', 'methodology', 'have', 'been', 'a', 'propellant', 'for', 'Reading', 'Comprehension', '(RC)', 'datasets', 'and', 'models.']",,,,Innovations,0,0,11,0,11,have been,0,38,47,38,47,['Innovations'],"['have', 'been']"
111,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,1,"One recent trend to challenge current RC models is to involve a model in the annotation process: humans create questions adversarially, such that the model fails to answer them correctly.","One recent trend to challenge current RC models is to involve a model in the annotation process: humans create questions adversarially, such that the model fails to answer them correctly.",,,"['One', 'recent', 'trend', 'to', 'challenge', 'current', 'RC', 'models', 'is', 'to', 'involve', 'a', 'model', 'in', 'the', 'annotation', 'process:', 'humans', 'create', 'questions', 'adversarially,', 'such', 'that', 'the', 'model', 'fails', 'to', 'answer', 'them', 'correctly.']",,,,,,,,,,,,,,,,"['recent', 'trend', 'humans']","['is', 'involve', 'create']"
112,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,2,"In this work we investigate this annotation approach and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop.","In this work we investigate this annotation approach and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop.",,,"['In', 'this', 'work', 'we', 'investigate', 'this', 'annotation', 'approach', 'and', 'apply', 'it', 'in', 'three', 'different', 'settings,', 'collecting', 'a', 'total', 'of', '36,000', 'samples', 'with', 'progressively', 'stronger', 'models', 'in', 'the', 'annotation', 'loop.']",,,,we,0,13,15,13,15,investigate and apply,0,16,62,16,62,['we'],"['investigate', 'apply', 'collecting']"
113,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,3,"This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalisation to data collected without a model.","This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalisation to data collected without a model.",,,"['This', 'allows', 'us', 'to', 'explore', 'questions', 'such', 'as', 'the', 'reproducibility', 'of', 'the', 'adversarial', 'effect,', 'transfer', 'from', 'data', 'collected', 'with', 'varying', 'model-in-the-loop', 'strengths,', 'and', 'generalisation', 'to', 'data', 'collected', 'without', 'a', 'model.']",,,,This,0,0,4,0,4,allows,0,5,11,5,11,['This'],"['allows', 'explore']"
114,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,4,"We find that training on adversarially collected samples leads to strong generalisation to non-adversarially collected datasets, yet with progressive deterioration as the model-in-the-loop strength increases.","We find that training on adversarially collected samples leads to strong generalisation to non-adversarially collected datasets, yet with progressive deterioration as the model-in-the-loop strength increases.",,,"['We', 'find', 'that', 'training', 'on', 'adversarially', 'collected', 'samples', 'leads', 'to', 'strong', 'generalisation', 'to', 'non-adversarially', 'collected', 'datasets,', 'yet', 'with', 'progressive', 'deterioration', 'as', 'the', 'model-in-the-loop', 'strength', 'increases.']",,,,We,0,0,2,0,2,find,0,3,7,3,7,['We'],['find']
115,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,5,"Furthermore we find that stronger models can still learn from datasets collected with substantially weaker models in the loop: When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 36.0F1 on questions that it cannot answer when trained on SQuAD - only marginally lower than when trained on data collected using RoBERTa itself.","Furthermore we find that stronger models can still learn from datasets collected with substantially weaker models in the loop: When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 36.0F1 on questions that it cannot answer when trained on SQuAD - only marginally lower than when trained on data collected using RoBERTa itself.",,,"['Furthermore', 'we', 'find', 'that', 'stronger', 'models', 'can', 'still', 'learn', 'from', 'datasets', 'collected', 'with', 'substantially', 'weaker', 'models', 'in', 'the', 'loop:', 'When', 'trained', 'on', 'data', 'collected', 'with', 'a', 'BiDAF', 'model', 'in', 'the', 'loop,', 'RoBERTa', 'achieves', '36.0F1', 'on', 'questions', 'that', 'it', 'cannot', 'answer', 'when', 'trained', 'on', 'SQuAD', '-', 'only', 'marginally', 'lower', 'than', 'when', 'trained', 'on', 'data', 'collected', 'using', 'RoBERTa', 'itself.']",,,,we,0,12,14,12,14,find,0,15,19,15,19,['we'],"['find', 'can', 'learn', 'achieves']"
116,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Title,0,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension.,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension.,,,"['Beat', 'the', 'AI:', 'Investigating', 'Adversarial', 'Human', 'Annotations', 'for', 'Reading', 'Comprehension.']",,,,,,,,,,Beat,0,0,4,0,4,[],['Beat']
117,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,0,"Machine reading comprehension (MRC) is one of the primary challenges in natural language understanding (NLU), its objective is to give the correct answer based on the questions asked from the specified context.","Machine reading comprehension (MRC) is one of the primary challenges in natural language understanding (NLU), its objective is to give the correct answer based on the questions asked from the specified context.",,,"['Machine', 'reading', 'comprehension', '(MRC)', 'is', 'one', 'of', 'the', 'primary', 'challenges', 'in', 'natural', 'language', 'understanding', '(NLU),', 'its', 'objective', 'is', 'to', 'give', 'the', 'correct', 'answer', 'based', 'on', 'the', 'questions', 'asked', 'from', 'the', 'specified', 'context.']",,,,objective,0,119,128,119,128,is,0,129,131,129,131,"['Machine', 'reading', 'comprehension', 'MRC', 'objective']","['is', 'is', 'give', 'based']"
118,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,1,"Nowadays, attention mechanisms have been widely used in reading comprehension tasks.","Nowadays, attention mechanisms have been widely used in reading comprehension tasks.",,,"['Nowadays,', 'attention', 'mechanisms', 'have', 'been', 'widely', 'used', 'in', 'reading', 'comprehension', 'tasks.']",,,,attention mechanisms,0,11,31,11,31,have been used,0,32,53,32,53,"['attention', 'mechanisms']","['have', 'been', 'used']"
119,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,2,"In this chapter, based on the analysis of two state-of-the-art attention mechanisms, Coattention and Multi-head Attention, the Transformer-based Coattention (TBC) is proposed.","In this chapter, based on the analysis of two state-of-the-art attention mechanisms, Coattention and Multi-head Attention, the Transformer-based Coattention (TBC) is proposed.",,,"['In', 'this', 'chapter,', 'based', 'on', 'the', 'analysis', 'of', 'two', 'state-of-the-art', 'attention', 'mechanisms,', 'Coattention', 'and', 'Multi-head', 'Attention,', 'the', 'Transformer-based', 'Coattention', '(TBC)', 'is', 'proposed.']",,,,,,,,,,based,0,18,23,18,23,[],['based']
120,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,3,"Furthermore, a general hybrid scheme is proposed to incorporate the TBC into pretrained MRC models with little extra training cost.","Furthermore, a general hybrid scheme is proposed to incorporate the TBC into pretrained MRC models with little extra training cost.",,,"['Furthermore,', 'a', 'general', 'hybrid', 'scheme', 'is', 'proposed', 'to', 'incorporate', 'the', 'TBC', 'into', 'pretrained', 'MRC', 'models', 'with', 'little', 'extra', 'training', 'cost.']",,,,a general hybrid scheme,0,14,37,14,37,is proposed,0,38,49,38,49,"['a', 'general', 'hybrid', 'scheme']","['is', 'proposed', 'incorporate']"
121,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,4,Our experiments on Stanford Question Answering Dataset (SQuAD) and Discrete Reasoning Over the content of Paragraphs (DROP) show that our hybrid scheme make models achieve better performance.,Our experiments on Stanford Question Answering Dataset ( SQuAD ) and Discrete Reasoning Over the content of Paragraphs (,DROP,) show that our hybrid scheme make models achieve better performance .,"['Our', 'experiments', 'on', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', 'and', 'Discrete', 'Reasoning', 'Over', 'the', 'content', 'of', 'Paragraphs', '(', 'DROP', ')', 'show', 'that', 'our', 'hybrid', 'scheme', 'make', 'models', 'achieve', 'better', 'performance', '.']","(19, 20)","(120, 124)",0,experiments,0,4,15,4,15,show,2,128,132,2,6,['experiments'],['show']
122,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Title,0,Transformer-Based Coattention: Neural Architecture for Reading Comprehension.,Transformer-Based Coattention: Neural Architecture for Reading Comprehension.,,,"['Transformer-Based', 'Coattention:', 'Neural', 'Architecture', 'for', 'Reading', 'Comprehension.']",,,,Transformer Coattention Neural Architecture,0,0,53,0,53,,,,,,,"['Transformer', 'Coattention', 'Neural', 'Architecture']",[]
123,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,0,Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information.,Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information.,,,"['Large', 'pre-trained', 'language', 'models', '(LMs)', 'are', 'known', 'to', 'encode', 'substantial', 'amounts', 'of', 'linguistic', 'information.']",,,,Large pre - trained language models LMs,0,0,41,0,41,are known,0,44,53,44,53,"['Large', 'pre', '-', 'trained', 'language', 'models', 'LMs']","['are', 'known', 'encode']"
124,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,1,"However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only.","However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only.",,,"['However,', 'high-level', 'reasoning', 'skills,', 'such', 'as', 'numerical', 'reasoning,', 'are', 'difficult', 'to', 'learn', 'from', 'a', 'language-modeling', 'objective', 'only.']",,,,high level reasoning skills modeling objective,0,10,131,10,131,are,0,72,75,72,75,"['high', 'level', 'reasoning', 'skills', 'modeling', 'objective']",['are']
125,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,2,"Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility.","Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility.",,,"['Consequently,', 'existing', 'models', 'for', 'numerical', 'reasoning', 'have', 'used', 'specialized', 'architectures', 'with', 'limited', 'flexibility.']",,,,models,0,24,30,24,30,have used,0,55,64,55,64,['models'],"['have', 'used']"
126,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,3,"In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.","In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.",,,"['In', 'this', 'work,', 'we', 'show', 'that', 'numerical', 'reasoning', 'is', 'amenable', 'to', 'automatic', 'data', 'generation,', 'and', 'thus', 'one', 'can', 'inject', 'this', 'skill', 'into', 'pre-trained', 'LMs,', 'by', 'generating', 'large', 'amounts', 'of', 'data,', 'and', 'training', 'in', 'a', 'multi-task', 'setup.']",,,,we,0,15,17,15,17,show,0,18,22,18,22,['we'],['show']
127,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,4,"We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 $\rightarrow$ 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture.","We show that pre-training our model , GenBERT , on this data , dramatically improves performance on",DROP,"( 49.3 $ \rightarrow $ 72.3 F1 ) , reaching performance that matches state-of-the-art models of comparable size , while using a simple and general-purpose encoder-decoder architecture .","['We', 'show', 'that', 'pre-training', 'our', 'model', ',', 'GenBERT', ',', 'on', 'this', 'data', ',', 'dramatically', 'improves', 'performance', 'on', 'DROP', '(', '49.3', '$', '\\rightarrow', '$', '72.3', 'F1', ')', ',', 'reaching', 'performance', 'that', 'matches', 'state-of-the-art', 'models', 'of', 'comparable', 'size', ',', 'while', 'using', 'a', 'simple', 'and', 'general-purpose', 'encoder-decoder', 'architecture', '.']","(17, 18)","(99, 103)",0,We,0,0,2,0,2,show,0,3,7,3,7,['We'],['show']
128,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,5,"Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks.","Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks.",,,"['Moreover,', 'GenBERT', 'generalizes', 'well', 'to', 'math', 'word', 'problem', 'datasets,', 'while', 'maintaining', 'high', 'performance', 'on', 'standard', 'RC', 'tasks.']",,,,GenBERT,0,11,18,11,18,generalizes,0,19,30,19,30,['GenBERT'],['generalizes']
129,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,6,"Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.","Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.",,,"['Our', 'approach', 'provides', 'a', 'general', 'recipe', 'for', 'injecting', 'skills', 'into', 'large', 'pre-trained', 'LMs,', 'whenever', 'the', 'skill', 'is', 'amenable', 'to', 'automatic', 'data', 'augmentation.']",,,,approach,0,4,12,4,12,provides,0,13,21,13,21,['approach'],['provides']
130,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Title,0,Injecting Numerical Reasoning Skills into Language Models.,Injecting Numerical Reasoning Skills into Language Models.,,,"['Injecting', 'Numerical', 'Reasoning', 'Skills', 'into', 'Language', 'Models.']",,,,,,,,,,Injecting,0,0,9,0,9,[],['Injecting']
131,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,0,Standard test sets for supervised learning evaluate in-distribution generalization.,Standard test sets for supervised learning evaluate in-distribution generalization.,,,"['Standard', 'test', 'sets', 'for', 'supervised', 'learning', 'evaluate', 'in-distribution', 'generalization.']",,,,Standard test sets distribution generalization,0,0,84,0,84,evaluate,0,43,51,43,51,"['Standard', 'test', 'sets', 'distribution', 'generalization']",['evaluate']
132,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,1,"Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities.","Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities.",,,"['Unfortunately,', 'when', 'a', 'dataset', 'has', 'systematic', 'gaps', '(e.g.,', 'annotation', 'artifacts),', 'these', 'evaluations', 'are', 'misleading:', 'a', 'model', 'can', 'learn', 'simple', 'decision', 'rules', 'that', 'perform', 'well', 'on', 'the', 'test', 'set', 'but', 'do', 'not', 'capture', 'a', ""dataset's"", 'intended', 'capabilities.']",,,,,,,,,,,,,,,,"['these', 'evaluations', 'a', 'model']","['are', 'can', 'learn']"
133,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,2,We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data.,We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data.,,,"['We', 'propose', 'a', 'new', 'annotation', 'paradigm', 'for', 'NLP', 'that', 'helps', 'to', 'close', 'systematic', 'gaps', 'in', 'the', 'test', 'data.']",,,,We,0,0,2,0,2,propose,0,3,10,3,10,['We'],['propose']
134,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,3,"In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets.","In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets.",,,"['In', 'particular,', 'after', 'a', 'dataset', 'is', 'constructed,', 'we', 'recommend', 'that', 'the', 'dataset', 'authors', 'manually', 'perturb', 'the', 'test', 'instances', 'in', 'small', 'but', 'meaningful', 'ways', 'that', '(typically)', 'change', 'the', 'gold', 'label,', 'creating', 'contrast', 'sets.']",,,,we,0,49,51,49,51,recommend,0,52,61,52,61,['we'],['recommend']
135,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,4,"Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities.","Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities.",,,"['Contrast', 'sets', 'provide', 'a', 'local', 'view', 'of', 'a', ""model's"", 'decision', 'boundary,', 'which', 'can', 'be', 'used', 'to', 'more', 'accurately', 'evaluate', 'a', ""model's"", 'true', 'linguistic', 'capabilities.']",,,,Contrast sets,0,0,13,0,13,provide,0,14,21,14,21,"['Contrast', 'sets']",['provide']
136,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,5,"We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, IMDb sentiment analysis).","We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets ( e.g. ,",DROP,"reading comprehension , UD parsing , IMDb sentiment analysis ) .","['We', 'demonstrate', 'the', 'efficacy', 'of', 'contrast', 'sets', 'by', 'creating', 'them', 'for', '10', 'diverse', 'NLP', 'datasets', '(', 'e.g.', ',', 'DROP', 'reading', 'comprehension', ',', 'UD', 'parsing', ',', 'IMDb', 'sentiment', 'analysis', ')', '.']","(18, 19)","(98, 102)",0,We,0,0,2,0,2,demonstrate,0,3,14,3,14,['We'],['demonstrate']
137,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,6,"Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\% in some cases.","Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\% in some cases.",,,"['Although', 'our', 'contrast', 'sets', 'are', 'not', 'explicitly', 'adversarial,', 'model', 'performance', 'is', 'significantly', 'lower', 'on', 'them', 'than', 'on', 'the', 'original', 'test', 'sets---up', 'to', '25\\%', 'in', 'some', 'cases.']",,,,model performance,0,60,77,60,77,is,0,78,80,78,80,"['model', 'performance']",['is']
138,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,7,We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.,We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.,,,"['We', 'release', 'our', 'contrast', 'sets', 'as', 'new', 'evaluation', 'benchmarks', 'and', 'encourage', 'future', 'dataset', 'construction', 'efforts', 'to', 'follow', 'similar', 'annotation', 'processes.']",,,,We,0,0,2,0,2,release and encourage,0,3,71,3,71,['We'],"['release', 'encourage', 'follow']"
139,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Title,0,Evaluating NLP Models via Contrast Sets.,Evaluating NLP Models via Contrast Sets.,,,"['Evaluating', 'NLP', 'Models', 'via', 'Contrast', 'Sets.']",,,,,,,,,,Evaluating,0,0,10,0,10,[],['Evaluating']
140,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,0,The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks.,The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks.,,,"['The', 'ability', 'to', 'understand', 'and', 'work', 'with', 'numbers', '(numeracy)', 'is', 'critical', 'for', 'many', 'complex', 'reasoning', 'tasks.']",,,,The ability,0,0,11,0,11,is,0,61,63,61,63,"['The', 'ability']",['is']
141,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,1,"Currently, most NLP models treat numbers in text in the same way as other tokens---they embed them as distributed vectors.","Currently, most NLP models treat numbers in text in the same way as other tokens---they embed them as distributed vectors.",,,"['Currently,', 'most', 'NLP', 'models', 'treat', 'numbers', 'in', 'text', 'in', 'the', 'same', 'way', 'as', 'other', 'tokens---they', 'embed', 'them', 'as', 'distributed', 'vectors.']",,,,,,,,,,,,,,,,"['most', 'NLP', 'models', 'they']","['treat', 'embed']"
142,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,2,Is this enough to capture numeracy?,Is this enough to capture numeracy?,,,"['Is', 'this', 'enough', 'to', 'capture', 'numeracy?']",,,,this,0,3,7,3,7,Is,0,0,2,0,2,['this'],['Is']
143,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,3,We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset.,We begin by investigating the numerical reasoning capabilities of,a state - of - the - art question answering model on the DROP dataset,.,"['We', 'begin', 'by', 'investigating', 'the', 'numerical', 'reasoning', 'capabilities', 'of', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'question', 'answering', 'model', 'on', 'the', 'DROP', 'dataset', '.']","(9, 24)","(65, 134)",57,We,0,0,2,0,2,begin,0,3,8,3,8,['We'],['begin']
144,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,4,"We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy.",We find,this model,"excels on questions that require numerical reasoning , i.e. , it already captures numeracy .","['We', 'find', 'this', 'model', 'excels', 'on', 'questions', 'that', 'require', 'numerical', 'reasoning', ',', 'i.e.', ',', 'it', 'already', 'captures', 'numeracy', '.']","(2, 4)","(7, 17)",-1,We,0,0,2,0,2,find,0,3,7,3,7,['We'],['find']
145,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,4,"We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy.","We find this model excels on questions that require numerical reasoning , i.e. ,",it,already captures numeracy .,"['We', 'find', 'this', 'model', 'excels', 'on', 'questions', 'that', 'require', 'numerical', 'reasoning', ',', 'i.e.', ',', 'it', 'already', 'captures', 'numeracy', '.']","(14, 15)","(80, 82)",-1,We,0,0,2,0,2,find,0,3,7,3,7,['We'],['find']
146,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,5,"To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks.","To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks.",,,"['To', 'understand', 'how', 'this', 'capability', 'emerges,', 'we', 'probe', 'token', 'embedding', 'methods', '(e.g.,', 'BERT,', 'GloVe)', 'on', 'synthetic', 'list', 'maximum,', 'number', 'decoding,', 'and', 'addition', 'tasks.']",,,,we,0,44,46,44,46,probe,0,47,52,47,52,['we'],"['understand', 'probe']"
147,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,6,A surprising degree of numeracy is naturally present in standard embeddings.,A surprising degree of numeracy is naturally present in standard embeddings.,,,"['A', 'surprising', 'degree', 'of', 'numeracy', 'is', 'naturally', 'present', 'in', 'standard', 'embeddings.']",,,,A surprising degree,0,0,19,0,19,is,0,32,34,32,34,"['A', 'surprising', 'degree']",['is']
148,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,7,"For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000.","For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000.",,,"['For', 'example,', 'GloVe', 'and', 'word2vec', 'accurately', 'encode', 'magnitude', 'for', 'numbers', 'up', 'to', '1,000.']",,,,GloVe word2vec,0,14,32,14,32,encode,0,44,50,44,50,"['GloVe', 'word2vec']",['encode']
149,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,8,"Furthermore, character-level embeddings are even more precise---ELMo captures numeracy the best for all pre-trained methods---but BERT, which uses sub-word units, is less exact.","Furthermore, character-level embeddings are even more precise---ELMo captures numeracy the best for all pre-trained methods---but BERT, which uses sub-word units, is less exact.",,,"['Furthermore,', 'character-level', 'embeddings', 'are', 'even', 'more', 'precise---ELMo', 'captures', 'numeracy', 'the', 'best', 'for', 'all', 'pre-trained', 'methods---but', 'BERT,', 'which', 'uses', 'sub-word', 'units,', 'is', 'less', 'exact.']",,,,,,,,,,but,0,135,138,135,138,"['character', 'level', 'embeddings', 'ELMo', 'BERT', 'sub', '-', 'word', 'units']","['are', 'is']"
150,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Title,0,Do NLP Models Know Numbers?,Do NLP Models Know Numbers?,,,"['Do', 'NLP', 'Models', 'Know', 'Numbers?']",,,,NLP Models,0,3,13,3,13,Do Know,0,0,18,0,18,"['NLP', 'Models']","['Do', 'Know']"
151,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Title,1,Probing Numeracy in Embeddings.,Probing Numeracy in Embeddings.,,,"['Probing', 'Numeracy', 'in', 'Embeddings.']",,,,,,,,,,Probing,0,0,7,0,7,[],['Probing']
152,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,0,"In this work, we focus on the task of Automatic Question Generation (AQG) where given a passage and an answer the task is to generate the corresponding question.","In this work, we focus on the task of Automatic Question Generation (AQG) where given a passage and an answer the task is to generate the corresponding question.",,,"['In', 'this', 'work,', 'we', 'focus', 'on', 'the', 'task', 'of', 'Automatic', 'Question', 'Generation', '(AQG)', 'where', 'given', 'a', 'passage', 'and', 'an', 'answer', 'the', 'task', 'is', 'to', 'generate', 'the', 'corresponding', 'question.']",,,,we,0,15,17,15,17,focus,0,18,23,18,23,['we'],['focus']
153,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,1,It is desired that the generated question should be (i) grammatically correct (ii) answerable from the passage and (iii) specific to the given answer.,It is desired that the generated question should be (i) grammatically correct (ii) answerable from the passage and (iii) specific to the given answer.,,,"['It', 'is', 'desired', 'that', 'the', 'generated', 'question', 'should', 'be', '(i)', 'grammatically', 'correct', '(ii)', 'answerable', 'from', 'the', 'passage', 'and', '(iii)', 'specific', 'to', 'the', 'given', 'answer.']",,,,It,0,0,2,0,2,is desired,0,3,13,3,13,['It'],"['is', 'desired']"
154,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,2,An analysis of existing AQG models shows that they produce questions which do not adhere to one or more of {the above-mentioned qualities}.,An analysis of existing AQG models shows that they produce questions which do not adhere to one or more of {the above-mentioned qualities}.,,,"['An', 'analysis', 'of', 'existing', 'AQG', 'models', 'shows', 'that', 'they', 'produce', 'questions', 'which', 'do', 'not', 'adhere', 'to', 'one', 'or', 'more', 'of', '{the', 'above-mentioned', 'qualities}.']",,,,An analysis,0,0,11,0,11,shows,0,35,40,35,40,"['An', 'analysis']",['shows']
155,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,3,"In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement.","In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement.",,,"['In', 'particular,', 'the', 'generated', 'questions', 'look', 'like', 'an', 'incomplete', 'draft', 'of', 'the', 'desired', 'question', 'with', 'a', 'clear', 'scope', 'for', 'refinement.']",,,,the questions,0,16,39,16,39,look,0,40,44,40,44,"['the', 'questions']",['look']
156,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,4,"{To alleviate this shortcoming}, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it.","{To alleviate this shortcoming}, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it.",,,"['{To', 'alleviate', 'this', 'shortcoming},', 'we', 'propose', 'a', 'method', 'which', 'tries', 'to', 'mimic', 'the', 'human', 'process', 'of', 'generating', 'questions', 'by', 'first', 'creating', 'an', 'initial', 'draft', 'and', 'then', 'refining', 'it.']",,,,we,0,36,38,36,38,propose,0,39,46,39,46,['we'],"['alleviate', 'propose']"
157,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,5,"More specifically, we propose Refine Network (RefNet) which contains two decoders.","More specifically, we propose Refine Network (RefNet) which contains two decoders.",,,"['More', 'specifically,', 'we', 'propose', 'Refine', 'Network', '(RefNet)', 'which', 'contains', 'two', 'decoders.']",,,,we,0,20,22,20,22,propose,0,23,30,23,30,['we'],['propose']
158,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,6,The second decoder uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first decoder.,The second decoder uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first decoder.,,,"['The', 'second', 'decoder', 'uses', 'a', 'dual', 'attention', 'network', 'which', 'pays', 'attention', 'to', 'both', '(i)', 'the', 'original', 'passage', 'and', '(ii)', 'the', 'question', '(initial', 'draft)', 'generated', 'by', 'the', 'first', 'decoder.']",,,,The second decoder,0,0,18,0,18,uses,0,19,23,19,23,"['The', 'second', 'decoder']",['uses']
159,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,7,"In effect, it refines the question generated by the first decoder, thereby making it more correct and complete.","In effect, it refines the question generated by the first decoder, thereby making it more correct and complete.",,,"['In', 'effect,', 'it', 'refines', 'the', 'question', 'generated', 'by', 'the', 'first', 'decoder,', 'thereby', 'making', 'it', 'more', 'correct', 'and', 'complete.']",,,,it,0,12,14,12,14,refines,0,15,22,15,22,['it'],"['refines', 'making']"
160,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,8,"We evaluate RefNet on three datasets, \textit{viz.","We evaluate RefNet on three datasets, \textit{viz.",,,"['We', 'evaluate', 'RefNet', 'on', 'three', 'datasets,', '\\textit{viz.']",,,,We,0,0,2,0,2,evaluate,0,3,11,3,11,['We'],['evaluate']
161,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,9,"}, SQuAD, HOTPOT-QA, and DROP, and show that it outperforms existing state-of-the-art methods by 7-16\% on all of these datasets.","} , SQuAD , HOTPOT-QA , and",DROP,", and show that it outperforms existing state-of-the-art methods by 7-16\ % on all of these datasets .","['}', ',', 'SQuAD', ',', 'HOTPOT-QA', ',', 'and', 'DROP', ',', 'and', 'show', 'that', 'it', 'outperforms', 'existing', 'state-of-the-art', 'methods', 'by', '7-16\\', '%', 'on', 'all', 'of', 'these', 'datasets', '.']","(7, 8)","(27, 31)",0,SQuAD HOTPOT QA DROP,0,4,34,4,34,and and show,0,26,45,26,45,"['SQuAD', 'HOTPOT', 'QA', 'DROP']",['show']
162,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,10,"Lastly, we show that we can improve the quality of the second decoder on specific metrics, such as, fluency and answerability by explicitly rewarding revisions that improve on the corresponding metric during training.","Lastly, we show that we can improve the quality of the second decoder on specific metrics, such as, fluency and answerability by explicitly rewarding revisions that improve on the corresponding metric during training.",,,"['Lastly,', 'we', 'show', 'that', 'we', 'can', 'improve', 'the', 'quality', 'of', 'the', 'second', 'decoder', 'on', 'specific', 'metrics,', 'such', 'as,', 'fluency', 'and', 'answerability', 'by', 'explicitly', 'rewarding', 'revisions', 'that', 'improve', 'on', 'the', 'corresponding', 'metric', 'during', 'training.']",,,,we,0,9,11,9,11,show,0,12,16,12,16,['we'],['show']
163,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,11,The code has been made publicly available \footnote{this https URL},The code has been made publicly available \footnote{this https URL},,,"['The', 'code', 'has', 'been', 'made', 'publicly', 'available', '\\footnote{this', 'https', 'URL}']",,,,The code,0,0,8,0,8,has been made,0,9,22,9,22,"['The', 'code']","['has', 'been', 'made']"
164,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Title,0,Let's Ask Again: Refine Network for Automatic Question Generation.,Let's Ask Again: Refine Network for Automatic Question Generation.,,,"[""Let's"", 'Ask', 'Again:', 'Refine', 'Network', 'for', 'Automatic', 'Question', 'Generation.']",,,,,,,,,,Let Ask,0,0,10,0,10,[],"['Let', 'Ask']"
165,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,0,Data points such as compressive strengths of materials or patient lab results are often reported in scientific literature.,Data points such as compressive strengths of materials or patient lab results are often reported in scientific literature.,,,"['Data', 'points', 'such', 'as', 'compressive', 'strengths', 'of', 'materials', 'or', 'patient', 'lab', 'results', 'are', 'often', 'reported', 'in', 'scientific', 'literature.']",,,,Data points,0,0,11,0,11,are reported,0,78,96,78,96,"['Data', 'points']","['are', 'reported']"
166,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,1,"Though databases of such data exist, they are typically curated manually.","Though databases of such data exist, they are typically curated manually.",,,"['Though', 'databases', 'of', 'such', 'data', 'exist,', 'they', 'are', 'typically', 'curated', 'manually.']",,,,they,0,38,42,38,42,are curated,0,43,64,43,64,['they'],"['are', 'curated']"
167,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,2,"While automated extraction of measured quantities, such as 17.2 mV, is straightforward, it is much more challenging to capture information about such quantities, such as which property of which entity is being measured.","While automated extraction of measured quantities, such as 17.2 mV, is straightforward, it is much more challenging to capture information about such quantities, such as which property of which entity is being measured.",,,"['While', 'automated', 'extraction', 'of', 'measured', 'quantities,', 'such', 'as', '17.2', 'mV,', 'is', 'straightforward,', 'it', 'is', 'much', 'more', 'challenging', 'to', 'capture', 'information', 'about', 'such', 'quantities,', 'such', 'as', 'which', 'property', 'of', 'which', 'entity', 'is', 'being', 'measured.']",,,,it,0,91,93,91,93,is,0,94,96,94,96,['it'],"['is', 'capture']"
168,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,3,"Additionally, further context may be required identifying conditions, such as temperature or pressure, under which a measurement was performed.","Additionally, further context may be required identifying conditions, such as temperature or pressure, under which a measurement was performed.",,,"['Additionally,', 'further', 'context', 'may', 'be', 'required', 'identifying', 'conditions,', 'such', 'as', 'temperature', 'or', 'pressure,', 'under', 'which', 'a', 'measurement', 'was', 'performed.']",,,,further context,0,15,30,15,30,may be required,0,31,46,31,46,"['further', 'context']","['may', 'be', 'required', 'identifying']"
169,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,4,"This information on property, entity, and context is needed to place the measurements into a database, but little attention has been given to the problem.","This information on property, entity, and context is needed to place the measurements into a database, but little attention has been given to the problem.",,,"['This', 'information', 'on', 'property,', 'entity,', 'and', 'context', 'is', 'needed', 'to', 'place', 'the', 'measurements', 'into', 'a', 'database,', 'but', 'little', 'attention', 'has', 'been', 'given', 'to', 'the', 'problem.']",,,,,,,,,,but,0,106,109,106,109,"['This', 'information', 'little', 'attention']","['is', 'needed', 'place', 'has', 'been', 'given']"
170,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,5,We start to address this information extraction problem by applying machine comprehension and question answering techniques.,We start to address this information extraction problem by applying machine comprehension and question answering techniques.,,,"['We', 'start', 'to', 'address', 'this', 'information', 'extraction', 'problem', 'by', 'applying', 'machine', 'comprehension', 'and', 'question', 'answering', 'techniques.']",,,,We,0,0,2,0,2,start,0,3,8,3,8,['We'],"['start', 'address']"
171,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,6,"To our knowledge, our research is the first to apply neural language models to extracting such properties and entities.","To our knowledge, our research is the first to apply neural language models to extracting such properties and entities.",,,"['To', 'our', 'knowledge,', 'our', 'research', 'is', 'the', 'first', 'to', 'apply', 'neural', 'language', 'models', 'to', 'extracting', 'such', 'properties', 'and', 'entities.']",,,,research,0,23,31,23,31,is,0,32,34,32,34,['research'],['is']
172,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,7,We found that we can do so with a very small investment in training data generation.,We found that we can do so with a very small investment in training data generation.,,,"['We', 'found', 'that', 'we', 'can', 'do', 'so', 'with', 'a', 'very', 'small', 'investment', 'in', 'training', 'data', 'generation.']",,,,We,0,0,2,0,2,found,0,3,8,3,8,['We'],['found']
173,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,8,"We develop a multi-turn question answering solution, where initial questions provide answers that are plugged into templates for follow-on questions.","We develop a multi-turn question answering solution, where initial questions provide answers that are plugged into templates for follow-on questions.",,,"['We', 'develop', 'a', 'multi-turn', 'question', 'answering', 'solution,', 'where', 'initial', 'questions', 'provide', 'answers', 'that', 'are', 'plugged', 'into', 'templates', 'for', 'follow-on', 'questions.']",,,,We,0,0,2,0,2,develop,0,3,10,3,10,['We'],['develop']
174,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,9,"We use existing QA datasets, SQuAD and DROP, to do most of the training of our model and measure the effect of adding different amounts of the gold data.","We use existing QA datasets , SQuAD and",DROP,", to do most of the training of our model and measure the effect of adding different amounts of the gold data .","['We', 'use', 'existing', 'QA', 'datasets', ',', 'SQuAD', 'and', 'DROP', ',', 'to', 'do', 'most', 'of', 'the', 'training', 'of', 'our', 'model', 'and', 'measure', 'the', 'effect', 'of', 'adding', 'different', 'amounts', 'of', 'the', 'gold', 'data', '.']","(8, 9)","(39, 43)",0,We,0,0,2,0,2,use,0,3,6,3,6,['We'],"['use', 'do', 'measure']"
175,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,10,"Adding gold data representing only 0.1% of the SQuAD data, a SciBERT-based QA model’s exact match and partial overlap accuracies improve from .32 and .47, respectively, to .42 and .57.","Adding gold data representing only 0.1% of the SQuAD data, a SciBERT-based QA model’s exact match and partial overlap accuracies improve from .32 and .47, respectively, to .42 and .57.",,,"['Adding', 'gold', 'data', 'representing', 'only', '0.1%', 'of', 'the', 'SQuAD', 'data,', 'a', 'SciBERT-based', 'QA', 'model’s', 'exact', 'match', 'and', 'partial', 'overlap', 'accuracies', 'improve', 'from', '.32', 'and', '.47,', 'respectively,', 'to', '.42', 'and', '.57.']",,,,a SciBERT QA model ’s exact match partial overlap accuracies,0,61,133,61,133,improve,0,134,141,134,141,"['a', 'SciBERT', 'QA', 'model', 'exact', 'match', 'partial', 'overlap', 'accuracies']","['Adding', 'improve']"
176,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,11,We improve substantially over the state-art baseline (GROBID-quantities).,We improve substantially over the state-art baseline (GROBID-quantities).,,,"['We', 'improve', 'substantially', 'over', 'the', 'state-art', 'baseline', '(GROBID-quantities).']",,,,We,0,0,2,0,2,improve,0,3,10,3,10,['We'],['improve']
177,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Title,0,Automatic Construction of Measured Property Knowledge Bases through Multi-Turn Question Answering.,Automatic Construction of Measured Property Knowledge Bases through Multi-Turn Question Answering.,,,"['Automatic', 'Construction', 'of', 'Measured', 'Property', 'Knowledge', 'Bases', 'through', 'Multi-Turn', 'Question', 'Answering.']",,,,Automatic Construction,0,0,22,0,22,,,,,,,"['Automatic', 'Construction']",[]
178,https://www.semanticscholar.org/paper/NumNet%3A-Machine-Reading-Comprehension-with-Ran-Lin/730043364aed106241ef18ab3e3b5e316802a254,21,Abstract,0,"Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in human's reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems.","Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in human's reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems.",,,"['Numerical', 'reasoning,', 'such', 'as', 'addition,', 'subtraction,', 'sorting', 'and', 'counting', 'is', 'a', 'critical', 'skill', 'in', ""human's"", 'reading', 'comprehension,', 'which', 'has', 'not', 'been', 'well', 'considered', 'in', 'existing', 'machine', 'reading', 'comprehension', '(MRC)', 'systems.']",,,,Numerical reasoning,0,0,19,0,19,is,0,76,78,76,78,"['Numerical', 'reasoning']",['is']
179,https://www.semanticscholar.org/paper/NumNet%3A-Machine-Reading-Comprehension-with-Ran-Lin/730043364aed106241ef18ab3e3b5e316802a254,21,Abstract,1,"To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage.","To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage.",,,"['To', 'address', 'this', 'issue,', 'we', 'propose', 'a', 'numerical', 'MRC', 'model', 'named', 'as', 'NumNet,', 'which', 'utilizes', 'a', 'numerically-aware', 'graph', 'neural', 'network', 'to', 'consider', 'the', 'comparing', 'information', 'and', 'performs', 'numerical', 'reasoning', 'over', 'numbers', 'in', 'the', 'question', 'and', 'passage.']",,,,we,0,24,26,24,26,propose,0,27,34,27,34,['we'],"['address', 'propose']"
180,https://www.semanticscholar.org/paper/NumNet%3A-Machine-Reading-Comprehension-with-Ran-Lin/730043364aed106241ef18ab3e3b5e316802a254,21,Abstract,2,"Our system achieves an EM-score of 64.56% on the DROP dataset, outperforming all existing machine reading comprehension models by considering the numerical relations among numbers.",Our system achieves an EM-score of 64.56 % on the,DROP,"dataset , outperforming all existing machine reading comprehension models by considering the numerical relations among numbers .","['Our', 'system', 'achieves', 'an', 'EM-score', 'of', '64.56', '%', 'on', 'the', 'DROP', 'dataset', ',', 'outperforming', 'all', 'existing', 'machine', 'reading', 'comprehension', 'models', 'by', 'considering', 'the', 'numerical', 'relations', 'among', 'numbers', '.']","(10, 11)","(49, 53)",0,system,0,4,10,4,10,achieves outperforming,0,11,80,11,80,['system'],"['achieves', 'outperforming']"
181,https://www.semanticscholar.org/paper/NumNet%3A-Machine-Reading-Comprehension-with-Ran-Lin/730043364aed106241ef18ab3e3b5e316802a254,21,Title,0,NumNet: Machine Reading Comprehension with Numerical Reasoning.,NumNet: Machine Reading Comprehension with Numerical Reasoning.,,,"['NumNet:', 'Machine', 'Reading', 'Comprehension', 'with', 'Numerical', 'Reasoning.']",,,,NumNet Machine Reading Comprehension,0,0,38,0,38,,,,,,,"['NumNet', 'Machine', 'Reading', 'Comprehension']",[]
182,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,0,Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks.,Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks.,,,"['Recent', 'advances', 'in', 'NLP', 'demonstrate', 'the', 'effectiveness', 'of', 'training', 'large-scale', 'language', 'models', 'and', 'transferring', 'them', 'to', 'downstream', 'tasks.']",,,,Recent advances,0,0,15,0,15,demonstrate,0,23,34,23,34,"['Recent', 'advances']",['demonstrate']
183,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,1,Can fine-tuning these models on tasks other than language modeling further improve performance?,Can fine-tuning these models on tasks other than language modeling further improve performance?,,,"['Can', 'fine-tuning', 'these', 'models', 'on', 'tasks', 'other', 'than', 'language', 'modeling', 'further', 'improve', 'performance?']",,,,,,,,,,Can improve,0,0,84,0,84,[],"['Can', 'tuning', 'improve']"
184,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,2,"In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling).","In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling).",,,"['In', 'this', 'paper,', 'we', 'conduct', 'an', 'extensive', 'study', 'of', 'the', 'transferability', 'between', '33', 'NLP', 'tasks', 'across', 'three', 'broad', 'classes', 'of', 'problems', '(text', 'classification,', 'question', 'answering,', 'and', 'sequence', 'labeling).']",,,,we,0,16,18,16,18,conduct,0,19,26,19,26,['we'],['conduct']
185,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,3,"Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even when the source task is small or differs substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset).","Our results show that transfer learning is more beneficial than previously thought , especially when target task data is scarce , and can improve performance even when the source task is small or differs substantially from the target task ( e.g. , part-of-speech tagging transfers well to the",DROP,QA dataset ) .,"['Our', 'results', 'show', 'that', 'transfer', 'learning', 'is', 'more', 'beneficial', 'than', 'previously', 'thought', ',', 'especially', 'when', 'target', 'task', 'data', 'is', 'scarce', ',', 'and', 'can', 'improve', 'performance', 'even', 'when', 'the', 'source', 'task', 'is', 'small', 'or', 'differs', 'substantially', 'from', 'the', 'target', 'task', '(', 'e.g.', ',', 'part-of-speech', 'tagging', 'transfers', 'well', 'to', 'the', 'DROP', 'QA', 'dataset', ')', '.']","(48, 49)","(292, 296)",0,results,0,4,11,4,11,show,0,12,16,12,16,['results'],['show']
186,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,4,"We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size.","We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size.",,,"['We', 'also', 'develop', 'task', 'embeddings', 'that', 'can', 'be', 'used', 'to', 'predict', 'the', 'most', 'transferable', 'source', 'tasks', 'for', 'a', 'given', 'target', 'task,', 'and', 'we', 'validate', 'their', 'effectiveness', 'in', 'experiments', 'controlled', 'for', 'source', 'and', 'target', 'data', 'size.']",,,,,,,,,,and,0,121,124,121,124,"['We', 'we']","['develop', 'validate']"
187,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,5,"Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability.","Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability.",,,"['Overall,', 'our', 'experiments', 'reveal', 'that', 'factors', 'such', 'as', 'source', 'data', 'size,', 'task', 'and', 'domain', 'similarity,', 'and', 'task', 'complexity', 'all', 'play', 'a', 'role', 'in', 'determining', 'transferability.']",,,,experiments,0,14,25,14,25,reveal,0,26,32,26,32,['experiments'],['reveal']
188,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Title,0,Exploring and Predicting Transferability across NLP Tasks.,Exploring and Predicting Transferability across NLP Tasks.,,,"['Exploring', 'and', 'Predicting', 'Transferability', 'across', 'NLP', 'Tasks.']",,,,,,,,,,,,,,,,[],['Exploring']
189,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,0,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture.","Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture.",,,"['Neural', 'module', 'networks', '(NMNs)', 'are', 'a', 'popular', 'approach', 'for', 'modeling', 'compositionality:', 'they', 'achieve', 'high', 'accuracy', 'when', 'applied', 'to', 'problems', 'in', 'language', 'and', 'vision,', 'while', 'reflecting', 'the', 'compositional', 'structure', 'of', 'the', 'problem', 'in', 'the', 'network', 'architecture.']",,,,,,,,,,,,,,,,"['Neural', 'module', 'networks', 'NMNs', 'they']","['are', 'achieve']"
190,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,1,"However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour.","However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour.",,,"['However,', 'prior', 'work', 'implicitly', 'assumed', 'that', 'the', 'structure', 'of', 'the', 'network', 'modules,', 'describing', 'the', 'abstract', 'reasoning', 'process,', 'provides', 'a', 'faithful', 'explanation', 'of', 'the', ""model's"", 'reasoning;', 'that', 'is,', 'that', 'all', 'modules', 'perform', 'their', 'intended', 'behaviour.']",,,,prior,0,10,15,10,15,assumed,0,32,39,32,39,['prior'],['assumed']
191,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,2,"In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps.","In this work , we propose and conduct a systematic evaluation of","the intermediate outputs of NMNs on NLVR2 and DROP , two datasets which require composing multiple reasoning steps",.,"['In', 'this', 'work', ',', 'we', 'propose', 'and', 'conduct', 'a', 'systematic', 'evaluation', 'of', 'the', 'intermediate', 'outputs', 'of', 'NMNs', 'on', 'NLVR2', 'and', 'DROP', ',', 'two', 'datasets', 'which', 'require', 'composing', 'multiple', 'reasoning', 'steps', '.']","(12, 30)","(64, 178)",46,we,0,15,17,15,17,propose and conduct,0,18,37,18,37,['we'],"['propose', 'conduct']"
192,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,3,"We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour.",We find that,the intermediate outputs,"differ from the expected output , illustrating that the network structure does not provide a faithful explanation of model behaviour .","['We', 'find', 'that', 'the', 'intermediate', 'outputs', 'differ', 'from', 'the', 'expected', 'output', ',', 'illustrating', 'that', 'the', 'network', 'structure', 'does', 'not', 'provide', 'a', 'faithful', 'explanation', 'of', 'model', 'behaviour', '.']","(3, 6)","(12, 36)",-1,We,0,0,2,0,2,find,0,3,7,3,7,['We'],['find']
193,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,4,"To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.","To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",,,"['To', 'remedy', 'that,', 'we', 'train', 'the', 'model', 'with', 'auxiliary', 'supervision', 'and', 'propose', 'particular', 'choices', 'for', 'module', 'architecture', 'that', 'yield', 'much', 'better', 'faithfulness,', 'at', 'a', 'minimal', 'cost', 'to', 'accuracy.']",,,,we,0,17,19,17,19,train and propose,0,20,74,20,74,['we'],"['remedy', 'train', 'propose']"
194,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Title,0,Obtaining Faithful Interpretations from Compositional Neural Networks.,Obtaining Faithful Interpretations from Compositional Neural Networks.,,,"['Obtaining', 'Faithful', 'Interpretations', 'from', 'Compositional', 'Neural', 'Networks.']",,,,,,,,,,Obtaining,0,0,9,0,9,[],['Obtaining']
195,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,0,We present a method to represent input texts by contextualizing them jointly with dynamically retrieved textual encyclopedic background knowledge from multiple documents.,We present a method to represent input texts by contextualizing them jointly with dynamically retrieved textual encyclopedic background knowledge from multiple documents.,,,"['We', 'present', 'a', 'method', 'to', 'represent', 'input', 'texts', 'by', 'contextualizing', 'them', 'jointly', 'with', 'dynamically', 'retrieved', 'textual', 'encyclopedic', 'background', 'knowledge', 'from', 'multiple', 'documents.']",,,,We,0,0,2,0,2,present,0,3,10,3,10,['We'],['present']
196,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,1,We apply our method to reading comprehension tasks by encoding questions and passages together with background sentences about the entities they mention.,We apply our method to reading comprehension tasks by encoding questions and passages together with background sentences about the entities they mention.,,,"['We', 'apply', 'our', 'method', 'to', 'reading', 'comprehension', 'tasks', 'by', 'encoding', 'questions', 'and', 'passages', 'together', 'with', 'background', 'sentences', 'about', 'the', 'entities', 'they', 'mention.']",,,,We,0,0,2,0,2,apply,0,3,8,3,8,['We'],['apply']
197,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,2,We show that integrating background knowledge from text is effective for tasks focusing on factual reasoning and allows direct reuse of powerful pretrained BERT-style encoders.,We show that integrating background knowledge from text is effective for tasks focusing on factual reasoning and allows direct reuse of powerful pretrained BERT-style encoders.,,,"['We', 'show', 'that', 'integrating', 'background', 'knowledge', 'from', 'text', 'is', 'effective', 'for', 'tasks', 'focusing', 'on', 'factual', 'reasoning', 'and', 'allows', 'direct', 'reuse', 'of', 'powerful', 'pretrained', 'BERT-style', 'encoders.']",,,,We,0,0,2,0,2,show,0,3,7,3,7,['We'],['show']
198,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,3,"Moreover, knowledge integration can be further improved with suitable pretraining via a self-supervised masked language model objective over words in background-augmented input text.","Moreover, knowledge integration can be further improved with suitable pretraining via a self-supervised masked language model objective over words in background-augmented input text.",,,"['Moreover,', 'knowledge', 'integration', 'can', 'be', 'further', 'improved', 'with', 'suitable', 'pretraining', 'via', 'a', 'self-supervised', 'masked', 'language', 'model', 'objective', 'over', 'words', 'in', 'background-augmented', 'input', 'text.']",,,,knowledge integration,0,11,32,11,32,can be improved,0,33,56,33,56,"['knowledge', 'integration']","['can', 'be', 'improved']"
199,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,4,"On TriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable RoBERTa models which do not integrate background knowledge dynamically.","On TriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable RoBERTa models which do not integrate background knowledge dynamically.",,,"['On', 'TriviaQA,', 'our', 'approach', 'obtains', 'improvements', 'of', '1.6', 'to', '3.1', 'F1', 'over', 'comparable', 'RoBERTa', 'models', 'which', 'do', 'not', 'integrate', 'background', 'knowledge', 'dynamically.']",,,,approach,0,18,26,18,26,obtains,0,27,34,27,34,['approach'],['obtains']
200,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,5,"On MRQA, a large collection of diverse QA datasets, we see consistent gains in-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2 F1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1).","On MRQA, a large collection of diverse QA datasets, we see consistent gains in-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2 F1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1).",,,"['On', 'MRQA,', 'a', 'large', 'collection', 'of', 'diverse', 'QA', 'datasets,', 'we', 'see', 'consistent', 'gains', 'in-domain', 'along', 'with', 'large', 'improvements', 'out-of-domain', 'on', 'BioASQ', '(2.1', 'to', '4.2', 'F1),', 'TextbookQA', '(1.6', 'to', '2.0', 'F1),', 'and', 'DuoRC', '(1.1', 'to', '2.0', 'F1).']",,,,we,0,54,56,54,56,see,0,57,60,57,60,['we'],['see']
201,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Title,0,Contextualized Representations Using Textual Encyclopedic Knowledge.,Contextualized Representations Using Textual Encyclopedic Knowledge.,,,"['Contextualized', 'Representations', 'Using', 'Textual', 'Encyclopedic', 'Knowledge.']",,,,Representations,0,15,30,15,30,Using,0,31,36,31,36,['Representations'],['Using']
202,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,0,"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly.","Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly.",,,"['Models', 'for', 'reading', 'comprehension', '(RC)', 'commonly', 'restrict', 'their', 'output', 'space', 'to', 'the', 'set', 'of', 'all', 'single', 'contiguous', 'spans', 'from', 'the', 'input,', 'in', 'order', 'to', 'alleviate', 'the', 'learning', 'problem', 'and', 'avoid', 'the', 'need', 'for', 'a', 'model', 'that', 'generates', 'text', 'explicitly.']",,,,Models,0,0,6,0,6,restrict,0,49,57,49,57,['Models'],['restrict']
203,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,1,"However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text.","However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text.",,,"['However,', 'forcing', 'an', 'answer', 'to', 'be', 'a', 'single', 'span', 'can', 'be', 'restrictive,', 'and', 'some', 'recent', 'datasets', 'also', 'include', 'multi-span', 'questions,', 'i.e.,', 'questions', 'whose', 'answer', 'is', 'a', 'set', 'of', 'non-contiguous', 'spans', 'in', 'the', 'text.']",,,,,,,,,,and,0,69,72,69,72,"['some', 'recent', 'datasets']","['forcing', 'be', 'can', 'be', 'include']"
204,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,2,"Naturally, models that return single spans cannot answer these questions.","Naturally, models that return single spans cannot answer these questions.",,,"['Naturally,', 'models', 'that', 'return', 'single', 'spans', 'cannot', 'answer', 'these', 'questions.']",,,,models,0,12,18,12,18,can answer,0,44,58,44,58,['models'],"['can', 'answer']"
205,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,3,"In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not.","In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not.",,,"['In', 'this', 'work,', 'we', 'propose', 'a', 'simple', 'architecture', 'for', 'answering', 'multi-span', 'questions', 'by', 'casting', 'the', 'task', 'as', 'a', 'sequence', 'tagging', 'problem,', 'namely,', 'predicting', 'for', 'each', 'input', 'token', 'whether', 'it', 'should', 'be', 'part', 'of', 'the', 'output', 'or', 'not.']",,,,we,0,15,17,15,17,propose,0,18,25,18,25,['we'],['propose']
206,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,4,Our model substantially improves performance on span extraction questions from DROP and Quoref by 9.9 and 5.5 EM points respectively.,Our model substantially improves performance on span extraction questions from,DROP,and Quoref by 9.9 and 5.5 EM points respectively .,"['Our', 'model', 'substantially', 'improves', 'performance', 'on', 'span', 'extraction', 'questions', 'from', 'DROP', 'and', 'Quoref', 'by', '9.9', 'and', '5.5', 'EM', 'points', 'respectively', '.']","(10, 11)","(78, 82)",0,model,0,4,9,4,9,improves,0,24,32,24,32,['model'],['improves']
207,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Title,0,A Simple and Effective Model for Answering Multi-span Questions.,A Simple and Effective Model for Answering Multi-span Questions.,,,"['A', 'Simple', 'and', 'Effective', 'Model', 'for', 'Answering', 'Multi-span', 'Questions.']",,,,A Model,0,0,28,0,28,,,,,,,"['A', 'Model']",[]
208,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,0,"Complex, compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer.","Complex, compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer.",,,"['Complex,', 'compositional', 'reading', 'comprehension', 'datasets', 'require', 'performing', 'latent', 'sequential', 'decisions', 'that', 'are', 'learned', 'via', 'supervision', 'from', 'the', 'final', 'answer.']",,,,Complex compositional reading comprehension datasets,0,0,54,0,54,require,0,55,62,55,62,"['Complex', 'compositional', 'reading', 'comprehension', 'datasets']","['require', 'performing']"
209,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,1,"A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task.","A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task.",,,"['A', 'large', 'combinatorial', 'space', 'of', 'possible', 'decision', 'paths', 'that', 'result', 'in', 'the', 'same', 'answer,', 'compounded', 'by', 'the', 'lack', 'of', 'intermediate', 'supervision', 'to', 'help', 'choose', 'the', 'right', 'path,', 'makes', 'the', 'learning', 'particularly', 'hard', 'for', 'this', 'task.']",,,,A large combinatorial space,0,0,27,0,27,makes,0,171,176,171,176,"['A', 'large', 'combinatorial', 'space']",['makes']
210,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,2,"In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection.","In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection.",,,"['In', 'this', 'work,', 'we', 'study', 'the', 'benefits', 'of', 'collecting', 'intermediate', 'reasoning', 'supervision', 'along', 'with', 'the', 'answer', 'during', 'data', 'collection.']",,,,we,0,15,17,15,17,study,0,18,23,18,23,['we'],['study']
211,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,3,We find that these intermediate annotations can provide two-fold benefits.,We find that these intermediate annotations can provide two-fold benefits.,,,"['We', 'find', 'that', 'these', 'intermediate', 'annotations', 'can', 'provide', 'two-fold', 'benefits.']",,,,We,0,0,2,0,2,find,0,3,7,3,7,['We'],['find']
212,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,4,"First, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: DROP and Quoref.","First , we observe that for any collection budget , spending a fraction of it on intermediate annotations results in improved model performance , for two complex compositional datasets :",DROP,and Quoref .,"['First', ',', 'we', 'observe', 'that', 'for', 'any', 'collection', 'budget', ',', 'spending', 'a', 'fraction', 'of', 'it', 'on', 'intermediate', 'annotations', 'results', 'in', 'improved', 'model', 'performance', ',', 'for', 'two', 'complex', 'compositional', 'datasets', ':', 'DROP', 'and', 'Quoref', '.']","(30, 31)","(186, 190)",0,we,0,8,10,8,10,observe,0,11,18,11,18,['we'],['observe']
213,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,5,"Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.","Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.",,,"['Second,', 'these', 'annotations', 'encourage', 'the', 'model', 'to', 'learn', 'the', 'correct', 'latent', 'reasoning', 'steps,', 'helping', 'combat', 'some', 'of', 'the', 'biases', 'introduced', 'during', 'the', 'data', 'collection', 'process.']",,,,these annotations,0,9,26,9,26,encourage,0,27,36,27,36,"['these', 'annotations']","['encourage', 'learn', 'helping']"
214,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Title,0,Benefits of Intermediate Annotations in Reading Comprehension.,Benefits of Intermediate Annotations in Reading Comprehension.,,,"['Benefits', 'of', 'Intermediate', 'Annotations', 'in', 'Reading', 'Comprehension.']",,,,Benefits,0,0,8,0,8,,,,,,,['Benefits'],[]
215,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,0,Complex question answering over knowledge base (Complex KBQA) is challenging because it requires the compositional reasoning capability.,Complex question answering over knowledge base (Complex KBQA) is challenging because it requires the compositional reasoning capability.,,,"['Complex', 'question', 'answering', 'over', 'knowledge', 'base', '(Complex', 'KBQA)', 'is', 'challenging', 'because', 'it', 'requires', 'the', 'compositional', 'reasoning', 'capability.']",,,,Complex question,0,0,16,0,16,is,0,64,66,64,66,"['Complex', 'question']",['is']
216,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,1,"Existing benchmarks have three shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are either generated by templates, leading to poor diversity, or on a small scale; and 3) they mostly only consider the relations among entities but not attributes.","Existing benchmarks have three shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are either generated by templates, leading to poor diversity, or on a small scale; and 3) they mostly only consider the relations among entities but not attributes.",,,"['Existing', 'benchmarks', 'have', 'three', 'shortcomings', 'that', 'limit', 'the', 'development', 'of', 'Complex', 'KBQA:', '1)', 'they', 'only', 'provide', 'QA', 'pairs', 'without', 'explicit', 'reasoning', 'processes;', '2)', 'questions', 'are', 'either', 'generated', 'by', 'templates,', 'leading', 'to', 'poor', 'diversity,', 'or', 'on', 'a', 'small', 'scale;', 'and', '3)', 'they', 'mostly', 'only', 'consider', 'the', 'relations', 'among', 'entities', 'but', 'not', 'attributes.']",,,,,,,,,,,,,,,,"['benchmarks', 'they', 'questions', 'they']","['have', 'provide', 'are', 'generated', 'leading', 'consider']"
217,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,2,"To this end, we introduce KQA Pro, a large-scale dataset for Complex KBQA.","To this end, we introduce KQA Pro, a large-scale dataset for Complex KBQA.",,,"['To', 'this', 'end,', 'we', 'introduce', 'KQA', 'Pro,', 'a', 'large-scale', 'dataset', 'for', 'Complex', 'KBQA.']",,,,we,0,14,16,14,16,introduce,0,17,26,17,26,['we'],['introduce']
218,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,3,"We generate questions, SPARQLs, and functional programs with recursive templates and then paraphrase the questions by crowdsourcing, giving rise to around 120K diverse instances.","We generate questions, SPARQLs, and functional programs with recursive templates and then paraphrase the questions by crowdsourcing, giving rise to around 120K diverse instances.",,,"['We', 'generate', 'questions,', 'SPARQLs,', 'and', 'functional', 'programs', 'with', 'recursive', 'templates', 'and', 'then', 'paraphrase', 'the', 'questions', 'by', 'crowdsourcing,', 'giving', 'rise', 'to', 'around', '120K', 'diverse', 'instances.']",,,,We,0,0,2,0,2,generate and paraphrase,0,3,102,3,102,['We'],"['generate', 'paraphrase', 'crowdsourcing', 'giving']"
219,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,4,"The SPARQLs and programs depict the reasoning processes in various manners, which can benefit a large spectrum of QA methods.","The SPARQLs and programs depict the reasoning processes in various manners, which can benefit a large spectrum of QA methods.",,,"['The', 'SPARQLs', 'and', 'programs', 'depict', 'the', 'reasoning', 'processes', 'in', 'various', 'manners,', 'which', 'can', 'benefit', 'a', 'large', 'spectrum', 'of', 'QA', 'methods.']",,,,The SPARQLs programs,0,0,24,0,24,depict,0,25,31,25,31,"['The', 'SPARQLs', 'programs']",['depict']
220,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,5,"We contribute a unified codebase and conduct extensive evaluations for baselines and state-of-the-arts: a blind GRU obtains 31.58%, the best model achieves only 35.15%, and humans top at 97.5%, which offers great research potential to fill the gap.","We contribute a unified codebase and conduct extensive evaluations for baselines and state-of-the-arts: a blind GRU obtains 31.58%, the best model achieves only 35.15%, and humans top at 97.5%, which offers great research potential to fill the gap.",,,"['We', 'contribute', 'a', 'unified', 'codebase', 'and', 'conduct', 'extensive', 'evaluations', 'for', 'baselines', 'and', 'state-of-the-arts:', 'a', 'blind', 'GRU', 'obtains', '31.58%,', 'the', 'best', 'model', 'achieves', 'only', '35.15%,', 'and', 'humans', 'top', 'at', '97.5%,', 'which', 'offers', 'great', 'research', 'potential', 'to', 'fill', 'the', 'gap.']",,,,We,0,0,2,0,2,contribute and conduct and,0,3,183,3,183,"['We', 'a', 'blind', 'GRU', 'the', 'best', 'model', 'humans']","['contribute', 'conduct', 'obtains', 'achieves', 'top']"
221,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Title,0,KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base.,KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base.,,,"['KQA', 'Pro:', 'A', 'Large', 'Diagnostic', 'Dataset', 'for', 'Complex', 'Question', 'Answering', 'over', 'Knowledge', 'Base.']",,,,KQA Pro A Large Diagnostic,0,0,28,0,28,,,,,,,"['KQA', 'Pro', 'A', 'Large', 'Diagnostic']",[]
