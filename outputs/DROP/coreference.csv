,split_0,split_1,split_2,c_senttype,c_sentparts,c_subj_full,c_subj_split,c_subj_span0,c_subj_span1,c_subj_cspan0,c_subj_cspan1,c_subj_allspans,c_verb_full,c_verb_split,c_verb_span0,c_verb_span1,c_verb_cspan0,c_verb_cspan1,c_verb_allspans,c_subj_list,c_verb_list,d_averb,d_averb_s,d_averb_o,d_averb_relation,d_averb_split,d_averb_span0,d_averb_span1,d_averb_cspan0,d_averb_cspan1,d_root,d_root_full,d_root_s,d_root_o,d_root_split,d_root_span0,d_root_span1,d_root_cspan0,d_root_cspan1,fverb,fword,d_apos,d_apos_w,URL,ID,Type,Index,Text,split_tokens,split_anchor_span,split_anchor_indices,within_anchor_index
0,"Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task.",,,S,"['NP', 'VP', '.']",comprehension,0.0,8.0,21.0,8.0,21.0,"[(8, 21)]",has seen,0.0,22.0,39.0,22.0,39.0,"[(22, 25), (35, 39)]",['comprehension'],"['has', 'seen']",,,,,,,,,,recently,has recently,['Reading comprehension'],"['seen rapid progress , with systems matching humans on the most popular datasets for the task']",0,22,35,22,35,,,,,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,0,"Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task.","['Reading', 'comprehension', 'has', 'recently', 'seen', 'rapid', 'progress,', 'with', 'systems', 'matching', 'humans', 'on', 'the', 'most', 'popular', 'datasets', 'for', 'the', 'task.']",,,
1,"However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",a large body,0.0,10.0,22.0,10.0,22.0,"[(10, 11), (12, 17), (18, 22)]",has highlighted,0.0,31.0,46.0,31.0,46.0,"[(31, 34), (35, 46)]","['a', 'large', 'body']","['has', 'highlighted', 'showing']",,,,,,,,,,highlighted,has highlighted,"['However', ',', 'a large body of work']",['the brittleness of these systems'],0,31,47,31,47,,,,,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,1,"However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done.","['However,', 'a', 'large', 'body', 'of', 'work', 'has', 'highlighted', 'the', 'brittleness', 'of', 'these', 'systems,', 'showing', 'that', 'there', 'is', 'much', 'work', 'left', 'to', 'be', 'done.']",,,
2,We introduce,"a new English reading comprehension benchmark , DROP , which requires Discrete Reasoning Over the content of Paragraphs",.,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",introduce,0.0,3.0,12.0,3.0,12.0,"[(3, 12)]",['We'],['introduce'],requires,"['which', 'We', 'introduce a new English reading comprehension benchmark , DROP']",['Discrete Reasoning Over the content of Paragraphs'],0.0,1.0,74.0,83.0,61.0,70.0,requires,requires,"['which', 'We', 'introduce a new English reading comprehension benchmark , DROP']",['Discrete Reasoning Over the content of Paragraphs'],1,74,83,61,70,,,['VERB'],['requires'],https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,2,"We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs.","['We', 'introduce', 'a', 'new', 'English', 'reading', 'comprehension', 'benchmark', ',', 'DROP', ',', 'which', 'requires', 'Discrete', 'Reasoning', 'Over', 'the', 'content', 'of', 'Paragraphs', '.']","(2, 20)","(12, 131)",48.0
3,In,"this crowdsourced , adversarially - created , 96k - question benchmark",", a system must resolve references in a question , perhaps to multiple input positions , and perform discrete operations over them ( such as addition , counting , or sorting ) .",S,"['PP', ',', 'NP', 'VP', '.']",a system,2.0,76.0,84.0,2.0,10.0,"[(76, 77), (78, 84)]",must resolve and perform,2.0,85.0,174.0,11.0,100.0,"[(85, 89), (90, 97), (163, 166), (167, 174)]","['a', 'system']","['must', 'resolve', 'perform']",must resolve,"[', a system']",[],1.0,2.0,85.0,98.0,11.0,24.0,references,references,"['In this crowdsourced , adversarially - created , 96k - question benchmark , a system must resolve']",[],2,98,109,24,35,resolve,a,"['ADJ', 'ADP', 'VERB']","['crowdsourced', 'In', 'resolve']",https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,3,"In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting).","['In', 'this', 'crowdsourced', ',', 'adversarially', '-', 'created', ',', '96k', '-', 'question', 'benchmark', ',', 'a', 'system', 'must', 'resolve', 'references', 'in', 'a', 'question', ',', 'perhaps', 'to', 'multiple', 'input', 'positions', ',', 'and', 'perform', 'discrete', 'operations', 'over', 'them', '(', 'such', 'as', 'addition', ',', 'counting', ',', 'or', 'sorting', ')', '.']","(1, 12)","(2, 72)",-1.0
4,These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.,,,S,"['NP', 'VP', '.']",These operations,0.0,0.0,16.0,0.0,16.0,"[(0, 5), (6, 16)]",require,0.0,17.0,24.0,17.0,24.0,"[(17, 24)]","['These', 'operations']",['require'],,,,,,,,,,require,require,['These operations'],['a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets .'],0,17,25,17,25,,,,,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,4,These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.,"['These', 'operations', 'require', 'a', 'much', 'more', 'comprehensive', 'understanding', 'of', 'the', 'content', 'of', 'paragraphs', 'than', 'what', 'was', 'necessary', 'for', 'prior', 'datasets.']",,,
5,We apply state - of - the - art methods from both the reading comprehension and semantic parsing literature on,this dataset,"and show that the best systems only achieve 32.7 % F1 on our generalized accuracy metric , while expert human performance is 96.0 % .",S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",apply and show,0.0,3.0,132.0,3.0,132.0,"[(3, 8), (124, 127), (128, 132)]",['We'],"['apply', 'show']",,,,,,,,,,We,We,[],"['apply state - of - the - art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7 % F1 on our generalized accuracy metric , while expert human performance is 96.0 % .']",0,0,3,0,3,achieve,and,"['NOUN', 'ADP', 'NOUN']","['dataset', 'on', 'comprehension']",https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,5,"We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.0%.","['We', 'apply', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods', 'from', 'both', 'the', 'reading', 'comprehension', 'and', 'semantic', 'parsing', 'literature', 'on', 'this', 'dataset', 'and', 'show', 'that', 'the', 'best', 'systems', 'only', 'achieve', '32.7', '%', 'F1', 'on', 'our', 'generalized', 'accuracy', 'metric', ',', 'while', 'expert', 'human', 'performance', 'is', '96.0', '%', '.']","(20, 22)","(110, 122)",-1.0
6,We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.,,,S,"['NP', 'ADVP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",present,0.0,16.0,23.0,16.0,23.0,"[(16, 23)]",['We'],['present'],,,,,,,,,,We,We,[],['additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0 % F1 .'],0,0,3,0,3,,,,,https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Abstract,6,We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.,"['We', 'additionally', 'present', 'a', 'new', 'model', 'that', 'combines', 'reading', 'comprehension', 'methods', 'with', 'simple', 'numerical', 'reasoning', 'to', 'achieve', '47.0%', 'F1.']",,,
7,,DROP,: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs .,NP,"['NP', ':', 'NP', '.']",DROP A Reading Comprehension Benchmark,1.0,0.0,40.0,0.0,40.0,"[(0, 4), (7, 8), (9, 16), (17, 30), (31, 40)]",,,,,,,[],"['DROP', 'A', 'Reading', 'Comprehension', 'Benchmark']",[],,,,,,,,,,DROP,DROP,[],[': A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs .'],1,0,5,0,5,,A,['PROPN'],['DROP'],https://www.semanticscholar.org/paper/DROP%3A-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang/dda6fb309f62e2557a071522354d8c2c897a2805#citing-papers,0,Title,0,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs.,"['DROP', ':', 'A', 'Reading', 'Comprehension', 'Benchmark', 'Requiring', 'Discrete', 'Reasoning', 'Over', 'Paragraphs', '.']","(0, 1)","(0, 4)",0.0
8,"Machine reading comprehension, the task of evaluating a machine’s ability to comprehend a passage of text, has seen a surge in popularity in recent years.",,,S,"['NP', 'VP', '.']",Machine reading comprehension the task,0.0,0.0,40.0,0.0,40.0,"[(0, 7), (8, 15), (16, 29), (32, 35), (36, 40)]",has seen,0.0,110.0,118.0,110.0,118.0,"[(110, 113), (114, 118)]","['Machine', 'reading', 'comprehension', 'the', 'task']","['has', 'seen']",,,,,,,,,,seen,seen,"['the task of evaluating a machine ’s ability to comprehend a passage of text ,', 'has']","['a surge', 'in popularity', '.']",0,114,119,114,119,,,,,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,0,"Machine reading comprehension, the task of evaluating a machine’s ability to comprehend a passage of text, has seen a surge in popularity in recent years.","['Machine', 'reading', 'comprehension,', 'the', 'task', 'of', 'evaluating', 'a', 'machine’s', 'ability', 'to', 'comprehend', 'a', 'passage', 'of', 'text,', 'has', 'seen', 'a', 'surge', 'in', 'popularity', 'in', 'recent', 'years.']",,,
9,"There are many datasets that are targeted at reading comprehension, and many systems that perform as well as humans on some of these datasets.",,,S,"['NP', 'VP', '.']",,,,,,,[],are,0.0,6.0,9.0,6.0,9.0,"[(6, 9)]",[],['are'],,,,,,,,,,are,are,['many datasets'],"['that are and many systems that perform as well as humans on some of these datasets', 'targeted at reading comprehension ,', '.']",0,6,10,6,10,,,,,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,1,"There are many datasets that are targeted at reading comprehension, and many systems that perform as well as humans on some of these datasets.","['There', 'are', 'many', 'datasets', 'that', 'are', 'targeted', 'at', 'reading', 'comprehension,', 'and', 'many', 'systems', 'that', 'perform', 'as', 'well', 'as', 'humans', 'on', 'some', 'of', 'these', 'datasets.']",,,
10,"Despite all of this interest, there is no work that systematically defines what reading comprehension is.",,,S,"['PP', ',', 'NP', 'VP', '.']",,,,,,,[],is,0.0,37.0,39.0,37.0,39.0,"[(37, 39)]",[],['is'],,,,,,,,,,is,is,"['no work that systematically defines what reading comprehension is', 'Despite all of this interest']",['.'],0,37,40,37,40,,,,,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,2,"Despite all of this interest, there is no work that systematically defines what reading comprehension is.","['Despite', 'all', 'of', 'this', 'interest,', 'there', 'is', 'no', 'work', 'that', 'systematically', 'defines', 'what', 'reading', 'comprehension', 'is.']",,,
11,"In this work, we justify a question answering approach to reading comprehension and describe the various kinds of questions one might use to more fully test a system’s comprehension of a passage, moving beyond questions that only probe local predicate-argument structures.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,15.0,17.0,15.0,17.0,"[(15, 17)]",justify and describe,0.0,18.0,93.0,18.0,93.0,"[(18, 25), (81, 84), (85, 93)]",['we'],"['justify', 'describe']",,,,,,,,,,justify,justify,"[',', 'we']","['a question answering approach to reading comprehension and describe the various kinds of questions one might use to more fully test a system ’s comprehension of a passage , moving beyond questions that only probe local predicate - argument structures']",0,18,26,18,26,,,,,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,3,"In this work, we justify a question answering approach to reading comprehension and describe the various kinds of questions one might use to more fully test a system’s comprehension of a passage, moving beyond questions that only probe local predicate-argument structures.","['In', 'this', 'work,', 'we', 'justify', 'a', 'question', 'answering', 'approach', 'to', 'reading', 'comprehension', 'and', 'describe', 'the', 'various', 'kinds', 'of', 'questions', 'one', 'might', 'use', 'to', 'more', 'fully', 'test', 'a', 'system’s', 'comprehension', 'of', 'a', 'passage,', 'moving', 'beyond', 'questions', 'that', 'only', 'probe', 'local', 'predicate-argument', 'structures.']",,,
12,The main pitfall of this approach is that questions can easily have surface cues or other biases that allow a model to shortcut the intended reasoning process.,,,S,"['NP', 'VP', '.']",The main pitfall,0.0,0.0,16.0,0.0,16.0,"[(0, 3), (4, 8), (9, 16)]",is,0.0,34.0,36.0,34.0,36.0,"[(34, 36)]","['The', 'main', 'pitfall']",['is'],,,,,,,,,,is,is,['The main pitfall of this approach'],['that questions can easily have surface cues or other biases that allow a model to shortcut the intended reasoning process'],0,34,37,34,37,,,,,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,4,The main pitfall of this approach is that questions can easily have surface cues or other biases that allow a model to shortcut the intended reasoning process.,"['The', 'main', 'pitfall', 'of', 'this', 'approach', 'is', 'that', 'questions', 'can', 'easily', 'have', 'surface', 'cues', 'or', 'other', 'biases', 'that', 'allow', 'a', 'model', 'to', 'shortcut', 'the', 'intended', 'reasoning', 'process.']",,,
13,"We discuss ways proposed in current literature to mitigate these shortcuts, and we conclude with recommendations for future dataset collection efforts.",,,S,"['S', ',', 'CC', 'S', '.']",,,,,,,[],and,0.0,77.0,80.0,77.0,80.0,"[(77, 80)]","['We', 'we']","['discuss', 'conclude']",,,,,,,,,,discuss,discuss,['We'],"['ways proposed in current literature to mitigate these shortcuts ,', 'and we conclude with recommendations for future dataset collection efforts']",0,3,11,3,11,,,,,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Abstract,5,"We discuss ways proposed in current literature to mitigate these shortcuts, and we conclude with recommendations for future dataset collection efforts.","['We', 'discuss', 'ways', 'proposed', 'in', 'current', 'literature', 'to', 'mitigate', 'these', 'shortcuts,', 'and', 'we', 'conclude', 'with', 'recommendations', 'for', 'future', 'dataset', 'collection', 'efforts.']",,,
14,On Making Reading Comprehension More Comprehensive.,,,PP,"['PP', '.']",,,,,,,[],,,,,,,[],[],[],,,,,,,,,,Reading,Reading,[],['Comprehension More Comprehensive .'],0,10,18,10,18,,,,,https://www.semanticscholar.org/paper/On-Making-Reading-Comprehension-More-Comprehensive-Gardner-Berant/c2c165dd615d4fc31d4fef4b4acbcab1a1655983,1,Title,0,On Making Reading Comprehension More Comprehensive.,"['On', 'Making', 'Reading', 'Comprehension', 'More', 'Comprehensive.']",,,
15,Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension.,,,S,"['NP', 'VP', '.']",Recent powerful pre - trained language models,0.0,0.0,45.0,0.0,45.0,"[(0, 6), (7, 15), (16, 19), (20, 21), (22, 29), (30, 38), (39, 45)]",have achieved,0.0,46.0,59.0,46.0,59.0,"[(46, 50), (51, 59)]","['Recent', 'powerful', 'pre', '-', 'trained', 'language', 'models']","['have', 'achieved']",,,,,,,,,,achieved,have achieved,['Recent powerful pre - trained language models'],['remarkable performance on most of the popular datasets for reading comprehension'],0,46,60,46,60,,,,,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,0,Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension.,"['Recent', 'powerful', 'pre-trained', 'language', 'models', 'have', 'achieved', 'remarkable', 'performance', 'on', 'most', 'of', 'the', 'popular', 'datasets', 'for', 'reading', 'comprehension.']",,,
16,It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text.,,,S,"['NP', 'VP', '.']",It,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",is,0.0,3.0,5.0,3.0,5.0,"[(3, 5)]",['It'],['is'],,,,,,,,,,time,is time,['It'],['to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text'],0,3,11,3,11,,,,,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,1,It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text.,"['It', 'is', 'time', 'to', 'introduce', 'more', 'challenging', 'datasets', 'to', 'push', 'the', 'development', 'of', 'this', 'field', 'towards', 'more', 'comprehensive', 'reasoning', 'of', 'text.']",,,
17,"In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,16.0,18.0,16.0,18.0,"[(16, 18)]",introduce,0.0,19.0,28.0,19.0,28.0,"[(19, 28)]",['we'],['introduce'],,,,,,,,,,introduce,introduce,"[',', 'we']","['a new Reading Comprehension dataset', 'requiring', 'logical reasoning ( ReClor ) extracted from standardized graduate admission examinations .']",0,19,29,19,29,,,,,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,2,"In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations.","['In', 'this', 'paper,', 'we', 'introduce', 'a', 'new', 'Reading', 'Comprehension', 'dataset', 'requiring', 'logical', 'reasoning', '(ReClor)', 'extracted', 'from', 'standardized', 'graduate', 'admission', 'examinations.']",,,
18,"As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text.",,,S,"['SBAR', ',', 'NP', 'ADVP', 'VP', '.']",human datasets,0.0,29.0,55.0,29.0,55.0,"[(29, 34), (47, 55)]",contain,0.0,64.0,71.0,64.0,71.0,"[(64, 71)]","['human', 'datasets']",['contain'],,,,,,,,,,studies,studies,[],['human - annotated datasets'],0,11,19,11,19,,,,,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,3,"As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text.","['As', 'earlier', 'studies', 'suggest,', 'human-annotated', 'datasets', 'usually', 'contain', 'biases,', 'which', 'are', 'often', 'exploited', 'by', 'models', 'to', 'achieve', 'high', 'accuracy', 'without', 'truly', 'understanding', 'the', 'text.']",,,
19,"In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points and separate them into EASY set while the rest as HARD set.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,89.0,91.0,89.0,91.0,"[(89, 91)]",propose,0.0,92.0,99.0,92.0,99.0,"[(92, 99)]",['we'],"['propose', 'identify', 'separate']",,,,,,,,,,propose,propose,"[',', 'we']",[],0,92,100,92,100,,,,,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,4,"In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points and separate them into EASY set while the rest as HARD set.","['In', 'order', 'to', 'comprehensively', 'evaluate', 'the', 'logical', 'reasoning', 'ability', 'of', 'models', 'on', 'ReClor,', 'we', 'propose', 'to', 'identify', 'biased', 'data', 'points', 'and', 'separate', 'them', 'into', 'EASY', 'set', 'while', 'the', 'rest', 'as', 'HARD', 'set.']",,,
20,Empirical results show that the state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set.,,,S,"['NP', 'VP', '.']",Empirical results,0.0,0.0,17.0,0.0,17.0,"[(0, 9), (10, 17)]",show,0.0,18.0,22.0,18.0,22.0,"[(18, 22)]","['Empirical', 'results']",['show'],,,,,,,,,,results,results,['Empirical'],['show that the state - of - the - art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set'],0,10,18,10,18,,,,,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,5,Empirical results show that the state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set.,"['Empirical', 'results', 'show', 'that', 'the', 'state-of-the-art', 'models', 'have', 'an', 'outstanding', 'ability', 'to', 'capture', 'biases', 'contained', 'in', 'the', 'dataset', 'with', 'high', 'accuracy', 'on', 'EASY', 'set.']",,,
21,"However, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",they,0.0,10.0,14.0,10.0,14.0,"[(10, 14)]",struggle,0.0,15.0,23.0,15.0,23.0,"[(15, 23)]",['they'],"['struggle', 'indicating']",,,,,,,,,,struggle,struggle,"['However', ',', 'they']",[],0,15,24,15,24,,,,,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Abstract,6,"However, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models.","['However,', 'they', 'struggle', 'on', 'HARD', 'set', 'with', 'poor', 'performance', 'near', 'that', 'of', 'random', 'guess,', 'indicating', 'more', 'research', 'is', 'needed', 'to', 'essentially', 'enhance', 'the', 'logical', 'reasoning', 'ability', 'of', 'current', 'models.']",,,
22,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning.,,,NP,"['NP', ':', 'NP', '.']",ReClor A Reading Comprehension Dataset,0.0,0.0,40.0,0.0,40.0,"[(0, 6), (9, 10), (11, 18), (19, 32), (33, 40)]",,,,,,,[],"['ReClor', 'A', 'Reading', 'Comprehension', 'Dataset']",[],,,,,,,,,,ReClor,ReClor,[],"[':', 'A Reading Comprehension Dataset Requiring Logical Reasoning .']",0,0,7,0,7,,,,,https://www.semanticscholar.org/paper/ReClor%3A-A-Reading-Comprehension-Dataset-Requiring-Yu-Jiang/02ab4f8c51bfce8e723f98ad92410c67e7811a4b,2,Title,0,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning.,"['ReClor:', 'A', 'Reading', 'Comprehension', 'Dataset', 'Requiring', 'Logical', 'Reasoning.']",,,
23,"With models reaching human performance on many popular reading comprehension datasets in recent years ,","a new dataset , DROP ,",introduced questions that were expected to present a harder challenge for reading comprehension models .,S,"['PP', ',', 'NP', 'VP', '.']",a new dataset DROP,1.0,104.0,124.0,0.0,20.0,"[(104, 105), (106, 109), (110, 117), (120, 124)]",introduced,2.0,127.0,137.0,0.0,10.0,"[(127, 137)]","['a', 'new', 'dataset', 'DROP']",['introduced'],introduced,"['a', 'new dataset , DROP ,']",[],0.0,2.0,127.0,138.0,0.0,11.0,present,present,"['With models reaching human performance on many popular reading comprehension datasets in recent years , a new dataset , DROP , introduced questions that were expected', 'to']",['a harder challenge for reading comprehension models .'],2,170,178,43,51,introduced,introduced,"['VERB', 'NOUN', 'AUX']","['introduced', 'questions', 'were']",https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,0,"With models reaching human performance on many popular reading comprehension datasets in recent years, a new dataset, DROP, introduced questions that were expected to present a harder challenge for reading comprehension models.","['With', 'models', 'reaching', 'human', 'performance', 'on', 'many', 'popular', 'reading', 'comprehension', 'datasets', 'in', 'recent', 'years', ',', 'a', 'new', 'dataset', ',', 'DROP', ',', 'introduced', 'questions', 'that', 'were', 'expected', 'to', 'present', 'a', 'harder', 'challenge', 'for', 'reading', 'comprehension', 'models', '.']","(15, 21)","(103, 125)",16.0
24,"Among these new types of questions were ""multi-span questions"", questions whose answers consist of several spans from either the paragraph or the question itself.",,,SINV,"['PP', 'VP', '.']",,,,,,,[],were,0.0,35.0,39.0,35.0,39.0,"[(35, 39)]",[],['were'],,,,,,,,,,were,were,"['Among', 'these new types of questions', '"" questions "" , questions whose answers consist of several spans from either the paragraph or the question itself', 'multi - span']",['.'],0,35,40,35,40,,,,,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,1,"Among these new types of questions were ""multi-span questions"", questions whose answers consist of several spans from either the paragraph or the question itself.","['Among', 'these', 'new', 'types', 'of', 'questions', 'were', '""multi-span', 'questions"",', 'questions', 'whose', 'answers', 'consist', 'of', 'several', 'spans', 'from', 'either', 'the', 'paragraph', 'or', 'the', 'question', 'itself.']",,,
25,"Until now, only one model attempted to tackle multi-span questions as a part of its design.",,,S,"['PP', ',', 'NP', 'VP', '.']",model,0.0,21.0,26.0,21.0,26.0,"[(21, 26)]",attempted,0.0,27.0,36.0,27.0,36.0,"[(27, 36)]",['model'],"['attempted', 'tackle']",,,,,,,,,,model,model,['only one'],"['Until now , attempted to tackle .', 'multi - span questions as a part of its design']",0,21,27,21,27,,,,,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,2,"Until now, only one model attempted to tackle multi-span questions as a part of its design.","['Until', 'now,', 'only', 'one', 'model', 'attempted', 'to', 'tackle', 'multi-span', 'questions', 'as', 'a', 'part', 'of', 'its', 'design.']",,,
26,"In this work, we suggest a new approach for tackling multi-span questions, based on sequence tagging, which differs from previous approaches for answering span questions.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,15.0,17.0,15.0,17.0,"[(15, 17)]",suggest,0.0,18.0,25.0,18.0,25.0,"[(18, 25)]",['we'],['suggest'],,,,,,,,,,suggest,suggest,"['we', 'a new approach for tackling multi - span questions , based on sequence tagging ,']",['which differs from previous approaches for answering span questions .'],0,18,26,18,26,,,,,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,3,"In this work, we suggest a new approach for tackling multi-span questions, based on sequence tagging, which differs from previous approaches for answering span questions.","['In', 'this', 'work,', 'we', 'suggest', 'a', 'new', 'approach', 'for', 'tackling', 'multi-span', 'questions,', 'based', 'on', 'sequence', 'tagging,', 'which', 'differs', 'from', 'previous', 'approaches', 'for', 'answering', 'span', 'questions.']",,,
27,"We show that our approach leads to an absolute improvement of 29.7 EM and 15.1 F1 compared to existing state-of-the-art results, while not hurting performance on other question types.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",show,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",['We'],['show'],,,,,,,,,,show,show,['We'],[],0,3,8,3,8,,,,,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,4,"We show that our approach leads to an absolute improvement of 29.7 EM and 15.1 F1 compared to existing state-of-the-art results, while not hurting performance on other question types.","['We', 'show', 'that', 'our', 'approach', 'leads', 'to', 'an', 'absolute', 'improvement', 'of', '29.7', 'EM', 'and', '15.1', 'F1', 'compared', 'to', 'existing', 'state-of-the-art', 'results,', 'while', 'not', 'hurting', 'performance', 'on', 'other', 'question', 'types.']",,,
28,"Furthermore , we show that our model slightly eclipses the current state - of - the - art results on",the entire DROP dataset,.,S,"['ADVP', ',', 'NP', 'VP', '.']",we,0.0,14.0,16.0,14.0,16.0,"[(14, 16)]",show,0.0,17.0,21.0,17.0,21.0,"[(17, 21)]",['we'],['show'],eclipses,"['our model', 'slightly']","['the current state', '-', 'results on the entire DROP dataset']",-1.0,0.0,46.0,55.0,46.0,55.0,show,show,"[',', 'we']",[],0,17,22,17,22,,,"['NOUN', 'ADP', 'NOUN']","['dataset', 'on', 'results']",https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Abstract,5,"Furthermore, we show that our model slightly eclipses the current state-of-the-art results on the entire DROP dataset.","['Furthermore', ',', 'we', 'show', 'that', 'our', 'model', 'slightly', 'eclipses', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'the', 'entire', 'DROP', 'dataset', '.']","(20, 24)","(100, 123)",11.0
29,Tag-based Multi-Span Extraction in Reading Comprehension.,,,NP,"['NP', 'HYPH', 'VBN', 'NNP', 'HYPH', 'NP', 'PP', '.']",Tag Multi Span Extraction,0.0,0.0,35.0,0.0,35.0,"[(0, 3), (12, 17), (20, 24), (25, 35)]",based,0.0,6.0,11.0,6.0,11.0,"[(6, 11)]","['Tag', 'Multi', 'Span', 'Extraction']",['based'],,,,,,,,,,based,based,"['Tag', '-']","['Multi -', 'Span Extraction in Reading Comprehension .']",0,6,12,6,12,,,,,https://www.semanticscholar.org/paper/Tag-based-Multi-Span-Extraction-in-Reading-Efrat-Segal/8dc72cda1b37e4baa319ff4a9cfc8ad11214eb77,3,Title,0,Tag-based Multi-Span Extraction in Reading Comprehension.,"['Tag-based', 'Multi-Span', 'Extraction', 'in', 'Reading', 'Comprehension.']",,,
30,Machine comprehension of texts longer than a single sentence often requires coreference resolution.,,,S,"['NP', 'ADVP', 'VP', '.']",Machine comprehension,0.0,0.0,21.0,0.0,21.0,"[(0, 7), (8, 21)]",requires,0.0,67.0,75.0,67.0,75.0,"[(67, 75)]","['Machine', 'comprehension']",['requires'],,,,,,,,,,requires,requires,"['Machine comprehension of texts longer than a single sentence', 'often']","['coreference resolution', '.']",0,67,76,67,76,,,,,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,0,Machine comprehension of texts longer than a single sentence often requires coreference resolution.,"['Machine', 'comprehension', 'of', 'texts', 'longer', 'than', 'a', 'single', 'sentence', 'often', 'requires', 'coreference', 'resolution.']",,,
31,"However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",most current reading comprehension benchmarks,0.0,10.0,55.0,10.0,55.0,"[(10, 14), (15, 22), (23, 30), (31, 44), (45, 55)]",do contain and fail,0.0,56.0,117.0,56.0,117.0,"[(56, 58), (63, 70), (103, 106), (113, 117)]","['most', 'current', 'reading', 'comprehension', 'benchmarks']","['do', 'contain', 'fail', 'evaluate']",,,,,,,,,,phenomena,phenomena,"['However , most current reading comprehension benchmarks', 'coreferential']",['and hence fail to evaluate the ability of models to resolve coreference .'],0,93,103,93,103,,,,,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,1,"However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference.","['However,', 'most', 'current', 'reading', 'comprehension', 'benchmarks', 'do', 'not', 'contain', 'complex', 'coreferential', 'phenomena', 'and', 'hence', 'fail', 'to', 'evaluate', 'the', 'ability', 'of', 'models', 'to', 'resolve', 'coreference.']",,,
32,We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",present,0.0,3.0,10.0,3.0,10.0,"[(3, 10)]",['We'],['present'],,,,,,,,,,questions,questions,['We present a new crowdsourced dataset containing more than 24 K span - selection'],['that require resolving coreference among entities in over 4.7 K English paragraphs from Wikipedia .'],0,81,91,81,91,,,,,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,2,We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia.,"['We', 'present', 'a', 'new', 'crowdsourced', 'dataset', 'containing', 'more', 'than', '24K', 'span-selection', 'questions', 'that', 'require', 'resolving', 'coreference', 'among', 'entities', 'in', 'over', '4.7K', 'English', 'paragraphs', 'from', 'Wikipedia.']",,,
33,"Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning.",,,S,"['S', 'VP', '.']",,,,,,,[],is,0.0,46.0,48.0,46.0,48.0,"[(46, 48)]",[],"['Obtaining', 'focused', 'is']",,,,,,,,,,challenging,is challenging,['Obtaining questions focused on such phenomena'],['.'],0,46,61,46,61,,,,,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,3,"Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning.","['Obtaining', 'questions', 'focused', 'on', 'such', 'phenomena', 'is', 'challenging,', 'because', 'it', 'is', 'hard', 'to', 'avoid', 'lexical', 'cues', 'that', 'shortcut', 'complex', 'reasoning.']",,,
34,"We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",deal,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",['We'],['deal'],,,,,,,,,,helps,helps,"['We', 'which', 'deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop ,']",['.'],0,107,113,107,113,,,,,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,4,"We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues.","['We', 'deal', 'with', 'this', 'issue', 'by', 'using', 'a', 'strong', 'baseline', 'model', 'as', 'an', 'adversary', 'in', 'the', 'crowdsourcing', 'loop,', 'which', 'helps', 'crowdworkers', 'avoid', 'writing', 'questions', 'with', 'exploitable', 'surface', 'cues.']",,,
35,"We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark—the best model performance is 70.5 F1, while the estimated human performance is 93.4 F1.",,,S,"['S', ':', 'S', '.']",,,,,,,[],,,,,,,[],"['We', 'the', 'best', 'model', 'performance']","['show', 'perform', 'is']",,,,,,,,,,70.5,is 70.5,['that state - of - the - art reading comprehension models perform significantly worse than humans on this benchmark — the best model performance'],"['We show F1 .', ', while the estimated human performance is 93.4 F1']",0,152,160,152,160,,,,,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Abstract,5,"We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark—the best model performance is 70.5 F1, while the estimated human performance is 93.4 F1.","['We', 'show', 'that', 'state-of-the-art', 'reading', 'comprehension', 'models', 'perform', 'significantly', 'worse', 'than', 'humans', 'on', 'this', 'benchmark—the', 'best', 'model', 'performance', 'is', '70.5', 'F1,', 'while', 'the', 'estimated', 'human', 'performance', 'is', '93.4', 'F1.']",,,
36,Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning.,,,NP,"['NP', ':', 'NP', '.']",Quoref A Reading Comprehension,0.0,0.0,32.0,0.0,32.0,"[(0, 6), (9, 10), (11, 18), (19, 32)]",,,,,,,[],"['Quoref', 'A', 'Reading', 'Comprehension']",[],,,,,,,,,,Quoref,Quoref,[],"[':', 'A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning .']",0,0,7,0,7,,,,,https://www.semanticscholar.org/paper/Quoref%3A-A-Reading-Comprehension-Dataset-with-Dasigi-Liu/3838387ea8dd1bb8c2306be5a63c1c120075c5a2,4,Title,0,Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning.,"['Quoref:', 'A', 'Reading', 'Comprehension', 'Dataset', 'with', 'Questions', 'Requiring', 'Coreferential', 'Reasoning.']",,,
37,"Rapid progress has been made in the field of reading comprehension and question answering, where several systems have achieved human parity in some simplified settings.",,,S,"['NP', 'VP', '.']",Rapid progress,0.0,0.0,14.0,0.0,14.0,"[(0, 5), (6, 14)]",has been made,0.0,15.0,28.0,15.0,28.0,"[(15, 18), (19, 23), (24, 28)]","['Rapid', 'progress']","['has', 'been', 'made']",,,,,,,,,,made,has been made,['progress'],['.'],0,15,29,15,29,,,,,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,0,"Rapid progress has been made in the field of reading comprehension and question answering, where several systems have achieved human parity in some simplified settings.","['Rapid', 'progress', 'has', 'been', 'made', 'in', 'the', 'field', 'of', 'reading', 'comprehension', 'and', 'question', 'answering,', 'where', 'several', 'systems', 'have', 'achieved', 'human', 'parity', 'in', 'some', 'simplified', 'settings.']",,,
38,"However, the performance of these models degrades significantly when they are applied to more realistic scenarios, such as answers involve various types, multiple text strings are correct answers, or discrete reasoning abilities are required.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",the performance,0.0,10.0,25.0,10.0,25.0,"[(10, 13), (14, 25)]",are applied,0.0,75.0,86.0,75.0,86.0,"[(75, 78), (79, 86)]","['the', 'performance']","['are', 'applied']",,,,,,,,,,degrades,degrades,[],[],0,42,51,42,51,,,,,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,1,"However, the performance of these models degrades significantly when they are applied to more realistic scenarios, such as answers involve various types, multiple text strings are correct answers, or discrete reasoning abilities are required.","['However,', 'the', 'performance', 'of', 'these', 'models', 'degrades', 'significantly', 'when', 'they', 'are', 'applied', 'to', 'more', 'realistic', 'scenarios,', 'such', 'as', 'answers', 'involve', 'various', 'types,', 'multiple', 'text', 'strings', 'are', 'correct', 'answers,', 'or', 'discrete', 'reasoning', 'abilities', 'are', 'required.']",,,
39,"In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types (e.g., span, count, negation, and arithmetic expression) with a multi-span extraction method for dynamically producing one or multiple text spans.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,16.0,18.0,16.0,18.0,"[(16, 18)]",introduce,0.0,19.0,28.0,19.0,28.0,"[(19, 28)]",['we'],['introduce'],,,,,,,,,,introduce,introduce,"[',', 'we']",['the Multi - Type Multi - Span Network ( MTMSN )'],0,19,29,19,29,,,,,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,2,"In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types (e.g., span, count, negation, and arithmetic expression) with a multi-span extraction method for dynamically producing one or multiple text spans.","['In', 'this', 'paper,', 'we', 'introduce', 'the', 'Multi-Type', 'Multi-Span', 'Network', '(MTMSN),', 'a', 'neural', 'reading', 'comprehension', 'model', 'that', 'combines', 'a', 'multi-type', 'answer', 'predictor', 'designed', 'to', 'support', 'various', 'answer', 'types', '(e.g.,', 'span,', 'count,', 'negation,', 'and', 'arithmetic', 'expression)', 'with', 'a', 'multi-span', 'extraction', 'method', 'for', 'dynamically', 'producing', 'one', 'or', 'multiple', 'text', 'spans.']",,,
40,"In addition, an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction.",,,S,"['PP', ',', 'NP', 'VP', '.']",an arithmetic expression mechanism,0.0,14.0,58.0,14.0,58.0,"[(14, 16), (17, 27), (28, 38), (49, 58)]",is proposed,0.0,59.0,70.0,59.0,70.0,"[(59, 61), (62, 70)]","['an', 'arithmetic', 'expression', 'mechanism']","['is', 'proposed', 'rank']",,,,,,,,,,proposed,is proposed,[],['rank expression candidates for further confirming the prediction'],0,59,71,59,71,,,,,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,3,"In addition, an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction.","['In', 'addition,', 'an', 'arithmetic', 'expression', 'reranking', 'mechanism', 'is', 'proposed', 'to', 'rank', 'expression', 'candidates', 'for', 'further', 'confirming', 'the', 'prediction.']",,,
41,Experiments show that our model achieves 79.9 F1 on the,DROP,"hidden test set , creating new state-of-the-art results .",S,"['NP', 'VP', '.']",Experiments,0.0,0.0,11.0,0.0,11.0,"[(0, 11)]",show,0.0,12.0,16.0,12.0,16.0,"[(12, 16)]",['Experiments'],['show'],show,['Experiments'],[],-1.0,0.0,12.0,17.0,12.0,17.0,show,show,['Experiments'],[],0,12,17,12,17,hidden,hidden,"['NOUN', 'NOUN', 'ADP']","['DROP', 'test', 'on']",https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,4,"Experiments show that our model achieves 79.9 F1 on the DROP hidden test set, creating new state-of-the-art results.","['Experiments', 'show', 'that', 'our', 'model', 'achieves', '79.9', 'F1', 'on', 'the', 'DROP', 'hidden', 'test', 'set', ',', 'creating', 'new', 'state-of-the-art', 'results', '.']","(10, 11)","(55, 59)",0.0
42,Source code\footnote{\url{https://github.com/huminghao16/MTMSN}} is released to facilitate future work.,,,S,"['NP', 'VP', '.']",Source huminghao16 MTMSN,0.0,0.0,66.0,0.0,66.0,"[(0, 6), (47, 58), (61, 66)]",is released,0.0,71.0,82.0,71.0,82.0,"[(71, 73), (74, 82)]","['Source', 'huminghao16', 'MTMSN']","['is', 'released', 'facilitate']",,,,,,,,,,is,is,['Source code\\footnote{\\url{https://github.com / huminghao16 / MTMSN } }'],[],0,71,74,71,74,,,,,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Abstract,5,Source code\footnote{\url{https://github.com/huminghao16/MTMSN}} is released to facilitate future work.,"['Source', 'code\\footnote{\\url{https://github.com/huminghao16/MTMSN}}', 'is', 'released', 'to', 'facilitate', 'future', 'work.']",,,
43,A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning.,,,NP,"['DT', 'NNP', 'HYPH', 'NNP', 'NNP', 'HYPH', 'NP', 'PP', 'SBAR', '.']",A Multi Type Multi Span Network,0.0,0.0,35.0,0.0,35.0,"[(0, 1), (2, 7), (10, 14), (15, 20), (23, 27), (28, 35)]",,,,,,,[],"['A', 'Multi', 'Type', 'Multi', 'Span', 'Network']",[],,,,,,,,,,Type,Type,['A Multi -'],['Multi - Span Network'],0,10,15,10,15,,,,,https://www.semanticscholar.org/paper/A-Multi-Type-Multi-Span-Network-for-Reading-that-Hu-Peng/b285c067f2da04bf5647beb8853bfddf6d9b4e1b,5,Title,0,A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning.,"['A', 'Multi-Type', 'Multi-Span', 'Network', 'for', 'Reading', 'Comprehension', 'that', 'Requires', 'Discrete', 'Reasoning.']",,,
44,Machine Reading Comprehension (MRC) is the task of answering a question over a paragraph of text.,,,S,"['NP', 'VP', '.']",Machine Reading Comprehension MRC,0.0,0.0,35.0,0.0,35.0,"[(0, 7), (8, 15), (16, 29), (32, 35)]",is,0.0,38.0,40.0,38.0,40.0,"[(38, 40)]","['Machine', 'Reading', 'Comprehension', 'MRC']",['is'],,,,,,,,,,is,is,[],['the task of answering a question over a paragraph of text'],0,38,41,38,41,,,,,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,0,Machine Reading Comprehension (MRC) is the task of answering a question over a paragraph of text.,"['Machine', 'Reading', 'Comprehension', '(MRC)', 'is', 'the', 'task', 'of', 'answering', 'a', 'question', 'over', 'a', 'paragraph', 'of', 'text.']",,,
45,"While neural MRC systems gain popularity and achieve noticeable performance, issues are being raised with the methodology used to establish their performance, particularly concerning the data design of gold standards that are used to evaluate them.",,,S,"['SBAR', ',', 'NP', 'VP', '.']",issues,0.0,78.0,84.0,78.0,84.0,"[(78, 84)]",are being raised concerning,0.0,85.0,184.0,85.0,184.0,"[(85, 88), (89, 94), (95, 101), (174, 184)]",['issues'],"['are', 'being', 'raised', 'concerning']",,,,,,,,,,gain,gain,['neural MRC systems'],[],0,25,30,25,30,,,,,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,1,"While neural MRC systems gain popularity and achieve noticeable performance, issues are being raised with the methodology used to establish their performance, particularly concerning the data design of gold standards that are used to evaluate them.","['While', 'neural', 'MRC', 'systems', 'gain', 'popularity', 'and', 'achieve', 'noticeable', 'performance,', 'issues', 'are', 'being', 'raised', 'with', 'the', 'methodology', 'used', 'to', 'establish', 'their', 'performance,', 'particularly', 'concerning', 'the', 'data', 'design', 'of', 'gold', 'standards', 'that', 'are', 'used', 'to', 'evaluate', 'them.']",,,
46,"There is but a limited understanding of the challenges present in this data, which makes it hard to draw comparisons and formulate reliable hypotheses.",,,S,"['NP', 'VP', '.']",,,,,,,[],is but,0.0,6.0,12.0,6.0,12.0,"[(6, 8), (9, 12)]",[],['is'],,,,,,,,,,is,is,['a limited understanding of the challenges present in this data'],"[',', 'which makes it hard to draw comparisons and formulate reliable hypotheses']",0,6,9,6,9,,,,,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,2,"There is but a limited understanding of the challenges present in this data, which makes it hard to draw comparisons and formulate reliable hypotheses.","['There', 'is', 'but', 'a', 'limited', 'understanding', 'of', 'the', 'challenges', 'present', 'in', 'this', 'data,', 'which', 'makes', 'it', 'hard', 'to', 'draw', 'comparisons', 'and', 'formulate', 'reliable', 'hypotheses.']",,,
47,"As a first step towards alleviating the problem, this paper proposes a unifying framework to systematically investigate the present linguistic features, required reasoning and background knowledge and factual correctness on one hand, and the presence of lexical cues as a lower bound for the requirement of understanding on the other hand.",,,S,"['PP', ',', 'NP', 'VP', '.']",this paper,0.0,50.0,60.0,50.0,60.0,"[(50, 54), (55, 60)]",proposes,0.0,61.0,69.0,61.0,69.0,"[(61, 69)]","['this', 'paper']",['proposes'],,,,,,,,,,for,for,"['As a first step towards alleviating the problem , this paper proposes a unifying framework to systematically investigate the present linguistic features , required reasoning and background knowledge and factual correctness on one hand , and the presence of lexical cues as a lower bound']",['the requirement of understanding on the other hand .'],0,287,291,287,291,,,,,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,3,"As a first step towards alleviating the problem, this paper proposes a unifying framework to systematically investigate the present linguistic features, required reasoning and background knowledge and factual correctness on one hand, and the presence of lexical cues as a lower bound for the requirement of understanding on the other hand.","['As', 'a', 'first', 'step', 'towards', 'alleviating', 'the', 'problem,', 'this', 'paper', 'proposes', 'a', 'unifying', 'framework', 'to', 'systematically', 'investigate', 'the', 'present', 'linguistic', 'features,', 'required', 'reasoning', 'and', 'background', 'knowledge', 'and', 'factual', 'correctness', 'on', 'one', 'hand,', 'and', 'the', 'presence', 'of', 'lexical', 'cues', 'as', 'a', 'lower', 'bound', 'for', 'the', 'requirement', 'of', 'understanding', 'on', 'the', 'other', 'hand.']",,,
48,We propose a qualitative annotation schema for the first and a set of approximative metrics for the latter.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",propose,0.0,3.0,10.0,3.0,10.0,"[(3, 10)]",['We'],['propose'],,,,,,,,,,propose,propose,['We'],"['a', 'qualitative annotation schema for the first and a set of approximative metrics for the latter']",0,3,11,3,11,,,,,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,4,We propose a qualitative annotation schema for the first and a set of approximative metrics for the latter.,"['We', 'propose', 'a', 'qualitative', 'annotation', 'schema', 'for', 'the', 'first', 'and', 'a', 'set', 'of', 'approximative', 'metrics', 'for', 'the', 'latter.']",,,
49,"In a first application of the framework, we analyse modern MRC gold standards and present our findings: the absence of features that contribute towards lexical ambiguity, the varying factual correctness of the expected answers and the presence of lexical cues, all of which potentially lower the reading comprehension complexity and quality of the evaluation data.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,42.0,44.0,42.0,44.0,"[(42, 44)]",analyse and present,0.0,45.0,90.0,45.0,90.0,"[(45, 52), (79, 82), (83, 90)]",['we'],"['analyse', 'present']",,,,,,,,,,standards,standards,"['In a first application of the framework , we analyse modern MRC gold and present our findings : the absence of features that contribute towards lexical ambiguity , the varying factual correctness of the expected answers and the presence of lexical cues , all of which potentially lower the reading comprehension complexity and quality of the evaluation data .']",[],0,69,79,69,79,,,,,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Abstract,5,"In a first application of the framework, we analyse modern MRC gold standards and present our findings: the absence of features that contribute towards lexical ambiguity, the varying factual correctness of the expected answers and the presence of lexical cues, all of which potentially lower the reading comprehension complexity and quality of the evaluation data.","['In', 'a', 'first', 'application', 'of', 'the', 'framework,', 'we', 'analyse', 'modern', 'MRC', 'gold', 'standards', 'and', 'present', 'our', 'findings:', 'the', 'absence', 'of', 'features', 'that', 'contribute', 'towards', 'lexical', 'ambiguity,', 'the', 'varying', 'factual', 'correctness', 'of', 'the', 'expected', 'answers', 'and', 'the', 'presence', 'of', 'lexical', 'cues,', 'all', 'of', 'which', 'potentially', 'lower', 'the', 'reading', 'comprehension', 'complexity', 'and', 'quality', 'of', 'the', 'evaluation', 'data.']",,,
50,A Framework for Evaluation of Machine Reading Comprehension Gold Standards.,,,NP,"['NP', 'PP', '.']",A Framework,0.0,0.0,11.0,0.0,11.0,"[(0, 1), (2, 11)]",,,,,,,[],"['A', 'Framework']",[],,,,,,,,,,Framework,Framework,[],[],0,2,12,2,12,,,,,https://www.semanticscholar.org/paper/A-Framework-for-Evaluation-of-Machine-Reading-Gold-Schlegel-Valentino/e27031f5a47025eaedd65af3b4a48f07b5636514,6,Title,0,A Framework for Evaluation of Machine Reading Comprehension Gold Standards.,"['A', 'Framework', 'for', 'Evaluation', 'of', 'Machine', 'Reading', 'Comprehension', 'Gold', 'Standards.']",,,
51,"Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations.",,,S,"['S', 'VP', '.']",,,,,,,[],is,0.0,88.0,90.0,88.0,90.0,"[(88, 90)]",[],"['Answering', 'is']",,,,,,,,,,compositional,compositional,[],[],0,10,24,10,24,,,,,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,0,"Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations.","['Answering', 'compositional', 'questions', 'that', 'require', 'multiple', 'steps', 'of', 'reasoning', 'against', 'text', 'is', 'challenging,', 'especially', 'when', 'they', 'involve', 'discrete,', 'symbolic', 'operations.']",,,
52,"Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains.",,,S,"['NP', 'VP', '.']",Neural module networks NMNs,0.0,0.0,29.0,0.0,29.0,"[(0, 6), (7, 13), (14, 22), (25, 29)]",learn,0.0,32.0,37.0,32.0,37.0,"[(32, 37)]","['Neural', 'module', 'networks', 'NMNs']","['learn', 'parse']",,,,,,,,,,module,module,[],"['networks', '( NMNs ) learn to parse such questions as executable programs composed of learnable modules , performing well on synthetic visual QA domains .']",0,7,14,7,14,,,,,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,1,"Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains.","['Neural', 'module', 'networks', '(NMNs)', 'learn', 'to', 'parse', 'such', 'questions', 'as', 'executable', 'programs', 'composed', 'of', 'learnable', 'modules,', 'performing', 'well', 'on', 'synthetic', 'visual', 'QA', 'domains.']",,,
53,"However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",we,0.0,10.0,12.0,10.0,12.0,"[(10, 12)]",find,0.0,13.0,17.0,13.0,17.0,"[(13, 17)]",['we'],['find'],,,,,,,,,,find,find,"['However', ',', 'we']",[],0,13,18,13,18,,,,,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,2,"However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning.","['However,', 'we', 'find', 'that', 'it', 'is', 'challenging', 'to', 'learn', 'these', 'models', 'for', 'non-synthetic', 'questions', 'on', 'open-domain', 'text,', 'where', 'a', 'model', 'needs', 'to', 'deal', 'with', 'the', 'diversity', 'of', 'natural', 'language', 'and', 'perform', 'a', 'broader', 'range', 'of', 'reasoning.']",,,
54,"We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",extend,0.0,3.0,9.0,3.0,9.0,"[(3, 9)]",['We'],['extend'],,,,,,,,,,We,We,[],"['extend NMNs by : ( a ) introducing modules that reason over a paragraph of text , performing symbolic reasoning ( such as arithmetic , sorting , counting ) over numbers and dates in a probabilistic and differentiable manner ; and ( b ) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text .']",0,0,3,0,3,,,,,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,3,"We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text.","['We', 'extend', 'NMNs', 'by:', '(a)', 'introducing', 'modules', 'that', 'reason', 'over', 'a', 'paragraph', 'of', 'text,', 'performing', 'symbolic', 'reasoning', '(such', 'as', 'arithmetic,', 'sorting,', 'counting)', 'over', 'numbers', 'and', 'dates', 'in', 'a', 'probabilistic', 'and', 'differentiable', 'manner;', 'and', '(b)', 'proposing', 'an', 'unsupervised', 'auxiliary', 'loss', 'to', 'help', 'extract', 'arguments', 'associated', 'with', 'the', 'events', 'in', 'text.']",,,
55,"Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",we,0.0,15.0,17.0,15.0,17.0,"[(15, 17)]",show,0.0,18.0,22.0,18.0,22.0,"[(18, 22)]",['we'],['show'],,,,,,,,,,show,show,"['Additionally', ',', 'we']",[],0,18,23,18,23,,,,,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,4,"Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning.","['Additionally,', 'we', 'show', 'that', 'a', 'limited', 'amount', 'of', 'heuristically-obtained', 'question', 'program', 'and', 'intermediate', 'module', 'output', 'supervision', 'provides', 'sufficient', 'inductive', 'bias', 'for', 'accurate', 'learning.']",,,
56,Our proposed model significantly outperforms state-of-the-art models on a subset of the,DROP,dataset that poses a variety of reasoning challenges that are covered by our modules .,S,"['NP', 'VP', '.']",model,0.0,13.0,18.0,13.0,18.0,"[(13, 18)]",outperforms,0.0,33.0,44.0,33.0,44.0,"[(33, 44)]",['model'],['outperforms'],outperforms,[],['state - of - the - art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules .'],-1.0,0.0,33.0,45.0,33.0,45.0,model,model,[],"['significantly', 'outperforms state - of - the - art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules .']",0,13,19,13,19,poses,dataset,"['NOUN', 'ADP', 'NOUN']","['DROP', 'of', 'subset']",https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Abstract,5,Our proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.,"['Our', 'proposed', 'model', 'significantly', 'outperforms', 'state-of-the-art', 'models', 'on', 'a', 'subset', 'of', 'the', 'DROP', 'dataset', 'that', 'poses', 'a', 'variety', 'of', 'reasoning', 'challenges', 'that', 'are', 'covered', 'by', 'our', 'modules', '.']","(12, 13)","(87, 91)",0.0
57,Neural Module Networks for Reasoning over Text.,,,NP,"['NP', 'PP', 'PP', '.']",Neural Module Networks,0.0,0.0,22.0,0.0,22.0,"[(0, 6), (7, 13), (14, 22)]",,,,,,,[],"['Neural', 'Module', 'Networks']",[],,,,,,,,,,Module,Module,['Neural .'],['Networks'],0,7,14,7,14,,,,,https://www.semanticscholar.org/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2,7,Title,0,Neural Module Networks for Reasoning over Text.,"['Neural', 'Module', 'Networks', 'for', 'Reasoning', 'over', 'Text.']",,,
58,Numerical reasoning is often important to accurately understand the world.,,,S,"['NP', 'VP', '.']",Numerical reasoning,0.0,0.0,19.0,0.0,19.0,"[(0, 9), (10, 19)]",is,0.0,20.0,22.0,20.0,22.0,"[(20, 22)]","['Numerical', 'reasoning']","['is', 'understand']",,,,,,,,,,important,is important,[],['to accurately understand the world'],0,20,39,20,39,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,0,Numerical reasoning is often important to accurately understand the world.,"['Numerical', 'reasoning', 'is', 'often', 'important', 'to', 'accurately', 'understand', 'the', 'world.']",,,
59,"Recently, several format-specific datasets have been proposed, such as numerical reasoning in the settings of Natural Language Inference (NLI), Reading Comprehension (RC), and Question Answering (QA).",,,S,"['ADVP', ',', 'NP', 'VP', ',', 'PP', '.']",several format specific datasets,0.0,11.0,45.0,11.0,45.0,"[(11, 18), (19, 25), (28, 36), (37, 45)]",have been proposed,0.0,46.0,64.0,46.0,64.0,"[(46, 50), (51, 55), (56, 64)]","['several', 'format', 'specific', 'datasets']","['have', 'been', 'proposed']",,,,,,,,,,(,(,"['Recently , several format - specific datasets have been proposed , such as numerical reasoning in the settings of Natural Language Inference ( NLI ) , Reading Comprehension ( RC ) , and Question Answering']",['QA ) .'],3,205,207,4,6,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,1,"Recently, several format-specific datasets have been proposed, such as numerical reasoning in the settings of Natural Language Inference (NLI), Reading Comprehension (RC), and Question Answering (QA).","['Recently,', 'several', 'format-specific', 'datasets', 'have', 'been', 'proposed,', 'such', 'as', 'numerical', 'reasoning', 'in', 'the', 'settings', 'of', 'Natural', 'Language', 'Inference', '(NLI),', 'Reading', 'Comprehension', '(RC),', 'and', 'Question', 'Answering', '(QA).']",,,
60,Several format-specific models and architectures in response to those datasets have also been proposed.,,,S,"['NP', 'VP', '.']",Several format specific models architectures,0.0,0.0,50.0,0.0,50.0,"[(0, 7), (8, 14), (17, 25), (26, 32), (37, 50)]",have been proposed,0.0,81.0,104.0,81.0,104.0,"[(81, 85), (91, 95), (96, 104)]","['Several', 'format', 'specific', 'models', 'architectures']","['have', 'been', 'proposed']",,,,,,,,,,proposed,have been proposed,['Several format - specific models and architectures in response to those datasets'],[],0,81,105,81,105,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,2,Several format-specific models and architectures in response to those datasets have also been proposed.,"['Several', 'format-specific', 'models', 'and', 'architectures', 'in', 'response', 'to', 'those', 'datasets', 'have', 'also', 'been', 'proposed.']",,,
61,"However, there exists a strong need for a benchmark which can evaluate the abilities of models, in performing question format independent numerical reasoning, as (i) the numerical reasoning capabilities we want to teach are not controlled by question formats, (ii) for numerical reasoning technology to have the best possible application, it must be able to process language and reason in a way that is not exclusive to a single format, task, dataset or domain.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",,,,,,,[],exists,0.0,16.0,22.0,16.0,22.0,"[(16, 22)]",[],['exists'],,,,,,,,,,exclusive,is exclusive,['that'],[],0,409,426,409,426,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,3,"However, there exists a strong need for a benchmark which can evaluate the abilities of models, in performing question format independent numerical reasoning, as (i) the numerical reasoning capabilities we want to teach are not controlled by question formats, (ii) for numerical reasoning technology to have the best possible application, it must be able to process language and reason in a way that is not exclusive to a single format, task, dataset or domain.","['However,', 'there', 'exists', 'a', 'strong', 'need', 'for', 'a', 'benchmark', 'which', 'can', 'evaluate', 'the', 'abilities', 'of', 'models,', 'in', 'performing', 'question', 'format', 'independent', 'numerical', 'reasoning,', 'as', '(i)', 'the', 'numerical', 'reasoning', 'capabilities', 'we', 'want', 'to', 'teach', 'are', 'not', 'controlled', 'by', 'question', 'formats,', '(ii)', 'for', 'numerical', 'reasoning', 'technology', 'to', 'have', 'the', 'best', 'possible', 'application,', 'it', 'must', 'be', 'able', 'to', 'process', 'language', 'and', 'reason', 'in', 'a', 'way', 'that', 'is', 'not', 'exclusive', 'to', 'a', 'single', 'format,', 'task,', 'dataset', 'or', 'domain.']",,,
62,"In pursuit of this goal, we introduce NUMBERGAME, a multifaceted benchmark to evaluate model performance across numerical reasoning tasks of eight diverse formats.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,26.0,28.0,26.0,28.0,"[(26, 28)]",introduce,0.0,29.0,38.0,29.0,38.0,"[(29, 38)]",['we'],['introduce'],,,,,,,,,,introduce,introduce,['we'],"['NUMBERGAME , a multifaceted benchmark']",0,29,39,29,39,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,4,"In pursuit of this goal, we introduce NUMBERGAME, a multifaceted benchmark to evaluate model performance across numerical reasoning tasks of eight diverse formats.","['In', 'pursuit', 'of', 'this', 'goal,', 'we', 'introduce', 'NUMBERGAME,', 'a', 'multifaceted', 'benchmark', 'to', 'evaluate', 'model', 'performance', 'across', 'numerical', 'reasoning', 'tasks', 'of', 'eight', 'diverse', 'formats.']",,,
63,We add four existing question types in our compilation.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",add,0.0,3.0,6.0,3.0,6.0,"[(3, 6)]",['We'],['add'],,,,,,,,,,add,add,['We'],['four existing question types in our compilation'],0,3,7,3,7,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,5,We add four existing question types in our compilation.,"['We', 'add', 'four', 'existing', 'question', 'types', 'in', 'our', 'compilation.']",,,
64,"Two of the new types we add are about questions that require external numerical knowledge, commonsense knowledge and domain knowledge.",,,S,"['NP', 'VP', '.']",,,,,,,[],are,0.0,28.0,31.0,28.0,31.0,"[(28, 31)]",[],['are'],,,,,,,,,,are,are,['Two of the new types we add'],[],0,28,32,28,32,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,6,"Two of the new types we add are about questions that require external numerical knowledge, commonsense knowledge and domain knowledge.","['Two', 'of', 'the', 'new', 'types', 'we', 'add', 'are', 'about', 'questions', 'that', 'require', 'external', 'numerical', 'knowledge,', 'commonsense', 'knowledge', 'and', 'domain', 'knowledge.']",,,
65,"While recently many QA datasets involving external knowledge have been proposed, ours incorporates them in a numerical reasoning setting.",,,S,"['SBAR', ',', 'NP', 'VP', '.']",,,,,,,[],incorporates,0.0,87.0,99.0,87.0,99.0,"[(87, 99)]",[],['incorporates'],,,,,,,,,,incorporates,incorporates,"['While recently many QA datasets involving external knowledge have been proposed ,', 'ours']","['them', 'in a numerical reasoning setting']",0,87,100,87,100,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,7,"While recently many QA datasets involving external knowledge have been proposed, ours incorporates them in a numerical reasoning setting.","['While', 'recently', 'many', 'QA', 'datasets', 'involving', 'external', 'knowledge', 'have', 'been', 'proposed,', 'ours', 'incorporates', 'them', 'in', 'a', 'numerical', 'reasoning', 'setting.']",,,
66,Other types in our compilation build upon existing data sets.,,,S,"['NP', 'VP', '.']",Other types,0.0,0.0,11.0,0.0,11.0,"[(0, 5), (6, 11)]",,,,,,,[],"['Other', 'types']",[],,,,,,,,,,types,types,['Other'],[],0,6,12,6,12,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,8,Other types in our compilation build upon existing data sets.,"['Other', 'types', 'in', 'our', 'compilation', 'build', 'upon', 'existing', 'data', 'sets.']",,,
67,"For building a more practical numerical reasoning system, NUMBERGAME demands four capabilities beyond numerical reasoning: (i) detecting question format directly from data (ii) finding intermediate common format to which every format can be converted (iii) incorporating commonsense knowledge (iv) handling data imbalance across formats.",,,S,"['PP', ',', 'NP', 'VP', '.']",NUMBERGAME,0.0,59.0,69.0,59.0,69.0,"[(59, 69)]",demands,0.0,70.0,77.0,70.0,77.0,"[(70, 77)]",['NUMBERGAME'],['demands'],,,,,,,,,,),),"['For building a more practical numerical reasoning system , NUMBERGAME demands four capabilities beyond numerical reasoning : ( i ) detecting question format directly from data ( ii ) finding intermediate common format to which every format can be converted ( iii ) incorporating commonsense knowledge ( iv']",[],0,306,308,306,308,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,9,"For building a more practical numerical reasoning system, NUMBERGAME demands four capabilities beyond numerical reasoning: (i) detecting question format directly from data (ii) finding intermediate common format to which every format can be converted (iii) incorporating commonsense knowledge (iv) handling data imbalance across formats.","['For', 'building', 'a', 'more', 'practical', 'numerical', 'reasoning', 'system,', 'NUMBERGAME', 'demands', 'four', 'capabilities', 'beyond', 'numerical', 'reasoning:', '(i)', 'detecting', 'question', 'format', 'directly', 'from', 'data', '(ii)', 'finding', 'intermediate', 'common', 'format', 'to', 'which', 'every', 'format', 'can', 'be', 'converted', '(iii)', 'incorporating', 'commonsense', 'knowledge', '(iv)', 'handling', 'data', 'imbalance', 'across', 'formats.']",,,
68,"We build several baselines, including a new model based on knowledge hunting using a cheatsheet.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",build,0.0,3.0,8.0,3.0,8.0,"[(3, 8)]",['We'],['build'],,,,,,,,,,build,build,['We'],"['several baselines', ',']",0,3,9,3,9,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,10,"We build several baselines, including a new model based on knowledge hunting using a cheatsheet.","['We', 'build', 'several', 'baselines,', 'including', 'a', 'new', 'model', 'based', 'on', 'knowledge', 'hunting', 'using', 'a', 'cheatsheet.']",,,
69,"However, all baselines perform poorly in contrast to the human baselines, indicating the hardness of our benchmark.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",all baselines,0.0,10.0,23.0,10.0,23.0,"[(10, 13), (14, 23)]",perform,0.0,24.0,31.0,24.0,31.0,"[(24, 31)]","['all', 'baselines']","['perform', 'indicating']",,,,,,,,,,perform,perform,"['However', ',', 'all baselines']","['in contrast to the human baselines , indicating the hardness of our benchmark']",0,24,32,24,32,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,11,"However, all baselines perform poorly in contrast to the human baselines, indicating the hardness of our benchmark.","['However,', 'all', 'baselines', 'perform', 'poorly', 'in', 'contrast', 'to', 'the', 'human', 'baselines,', 'indicating', 'the', 'hardness', 'of', 'our', 'benchmark.']",,,
70,"Our work takes forward the recent progress in generic system development, demonstrating the scope of these under-explored tasks.",,,S,"['NP', 'VP', '.']",work,0.0,4.0,8.0,4.0,8.0,"[(4, 8)]",takes,0.0,9.0,14.0,9.0,14.0,"[(9, 14)]",['work'],['takes'],,,,,,,,,,takes,takes,"['Our', 'work']","['the recent progress', 'demonstrating the scope of these under - explored tasks']",0,9,15,9,15,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Abstract,12,"Our work takes forward the recent progress in generic system development, demonstrating the scope of these under-explored tasks.","['Our', 'work', 'takes', 'forward', 'the', 'recent', 'progress', 'in', 'generic', 'system', 'development,', 'demonstrating', 'the', 'scope', 'of', 'these', 'under-explored', 'tasks.']",,,
71,Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks.,,,FRAG,"['IN', 'NP', 'NP', ':', 'NP', '.']",Question Format Independent Numerical Reasoning A Set,0.0,8.0,63.0,8.0,63.0,"[(8, 16), (17, 23), (24, 35), (36, 45), (46, 55), (58, 59), (60, 63)]",,,,,,,[],"['Question', 'Format', 'Independent', 'Numerical', 'Reasoning', 'A', 'Set']",[],,,,,,,,,,Numerical,Numerical,"['Towards Question Format .', 'Independent']",[': A Set of Prerequisite Tasks'],0,36,46,36,46,,,,,https://www.semanticscholar.org/paper/Towards-Question-Format-Independent-Numerical-A-Set-Mishra-Mitra/72e6b14c081ad3e6a1c295b60e5c834837e6b304,8,Title,0,Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks.,"['Towards', 'Question', 'Format', 'Independent', 'Numerical', 'Reasoning:', 'A', 'Set', 'of', 'Prerequisite', 'Tasks.']",,,
72,"Integrating distributed representations with symbolic operations is essential for reading comprehension requiring complex reasoning, such as counting, sorting and arithmetics, but most existing approaches are hard to scale to more domains or more complex reasoning.",,,S,"['S', ',', 'CC', 'S', '.']",,,,,,,[],but,0.0,179.0,182.0,179.0,182.0,"[(179, 182)]","['existing', 'approaches']","['Integrating', 'is', 'are', 'scale']",,,,,,,,,,essential,is essential,['Integrating distributed representations with symbolic operations'],[],0,65,78,65,78,,,,,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,0,"Integrating distributed representations with symbolic operations is essential for reading comprehension requiring complex reasoning, such as counting, sorting and arithmetics, but most existing approaches are hard to scale to more domains or more complex reasoning.","['Integrating', 'distributed', 'representations', 'with', 'symbolic', 'operations', 'is', 'essential', 'for', 'reading', 'comprehension', 'requiring', 'complex', 'reasoning,', 'such', 'as', 'counting,', 'sorting', 'and', 'arithmetics,', 'but', 'most', 'existing', 'approaches', 'are', 'hard', 'to', 'scale', 'to', 'more', 'domains', 'or', 'more', 'complex', 'reasoning.']",,,
73,"In this work, we propose the Neural Symbolic Reader (NeRd), which includes a reader, e.g., BERT, to encode the passage and question, and a programmer, e.g., LSTM, to generate a program that is executed to produce the answer.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,15.0,17.0,15.0,17.0,"[(15, 17)]",propose,0.0,18.0,25.0,18.0,25.0,"[(18, 25)]",['we'],['propose'],,,,,,,,,,propose,propose,"['we', 'the Neural Symbolic Reader ( NeRd ) a reader , e.g. , BERT ,', ',']","['which includes to encode the passage and question , and a programmer , e.g. , LSTM , to generate a program that is executed to produce the answer .']",0,18,26,18,26,,,,,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,1,"In this work, we propose the Neural Symbolic Reader (NeRd), which includes a reader, e.g., BERT, to encode the passage and question, and a programmer, e.g., LSTM, to generate a program that is executed to produce the answer.","['In', 'this', 'work,', 'we', 'propose', 'the', 'Neural', 'Symbolic', 'Reader', '(NeRd),', 'which', 'includes', 'a', 'reader,', 'e.g.,', 'BERT,', 'to', 'encode', 'the', 'passage', 'and', 'question,', 'and', 'a', 'programmer,', 'e.g.,', 'LSTM,', 'to', 'generate', 'a', 'program', 'that', 'is', 'executed', 'to', 'produce', 'the', 'answer.']",,,
74,"Compared to previous works, NeRd is more scalable in two aspects: (1) domain-agnostic, i.e., the same neural architecture works for different domains; (2) compositional, i.e., when needed, complex programs can be generated by recursively applying the predefined operators, which become executable and interpretable representations for more complex reasoning.",,,S,"['PP', ',', 'NP', 'VP', '.']",NeRd,0.0,29.0,33.0,29.0,33.0,"[(29, 33)]",is,0.0,34.0,36.0,34.0,36.0,"[(34, 36)]",['NeRd'],['is'],,,,,,,,,,scalable,is scalable,[],"[': ( 1 ) domain - agnostic , i.e. , the same neural architecture works for different domains', '( 2 ) compositional , i.e. , when needed , complex programs can be generated by recursively applying the predefined operators , which become executable and interpretable representations for more complex reasoning .']",0,34,51,34,51,,,,,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,2,"Compared to previous works, NeRd is more scalable in two aspects: (1) domain-agnostic, i.e., the same neural architecture works for different domains; (2) compositional, i.e., when needed, complex programs can be generated by recursively applying the predefined operators, which become executable and interpretable representations for more complex reasoning.","['Compared', 'to', 'previous', 'works,', 'NeRd', 'is', 'more', 'scalable', 'in', 'two', 'aspects:', '(1)', 'domain-agnostic,', 'i.e.,', 'the', 'same', 'neural', 'architecture', 'works', 'for', 'different', 'domains;', '(2)', 'compositional,', 'i.e.,', 'when', 'needed,', 'complex', 'programs', 'can', 'be', 'generated', 'by', 'recursively', 'applying', 'the', 'predefined', 'operators,', 'which', 'become', 'executable', 'and', 'interpretable', 'representations', 'for', 'more', 'complex', 'reasoning.']",,,
75,"Furthermore, to overcome the challenge of training NeRd with weak supervision, we apply data augmentation techniques and hard Expectation-Maximization (EM) with thresholding.",,,S,"['ADVP', ',', 'S', ',', 'NP', 'VP', '.']",we,0.0,81.0,83.0,81.0,83.0,"[(81, 83)]",apply and,0.0,84.0,122.0,84.0,122.0,"[(84, 89), (119, 122)]",['we'],"['overcome', 'apply']",,,,,,,,,,apply,apply,[],['data augmentation techniques and hard Expectation - Maximization ( EM ) with thresholding'],0,84,90,84,90,,,,,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,3,"Furthermore, to overcome the challenge of training NeRd with weak supervision, we apply data augmentation techniques and hard Expectation-Maximization (EM) with thresholding.","['Furthermore,', 'to', 'overcome', 'the', 'challenge', 'of', 'training', 'NeRd', 'with', 'weak', 'supervision,', 'we', 'apply', 'data', 'augmentation', 'techniques', 'and', 'hard', 'Expectation-Maximization', '(EM)', 'with', 'thresholding.']",,,
76,On,DROP,", a challenging reading comprehension dataset that requires discrete reasoning , NeRd achieves 2.5 % /1.8 % absolute improvement over the state-of-the-art on EM/F1 metrics .",S,"['PP', 'NP', 'VP', '.']",NeRd,2.0,89.0,93.0,81.0,85.0,"[(89, 93)]",achieves,2.0,94.0,102.0,86.0,94.0,"[(94, 102)]",['NeRd'],['achieves'],,,,,,,,,,comprehension,comprehension,"['a challenging', 'reading']","['On DROP , dataset that requires discrete reasoning , NeRd achieves 2.5 % /1.8 % absolute improvement over the state - of - the - art on EM / F1 metrics .']",2,32,46,24,38,challenging,a,"['NOUN', 'ADP', 'NOUN']","['DROP', 'On', 'dataset']",https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,4,"On DROP, a challenging reading comprehension dataset that requires discrete reasoning, NeRd achieves 2.5%/1.8% absolute improvement over the state-of-the-art on EM/F1 metrics.","['On', 'DROP', ',', 'a', 'challenging', 'reading', 'comprehension', 'dataset', 'that', 'requires', 'discrete', 'reasoning', ',', 'NeRd', 'achieves', '2.5', '%', '/1.8', '%', 'absolute', 'improvement', 'over', 'the', 'state-of-the-art', 'on', 'EM/F1', 'metrics', '.']","(1, 2)","(2, 6)",0.0
77,"With the same architecture, NeRd significantly outperforms the baselines on MathQA, a math problem benchmark that requires multiple steps of reasoning, by 25.5% absolute increment on accuracy when trained on all the annotated programs.",,,S,"['PP', ',', 'NP', 'VP', '.']",NeRd,0.0,29.0,33.0,29.0,33.0,"[(29, 33)]",outperforms,0.0,48.0,59.0,48.0,59.0,"[(48, 59)]",['NeRd'],['outperforms'],,,,,,,,,,outperforms,outperforms,"['the same architecture', ', NeRd', 'the baselines on MathQA']",[],0,48,60,48,60,,,,,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,5,"With the same architecture, NeRd significantly outperforms the baselines on MathQA, a math problem benchmark that requires multiple steps of reasoning, by 25.5% absolute increment on accuracy when trained on all the annotated programs.","['With', 'the', 'same', 'architecture,', 'NeRd', 'significantly', 'outperforms', 'the', 'baselines', 'on', 'MathQA,', 'a', 'math', 'problem', 'benchmark', 'that', 'requires', 'multiple', 'steps', 'of', 'reasoning,', 'by', '25.5%', 'absolute', 'increment', 'on', 'accuracy', 'when', 'trained', 'on', 'all', 'the', 'annotated', 'programs.']",,,
78,"More importantly, NeRd still beats the baselines even when only 20% of the program annotations are given.",,,S,"['ADVP', ',', 'NP', 'ADVP', 'VP', '.']",NeRd,0.0,19.0,23.0,19.0,23.0,"[(19, 23)]",beats,0.0,30.0,35.0,30.0,35.0,"[(30, 35)]",['NeRd'],['beats'],,,,,,,,,,beats,beats,['still'],['the baselines'],0,30,36,30,36,,,,,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Abstract,6,"More importantly, NeRd still beats the baselines even when only 20% of the program annotations are given.","['More', 'importantly,', 'NeRd', 'still', 'beats', 'the', 'baselines', 'even', 'when', 'only', '20%', 'of', 'the', 'program', 'annotations', 'are', 'given.']",,,
79,Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension.,,,NP,"['NP', ':', 'NP', '.']",Neural Symbolic Reader Scalable Integration,0.0,0.0,45.0,0.0,45.0,"[(0, 6), (7, 15), (16, 22), (25, 33), (34, 45)]",,,,,,,[],"['Neural', 'Symbolic', 'Reader', 'Scalable', 'Integration']",[],,,,,,,,,,Symbolic,Symbolic,['Neural .'],"['Reader Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension', ':']",0,7,16,7,16,,,,,https://www.semanticscholar.org/paper/Neural-Symbolic-Reader%3A-Scalable-Integration-of-and-Chen-Liang/b9a5aa5db8836744ff2073e8368520b7a614049f,9,Title,0,Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension.,"['Neural', 'Symbolic', 'Reader:', 'Scalable', 'Integration', 'of', 'Distributed', 'and', 'Symbolic', 'Representations', 'for', 'Reading', 'Comprehension.']",,,
80,A key component of successfully reading a passage of text is the ability to apply knowledge gained from the passage to a new situation.,,,S,"['NP', 'VP', '.']",A key component,0.0,0.0,15.0,0.0,15.0,"[(0, 1), (2, 5), (6, 15)]",is,0.0,58.0,60.0,58.0,60.0,"[(58, 60)]","['A', 'key', 'component']",['is'],,,,,,,,,,the,is the,['A key component of successfully reading a passage of text'],['ability to apply knowledge gained from the passage to a new situation'],0,58,65,58,65,,,,,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,0,A key component of successfully reading a passage of text is the ability to apply knowledge gained from the passage to a new situation.,"['A', 'key', 'component', 'of', 'successfully', 'reading', 'a', 'passage', 'of', 'text', 'is', 'the', 'ability', 'to', 'apply', 'knowledge', 'gained', 'from', 'the', 'passage', 'to', 'a', 'new', 'situation.']",,,
81,"In order to facilitate progress on this kind of reading, we present ROPES, a challenging benchmark for reading comprehension targeting Reasoning Over Paragraph Effects in Situations.",,,S,"['SBAR', ',', 'NP', 'VP', '.']",we,0.0,58.0,60.0,58.0,60.0,"[(58, 60)]",present,0.0,61.0,68.0,61.0,68.0,"[(61, 68)]",['we'],['present'],,,,,,,,,,present,present,"['order to facilitate progress on', 'this kind of reading', ',', 'we']","['ROPES , a challenging benchmark for reading comprehension targeting Reasoning Over Paragraph Effects in Situations']",0,61,69,61,69,,,,,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,1,"In order to facilitate progress on this kind of reading, we present ROPES, a challenging benchmark for reading comprehension targeting Reasoning Over Paragraph Effects in Situations.","['In', 'order', 'to', 'facilitate', 'progress', 'on', 'this', 'kind', 'of', 'reading,', 'we', 'present', 'ROPES,', 'a', 'challenging', 'benchmark', 'for', 'reading', 'comprehension', 'targeting', 'Reasoning', 'Over', 'Paragraph', 'Effects', 'in', 'Situations.']",,,
82,"We target expository language describing causes and effects (e.g., ""animal pollinators increase efficiency of fertilization in flowers""), as they have clear implications for new situations.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",target,0.0,3.0,9.0,3.0,9.0,"[(3, 9)]",['We'],['target'],,,,,,,,,,implications,implications,"['We target expository language describing causes and effects ( e.g. , "" animal pollinators increase efficiency of fertilization in flowers "" ) , as they have clear .']",[],0,163,176,163,176,,,,,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,2,"We target expository language describing causes and effects (e.g., ""animal pollinators increase efficiency of fertilization in flowers""), as they have clear implications for new situations.","['We', 'target', 'expository', 'language', 'describing', 'causes', 'and', 'effects', '(e.g.,', '""animal', 'pollinators', 'increase', 'efficiency', 'of', 'fertilization', 'in', 'flowers""),', 'as', 'they', 'have', 'clear', 'implications', 'for', 'new', 'situations.']",,,
83,"A system is presented a background passage containing at least one of these relations, a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the background passage in the context of the situation.",,,S,"['NP', 'VP', '.']",A system,0.0,0.0,8.0,0.0,8.0,"[(0, 1), (2, 8)]",is presented,0.0,9.0,21.0,9.0,21.0,"[(9, 11), (12, 21)]","['A', 'system']","['is', 'presented']",,,,,,,,,,presented,is presented,['A system'],"['a background passage containing at least one of these relations , a novel situation that uses this background , and questions that require reasoning about effects of the relationships in the background passage in the context of the situation']",0,9,22,9,22,,,,,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,3,"A system is presented a background passage containing at least one of these relations, a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the background passage in the context of the situation.","['A', 'system', 'is', 'presented', 'a', 'background', 'passage', 'containing', 'at', 'least', 'one', 'of', 'these', 'relations,', 'a', 'novel', 'situation', 'that', 'uses', 'this', 'background,', 'and', 'questions', 'that', 'require', 'reasoning', 'about', 'effects', 'of', 'the', 'relationships', 'in', 'the', 'background', 'passage', 'in', 'the', 'context', 'of', 'the', 'situation.']",,,
84,"We collect background passages from science textbooks and Wikipedia that contain such phenomena, and ask crowd workers to author situations, questions, and answers, resulting in a 14,102 question dataset.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",collect and ask resulting,0.0,3.0,178.0,3.0,178.0,"[(3, 10), (98, 101), (102, 105), (169, 178)]",['We'],"['collect', 'ask', 'resulting']",,,,,,,,,,We,We,[],"['collect background passages from science textbooks and Wikipedia that contain such phenomena , and ask crowd workers to author situations , questions , and answers , resulting in a 14,102 question dataset .']",0,0,3,0,3,,,,,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,4,"We collect background passages from science textbooks and Wikipedia that contain such phenomena, and ask crowd workers to author situations, questions, and answers, resulting in a 14,102 question dataset.","['We', 'collect', 'background', 'passages', 'from', 'science', 'textbooks', 'and', 'Wikipedia', 'that', 'contain', 'such', 'phenomena,', 'and', 'ask', 'crowd', 'workers', 'to', 'author', 'situations,', 'questions,', 'and', 'answers,', 'resulting', 'in', 'a', '14,102', 'question', 'dataset.']",,,
85,We analyze the challenges of this task and evaluate the performance of state-of-the-art reading comprehension models.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",analyze and evaluate,0.0,3.0,51.0,3.0,51.0,"[(3, 10), (39, 42), (43, 51)]",['We'],"['analyze', 'evaluate']",,,,,,,,,,analyze,analyze,['We'],"['the challenges of this task and', 'evaluate the performance of state - of - the - art reading comprehension models .']",0,3,11,3,11,,,,,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,5,We analyze the challenges of this task and evaluate the performance of state-of-the-art reading comprehension models.,"['We', 'analyze', 'the', 'challenges', 'of', 'this', 'task', 'and', 'evaluate', 'the', 'performance', 'of', 'state-of-the-art', 'reading', 'comprehension', 'models.']",,,
86,"The best model performs only slightly better than randomly guessing an answer of the correct type, at 51.9% F1, well below the human performance of 89.0%.",,,S,"['NP', 'VP', '.']",The best model,0.0,0.0,14.0,0.0,14.0,"[(0, 3), (4, 8), (9, 14)]",performs,0.0,15.0,23.0,15.0,23.0,"[(15, 23)]","['The', 'best', 'model']",['performs'],,,,,,,,,,performs,performs,['The best model'],"['only slightly better than randomly guessing an answer of the correct type , at 51.9 % F1 , well below the human performance of 89.0 % .']",0,15,24,15,24,,,,,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Abstract,6,"The best model performs only slightly better than randomly guessing an answer of the correct type, at 51.9% F1, well below the human performance of 89.0%.","['The', 'best', 'model', 'performs', 'only', 'slightly', 'better', 'than', 'randomly', 'guessing', 'an', 'answer', 'of', 'the', 'correct', 'type,', 'at', '51.9%', 'F1,', 'well', 'below', 'the', 'human', 'performance', 'of', '89.0%.']",,,
87,Reasoning Over Paragraph Effects in Situations.,,,S,"['VP', '.']",,,,,,,[],Reasoning,0.0,0.0,9.0,0.0,9.0,"[(0, 9)]",[],['Reasoning'],,,,,,,,,,Paragraph,Paragraph,['Over'],[],0,15,25,15,25,,,,,https://www.semanticscholar.org/paper/Reasoning-Over-Paragraph-Effects-in-Situations-Lin-Tafjord/2ebb01d08022a52c1025302379873dedb5100035,10,Title,0,Reasoning Over Paragraph Effects in Situations.,"['Reasoning', 'Over', 'Paragraph', 'Effects', 'in', 'Situations.']",,,
88,"Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers.",,,S,"['S', ',', 'CC', 'S', '.']",,,,,,,[],but,0.0,89.0,92.0,89.0,92.0,"[(89, 92)]","['comprehension', 'models', 'it']","['have', 'been', 'applied', 'is']",,,,,,,,,,applied,applied,[],[],0,52,60,52,60,,,,,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,0,"Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers.","['Reading', 'comprehension', 'models', 'have', 'been', 'successfully', 'applied', 'to', 'extractive', 'text', 'answers,', 'but', 'it', 'is', 'unclear', 'how', 'best', 'to', 'generalize', 'these', 'models', 'to', 'abstractive', 'numerical', 'answers.']",,,
89,We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",enable,0.0,3.0,9.0,3.0,9.0,"[(3, 9)]",['We'],['enable'],,,,,,,,,,enable,enable,['We'],"['a BERT - based reading comprehension model to perform lightweight numerical reasoning', '.']",0,3,10,3,10,,,,,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,1,We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning.,"['We', 'enable', 'a', 'BERT-based', 'reading', 'comprehension', 'model', 'to', 'perform', 'lightweight', 'numerical', 'reasoning.']",,,
90,We augment the model with a predefined set of executable 'programs' which encompass simple arithmetic as well as extraction.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",augment,0.0,3.0,10.0,3.0,10.0,"[(3, 10)]",['We'],['augment'],,,,,,,,,,as,as,"['which encompass', 'simple arithmetic', 'as well']",['extraction'],0,112,115,112,115,,,,,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,2,We augment the model with a predefined set of executable 'programs' which encompass simple arithmetic as well as extraction.,"['We', 'augment', 'the', 'model', 'with', 'a', 'predefined', 'set', 'of', 'executable', ""'programs'"", 'which', 'encompass', 'simple', 'arithmetic', 'as', 'well', 'as', 'extraction.']",,,
91,"Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it.",,,S,"['PP', ',', 'NP', 'VP', '.']",the model,0.0,61.0,70.0,61.0,70.0,"[(61, 64), (65, 70)]",can pick and execute,0.0,71.0,101.0,71.0,101.0,"[(71, 74), (75, 79), (90, 93), (94, 101)]","['the', 'model']","['can', 'pick', 'execute']",,,,,,,,,,pick,pick,"['Rather than having to learn to manipulate numbers directly', 'can']","[', the model a program', 'and execute it', '.']",0,75,80,75,80,,,,,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,3,"Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it.","['Rather', 'than', 'having', 'to', 'learn', 'to', 'manipulate', 'numbers', 'directly,', 'the', 'model', 'can', 'pick', 'a', 'program', 'and', 'execute', 'it.']",,,
92,On the recent Discrete Reasoning Over Passages (,DROP,") dataset , designed to challenge reading comprehension models , we show a 33 % absolute improvement by adding shallow programs .",S,"['PP', 'NP', 'VP', '.']",we,2.0,119.0,121.0,65.0,67.0,"[(119, 121)]",show,2.0,122.0,126.0,68.0,72.0,"[(122, 126)]",['we'],['show'],show,"['we', ',']",['a 33 % absolute improvement'],1.0,2.0,122.0,127.0,68.0,73.0,show,show,"['we', ',']",['a 33 % absolute improvement'],2,122,127,68,73,designed,dataset,"['PROPN', 'NOUN', 'ADP']","['DROP', 'dataset', 'On']",https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,4,"On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a 33% absolute improvement by adding shallow programs.","['On', 'the', 'recent', 'Discrete', 'Reasoning', 'Over', 'Passages', '(', 'DROP', ')', 'dataset', ',', 'designed', 'to', 'challenge', 'reading', 'comprehension', 'models', ',', 'we', 'show', 'a', '33', '%', 'absolute', 'improvement', 'by', 'adding', 'shallow', 'programs', '.']","(8, 9)","(48, 52)",0.0
93,"The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.",,,S,"['NP', 'VP', '.']",The model,0.0,0.0,9.0,0.0,9.0,"[(0, 3), (4, 9)]",can learn,0.0,10.0,19.0,10.0,19.0,"[(10, 13), (14, 19)]","['The', 'model']","['can', 'learn', 'predict']",,,,,,,,,,learn,can learn,['The model'],['.'],0,10,20,10,20,,,,,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Abstract,5,"The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.","['The', 'model', 'can', 'learn', 'to', 'predict', 'new', 'operations', 'when', 'appropriate', 'in', 'a', 'math', 'word', 'problem', 'setting', '(Roy', 'and', 'Roth,', '2015)', 'with', 'very', 'few', 'training', 'examples.']",,,
94,Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension.,,,S,"['VBG', 'NP', 'NP', '.']",BERT a Calculator,0.0,7.0,24.0,7.0,24.0,"[(7, 11), (12, 13), (14, 24)]",Giving,0.0,0.0,6.0,0.0,6.0,"[(0, 6)]","['BERT', 'a', 'Calculator']",['Giving'],,,,,,,,,,BERT,BERT,['Giving .'],"['a', 'Calculator', ':', 'Finding Operations and Arguments with Reading Comprehension']",0,7,12,7,12,,,,,https://www.semanticscholar.org/paper/Giving-BERT-a-Calculator%3A-Finding-Operations-and-Andor-He/52fa450740913a6cdcb4d9395b45e203f46cab79,11,Title,0,Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension.,"['Giving', 'BERT', 'a', 'Calculator:', 'Finding', 'Operations', 'and', 'Arguments', 'with', 'Reading', 'Comprehension.']",,,
95,"Machine reading comprehension (MRC) aims to teach machines to read and comprehend human languages, which is a long-standing goal of natural language processing (NLP).",,,S,"['NP', 'VP', '.']",Machine reading comprehension MRC,0.0,0.0,35.0,0.0,35.0,"[(0, 7), (8, 15), (16, 29), (32, 35)]",aims,0.0,38.0,42.0,38.0,42.0,"[(38, 42)]","['Machine', 'reading', 'comprehension', 'MRC']","['aims', 'teach', 'read', 'comprehend']",,,,,,,,,,reading,reading,[],"['Machine comprehension ( MRC ) aims to teach machines to read and comprehend human languages , which is a long - standing goal of natural language processing ( NLP ) .']",0,8,16,8,16,,,,,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,0,"Machine reading comprehension (MRC) aims to teach machines to read and comprehend human languages, which is a long-standing goal of natural language processing (NLP).","['Machine', 'reading', 'comprehension', '(MRC)', 'aims', 'to', 'teach', 'machines', 'to', 'read', 'and', 'comprehend', 'human', 'languages,', 'which', 'is', 'a', 'long-standing', 'goal', 'of', 'natural', 'language', 'processing', '(NLP).']",,,
96,"With the burst of deep neural networks and the evolution of contextualized language models (CLMs), the research of MRC has experienced two significant breakthroughs.",,,S,"['PP', ',', 'NP', 'VP', '.']",the research,0.0,102.0,114.0,102.0,114.0,"[(102, 105), (106, 114)]",has experienced,0.0,122.0,137.0,122.0,137.0,"[(122, 125), (126, 137)]","['the', 'research']","['has', 'experienced']",,,,,,,,,,experienced,experienced,"['the research of MRC', 'With the burst of deep neural networks and the evolution of contextualized language models ( CLMs ) , has']","['two significant breakthroughs', '.']",0,126,138,126,138,,,,,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,1,"With the burst of deep neural networks and the evolution of contextualized language models (CLMs), the research of MRC has experienced two significant breakthroughs.","['With', 'the', 'burst', 'of', 'deep', 'neural', 'networks', 'and', 'the', 'evolution', 'of', 'contextualized', 'language', 'models', '(CLMs),', 'the', 'research', 'of', 'MRC', 'has', 'experienced', 'two', 'significant', 'breakthroughs.']",,,
97,"MRC and CLM, as a phenomenon, have a great impact on the NLP community.",,,S,"['NP', ',', 'PP', ',', 'VP', '.']",MRC CLM,0.0,0.0,11.0,0.0,11.0,"[(0, 3), (8, 11)]",have,0.0,32.0,36.0,32.0,36.0,"[(32, 36)]","['MRC', 'CLM']",['have'],,,,,,,,,,have,have,"['MRC and CLM , as a phenomenon', ',']",['a great impact on the NLP community'],0,32,37,32,37,,,,,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,2,"MRC and CLM, as a phenomenon, have a great impact on the NLP community.","['MRC', 'and', 'CLM,', 'as', 'a', 'phenomenon,', 'have', 'a', 'great', 'impact', 'on', 'the', 'NLP', 'community.']",,,
98,"In this survey, we provide a comprehensive and comparative review on MRC covering overall research topics about 1) the origin and development of MRC and CLM, with a particular focus on the role of CLMs; 2) the impact of MRC and CLM to the NLP community; 3) the definition, datasets, and evaluation of MRC; 4) general MRC architecture and technical methods in the view of two-stage Encoder-Decoder solving architecture from the insights of the cognitive process of humans; 5) previous highlights, emerging topics, and our empirical analysis, among which we especially focus on what works in different periods of MRC researches.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,17.0,19.0,17.0,19.0,"[(17, 19)]",provide covering,0.0,20.0,82.0,20.0,82.0,"[(20, 27), (74, 82)]",['we'],"['provide', 'covering']",,,,,,,,,,in,in,"['and', '5 ) previous highlights , emerging our empirical analysis', 'what', 'In this survey , we provide a comprehensive and comparative review on MRC covering overall research topics about 1 ) the origin and development of MRC and CLM , with a particular focus on the role of CLMs ; 2 ) the impact of MRC and CLM to the NLP community ; 3 ) the definition , datasets , and evaluation of MRC ; 4 ) general MRC architecture and technical methods in the view of two - stage Encoder - Decoder solving architecture from the insights of the cognitive process of humans ; which we especially focus on works']","['topics , , among different periods of MRC researches .']",0,607,610,607,610,,,,,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,3,"In this survey, we provide a comprehensive and comparative review on MRC covering overall research topics about 1) the origin and development of MRC and CLM, with a particular focus on the role of CLMs; 2) the impact of MRC and CLM to the NLP community; 3) the definition, datasets, and evaluation of MRC; 4) general MRC architecture and technical methods in the view of two-stage Encoder-Decoder solving architecture from the insights of the cognitive process of humans; 5) previous highlights, emerging topics, and our empirical analysis, among which we especially focus on what works in different periods of MRC researches.","['In', 'this', 'survey,', 'we', 'provide', 'a', 'comprehensive', 'and', 'comparative', 'review', 'on', 'MRC', 'covering', 'overall', 'research', 'topics', 'about', '1)', 'the', 'origin', 'and', 'development', 'of', 'MRC', 'and', 'CLM,', 'with', 'a', 'particular', 'focus', 'on', 'the', 'role', 'of', 'CLMs;', '2)', 'the', 'impact', 'of', 'MRC', 'and', 'CLM', 'to', 'the', 'NLP', 'community;', '3)', 'the', 'definition,', 'datasets,', 'and', 'evaluation', 'of', 'MRC;', '4)', 'general', 'MRC', 'architecture', 'and', 'technical', 'methods', 'in', 'the', 'view', 'of', 'two-stage', 'Encoder-Decoder', 'solving', 'architecture', 'from', 'the', 'insights', 'of', 'the', 'cognitive', 'process', 'of', 'humans;', '5)', 'previous', 'highlights,', 'emerging', 'topics,', 'and', 'our', 'empirical', 'analysis,', 'among', 'which', 'we', 'especially', 'focus', 'on', 'what', 'works', 'in', 'different', 'periods', 'of', 'MRC', 'researches.']",,,
99,We propose a full-view categorization and new taxonomies on these topics.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",propose,0.0,3.0,10.0,3.0,10.0,"[(3, 10)]",['We'],['propose'],,,,,,,,,,propose,propose,['We'],['a full - view categorization and new taxonomies on these topics'],0,3,11,3,11,,,,,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,4,We propose a full-view categorization and new taxonomies on these topics.,"['We', 'propose', 'a', 'full-view', 'categorization', 'and', 'new', 'taxonomies', 'on', 'these', 'topics.']",,,
100,The primary views we have arrived at are that 1) MRC boosts the progress from language processing to understanding; 2) the rapid improvement of MRC systems greatly benefits from the development of CLMs; 3) the theme of MRC is gradually moving from shallow text matching to cognitive reasoning.,,,S,"['NP', 'VP', '.']",The primary views,0.0,0.0,17.0,0.0,17.0,"[(0, 3), (4, 11), (12, 17)]",are,0.0,37.0,40.0,37.0,40.0,"[(37, 40)]","['The', 'primary', 'views']",['are'],,,,,,,,,,are,are,"['The primary views we have arrived at', '1 ) MRC boosts the progress from language processing to understanding ; 2 ) the rapid improvement of MRC systems greatly benefits from the development of CLMs']",['3 ) the theme of MRC is gradually moving from shallow text matching to cognitive reasoning .'],0,37,41,37,41,,,,,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Abstract,5,The primary views we have arrived at are that 1) MRC boosts the progress from language processing to understanding; 2) the rapid improvement of MRC systems greatly benefits from the development of CLMs; 3) the theme of MRC is gradually moving from shallow text matching to cognitive reasoning.,"['The', 'primary', 'views', 'we', 'have', 'arrived', 'at', 'are', 'that', '1)', 'MRC', 'boosts', 'the', 'progress', 'from', 'language', 'processing', 'to', 'understanding;', '2)', 'the', 'rapid', 'improvement', 'of', 'MRC', 'systems', 'greatly', 'benefits', 'from', 'the', 'development', 'of', 'CLMs;', '3)', 'the', 'theme', 'of', 'MRC', 'is', 'gradually', 'moving', 'from', 'shallow', 'text', 'matching', 'to', 'cognitive', 'reasoning.']",,,
101,Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond.,,,NP,"['NP', ':', 'NP', '.']",Machine Reading Comprehension The Role,0.0,0.0,40.0,0.0,40.0,"[(0, 7), (8, 15), (16, 29), (32, 35), (36, 40)]",,,,,,,[],"['Machine', 'Reading', 'Comprehension', 'The', 'Role']",[],,,,,,,,,,Comprehension,Comprehension,['Machine Reading'],['The Role of Contextualized Language Models and Beyond .'],0,16,30,16,30,,,,,https://www.semanticscholar.org/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af,12,Title,0,Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond.,"['Machine', 'Reading', 'Comprehension:', 'The', 'Role', 'of', 'Contextualized', 'Language', 'Models', 'and', 'Beyond.']",,,
102,Many question answering (QA) tasks only provide weak supervision for how the answer should be computed.,,,S,"['NP', 'ADVP', 'VP', '.']",Many question QA tasks,0.0,0.0,36.0,0.0,36.0,"[(0, 4), (5, 13), (26, 28), (31, 36)]",provide,0.0,42.0,49.0,42.0,49.0,"[(42, 49)]","['Many', 'question', 'QA', 'tasks']",['provide'],,,,,,,,,,question,question,['Many'],[],0,5,14,5,14,,,,,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,0,Many question answering (QA) tasks only provide weak supervision for how the answer should be computed.,"['Many', 'question', 'answering', '(QA)', 'tasks', 'only', 'provide', 'weak', 'supervision', 'for', 'how', 'the', 'answer', 'should', 'be', 'computed.']",,,
103,"For example , TriviaQA answers are entities that can be mentioned multiple times in supporting documents , while",DROP,answers can be computed by deriving many different equations from numbers in the reference text .,S,"['PP', ',', 'NP', 'VP', '.']",TriviaQA answers,0.0,14.0,30.0,14.0,30.0,"[(14, 22), (23, 30)]",are,0.0,31.0,34.0,31.0,34.0,"[(31, 34)]","['TriviaQA', 'answers']",['are'],can be computed,['DROP answers'],[],1.0,2.0,126.0,142.0,8.0,24.0,are,are,"['For example , TriviaQA answers entities that can be mentioned multiple times in supporting documents , while DROP answers can be computed by deriving many different equations from numbers in the reference text .']",[],0,31,35,31,35,be,answers,"['NOUN', 'VERB', 'VERB']","['DROP', 'computed', 'mentioned']",https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,1,"For example, TriviaQA answers are entities that can be mentioned multiple times in supporting documents, while DROP answers can be computed by deriving many different equations from numbers in the reference text.","['For', 'example', ',', 'TriviaQA', 'answers', 'are', 'entities', 'that', 'can', 'be', 'mentioned', 'multiple', 'times', 'in', 'supporting', 'documents', ',', 'while', 'DROP', 'answers', 'can', 'be', 'computed', 'by', 'deriving', 'many', 'different', 'equations', 'from', 'numbers', 'in', 'the', 'reference', 'text', '.']","(18, 19)","(112, 116)",0.0
104,"In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible ""solutions"" (e.g.",,,S,"['PP', ',', 'NP', 'VP']",we,0.0,16.0,18.0,16.0,18.0,"[(16, 18)]",show,0.0,19.0,23.0,19.0,23.0,"[(19, 23)]",['we'],['show'],,,,,,,,,,possible,is possible,"[', we show', 'it']",[],0,27,39,27,39,,,,,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,2,"In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible ""solutions"" (e.g.","['In', 'this', 'paper,', 'we', 'show', 'it', 'is', 'possible', 'to', 'convert', 'such', 'tasks', 'into', 'discrete', 'latent', 'variable', 'learning', 'problems', 'with', 'a', 'precomputed,', 'task-specific', 'set', 'of', 'possible', '""solutions""', '(e.g.']",,,
105,different mentions or equations) that contains one correct option.,,,NP,"['NP', '-RRB-', 'SBAR', '.']",different mentions equations,0.0,0.0,31.0,0.0,31.0,"[(0, 9), (10, 18), (22, 31)]",,,,,,,[],"['different', 'mentions', 'equations']",[],,,,,,,,,,mentions,mentions,['different'],['equations ) that contains one correct option .'],0,10,19,10,19,,,,,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,3,different mentions or equations) that contains one correct option.,"['different', 'mentions', 'or', 'equations)', 'that', 'contains', 'one', 'correct', 'option.']",,,
106,We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update.,,,S,"['NP', 'ADVP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",develop,0.0,8.0,15.0,8.0,15.0,"[(8, 15)]",['We'],['develop'],,,,,,,,,,develop,develop,"['We', 'then']",['a hard EM learning scheme that computes gradients relative to the most likely solution at each update .'],0,8,16,8,16,,,,,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,4,We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update.,"['We', 'then', 'develop', 'a', 'hard', 'EM', 'learning', 'scheme', 'that', 'computes', 'gradients', 'relative', 'to', 'the', 'most', 'likely', 'solution', 'at', 'each', 'update.']",,,
107,"Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2--10%, and achieves the state-of-the-art on five of them.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,25.0,27.0,25.0,27.0,"[(25, 27)]",show,0.0,28.0,32.0,28.0,32.0,"[(28, 32)]",['we'],['show'],,,,,,,,,,show,show,"[',', 'we']",[],0,28,33,28,33,,,,,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,5,"Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2--10%, and achieves the state-of-the-art on five of them.","['Despite', 'its', 'simplicity,', 'we', 'show', 'that', 'this', 'approach', 'significantly', 'outperforms', 'previous', 'methods', 'on', 'six', 'QA', 'tasks,', 'including', 'absolute', 'gains', 'of', '2--10%,', 'and', 'achieves', 'the', 'state-of-the-art', 'on', 'five', 'of', 'them.']",,,
108,"Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.",,,S,"['S', 'VP', '.']",,,,,,,[],is,0.0,61.0,63.0,61.0,63.0,"[(61, 63)]",[],"['Using', 'is']",,,,,,,,,,key,is key,"['Using', 'hard updates instead of maximizing marginal likelihood']","['these results', 'as it encourages the model to find the one correct answer , which we show through detailed qualitative analysis']",0,61,68,61,68,,,,,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Abstract,6,"Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.","['Using', 'hard', 'updates', 'instead', 'of', 'maximizing', 'marginal', 'likelihood', 'is', 'key', 'to', 'these', 'results', 'as', 'it', 'encourages', 'the', 'model', 'to', 'find', 'the', 'one', 'correct', 'answer,', 'which', 'we', 'show', 'through', 'detailed', 'qualitative', 'analysis.']",,,
109,A Discrete Hard EM Approach for Weakly Supervised Question Answering.,,,NP,"['NP', 'PP', '.']",A Discrete Hard EM Approach,0.0,0.0,27.0,0.0,27.0,"[(0, 1), (2, 10), (11, 15), (16, 18), (19, 27)]",,,,,,,[],"['A', 'Discrete', 'Hard', 'EM', 'Approach']",[],,,,,,,,,,Approach,Approach,"['A Discrete Hard', 'EM']",[],0,19,28,19,28,,,,,https://www.semanticscholar.org/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c,13,Title,0,A Discrete Hard EM Approach for Weakly Supervised Question Answering.,"['A', 'Discrete', 'Hard', 'EM', 'Approach', 'for', 'Weakly', 'Supervised', 'Question', 'Answering.']",,,
110,Innovations in annotation methodology have been a propellant for Reading Comprehension (RC) datasets and models.,,,S,"['NP', 'VP', '.']",Innovations,0.0,0.0,11.0,0.0,11.0,"[(0, 11)]",have been,0.0,38.0,47.0,38.0,47.0,"[(38, 42), (43, 47)]",['Innovations'],"['have', 'been']",,,,,,,,,,(,(,"['Innovations in annotation methodology have been a propellant for', 'Reading Comprehension RC ) datasets and models .']",[],0,87,89,87,89,,,,,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,0,Innovations in annotation methodology have been a propellant for Reading Comprehension (RC) datasets and models.,"['Innovations', 'in', 'annotation', 'methodology', 'have', 'been', 'a', 'propellant', 'for', 'Reading', 'Comprehension', '(RC)', 'datasets', 'and', 'models.']",,,
111,"One recent trend to challenge current RC models is to involve a model in the annotation process: humans create questions adversarially, such that the model fails to answer them correctly.",,,S,"['S', ':', 'S', '.']",,,,,,,[],,,,,,,[],"['recent', 'trend', 'humans']","['is', 'involve', 'create']",,,,,,,,,,to,is to,[],"['involve a model in the annotation process : humans create questions adversarially , such that the model fails to answer them correctly .']",0,48,54,48,54,,,,,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,1,"One recent trend to challenge current RC models is to involve a model in the annotation process: humans create questions adversarially, such that the model fails to answer them correctly.","['One', 'recent', 'trend', 'to', 'challenge', 'current', 'RC', 'models', 'is', 'to', 'involve', 'a', 'model', 'in', 'the', 'annotation', 'process:', 'humans', 'create', 'questions', 'adversarially,', 'such', 'that', 'the', 'model', 'fails', 'to', 'answer', 'them', 'correctly.']",,,
112,"In this work we investigate this annotation approach and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop.",,,S,"['PP', 'NP', 'VP', '.']",we,0.0,13.0,15.0,13.0,15.0,"[(13, 15)]",investigate and apply,0.0,16.0,62.0,16.0,62.0,"[(16, 27), (53, 56), (57, 62)]",['we'],"['investigate', 'apply', 'collecting']",,,,,,,,,,investigate,investigate,"['this work', 'we']","['this annotation approach and apply it in three different settings , collecting a total of 36,000 samples with progressively stronger models in the annotation loop']",0,16,28,16,28,,,,,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,2,"In this work we investigate this annotation approach and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop.","['In', 'this', 'work', 'we', 'investigate', 'this', 'annotation', 'approach', 'and', 'apply', 'it', 'in', 'three', 'different', 'settings,', 'collecting', 'a', 'total', 'of', '36,000', 'samples', 'with', 'progressively', 'stronger', 'models', 'in', 'the', 'annotation', 'loop.']",,,
113,"This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalisation to data collected without a model.",,,S,"['NP', 'VP', '.']",This,0.0,0.0,4.0,0.0,4.0,"[(0, 4)]",allows,0.0,5.0,11.0,5.0,11.0,"[(5, 11)]",['This'],"['allows', 'explore']",,,,,,,,,,allows,allows,['This'],[],0,5,12,5,12,,,,,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,3,"This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalisation to data collected without a model.","['This', 'allows', 'us', 'to', 'explore', 'questions', 'such', 'as', 'the', 'reproducibility', 'of', 'the', 'adversarial', 'effect,', 'transfer', 'from', 'data', 'collected', 'with', 'varying', 'model-in-the-loop', 'strengths,', 'and', 'generalisation', 'to', 'data', 'collected', 'without', 'a', 'model.']",,,
114,"We find that training on adversarially collected samples leads to strong generalisation to non-adversarially collected datasets, yet with progressive deterioration as the model-in-the-loop strength increases.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",find,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",['We'],['find'],,,,,,,,,,find,find,['We'],[],0,3,8,3,8,,,,,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,4,"We find that training on adversarially collected samples leads to strong generalisation to non-adversarially collected datasets, yet with progressive deterioration as the model-in-the-loop strength increases.","['We', 'find', 'that', 'training', 'on', 'adversarially', 'collected', 'samples', 'leads', 'to', 'strong', 'generalisation', 'to', 'non-adversarially', 'collected', 'datasets,', 'yet', 'with', 'progressive', 'deterioration', 'as', 'the', 'model-in-the-loop', 'strength', 'increases.']",,,
115,"Furthermore we find that stronger models can still learn from datasets collected with substantially weaker models in the loop: When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 36.0F1 on questions that it cannot answer when trained on SQuAD - only marginally lower than when trained on data collected using RoBERTa itself.",,,S,"['ADVP', 'NP', 'VP', '.']",we,0.0,12.0,14.0,12.0,14.0,"[(12, 14)]",find,0.0,15.0,19.0,15.0,19.0,"[(15, 19)]",['we'],"['find', 'can', 'learn', 'achieves']",,,,,,,,,,find,find,"['Furthermore', 'we']",[],0,15,20,15,20,,,,,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Abstract,5,"Furthermore we find that stronger models can still learn from datasets collected with substantially weaker models in the loop: When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 36.0F1 on questions that it cannot answer when trained on SQuAD - only marginally lower than when trained on data collected using RoBERTa itself.","['Furthermore', 'we', 'find', 'that', 'stronger', 'models', 'can', 'still', 'learn', 'from', 'datasets', 'collected', 'with', 'substantially', 'weaker', 'models', 'in', 'the', 'loop:', 'When', 'trained', 'on', 'data', 'collected', 'with', 'a', 'BiDAF', 'model', 'in', 'the', 'loop,', 'RoBERTa', 'achieves', '36.0F1', 'on', 'questions', 'that', 'it', 'cannot', 'answer', 'when', 'trained', 'on', 'SQuAD', '-', 'only', 'marginally', 'lower', 'than', 'when', 'trained', 'on', 'data', 'collected', 'using', 'RoBERTa', 'itself.']",,,
116,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension.,,,S,"['VP', '.']",,,,,,,[],Beat,0.0,0.0,4.0,0.0,4.0,"[(0, 4)]",[],['Beat'],,,,,,,,,,Adversarial,Adversarial,['Investigating Human Annotations for Reading Comprehension .'],[],0,28,40,28,40,,,,,https://www.semanticscholar.org/paper/Beat-the-AI%3A-Investigating-Adversarial-Human-for-Bartolo-Roberts/693cce5d9764f9e9e0c9c583bf840ac019e2179f,14,Title,0,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension.,"['Beat', 'the', 'AI:', 'Investigating', 'Adversarial', 'Human', 'Annotations', 'for', 'Reading', 'Comprehension.']",,,
117,"Machine reading comprehension (MRC) is one of the primary challenges in natural language understanding (NLU), its objective is to give the correct answer based on the questions asked from the specified context.",,,S,"['S', ',', 'NP', 'VP', '.']",objective,0.0,119.0,128.0,119.0,128.0,"[(119, 128)]",is,0.0,129.0,131.0,129.0,131.0,"[(129, 131)]","['Machine', 'reading', 'comprehension', 'MRC', 'objective']","['is', 'is', 'give', 'based']",,,,,,,,,,comprehension,comprehension,[],"['(', 'Machine MRC ) is one of the primary challenges in natural language understanding ( NLU ) , its objective is to give the correct answer based on the questions asked from the specified context .']",0,16,30,16,30,,,,,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,0,"Machine reading comprehension (MRC) is one of the primary challenges in natural language understanding (NLU), its objective is to give the correct answer based on the questions asked from the specified context.","['Machine', 'reading', 'comprehension', '(MRC)', 'is', 'one', 'of', 'the', 'primary', 'challenges', 'in', 'natural', 'language', 'understanding', '(NLU),', 'its', 'objective', 'is', 'to', 'give', 'the', 'correct', 'answer', 'based', 'on', 'the', 'questions', 'asked', 'from', 'the', 'specified', 'context.']",,,
118,"Nowadays, attention mechanisms have been widely used in reading comprehension tasks.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",attention mechanisms,0.0,11.0,31.0,11.0,31.0,"[(11, 20), (21, 31)]",have been used,0.0,32.0,53.0,32.0,53.0,"[(32, 36), (37, 41), (49, 53)]","['attention', 'mechanisms']","['have', 'been', 'used']",,,,,,,,,,widely,have been widely,"['Nowadays , attention mechanisms']","['used in reading comprehension tasks', '.']",0,32,49,32,49,,,,,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,1,"Nowadays, attention mechanisms have been widely used in reading comprehension tasks.","['Nowadays,', 'attention', 'mechanisms', 'have', 'been', 'widely', 'used', 'in', 'reading', 'comprehension', 'tasks.']",,,
119,"In this chapter, based on the analysis of two state-of-the-art attention mechanisms, Coattention and Multi-head Attention, the Transformer-based Coattention (TBC) is proposed.",,,S,"['PP', ',', 'VBN', 'PP', '.']",,,,,,,[],based,0.0,18.0,23.0,18.0,23.0,"[(18, 23)]",[],['based'],,,,,,,,,,proposed,proposed,"['Coattention', '-', 'Multi head', 'Attention', ',', 'the Transformer - based Coattention ( TBC )', 'is']",['.'],3,181,190,5,14,,,,,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,2,"In this chapter, based on the analysis of two state-of-the-art attention mechanisms, Coattention and Multi-head Attention, the Transformer-based Coattention (TBC) is proposed.","['In', 'this', 'chapter,', 'based', 'on', 'the', 'analysis', 'of', 'two', 'state-of-the-art', 'attention', 'mechanisms,', 'Coattention', 'and', 'Multi-head', 'Attention,', 'the', 'Transformer-based', 'Coattention', '(TBC)', 'is', 'proposed.']",,,
120,"Furthermore, a general hybrid scheme is proposed to incorporate the TBC into pretrained MRC models with little extra training cost.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",a general hybrid scheme,0.0,14.0,37.0,14.0,37.0,"[(14, 15), (16, 23), (24, 30), (31, 37)]",is proposed,0.0,38.0,49.0,38.0,49.0,"[(38, 40), (41, 49)]","['a', 'general', 'hybrid', 'scheme']","['is', 'proposed', 'incorporate']",,,,,,,,,,proposed,is proposed,[],"['to', 'incorporate the TBC into pretrained MRC models with little extra training cost', '.']",0,38,50,38,50,,,,,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,3,"Furthermore, a general hybrid scheme is proposed to incorporate the TBC into pretrained MRC models with little extra training cost.","['Furthermore,', 'a', 'general', 'hybrid', 'scheme', 'is', 'proposed', 'to', 'incorporate', 'the', 'TBC', 'into', 'pretrained', 'MRC', 'models', 'with', 'little', 'extra', 'training', 'cost.']",,,
121,Our experiments on Stanford Question Answering Dataset ( SQuAD ) and Discrete Reasoning Over the content of Paragraphs (,DROP,) show that our hybrid scheme make models achieve better performance .,S,"['NP', 'VP', '.']",experiments,0.0,4.0,15.0,4.0,15.0,"[(4, 15)]",show,2.0,128.0,132.0,2.0,6.0,"[(128, 132)]",['experiments'],['show'],,,,,,,,,,experiments,experiments,[],[],0,4,16,4,16,show,show,"['PROPN', 'PUNCT', 'ADP']","['DROP', ')', 'of']",https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Abstract,4,Our experiments on Stanford Question Answering Dataset (SQuAD) and Discrete Reasoning Over the content of Paragraphs (DROP) show that our hybrid scheme make models achieve better performance.,"['Our', 'experiments', 'on', 'Stanford', 'Question', 'Answering', 'Dataset', '(', 'SQuAD', ')', 'and', 'Discrete', 'Reasoning', 'Over', 'the', 'content', 'of', 'Paragraphs', '(', 'DROP', ')', 'show', 'that', 'our', 'hybrid', 'scheme', 'make', 'models', 'achieve', 'better', 'performance', '.']","(19, 20)","(120, 124)",0.0
122,Transformer-Based Coattention: Neural Architecture for Reading Comprehension.,,,NP,"['NP', 'HYPH', 'NP', '.']",Transformer Coattention Neural Architecture,0.0,0.0,53.0,0.0,53.0,"[(0, 11), (20, 31), (34, 40), (41, 53)]",,,,,,,[],"['Transformer', 'Coattention', 'Neural', 'Architecture']",[],,,,,,,,,,Transformer,Transformer,[],"['-', 'Based', 'Coattention', ':', 'Neural Architecture for Reading Comprehension']",0,0,12,0,12,,,,,https://www.semanticscholar.org/paper/Transformer-Based-Coattention%3A-Neural-Architecture-Wang-Tang/2c96dc4275480ec7099c9aaaf7b78c220a1a7844,15,Title,0,Transformer-Based Coattention: Neural Architecture for Reading Comprehension.,"['Transformer-Based', 'Coattention:', 'Neural', 'Architecture', 'for', 'Reading', 'Comprehension.']",,,
123,Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information.,,,S,"['NP', 'VP', '.']",Large pre - trained language models LMs,0.0,0.0,41.0,0.0,41.0,"[(0, 5), (6, 9), (10, 11), (12, 19), (20, 28), (29, 35), (38, 41)]",are known,0.0,44.0,53.0,44.0,53.0,"[(44, 47), (48, 53)]","['Large', 'pre', '-', 'trained', 'language', 'models', 'LMs']","['are', 'known', 'encode']",,,,,,,,,,known,known,['Large pre - trained language models ( LMs ) are'],['.'],0,48,54,48,54,,,,,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,0,Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information.,"['Large', 'pre-trained', 'language', 'models', '(LMs)', 'are', 'known', 'to', 'encode', 'substantial', 'amounts', 'of', 'linguistic', 'information.']",,,
124,"However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only.",,,S,"['ADVP', ',', 'NP', 'VBP', 'ADJP', 'HYPH', 'NP', '.']",high level reasoning skills modeling objective,0.0,10.0,131.0,10.0,131.0,"[(10, 14), (17, 22), (23, 32), (33, 39), (113, 121), (122, 131)]",are,0.0,72.0,75.0,72.0,75.0,"[(72, 75)]","['high', 'level', 'reasoning', 'skills', 'modeling', 'objective']",['are'],,,,,,,,,,difficult,are difficult,"['However', ',', 'high - level reasoning skills , such as numerical reasoning ,']",['.'],0,72,86,72,86,,,,,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,1,"However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only.","['However,', 'high-level', 'reasoning', 'skills,', 'such', 'as', 'numerical', 'reasoning,', 'are', 'difficult', 'to', 'learn', 'from', 'a', 'language-modeling', 'objective', 'only.']",,,
125,"Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",models,0.0,24.0,30.0,24.0,30.0,"[(24, 30)]",have used,0.0,55.0,64.0,55.0,64.0,"[(55, 59), (60, 64)]",['models'],"['have', 'used']",,,,,,,,,,used,have used,"[', existing models for numerical reasoning', 'Consequently']","['specialized architectures limited flexibility', '.']",0,55,65,55,65,,,,,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,2,"Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility.","['Consequently,', 'existing', 'models', 'for', 'numerical', 'reasoning', 'have', 'used', 'specialized', 'architectures', 'with', 'limited', 'flexibility.']",,,
126,"In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,15.0,17.0,15.0,17.0,"[(15, 17)]",show,0.0,18.0,22.0,18.0,22.0,"[(18, 22)]",['we'],['show'],,,,,,,,,,show,show,"[',', 'we']",[],0,18,23,18,23,,,,,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,3,"In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.","['In', 'this', 'work,', 'we', 'show', 'that', 'numerical', 'reasoning', 'is', 'amenable', 'to', 'automatic', 'data', 'generation,', 'and', 'thus', 'one', 'can', 'inject', 'this', 'skill', 'into', 'pre-trained', 'LMs,', 'by', 'generating', 'large', 'amounts', 'of', 'data,', 'and', 'training', 'in', 'a', 'multi-task', 'setup.']",,,
127,"We show that pre-training our model , GenBERT , on this data , dramatically improves performance on",DROP,"( 49.3 $ \rightarrow $ 72.3 F1 ) , reaching performance that matches state-of-the-art models of comparable size , while using a simple and general-purpose encoder-decoder architecture .",S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",show,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",['We'],['show'],improves,"['pre - training our model , GenBERT , on this data ,']",[],-1.0,0.0,78.0,87.0,78.0,87.0,show,show,['We'],[],0,3,8,3,8,reaching,49.3,"['PROPN', 'ADP', 'VERB']","['DROP', 'on', 'improves']",https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,4,"We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 $\rightarrow$ 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture.","['We', 'show', 'that', 'pre-training', 'our', 'model', ',', 'GenBERT', ',', 'on', 'this', 'data', ',', 'dramatically', 'improves', 'performance', 'on', 'DROP', '(', '49.3', '$', '\\rightarrow', '$', '72.3', 'F1', ')', ',', 'reaching', 'performance', 'that', 'matches', 'state-of-the-art', 'models', 'of', 'comparable', 'size', ',', 'while', 'using', 'a', 'simple', 'and', 'general-purpose', 'encoder-decoder', 'architecture', '.']","(17, 18)","(99, 103)",0.0
128,"Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",GenBERT,0.0,11.0,18.0,11.0,18.0,"[(11, 18)]",generalizes,0.0,19.0,30.0,19.0,30.0,"[(19, 30)]",['GenBERT'],['generalizes'],,,,,,,,,,problem,problem,"['Moreover , GenBERT generalizes well to math', 'word']","['datasets , while maintaining high performance on standard RC tasks', '.']",0,49,57,49,57,,,,,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,5,"Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks.","['Moreover,', 'GenBERT', 'generalizes', 'well', 'to', 'math', 'word', 'problem', 'datasets,', 'while', 'maintaining', 'high', 'performance', 'on', 'standard', 'RC', 'tasks.']",,,
129,"Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.",,,S,"['NP', 'VP', '.']",approach,0.0,4.0,12.0,4.0,12.0,"[(4, 12)]",provides,0.0,13.0,21.0,13.0,21.0,"[(13, 21)]",['approach'],['provides'],,,,,,,,,,provides,provides,['Our approach'],"['a general recipe for injecting skills into', 'large pre - trained LMs , whenever the skill is amenable to automatic data augmentation', '.']",0,13,22,13,22,,,,,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Abstract,6,"Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.","['Our', 'approach', 'provides', 'a', 'general', 'recipe', 'for', 'injecting', 'skills', 'into', 'large', 'pre-trained', 'LMs,', 'whenever', 'the', 'skill', 'is', 'amenable', 'to', 'automatic', 'data', 'augmentation.']",,,
130,Injecting Numerical Reasoning Skills into Language Models.,,,S,"['VP', '.']",,,,,,,[],Injecting,0.0,0.0,9.0,0.0,9.0,"[(0, 9)]",[],['Injecting'],,,,,,,,,,Numerical,Numerical,['Injecting'],['Reasoning Skills into Language Models'],0,10,20,10,20,,,,,https://www.semanticscholar.org/paper/Injecting-Numerical-Reasoning-Skills-into-Language-Geva-Gupta/3dd61d97827e3f380bf9304101149a3f865051fc,16,Title,0,Injecting Numerical Reasoning Skills into Language Models.,"['Injecting', 'Numerical', 'Reasoning', 'Skills', 'into', 'Language', 'Models.']",,,
131,Standard test sets for supervised learning evaluate in-distribution generalization.,,,NP,"['NP', 'PP', 'VB', 'PP', 'HYPH', 'NP', '.']",Standard test sets distribution generalization,0.0,0.0,84.0,0.0,84.0,"[(0, 8), (9, 13), (14, 18), (57, 69), (70, 84)]",evaluate,0.0,43.0,51.0,43.0,51.0,"[(43, 51)]","['Standard', 'test', 'sets', 'distribution', 'generalization']",['evaluate'],,,,,,,,,,test,test,['Standard'],['sets for supervised learning evaluate in - distribution generalization'],0,9,14,9,14,,,,,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,0,Standard test sets for supervised learning evaluate in-distribution generalization.,"['Standard', 'test', 'sets', 'for', 'supervised', 'learning', 'evaluate', 'in-distribution', 'generalization.']",,,
132,"Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities.",,,S,"['S', ':', 'S', '.']",,,,,,,[],,,,,,,[],"['these', 'evaluations', 'a', 'model']","['are', 'can', 'learn']",,,,,,,,,,misleading,are misleading,"['annotation artifacts ) ,', 'these evaluations', 'Unfortunately ,', 'when a dataset has systematic gaps ( e.g. ,']","[':', 'a model can learn simple decision rules perform well on the test set']",0,103,118,103,118,,,,,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,1,"Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset's intended capabilities.","['Unfortunately,', 'when', 'a', 'dataset', 'has', 'systematic', 'gaps', '(e.g.,', 'annotation', 'artifacts),', 'these', 'evaluations', 'are', 'misleading:', 'a', 'model', 'can', 'learn', 'simple', 'decision', 'rules', 'that', 'perform', 'well', 'on', 'the', 'test', 'set', 'but', 'do', 'not', 'capture', 'a', ""dataset's"", 'intended', 'capabilities.']",,,
133,We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",propose,0.0,3.0,10.0,3.0,10.0,"[(3, 10)]",['We'],['propose'],,,,,,,,,,propose,propose,['We'],"['a new annotation paradigm for NLP', '.']",0,3,11,3,11,,,,,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,2,We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data.,"['We', 'propose', 'a', 'new', 'annotation', 'paradigm', 'for', 'NLP', 'that', 'helps', 'to', 'close', 'systematic', 'gaps', 'in', 'the', 'test', 'data.']",,,
134,"In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets.",,,S,"['PP', ',', 'SBAR', ',', 'NP', 'VP', '.']",we,0.0,49.0,51.0,49.0,51.0,"[(49, 51)]",recommend,0.0,52.0,61.0,52.0,61.0,"[(52, 61)]",['we'],['recommend'],,,,,,,,,,recommend,recommend,"[',', 'we', 'In particular , after a dataset is constructed']","['but', 'ways that ( typically ) change the gold label , creating contrast sets .']",0,52,62,52,62,,,,,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,3,"In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets.","['In', 'particular,', 'after', 'a', 'dataset', 'is', 'constructed,', 'we', 'recommend', 'that', 'the', 'dataset', 'authors', 'manually', 'perturb', 'the', 'test', 'instances', 'in', 'small', 'but', 'meaningful', 'ways', 'that', '(typically)', 'change', 'the', 'gold', 'label,', 'creating', 'contrast', 'sets.']",,,
135,"Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities.",,,S,"['NP', 'VP', '.']",Contrast sets,0.0,0.0,13.0,0.0,13.0,"[(0, 8), (9, 13)]",provide,0.0,14.0,21.0,14.0,21.0,"[(14, 21)]","['Contrast', 'sets']",['provide'],,,,,,,,,,sets,sets,['Contrast'],"[""provide a local view of a model 's decision boundary , which can be used to more accurately evaluate a model 's true linguistic capabilities .""]",0,9,14,9,14,,,,,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,4,"Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities.","['Contrast', 'sets', 'provide', 'a', 'local', 'view', 'of', 'a', ""model's"", 'decision', 'boundary,', 'which', 'can', 'be', 'used', 'to', 'more', 'accurately', 'evaluate', 'a', ""model's"", 'true', 'linguistic', 'capabilities.']",,,
136,"We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets ( e.g. ,",DROP,"reading comprehension , UD parsing , IMDb sentiment analysis ) .",S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",demonstrate,0.0,3.0,14.0,3.0,14.0,"[(3, 14)]",['We'],['demonstrate'],,,,,,,,,,analysis,analysis,"['10 diverse NLP datasets ( e.g. , DROP reading comprehension ,', 'UD parsing', 'IMDb']",['We demonstrate the efficacy of contrast sets by creating them for ) .'],2,156,165,52,61,reading,reading,"['NOUN', 'ADV', 'NOUN']","['DROP', 'e.g.', 'analysis']",https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,5,"We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, IMDb sentiment analysis).","['We', 'demonstrate', 'the', 'efficacy', 'of', 'contrast', 'sets', 'by', 'creating', 'them', 'for', '10', 'diverse', 'NLP', 'datasets', '(', 'e.g.', ',', 'DROP', 'reading', 'comprehension', ',', 'UD', 'parsing', ',', 'IMDb', 'sentiment', 'analysis', ')', '.']","(18, 19)","(98, 102)",0.0
137,"Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\% in some cases.",,,S,"['SBAR', ',', 'NP', 'VP', '.']",model performance,0.0,60.0,77.0,60.0,77.0,"[(60, 65), (66, 77)]",is,0.0,78.0,80.0,78.0,80.0,"[(78, 80)]","['model', 'performance']",['is'],,,,,,,,,,significantly,is significantly,[],['lower on them than on the original test sets --- up to 25\\% in some cases .'],0,78,95,78,95,,,,,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,6,"Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\% in some cases.","['Although', 'our', 'contrast', 'sets', 'are', 'not', 'explicitly', 'adversarial,', 'model', 'performance', 'is', 'significantly', 'lower', 'on', 'them', 'than', 'on', 'the', 'original', 'test', 'sets---up', 'to', '25\\%', 'in', 'some', 'cases.']",,,
138,We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",release and encourage,0.0,3.0,71.0,3.0,71.0,"[(3, 10), (58, 61), (62, 71)]",['We'],"['release', 'encourage', 'follow']",,,,,,,,,,release,release,[],"['our contrast sets as new evaluation benchmarks and', 'encourage future dataset construction efforts to follow similar annotation processes .']",0,3,11,3,11,,,,,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Abstract,7,We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.,"['We', 'release', 'our', 'contrast', 'sets', 'as', 'new', 'evaluation', 'benchmarks', 'and', 'encourage', 'future', 'dataset', 'construction', 'efforts', 'to', 'follow', 'similar', 'annotation', 'processes.']",,,
139,Evaluating NLP Models via Contrast Sets.,,,S,"['VP', '.']",,,,,,,[],Evaluating,0.0,0.0,10.0,0.0,10.0,"[(0, 10)]",[],['Evaluating'],,,,,,,,,,Models,Models,"['Evaluating', 'NLP']",[],0,15,22,15,22,,,,,https://www.semanticscholar.org/paper/Evaluating-NLP-Models-via-Contrast-Sets-Gardner-Artzi/9fec5868542b4d9070306f1418d1d21666226e90,17,Title,0,Evaluating NLP Models via Contrast Sets.,"['Evaluating', 'NLP', 'Models', 'via', 'Contrast', 'Sets.']",,,
140,The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks.,,,S,"['NP', 'VP', '.']",The ability,0.0,0.0,11.0,0.0,11.0,"[(0, 3), (4, 11)]",is,0.0,61.0,63.0,61.0,63.0,"[(61, 63)]","['The', 'ability']",['is'],,,,,,,,,,critical,is critical,"['The', 'ability to understand and work with numbers ( numeracy )']",[],0,61,73,61,73,,,,,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,0,The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks.,"['The', 'ability', 'to', 'understand', 'and', 'work', 'with', 'numbers', '(numeracy)', 'is', 'critical', 'for', 'many', 'complex', 'reasoning', 'tasks.']",,,
141,"Currently, most NLP models treat numbers in text in the same way as other tokens---they embed them as distributed vectors.",,,S,"['S', ':', 'S', '.']",,,,,,,[],,,,,,,[],"['most', 'NLP', 'models', 'they']","['treat', 'embed']",,,,,,,,,,as,as,"['Currently , most NLP models treat numbers in text in the same way']",['other tokens --- they embed them as distributed vectors .'],0,66,69,66,69,,,,,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,1,"Currently, most NLP models treat numbers in text in the same way as other tokens---they embed them as distributed vectors.","['Currently,', 'most', 'NLP', 'models', 'treat', 'numbers', 'in', 'text', 'in', 'the', 'same', 'way', 'as', 'other', 'tokens---they', 'embed', 'them', 'as', 'distributed', 'vectors.']",,,
142,Is this enough to capture numeracy?,,,SQ,"['VBZ', 'NP', 'ADJP', '.']",this,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",Is,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",['this'],['Is'],,,,,,,,,,enough,enough,"['Is', 'this']",['to capture numeracy'],0,8,15,8,15,,,,,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,2,Is this enough to capture numeracy?,"['Is', 'this', 'enough', 'to', 'capture', 'numeracy?']",,,
143,We begin by investigating the numerical reasoning capabilities of,a state - of - the - art question answering model on the DROP dataset,.,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",begin,0.0,3.0,8.0,3.0,8.0,"[(3, 8)]",['We'],['begin'],,,,,,,,,,question,question,"['We', 'begin by investigating the numerical reasoning capabilities of a state - of -', 'the - art']","['answering on the DROP dataset', 'model']",1,91,100,25,34,,,['NOUN'],['question'],https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,3,We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset.,"['We', 'begin', 'by', 'investigating', 'the', 'numerical', 'reasoning', 'capabilities', 'of', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'question', 'answering', 'model', 'on', 'the', 'DROP', 'dataset', '.']","(9, 24)","(65, 134)",57.0
144,We find,this model,"excels on questions that require numerical reasoning , i.e. , it already captures numeracy .",S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",find,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",['We'],['find'],find,['We'],"['this model excels on questions that require numerical reasoning , i.e. , it already captures numeracy .']",-1.0,0.0,3.0,8.0,3.0,8.0,find,find,['We'],"['this model excels on questions that require numerical reasoning , i.e. , it already captures numeracy .']",0,3,8,3,8,excels,excels,"['NOUN', 'VERB']","['excels', 'find']",https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,4,"We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy.","['We', 'find', 'this', 'model', 'excels', 'on', 'questions', 'that', 'require', 'numerical', 'reasoning', ',', 'i.e.', ',', 'it', 'already', 'captures', 'numeracy', '.']","(2, 4)","(7, 17)",-1.0
145,"We find this model excels on questions that require numerical reasoning , i.e. ,",it,already captures numeracy .,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",find,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",['We'],['find'],captures,"['it', 'already']",['numeracy'],1.0,2.0,92.0,101.0,8.0,17.0,find,find,['We'],"['this model excels on questions that require numerical reasoning , i.e. , it already captures numeracy .']",0,3,8,3,8,captures,already,"['PRON', 'VERB', 'X']","['it', 'captures', 'i.e.']",https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,4,"We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy.","['We', 'find', 'this', 'model', 'excels', 'on', 'questions', 'that', 'require', 'numerical', 'reasoning', ',', 'i.e.', ',', 'it', 'already', 'captures', 'numeracy', '.']","(14, 15)","(80, 82)",-1.0
146,"To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks.",,,S,"['S', ',', 'NP', 'VP', '.']",we,0.0,44.0,46.0,44.0,46.0,"[(44, 46)]",probe,0.0,47.0,52.0,47.0,52.0,"[(47, 52)]",['we'],"['understand', 'probe']",,,,,,,,,,probe,probe,"['To understand how this capability emerges', ',', 'we']","['token embedding methods ( e.g. , BERT , GloVe ) on synthetic list maximum , number decoding , and addition tasks .']",0,47,53,47,53,,,,,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,5,"To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks.","['To', 'understand', 'how', 'this', 'capability', 'emerges,', 'we', 'probe', 'token', 'embedding', 'methods', '(e.g.,', 'BERT,', 'GloVe)', 'on', 'synthetic', 'list', 'maximum,', 'number', 'decoding,', 'and', 'addition', 'tasks.']",,,
147,A surprising degree of numeracy is naturally present in standard embeddings.,,,S,"['NP', 'VP', '.']",A surprising degree,0.0,0.0,19.0,0.0,19.0,"[(0, 1), (2, 12), (13, 19)]",is,0.0,32.0,34.0,32.0,34.0,"[(32, 34)]","['A', 'surprising', 'degree']",['is'],,,,,,,,,,naturally,is naturally,['A surprising degree of numeracy'],['present in standard embeddings'],0,32,45,32,45,,,,,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,6,A surprising degree of numeracy is naturally present in standard embeddings.,"['A', 'surprising', 'degree', 'of', 'numeracy', 'is', 'naturally', 'present', 'in', 'standard', 'embeddings.']",,,
148,"For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000.",,,S,"['PP', ',', 'NP', 'VP', '.']",GloVe word2vec,0.0,14.0,32.0,14.0,32.0,"[(14, 19), (24, 32)]",encode,0.0,44.0,50.0,44.0,50.0,"[(44, 50)]","['GloVe', 'word2vec']",['encode'],,,,,,,,,,",",",","['For', 'example']","['GloVe and word2vec', 'accurately encode magnitude for numbers up to 1,000 .']",0,12,14,12,14,,,,,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,7,"For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000.","['For', 'example,', 'GloVe', 'and', 'word2vec', 'accurately', 'encode', 'magnitude', 'for', 'numbers', 'up', 'to', '1,000.']",,,
149,"Furthermore, character-level embeddings are even more precise---ELMo captures numeracy the best for all pre-trained methods---but BERT, which uses sub-word units, is less exact.",,,S,"['S', 'CC', 'S', '.']",,,,,,,[],but,0.0,135.0,138.0,135.0,138.0,"[(135, 138)]","['character', 'level', 'embeddings', 'ELMo', 'BERT', 'sub', '-', 'word', 'units']","['are', 'is']",,,,,,,,,,precise,are precise,['more'],['--- ELMo captures numeracy the best for all pre - trained methods ---'],0,43,65,43,65,,,,,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Abstract,8,"Furthermore, character-level embeddings are even more precise---ELMo captures numeracy the best for all pre-trained methods---but BERT, which uses sub-word units, is less exact.","['Furthermore,', 'character-level', 'embeddings', 'are', 'even', 'more', 'precise---ELMo', 'captures', 'numeracy', 'the', 'best', 'for', 'all', 'pre-trained', 'methods---but', 'BERT,', 'which', 'uses', 'sub-word', 'units,', 'is', 'less', 'exact.']",,,
150,Do NLP Models Know Numbers?,,,SQ,"['VBP', 'NP', 'VP', '.']",NLP Models,0.0,3.0,13.0,3.0,13.0,"[(3, 6), (7, 13)]",Do Know,0.0,0.0,18.0,0.0,18.0,"[(0, 2), (14, 18)]","['NLP', 'Models']","['Do', 'Know']",,,,,,,,,,Models,Do Models,[],[],0,0,14,0,14,,,,,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Title,0,Do NLP Models Know Numbers?,"['Do', 'NLP', 'Models', 'Know', 'Numbers?']",,,
151,Probing Numeracy in Embeddings.,,,S,"['VP', '.']",,,,,,,[],Probing,0.0,0.0,7.0,0.0,7.0,"[(0, 7)]",[],['Probing'],,,,,,,,,,Numeracy,Numeracy,['Probing'],[],0,8,17,8,17,,,,,https://www.semanticscholar.org/paper/Do-NLP-Models-Know-Numbers-Probing-Numeracy-in-Wallace-Wang/0427110f0e79f41e69a8eb00a3ec8868bac26a4f,18,Title,1,Probing Numeracy in Embeddings.,"['Probing', 'Numeracy', 'in', 'Embeddings.']",,,
152,"In this work, we focus on the task of Automatic Question Generation (AQG) where given a passage and an answer the task is to generate the corresponding question.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,15.0,17.0,15.0,17.0,"[(15, 17)]",focus,0.0,18.0,23.0,18.0,23.0,"[(18, 23)]",['we'],['focus'],,,,,,,,,,focus,focus,['we'],[],0,18,24,18,24,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,0,"In this work, we focus on the task of Automatic Question Generation (AQG) where given a passage and an answer the task is to generate the corresponding question.","['In', 'this', 'work,', 'we', 'focus', 'on', 'the', 'task', 'of', 'Automatic', 'Question', 'Generation', '(AQG)', 'where', 'given', 'a', 'passage', 'and', 'an', 'answer', 'the', 'task', 'is', 'to', 'generate', 'the', 'corresponding', 'question.']",,,
153,It is desired that the generated question should be (i) grammatically correct (ii) answerable from the passage and (iii) specific to the given answer.,,,S,"['NP', 'VP', '.']",It,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",is desired,0.0,3.0,13.0,3.0,13.0,"[(3, 5), (6, 13)]",['It'],"['is', 'desired']",,,,,,,,,,desired,is desired,['It'],[],0,3,14,3,14,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,1,It is desired that the generated question should be (i) grammatically correct (ii) answerable from the passage and (iii) specific to the given answer.,"['It', 'is', 'desired', 'that', 'the', 'generated', 'question', 'should', 'be', '(i)', 'grammatically', 'correct', '(ii)', 'answerable', 'from', 'the', 'passage', 'and', '(iii)', 'specific', 'to', 'the', 'given', 'answer.']",,,
154,An analysis of existing AQG models shows that they produce questions which do not adhere to one or more of {the above-mentioned qualities}.,,,S,"['NP', 'VP', '.']",An analysis,0.0,0.0,11.0,0.0,11.0,"[(0, 2), (3, 11)]",shows,0.0,35.0,40.0,35.0,40.0,"[(35, 40)]","['An', 'analysis']",['shows'],,,,,,,,,,shows,shows,['An analysis of existing AQG models'],[],0,35,41,35,41,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,2,An analysis of existing AQG models shows that they produce questions which do not adhere to one or more of {the above-mentioned qualities}.,"['An', 'analysis', 'of', 'existing', 'AQG', 'models', 'shows', 'that', 'they', 'produce', 'questions', 'which', 'do', 'not', 'adhere', 'to', 'one', 'or', 'more', 'of', '{the', 'above-mentioned', 'qualities}.']",,,
155,"In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement.",,,S,"['PP', ',', 'NP', 'VP', '.']",the questions,0.0,16.0,39.0,16.0,39.0,"[(16, 19), (30, 39)]",look,0.0,40.0,44.0,40.0,44.0,"[(40, 44)]","['the', 'questions']",['look'],,,,,,,,,,look,look,['the generated questions'],['question with a clear scope for refinement'],0,40,45,40,45,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,3,"In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement.","['In', 'particular,', 'the', 'generated', 'questions', 'look', 'like', 'an', 'incomplete', 'draft', 'of', 'the', 'desired', 'question', 'with', 'a', 'clear', 'scope', 'for', 'refinement.']",,,
156,"{To alleviate this shortcoming}, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it.",,,S,"['-LRB-', 'S', ',', 'NP', 'VP', '.']",we,0.0,36.0,38.0,36.0,38.0,"[(36, 38)]",propose,0.0,39.0,46.0,39.0,46.0,"[(39, 46)]",['we'],"['alleviate', 'propose']",,,,,,,,,,propose,propose,"['{ To alleviate this shortcoming }', ',', 'we']",['a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it .'],0,39,47,39,47,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,4,"{To alleviate this shortcoming}, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it.","['{To', 'alleviate', 'this', 'shortcoming},', 'we', 'propose', 'a', 'method', 'which', 'tries', 'to', 'mimic', 'the', 'human', 'process', 'of', 'generating', 'questions', 'by', 'first', 'creating', 'an', 'initial', 'draft', 'and', 'then', 'refining', 'it.']",,,
157,"More specifically, we propose Refine Network (RefNet) which contains two decoders.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",we,0.0,20.0,22.0,20.0,22.0,"[(20, 22)]",propose,0.0,23.0,30.0,23.0,30.0,"[(23, 30)]",['we'],['propose'],,,,,,,,,,propose,propose,['we'],"['Refine Network ( RefNet ) which contains two decoders', '.']",0,23,31,23,31,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,5,"More specifically, we propose Refine Network (RefNet) which contains two decoders.","['More', 'specifically,', 'we', 'propose', 'Refine', 'Network', '(RefNet)', 'which', 'contains', 'two', 'decoders.']",,,
158,The second decoder uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first decoder.,,,S,"['NP', 'VP', '.']",The second decoder,0.0,0.0,18.0,0.0,18.0,"[(0, 3), (4, 10), (11, 18)]",uses,0.0,19.0,23.0,19.0,23.0,"[(19, 23)]","['The', 'second', 'decoder']",['uses'],,,,,,,,,,pays,pays,"['The second decoder uses a dual attention network', 'which']",['attention'],0,55,60,55,60,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,6,The second decoder uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first decoder.,"['The', 'second', 'decoder', 'uses', 'a', 'dual', 'attention', 'network', 'which', 'pays', 'attention', 'to', 'both', '(i)', 'the', 'original', 'passage', 'and', '(ii)', 'the', 'question', '(initial', 'draft)', 'generated', 'by', 'the', 'first', 'decoder.']",,,
159,"In effect, it refines the question generated by the first decoder, thereby making it more correct and complete.",,,S,"['PP', ',', 'NP', 'VP', '.']",it,0.0,12.0,14.0,12.0,14.0,"[(12, 14)]",refines,0.0,15.0,22.0,15.0,22.0,"[(15, 22)]",['it'],"['refines', 'making']",,,,,,,,,,refines,refines,['it'],['the question generated by the first decoder'],0,15,23,15,23,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,7,"In effect, it refines the question generated by the first decoder, thereby making it more correct and complete.","['In', 'effect,', 'it', 'refines', 'the', 'question', 'generated', 'by', 'the', 'first', 'decoder,', 'thereby', 'making', 'it', 'more', 'correct', 'and', 'complete.']",,,
160,"We evaluate RefNet on three datasets, \textit{viz.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",evaluate,0.0,3.0,11.0,3.0,11.0,"[(3, 11)]",['We'],['evaluate'],,,,,,,,,,evaluate,evaluate,['We'],"['RefNet on three datasets , \\textit{viz']",0,3,12,3,12,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,8,"We evaluate RefNet on three datasets, \textit{viz.","['We', 'evaluate', 'RefNet', 'on', 'three', 'datasets,', '\\textit{viz.']",,,
161,"} , SQuAD , HOTPOT-QA , and",DROP,", and show that it outperforms existing state-of-the-art methods by 7-16\ % on all of these datasets .",S,"['-RRB-', ',', 'NP', ',', 'NP', 'HYPH', 'NNP', ',', 'CC', 'NN', ',', 'CC', 'VP', '.']",SQuAD HOTPOT QA DROP,0.0,4.0,34.0,4.0,34.0,"[(4, 9), (12, 18), (21, 23), (30, 34)]",and and show,0.0,26.0,45.0,26.0,45.0,"[(26, 29), (37, 40), (41, 45)]","['SQuAD', 'HOTPOT', 'QA', 'DROP']",['show'],show,"['} , SQuAD , HOTPOT - QA , and DROP']",['that it outperforms existing state - of - the - art methods by 7 - 16\\ % on all of these datasets .'],1.0,2.0,41.0,46.0,8.0,13.0,",",",",[],"['} , SQuAD , HOTPOT - QA , and DROP and show that it outperforms existing state - of - the - art methods by 7 - 16\\ % on all of these datasets .']",2,35,37,2,4,show,and,"['NOUN', 'PUNCT', 'PROPN']","['DROP', ',', 'QA']",https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,9,"}, SQuAD, HOTPOT-QA, and DROP, and show that it outperforms existing state-of-the-art methods by 7-16\% on all of these datasets.","['}', ',', 'SQuAD', ',', 'HOTPOT-QA', ',', 'and', 'DROP', ',', 'and', 'show', 'that', 'it', 'outperforms', 'existing', 'state-of-the-art', 'methods', 'by', '7-16\\', '%', 'on', 'all', 'of', 'these', 'datasets', '.']","(7, 8)","(27, 31)",0.0
162,"Lastly, we show that we can improve the quality of the second decoder on specific metrics, such as, fluency and answerability by explicitly rewarding revisions that improve on the corresponding metric during training.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",we,0.0,9.0,11.0,9.0,11.0,"[(9, 11)]",show,0.0,12.0,16.0,12.0,16.0,"[(12, 16)]",['we'],['show'],,,,,,,,,,show,show,"['Lastly', ',', 'we']",[],0,12,17,12,17,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,10,"Lastly, we show that we can improve the quality of the second decoder on specific metrics, such as, fluency and answerability by explicitly rewarding revisions that improve on the corresponding metric during training.","['Lastly,', 'we', 'show', 'that', 'we', 'can', 'improve', 'the', 'quality', 'of', 'the', 'second', 'decoder', 'on', 'specific', 'metrics,', 'such', 'as,', 'fluency', 'and', 'answerability', 'by', 'explicitly', 'rewarding', 'revisions', 'that', 'improve', 'on', 'the', 'corresponding', 'metric', 'during', 'training.']",,,
163,The code has been made publicly available \footnote{this https URL},,,S,"['NP', 'VP']",The code,0.0,0.0,8.0,0.0,8.0,"[(0, 3), (4, 8)]",has been made,0.0,9.0,22.0,9.0,22.0,"[(9, 12), (13, 17), (18, 22)]","['The', 'code']","['has', 'been', 'made']",,,,,,,,,,made,has been made,['The code'],['available \\footnote{this https URL }'],0,9,23,9,23,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Abstract,11,The code has been made publicly available \footnote{this https URL},"['The', 'code', 'has', 'been', 'made', 'publicly', 'available', '\\footnote{this', 'https', 'URL}']",,,
164,Let's Ask Again: Refine Network for Automatic Question Generation.,,,S,"['VP', '.']",,,,,,,[],Let Ask,0.0,0.0,10.0,0.0,10.0,"[(0, 3), (7, 10)]",[],"['Let', 'Ask']",,,,,,,,,,Let,Let,[],"[""'s"", 'Ask Again : Refine Network for Automatic Question Generation', '.']",0,0,4,0,4,,,,,https://www.semanticscholar.org/paper/Let's-Ask-Again%3A-Refine-Network-for-Automatic-Nema-Mohankumar/5ea017faae8706d07a8ebc2a321969a899e8fad9,19,Title,0,Let's Ask Again: Refine Network for Automatic Question Generation.,"[""Let's"", 'Ask', 'Again:', 'Refine', 'Network', 'for', 'Automatic', 'Question', 'Generation.']",,,
165,Data points such as compressive strengths of materials or patient lab results are often reported in scientific literature.,,,S,"['NP', 'VP', '.']",Data points,0.0,0.0,11.0,0.0,11.0,"[(0, 4), (5, 11)]",are reported,0.0,78.0,96.0,78.0,96.0,"[(78, 81), (88, 96)]","['Data', 'points']","['are', 'reported']",,,,,,,,,,reported,reported,"['Data points such as compressive strengths of materials or patient lab results are', 'often']",[],0,88,97,88,97,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,0,Data points such as compressive strengths of materials or patient lab results are often reported in scientific literature.,"['Data', 'points', 'such', 'as', 'compressive', 'strengths', 'of', 'materials', 'or', 'patient', 'lab', 'results', 'are', 'often', 'reported', 'in', 'scientific', 'literature.']",,,
166,"Though databases of such data exist, they are typically curated manually.",,,S,"['SBAR', ',', 'NP', 'VP', '.']",they,0.0,38.0,42.0,38.0,42.0,"[(38, 42)]",are curated,0.0,43.0,64.0,43.0,64.0,"[(43, 46), (57, 64)]",['they'],"['are', 'curated']",,,,,,,,,,curated,curated,"['they are', 'Though databases of such data exist , typically']","['manually', '.']",0,57,65,57,65,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,1,"Though databases of such data exist, they are typically curated manually.","['Though', 'databases', 'of', 'such', 'data', 'exist,', 'they', 'are', 'typically', 'curated', 'manually.']",,,
167,"While automated extraction of measured quantities, such as 17.2 mV, is straightforward, it is much more challenging to capture information about such quantities, such as which property of which entity is being measured.",,,S,"['SBAR', ',', 'NP', 'VP', '.']",it,0.0,91.0,93.0,91.0,93.0,"[(91, 93)]",is,0.0,94.0,96.0,94.0,96.0,"[(94, 96)]",['it'],"['is', 'capture']",,,,,,,,,,17.2,17.2,[],"['mV', ',']",0,60,65,60,65,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,2,"While automated extraction of measured quantities, such as 17.2 mV, is straightforward, it is much more challenging to capture information about such quantities, such as which property of which entity is being measured.","['While', 'automated', 'extraction', 'of', 'measured', 'quantities,', 'such', 'as', '17.2', 'mV,', 'is', 'straightforward,', 'it', 'is', 'much', 'more', 'challenging', 'to', 'capture', 'information', 'about', 'such', 'quantities,', 'such', 'as', 'which', 'property', 'of', 'which', 'entity', 'is', 'being', 'measured.']",,,
168,"Additionally, further context may be required identifying conditions, such as temperature or pressure, under which a measurement was performed.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",further context,0.0,15.0,30.0,15.0,30.0,"[(15, 22), (23, 30)]",may be required,0.0,31.0,46.0,31.0,46.0,"[(31, 34), (35, 37), (38, 46)]","['further', 'context']","['may', 'be', 'required', 'identifying']",,,,,,,,,,required,may be required,[],"['under which was performed', '.']",0,31,47,31,47,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,3,"Additionally, further context may be required identifying conditions, such as temperature or pressure, under which a measurement was performed.","['Additionally,', 'further', 'context', 'may', 'be', 'required', 'identifying', 'conditions,', 'such', 'as', 'temperature', 'or', 'pressure,', 'under', 'which', 'a', 'measurement', 'was', 'performed.']",,,
169,"This information on property, entity, and context is needed to place the measurements into a database, but little attention has been given to the problem.",,,S,"['S', ',', 'CC', 'S', '.']",,,,,,,[],but,0.0,106.0,109.0,106.0,109.0,"[(106, 109)]","['This', 'information', 'little', 'attention']","['is', 'needed', 'place', 'has', 'been', 'given']",,,,,,,,,,needed,is needed,"['This', 'information on property , entity , and context']","[',']",0,52,62,52,62,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,4,"This information on property, entity, and context is needed to place the measurements into a database, but little attention has been given to the problem.","['This', 'information', 'on', 'property,', 'entity,', 'and', 'context', 'is', 'needed', 'to', 'place', 'the', 'measurements', 'into', 'a', 'database,', 'but', 'little', 'attention', 'has', 'been', 'given', 'to', 'the', 'problem.']",,,
170,We start to address this information extraction problem by applying machine comprehension and question answering techniques.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",start,0.0,3.0,8.0,3.0,8.0,"[(3, 8)]",['We'],"['start', 'address']",,,,,,,,,,start,start,['We'],[],0,3,9,3,9,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,5,We start to address this information extraction problem by applying machine comprehension and question answering techniques.,"['We', 'start', 'to', 'address', 'this', 'information', 'extraction', 'problem', 'by', 'applying', 'machine', 'comprehension', 'and', 'question', 'answering', 'techniques.']",,,
171,"To our knowledge, our research is the first to apply neural language models to extracting such properties and entities.",,,S,"['PP', ',', 'NP', 'VP', '.']",research,0.0,23.0,31.0,23.0,31.0,"[(23, 31)]",is,0.0,32.0,34.0,32.0,34.0,"[(32, 34)]",['research'],['is'],,,,,,,,,,the,is the,[],"['first', 'to apply neural language models to extracting such properties and entities']",0,32,39,32,39,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,6,"To our knowledge, our research is the first to apply neural language models to extracting such properties and entities.","['To', 'our', 'knowledge,', 'our', 'research', 'is', 'the', 'first', 'to', 'apply', 'neural', 'language', 'models', 'to', 'extracting', 'such', 'properties', 'and', 'entities.']",,,
172,We found that we can do so with a very small investment in training data generation.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",found,0.0,3.0,8.0,3.0,8.0,"[(3, 8)]",['We'],['found'],,,,,,,,,,found,found,['We'],[],0,3,9,3,9,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,7,We found that we can do so with a very small investment in training data generation.,"['We', 'found', 'that', 'we', 'can', 'do', 'so', 'with', 'a', 'very', 'small', 'investment', 'in', 'training', 'data', 'generation.']",,,
173,"We develop a multi-turn question answering solution, where initial questions provide answers that are plugged into templates for follow-on questions.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",develop,0.0,3.0,10.0,3.0,10.0,"[(3, 10)]",['We'],['develop'],,,,,,,,,,question,question,"[', where initial questions provide answers that are plugged into templates for follow - on questions', 'develop', 'a multi -', 'turn']",['solution'],0,26,35,26,35,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,8,"We develop a multi-turn question answering solution, where initial questions provide answers that are plugged into templates for follow-on questions.","['We', 'develop', 'a', 'multi-turn', 'question', 'answering', 'solution,', 'where', 'initial', 'questions', 'provide', 'answers', 'that', 'are', 'plugged', 'into', 'templates', 'for', 'follow-on', 'questions.']",,,
174,"We use existing QA datasets , SQuAD and",DROP,", to do most of the training of our model and measure the effect of adding different amounts of the gold data .",S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",use,0.0,3.0,6.0,3.0,6.0,"[(3, 6)]",['We'],"['use', 'do', 'measure']",use,['We'],"['existing QA datasets , SQuAD and DROP , to do most of the training of our model and measure the effect of adding different amounts of the gold data .']",-1.0,0.0,3.0,7.0,3.0,7.0,use,use,['We'],"['existing QA datasets , SQuAD and DROP , to do most of the training of our model and measure the effect of adding different amounts of the gold data .']",0,3,7,3,7,do,to,"['PROPN', 'NOUN', 'VERB']","['DROP', 'datasets', 'use']",https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,9,"We use existing QA datasets, SQuAD and DROP, to do most of the training of our model and measure the effect of adding different amounts of the gold data.","['We', 'use', 'existing', 'QA', 'datasets', ',', 'SQuAD', 'and', 'DROP', ',', 'to', 'do', 'most', 'of', 'the', 'training', 'of', 'our', 'model', 'and', 'measure', 'the', 'effect', 'of', 'adding', 'different', 'amounts', 'of', 'the', 'gold', 'data', '.']","(8, 9)","(39, 43)",0.0
175,"Adding gold data representing only 0.1% of the SQuAD data, a SciBERT-based QA model’s exact match and partial overlap accuracies improve from .32 and .47, respectively, to .42 and .57.",,,S,"['S', ',', 'NP', 'VP', '.']",a SciBERT QA model ’s exact match partial overlap accuracies,0.0,61.0,133.0,61.0,133.0,"[(61, 62), (63, 70), (79, 81), (82, 87), (88, 90), (91, 96), (97, 102), (107, 114), (115, 122), (123, 133)]",improve,0.0,134.0,141.0,134.0,141.0,"[(134, 141)]","['a', 'SciBERT', 'QA', 'model', 'exact', 'match', 'partial', 'overlap', 'accuracies']","['Adding', 'improve']",,,,,,,,,,accuracies,accuracies,"['Adding gold data representing only 0.1 % of the SQuAD data , a SciBERT - based QA model ’s exact match and overlap improve from .32 and .47 , respectively , to .42 and .57 .']",[],0,123,134,123,134,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,10,"Adding gold data representing only 0.1% of the SQuAD data, a SciBERT-based QA model’s exact match and partial overlap accuracies improve from .32 and .47, respectively, to .42 and .57.","['Adding', 'gold', 'data', 'representing', 'only', '0.1%', 'of', 'the', 'SQuAD', 'data,', 'a', 'SciBERT-based', 'QA', 'model’s', 'exact', 'match', 'and', 'partial', 'overlap', 'accuracies', 'improve', 'from', '.32', 'and', '.47,', 'respectively,', 'to', '.42', 'and', '.57.']",,,
176,We improve substantially over the state-art baseline (GROBID-quantities).,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",improve,0.0,3.0,10.0,3.0,10.0,"[(3, 10)]",['We'],['improve'],,,,,,,,,,improve,improve,['We'],['substantially over the state - art baseline ( GROBID - quantities ) .'],0,3,11,3,11,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Abstract,11,We improve substantially over the state-art baseline (GROBID-quantities).,"['We', 'improve', 'substantially', 'over', 'the', 'state-art', 'baseline', '(GROBID-quantities).']",,,
177,Automatic Construction of Measured Property Knowledge Bases through Multi-Turn Question Answering.,,,NP,"['NP', 'PP', 'PP', '.']",Automatic Construction,0.0,0.0,22.0,0.0,22.0,"[(0, 9), (10, 22)]",,,,,,,[],"['Automatic', 'Construction']",[],,,,,,,,,,Bases,Bases,['Automatic Construction of Measured Property Knowledge'],[],0,54,60,54,60,,,,,https://www.semanticscholar.org/paper/Automatic-Construction-of-Measured-Property-Bases/dd545488d4f07504f38d510205552ac849c0a4a5,20,Title,0,Automatic Construction of Measured Property Knowledge Bases through Multi-Turn Question Answering.,"['Automatic', 'Construction', 'of', 'Measured', 'Property', 'Knowledge', 'Bases', 'through', 'Multi-Turn', 'Question', 'Answering.']",,,
178,"Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in human's reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems.",,,S,"['NP', 'VP', '.']",Numerical reasoning,0.0,0.0,19.0,0.0,19.0,"[(0, 9), (10, 19)]",is,0.0,76.0,78.0,76.0,78.0,"[(76, 78)]","['Numerical', 'reasoning']",['is'],,,,,,,,,,skill,is skill,[],"[',', 'which has not been well considered in existing machine reading comprehension ( MRC ) systems .']",0,76,96,76,96,,,,,https://www.semanticscholar.org/paper/NumNet%3A-Machine-Reading-Comprehension-with-Ran-Lin/730043364aed106241ef18ab3e3b5e316802a254,21,Abstract,0,"Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in human's reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems.","['Numerical', 'reasoning,', 'such', 'as', 'addition,', 'subtraction,', 'sorting', 'and', 'counting', 'is', 'a', 'critical', 'skill', 'in', ""human's"", 'reading', 'comprehension,', 'which', 'has', 'not', 'been', 'well', 'considered', 'in', 'existing', 'machine', 'reading', 'comprehension', '(MRC)', 'systems.']",,,
179,"To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage.",,,S,"['S', ',', 'NP', 'VP', '.']",we,0.0,24.0,26.0,24.0,26.0,"[(24, 26)]",propose,0.0,27.0,34.0,27.0,34.0,"[(27, 34)]",['we'],"['address', 'propose']",,,,,,,,,,propose,propose,"['To', 'we', 'address this issue']","['a numerical MRC model named as NumNet , utilizes a numerically - aware graph neural network', 'which']",0,27,35,27,35,,,,,https://www.semanticscholar.org/paper/NumNet%3A-Machine-Reading-Comprehension-with-Ran-Lin/730043364aed106241ef18ab3e3b5e316802a254,21,Abstract,1,"To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage.","['To', 'address', 'this', 'issue,', 'we', 'propose', 'a', 'numerical', 'MRC', 'model', 'named', 'as', 'NumNet,', 'which', 'utilizes', 'a', 'numerically-aware', 'graph', 'neural', 'network', 'to', 'consider', 'the', 'comparing', 'information', 'and', 'performs', 'numerical', 'reasoning', 'over', 'numbers', 'in', 'the', 'question', 'and', 'passage.']",,,
180,Our system achieves an EM-score of 64.56 % on the,DROP,"dataset , outperforming all existing machine reading comprehension models by considering the numerical relations among numbers .",S,"['NP', 'VP', '.']",system,0.0,4.0,10.0,4.0,10.0,"[(4, 10)]",achieves outperforming,0.0,11.0,80.0,11.0,80.0,"[(11, 19), (67, 80)]",['system'],"['achieves', 'outperforming']",achieves,['Our system'],"['an EM - score of 64.56 % on the DROP dataset', '.']",-1.0,0.0,11.0,20.0,11.0,20.0,achieves,achieves,['Our system'],"['an EM - score of 64.56 % on the DROP dataset', '.']",0,11,20,11,20,outperforming,dataset,"['NOUN', 'NOUN', 'ADP']","['DROP', 'dataset', 'on']",https://www.semanticscholar.org/paper/NumNet%3A-Machine-Reading-Comprehension-with-Ran-Lin/730043364aed106241ef18ab3e3b5e316802a254,21,Abstract,2,"Our system achieves an EM-score of 64.56% on the DROP dataset, outperforming all existing machine reading comprehension models by considering the numerical relations among numbers.","['Our', 'system', 'achieves', 'an', 'EM-score', 'of', '64.56', '%', 'on', 'the', 'DROP', 'dataset', ',', 'outperforming', 'all', 'existing', 'machine', 'reading', 'comprehension', 'models', 'by', 'considering', 'the', 'numerical', 'relations', 'among', 'numbers', '.']","(10, 11)","(49, 53)",0.0
181,NumNet: Machine Reading Comprehension with Numerical Reasoning.,,,NP,"['NP', ':', 'NP', '.']",NumNet Machine Reading Comprehension,0.0,0.0,38.0,0.0,38.0,"[(0, 6), (9, 16), (17, 24), (25, 38)]",,,,,,,[],"['NumNet', 'Machine', 'Reading', 'Comprehension']",[],,,,,,,,,,NumNet,NumNet,[],"[':', 'Machine Reading Comprehension with Numerical Reasoning']",0,0,7,0,7,,,,,https://www.semanticscholar.org/paper/NumNet%3A-Machine-Reading-Comprehension-with-Ran-Lin/730043364aed106241ef18ab3e3b5e316802a254,21,Title,0,NumNet: Machine Reading Comprehension with Numerical Reasoning.,"['NumNet:', 'Machine', 'Reading', 'Comprehension', 'with', 'Numerical', 'Reasoning.']",,,
182,Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks.,,,S,"['NP', 'VP', '.']",Recent advances,0.0,0.0,15.0,0.0,15.0,"[(0, 6), (7, 15)]",demonstrate,0.0,23.0,34.0,23.0,34.0,"[(23, 34)]","['Recent', 'advances']",['demonstrate'],,,,,,,,,,demonstrate,demonstrate,"['Recent', 'advances in NLP']","['the effectiveness of training large - scale language models and transferring them to downstream tasks', '.']",0,23,35,23,35,,,,,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,0,Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks.,"['Recent', 'advances', 'in', 'NLP', 'demonstrate', 'the', 'effectiveness', 'of', 'training', 'large-scale', 'language', 'models', 'and', 'transferring', 'them', 'to', 'downstream', 'tasks.']",,,
183,Can fine-tuning these models on tasks other than language modeling further improve performance?,,,SQ,"['MD', 'ADVP', 'HYPH', 'S', 'VP', '.']",,,,,,,[],Can improve,0.0,0.0,84.0,0.0,84.0,"[(0, 3), (77, 84)]",[],"['Can', 'tuning', 'improve']",,,,,,,,,,tuning,Can tuning,['fine'],['these models'],0,0,18,0,18,,,,,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,1,Can fine-tuning these models on tasks other than language modeling further improve performance?,"['Can', 'fine-tuning', 'these', 'models', 'on', 'tasks', 'other', 'than', 'language', 'modeling', 'further', 'improve', 'performance?']",,,
184,"In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling).",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,16.0,18.0,16.0,18.0,"[(16, 18)]",conduct,0.0,19.0,26.0,19.0,26.0,"[(19, 26)]",['we'],['conduct'],,,,,,,,,,question,question,"['In this paper , we conduct an extensive study of the transferability between 33 NLP tasks across', 'three broad classes of', 'problems ( text classification', ',']","['answering', ', and sequence labeling )', '.']",0,153,162,153,162,,,,,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,2,"In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling).","['In', 'this', 'paper,', 'we', 'conduct', 'an', 'extensive', 'study', 'of', 'the', 'transferability', 'between', '33', 'NLP', 'tasks', 'across', 'three', 'broad', 'classes', 'of', 'problems', '(text', 'classification,', 'question', 'answering,', 'and', 'sequence', 'labeling).']",,,
185,"Our results show that transfer learning is more beneficial than previously thought , especially when target task data is scarce , and can improve performance even when the source task is small or differs substantially from the target task ( e.g. , part-of-speech tagging transfers well to the",DROP,QA dataset ) .,S,"['NP', 'VP', '.']",results,0.0,4.0,11.0,4.0,11.0,"[(4, 11)]",show,0.0,12.0,16.0,12.0,16.0,"[(12, 16)]",['results'],['show'],improve,[],"['performance', 'substantially from the target task ( e.g. , part - of - speech tagging transfers well to the DROP QA dataset ) .']",-1.0,0.0,138.0,146.0,138.0,146.0,results,results,['Our'],"['show that transfer learning is more beneficial than previously thought , especially when target task data is scarce , and can improve performance even when the source task is small or differs substantially from the target task ( e.g. , part - of - speech tagging transfers well to the DROP QA dataset ) .']",0,4,12,4,12,,QA,"['NOUN', 'PUNCT', 'ADP']","['DROP', ')', 'to']",https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,3,"Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even when the source task is small or differs substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset).","['Our', 'results', 'show', 'that', 'transfer', 'learning', 'is', 'more', 'beneficial', 'than', 'previously', 'thought', ',', 'especially', 'when', 'target', 'task', 'data', 'is', 'scarce', ',', 'and', 'can', 'improve', 'performance', 'even', 'when', 'the', 'source', 'task', 'is', 'small', 'or', 'differs', 'substantially', 'from', 'the', 'target', 'task', '(', 'e.g.', ',', 'part-of-speech', 'tagging', 'transfers', 'well', 'to', 'the', 'DROP', 'QA', 'dataset', ')', '.']","(48, 49)","(292, 296)",0.0
186,"We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size.",,,S,"['S', ',', 'CC', 'S', '.']",,,,,,,[],and,0.0,121.0,124.0,121.0,124.0,"[(121, 124)]","['We', 'we']","['develop', 'validate']",,,,,,,,,,develop,develop,['We'],"['task embeddings', 'that can be used to predict the most transferable source tasks for a given target task , and we validate their effectiveness in experiments controlled for source and target data size']",0,8,16,8,16,,,,,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,4,"We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size.","['We', 'also', 'develop', 'task', 'embeddings', 'that', 'can', 'be', 'used', 'to', 'predict', 'the', 'most', 'transferable', 'source', 'tasks', 'for', 'a', 'given', 'target', 'task,', 'and', 'we', 'validate', 'their', 'effectiveness', 'in', 'experiments', 'controlled', 'for', 'source', 'and', 'target', 'data', 'size.']",,,
187,"Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",experiments,0.0,14.0,25.0,14.0,25.0,"[(14, 25)]",reveal,0.0,26.0,32.0,26.0,32.0,"[(26, 32)]",['experiments'],['reveal'],,,,,,,,,,reveal,reveal,['our experiments'],[],0,26,33,26,33,,,,,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Abstract,5,"Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability.","['Overall,', 'our', 'experiments', 'reveal', 'that', 'factors', 'such', 'as', 'source', 'data', 'size,', 'task', 'and', 'domain', 'similarity,', 'and', 'task', 'complexity', 'all', 'play', 'a', 'role', 'in', 'determining', 'transferability.']",,,
188,Exploring and Predicting Transferability across NLP Tasks.,,,S,"['S', 'PP', '.']",,,,,,,[],,,,,,,[],[],['Exploring'],,,,,,,,,,Transferability,Transferability,['Exploring and Predicting'],[],0,25,41,25,41,,,,,https://www.semanticscholar.org/paper/Exploring-and-Predicting-Transferability-across-NLP-Vu-Wang/ee026b977120087c76819959649e1d4fd42510f0,22,Title,0,Exploring and Predicting Transferability across NLP Tasks.,"['Exploring', 'and', 'Predicting', 'Transferability', 'across', 'NLP', 'Tasks.']",,,
189,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture.",,,S,"['S', ':', 'S', '.']",,,,,,,[],,,,,,,[],"['Neural', 'module', 'networks', 'NMNs', 'they']","['are', 'achieve']",,,,,,,,,,),),"['(', 'NMNs']",[],0,30,32,30,32,,,,,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,0,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture.","['Neural', 'module', 'networks', '(NMNs)', 'are', 'a', 'popular', 'approach', 'for', 'modeling', 'compositionality:', 'they', 'achieve', 'high', 'accuracy', 'when', 'applied', 'to', 'problems', 'in', 'language', 'and', 'vision,', 'while', 'reflecting', 'the', 'compositional', 'structure', 'of', 'the', 'problem', 'in', 'the', 'network', 'architecture.']",,,
190,"However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",prior,0.0,10.0,15.0,10.0,15.0,"[(10, 15)]",assumed,0.0,32.0,39.0,32.0,39.0,"[(32, 39)]",['prior'],['assumed'],,,,,,,,,,assumed,assumed,"['However', ',', 'implicitly']",[],0,32,40,32,40,,,,,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,1,"However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour.","['However,', 'prior', 'work', 'implicitly', 'assumed', 'that', 'the', 'structure', 'of', 'the', 'network', 'modules,', 'describing', 'the', 'abstract', 'reasoning', 'process,', 'provides', 'a', 'faithful', 'explanation', 'of', 'the', ""model's"", 'reasoning;', 'that', 'is,', 'that', 'all', 'modules', 'perform', 'their', 'intended', 'behaviour.']",,,
191,"In this work , we propose and conduct a systematic evaluation of","the intermediate outputs of NMNs on NLVR2 and DROP , two datasets which require composing multiple reasoning steps",.,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,15.0,17.0,15.0,17.0,"[(15, 17)]",propose and conduct,0.0,18.0,37.0,18.0,37.0,"[(18, 25), (26, 29), (30, 37)]",['we'],"['propose', 'conduct']",conduct,[],"['a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP , two datasets which require composing multiple reasoning steps .']",-1.0,0.0,30.0,38.0,30.0,38.0,propose,propose,"[',', 'we']",[],0,18,26,18,26,,,"['NOUN', 'ADP', 'NOUN']","['outputs', 'of', 'evaluation']",https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,2,"In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps.","['In', 'this', 'work', ',', 'we', 'propose', 'and', 'conduct', 'a', 'systematic', 'evaluation', 'of', 'the', 'intermediate', 'outputs', 'of', 'NMNs', 'on', 'NLVR2', 'and', 'DROP', ',', 'two', 'datasets', 'which', 'require', 'composing', 'multiple', 'reasoning', 'steps', '.']","(12, 30)","(64, 178)",46.0
192,We find that,the intermediate outputs,"differ from the expected output , illustrating that the network structure does not provide a faithful explanation of model behaviour .",S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",find,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",['We'],['find'],differ,['the intermediate outputs'],[],1.0,2.0,38.0,45.0,0.0,7.0,find,find,['We'],[],0,3,8,3,8,differ,differ,"['NOUN', 'VERB', 'VERB']","['outputs', 'differ', 'find']",https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,3,"We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour.","['We', 'find', 'that', 'the', 'intermediate', 'outputs', 'differ', 'from', 'the', 'expected', 'output', ',', 'illustrating', 'that', 'the', 'network', 'structure', 'does', 'not', 'provide', 'a', 'faithful', 'explanation', 'of', 'model', 'behaviour', '.']","(3, 6)","(12, 36)",-1.0
193,"To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",,,S,"['S', ',', 'NP', 'VP', '.']",we,0.0,17.0,19.0,17.0,19.0,"[(17, 19)]",train and propose,0.0,20.0,74.0,20.0,74.0,"[(20, 25), (63, 66), (67, 74)]",['we'],"['remedy', 'train', 'propose']",,,,,,,,,,remedy,To remedy,[],"['we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness , at a minimal cost to accuracy .']",0,0,10,0,10,,,,,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Abstract,4,"To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.","['To', 'remedy', 'that,', 'we', 'train', 'the', 'model', 'with', 'auxiliary', 'supervision', 'and', 'propose', 'particular', 'choices', 'for', 'module', 'architecture', 'that', 'yield', 'much', 'better', 'faithfulness,', 'at', 'a', 'minimal', 'cost', 'to', 'accuracy.']",,,
194,Obtaining Faithful Interpretations from Compositional Neural Networks.,,,S,"['VP', '.']",,,,,,,[],Obtaining,0.0,0.0,9.0,0.0,9.0,"[(0, 9)]",[],['Obtaining'],,,,,,,,,,Obtaining,Obtaining,[],['Faithful Interpretations'],0,0,10,0,10,,,,,https://www.semanticscholar.org/paper/Obtaining-Faithful-Interpretations-from-Neural-Subramanian-Bogin/e4b1b8bee66c1bb438f72b36a9d2cfefd8e105a8,23,Title,0,Obtaining Faithful Interpretations from Compositional Neural Networks.,"['Obtaining', 'Faithful', 'Interpretations', 'from', 'Compositional', 'Neural', 'Networks.']",,,
195,We present a method to represent input texts by contextualizing them jointly with dynamically retrieved textual encyclopedic background knowledge from multiple documents.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",present,0.0,3.0,10.0,3.0,10.0,"[(3, 10)]",['We'],['present'],,,,,,,,,,present,present,['We'],['a method to represent input texts by contextualizing them jointly with dynamically retrieved textual encyclopedic background knowledge from multiple documents .'],0,3,11,3,11,,,,,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,0,We present a method to represent input texts by contextualizing them jointly with dynamically retrieved textual encyclopedic background knowledge from multiple documents.,"['We', 'present', 'a', 'method', 'to', 'represent', 'input', 'texts', 'by', 'contextualizing', 'them', 'jointly', 'with', 'dynamically', 'retrieved', 'textual', 'encyclopedic', 'background', 'knowledge', 'from', 'multiple', 'documents.']",,,
196,We apply our method to reading comprehension tasks by encoding questions and passages together with background sentences about the entities they mention.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",apply,0.0,3.0,8.0,3.0,8.0,"[(3, 8)]",['We'],['apply'],,,,,,,,,,method,method,['We apply'],[],0,13,20,13,20,,,,,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,1,We apply our method to reading comprehension tasks by encoding questions and passages together with background sentences about the entities they mention.,"['We', 'apply', 'our', 'method', 'to', 'reading', 'comprehension', 'tasks', 'by', 'encoding', 'questions', 'and', 'passages', 'together', 'with', 'background', 'sentences', 'about', 'the', 'entities', 'they', 'mention.']",,,
197,We show that integrating background knowledge from text is effective for tasks focusing on factual reasoning and allows direct reuse of powerful pretrained BERT-style encoders.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",show,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",['We'],['show'],,,,,,,,,,BERT,BERT,"['We show that integrating background knowledge from text is effective for tasks focusing on factual reasoning and allows direct reuse of powerful style encoders .', 'pretrained']",['-'],0,156,161,156,161,,,,,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,2,We show that integrating background knowledge from text is effective for tasks focusing on factual reasoning and allows direct reuse of powerful pretrained BERT-style encoders.,"['We', 'show', 'that', 'integrating', 'background', 'knowledge', 'from', 'text', 'is', 'effective', 'for', 'tasks', 'focusing', 'on', 'factual', 'reasoning', 'and', 'allows', 'direct', 'reuse', 'of', 'powerful', 'pretrained', 'BERT-style', 'encoders.']",,,
198,"Moreover, knowledge integration can be further improved with suitable pretraining via a self-supervised masked language model objective over words in background-augmented input text.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",knowledge integration,0.0,11.0,32.0,11.0,32.0,"[(11, 20), (21, 32)]",can be improved,0.0,33.0,56.0,33.0,56.0,"[(33, 36), (37, 39), (48, 56)]","['knowledge', 'integration']","['can', 'be', 'improved']",,,,,,,,,,further,can be further,"['Moreover , knowledge integration']",['improved with suitable pretraining via a self - supervised masked language model objective over words in background - augmented input text'],0,33,48,33,48,,,,,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,3,"Moreover, knowledge integration can be further improved with suitable pretraining via a self-supervised masked language model objective over words in background-augmented input text.","['Moreover,', 'knowledge', 'integration', 'can', 'be', 'further', 'improved', 'with', 'suitable', 'pretraining', 'via', 'a', 'self-supervised', 'masked', 'language', 'model', 'objective', 'over', 'words', 'in', 'background-augmented', 'input', 'text.']",,,
199,"On TriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable RoBERTa models which do not integrate background knowledge dynamically.",,,S,"['PP', ',', 'NP', 'VP', '.']",approach,0.0,18.0,26.0,18.0,26.0,"[(18, 26)]",obtains,0.0,27.0,34.0,27.0,34.0,"[(27, 34)]",['approach'],['obtains'],,,,,,,,,,obtains,obtains,[],['improvements of 1.6 to 3.1 F1 over comparable RoBERTa models which do not integrate background knowledge dynamically .'],0,27,35,27,35,,,,,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,4,"On TriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable RoBERTa models which do not integrate background knowledge dynamically.","['On', 'TriviaQA,', 'our', 'approach', 'obtains', 'improvements', 'of', '1.6', 'to', '3.1', 'F1', 'over', 'comparable', 'RoBERTa', 'models', 'which', 'do', 'not', 'integrate', 'background', 'knowledge', 'dynamically.']",,,
200,"On MRQA, a large collection of diverse QA datasets, we see consistent gains in-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2 F1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1).",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,54.0,56.0,54.0,56.0,"[(54, 56)]",see,0.0,57.0,60.0,57.0,60.0,"[(57, 60)]",['we'],['see'],,,,,,,,,,see,see,['we'],"['consistent', 'gains in - domain along with large improvements out - of - domain on BioASQ ( 2.1 to 4.2 F1 ) , TextbookQA ( 1.6 to 2.0 F1 ) , and DuoRC ( 1.1 to 2.0 F1 ) .']",0,57,61,57,61,,,,,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Abstract,5,"On MRQA, a large collection of diverse QA datasets, we see consistent gains in-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2 F1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1).","['On', 'MRQA,', 'a', 'large', 'collection', 'of', 'diverse', 'QA', 'datasets,', 'we', 'see', 'consistent', 'gains', 'in-domain', 'along', 'with', 'large', 'improvements', 'out-of-domain', 'on', 'BioASQ', '(2.1', 'to', '4.2', 'F1),', 'TextbookQA', '(1.6', 'to', '2.0', 'F1),', 'and', 'DuoRC', '(1.1', 'to', '2.0', 'F1).']",,,
201,Contextualized Representations Using Textual Encyclopedic Knowledge.,,,S,"['NP', 'VP', '.']",Representations,0.0,15.0,30.0,15.0,30.0,"[(15, 30)]",Using,0.0,31.0,36.0,31.0,36.0,"[(31, 36)]",['Representations'],['Using'],,,,,,,,,,Representations,Representations,[],['Using Textual Encyclopedic Knowledge .'],0,15,31,15,31,,,,,https://www.semanticscholar.org/paper/Contextualized-Representations-Using-Textual-Joshi-Lee/612f49eb1da46668b30f051bd3abc3b8ff6cc7b9,24,Title,0,Contextualized Representations Using Textual Encyclopedic Knowledge.,"['Contextualized', 'Representations', 'Using', 'Textual', 'Encyclopedic', 'Knowledge.']",,,
202,"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly.",,,S,"['NP', 'VP', '.']",Models,0.0,0.0,6.0,0.0,6.0,"[(0, 6)]",restrict,0.0,49.0,57.0,49.0,57.0,"[(49, 57)]",['Models'],['restrict'],,,,,,,,,,RC,RC,"['Models for reading', 'comprehension (']","[') commonly restrict their output space to the set of all single contiguous spans from the input , in order to alleviate the learning problem and avoid the need for a model that generates text explicitly .']",0,35,38,35,38,,,,,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,0,"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly.","['Models', 'for', 'reading', 'comprehension', '(RC)', 'commonly', 'restrict', 'their', 'output', 'space', 'to', 'the', 'set', 'of', 'all', 'single', 'contiguous', 'spans', 'from', 'the', 'input,', 'in', 'order', 'to', 'alleviate', 'the', 'learning', 'problem', 'and', 'avoid', 'the', 'need', 'for', 'a', 'model', 'that', 'generates', 'text', 'explicitly.']",,,
203,"However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text.",,,S,"['S', ',', 'CC', 'S', '.']",,,,,,,[],and,0.0,69.0,72.0,69.0,72.0,"[(69, 72)]","['some', 'recent', 'datasets']","['forcing', 'be', 'can', 'be', 'include']",,,,,,,,,,restrictive,can be restrictive,"['However', ',', 'forcing']","['some recent datasets also include multi - span questions , i.e. , questions whose answer is a set of non - contiguous spans in the text']",0,48,67,48,67,,,,,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,1,"However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text.","['However,', 'forcing', 'an', 'answer', 'to', 'be', 'a', 'single', 'span', 'can', 'be', 'restrictive,', 'and', 'some', 'recent', 'datasets', 'also', 'include', 'multi-span', 'questions,', 'i.e.,', 'questions', 'whose', 'answer', 'is', 'a', 'set', 'of', 'non-contiguous', 'spans', 'in', 'the', 'text.']",,,
204,"Naturally, models that return single spans cannot answer these questions.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",models,0.0,12.0,18.0,12.0,18.0,"[(12, 18)]",can answer,0.0,44.0,58.0,44.0,58.0,"[(44, 47), (52, 58)]",['models'],"['can', 'answer']",,,,,,,,,,answer,can answer,"['Naturally', ', models that return single spans']",['these questions'],0,44,59,44,59,,,,,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,2,"Naturally, models that return single spans cannot answer these questions.","['Naturally,', 'models', 'that', 'return', 'single', 'spans', 'cannot', 'answer', 'these', 'questions.']",,,
205,"In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,15.0,17.0,15.0,17.0,"[(15, 17)]",propose,0.0,18.0,25.0,18.0,25.0,"[(18, 25)]",['we'],['propose'],,,,,,,,,,propose,propose,['we'],['a simple architecture'],0,18,26,18,26,,,,,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,3,"In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not.","['In', 'this', 'work,', 'we', 'propose', 'a', 'simple', 'architecture', 'for', 'answering', 'multi-span', 'questions', 'by', 'casting', 'the', 'task', 'as', 'a', 'sequence', 'tagging', 'problem,', 'namely,', 'predicting', 'for', 'each', 'input', 'token', 'whether', 'it', 'should', 'be', 'part', 'of', 'the', 'output', 'or', 'not.']",,,
206,Our model substantially improves performance on span extraction questions from,DROP,and Quoref by 9.9 and 5.5 EM points respectively .,S,"['NP', 'VP', '.']",model,0.0,4.0,9.0,4.0,9.0,"[(4, 9)]",improves,0.0,24.0,32.0,24.0,32.0,"[(24, 32)]",['model'],['improves'],improves,['Our model'],[],-1.0,0.0,24.0,33.0,24.0,33.0,improves,improves,['Our model'],[],0,24,33,24,33,,and,"['PROPN', 'ADP', 'NOUN']","['DROP', 'from', 'questions']",https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Abstract,4,Our model substantially improves performance on span extraction questions from DROP and Quoref by 9.9 and 5.5 EM points respectively.,"['Our', 'model', 'substantially', 'improves', 'performance', 'on', 'span', 'extraction', 'questions', 'from', 'DROP', 'and', 'Quoref', 'by', '9.9', 'and', '5.5', 'EM', 'points', 'respectively', '.']","(10, 11)","(78, 82)",0.0
207,A Simple and Effective Model for Answering Multi-span Questions.,,,NP,"['NP', 'PP', '.']",A Model,0.0,0.0,28.0,0.0,28.0,"[(0, 1), (23, 28)]",,,,,,,[],"['A', 'Model']",[],,,,,,,,,,Simple,Simple,['A'],['Model for Answering Multi - span Questions .'],0,2,9,2,9,,,,,https://www.semanticscholar.org/paper/A-Simple-and-Effective-Model-for-Answering-Segal-Efrat/0e21b1f462703078fc7b0459209634ea4c9073ad,25,Title,0,A Simple and Effective Model for Answering Multi-span Questions.,"['A', 'Simple', 'and', 'Effective', 'Model', 'for', 'Answering', 'Multi-span', 'Questions.']",,,
208,"Complex, compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer.",,,S,"['NP', 'VP', '.']",Complex compositional reading comprehension datasets,0.0,0.0,54.0,0.0,54.0,"[(0, 7), (10, 23), (24, 31), (32, 45), (46, 54)]",require,0.0,55.0,62.0,55.0,62.0,"[(55, 62)]","['Complex', 'compositional', 'reading', 'comprehension', 'datasets']","['require', 'performing']",,,,,,,,,,require,require,"['Complex', ', compositional reading comprehension datasets']",['performing latent sequential decisions that are learned via supervision from the final answer .'],0,55,63,55,63,,,,,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,0,"Complex, compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer.","['Complex,', 'compositional', 'reading', 'comprehension', 'datasets', 'require', 'performing', 'latent', 'sequential', 'decisions', 'that', 'are', 'learned', 'via', 'supervision', 'from', 'the', 'final', 'answer.']",,,
209,"A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task.",,,S,"['NP', 'VP', '.']",A large combinatorial space,0.0,0.0,27.0,0.0,27.0,"[(0, 1), (2, 7), (8, 21), (22, 27)]",makes,0.0,171.0,176.0,171.0,176.0,"[(171, 176)]","['A', 'large', 'combinatorial', 'space']",['makes'],,,,,,,,,,paths,paths,['A large combinatorial space of'],"['that result in the same answer , compounded by the lack of intermediate supervision to help choose the right path , makes the learning particularly hard for this task .']",0,49,55,49,55,,,,,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,1,"A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task.","['A', 'large', 'combinatorial', 'space', 'of', 'possible', 'decision', 'paths', 'that', 'result', 'in', 'the', 'same', 'answer,', 'compounded', 'by', 'the', 'lack', 'of', 'intermediate', 'supervision', 'to', 'help', 'choose', 'the', 'right', 'path,', 'makes', 'the', 'learning', 'particularly', 'hard', 'for', 'this', 'task.']",,,
210,"In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,15.0,17.0,15.0,17.0,"[(15, 17)]",study,0.0,18.0,23.0,18.0,23.0,"[(18, 23)]",['we'],['study'],,,,,,,,,,study,study,"[',', 'we']","['the benefits of collecting intermediate reasoning supervision along with the answer during data collection', '.']",0,18,24,18,24,,,,,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,2,"In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection.","['In', 'this', 'work,', 'we', 'study', 'the', 'benefits', 'of', 'collecting', 'intermediate', 'reasoning', 'supervision', 'along', 'with', 'the', 'answer', 'during', 'data', 'collection.']",,,
211,We find that these intermediate annotations can provide two-fold benefits.,,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",find,0.0,3.0,7.0,3.0,7.0,"[(3, 7)]",['We'],['find'],,,,,,,,,,find,find,['We'],[],0,3,8,3,8,,,,,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,3,We find that these intermediate annotations can provide two-fold benefits.,"['We', 'find', 'that', 'these', 'intermediate', 'annotations', 'can', 'provide', 'two-fold', 'benefits.']",,,
212,"First , we observe that for any collection budget , spending a fraction of it on intermediate annotations results in improved model performance , for two complex compositional datasets :",DROP,and Quoref .,S,"['ADVP', ',', 'NP', 'VP', '.']",we,0.0,8.0,10.0,8.0,10.0,"[(8, 10)]",observe,0.0,11.0,18.0,11.0,18.0,"[(11, 18)]",['we'],['observe'],spending,[],['a fraction of it'],-1.0,0.0,52.0,61.0,52.0,61.0,observe,observe,"[',', 'we']","['that', ',']",0,11,19,11,19,,and,"['PROPN', 'ADP', 'NOUN']","['DROP', 'for', 'results']",https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,4,"First, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: DROP and Quoref.","['First', ',', 'we', 'observe', 'that', 'for', 'any', 'collection', 'budget', ',', 'spending', 'a', 'fraction', 'of', 'it', 'on', 'intermediate', 'annotations', 'results', 'in', 'improved', 'model', 'performance', ',', 'for', 'two', 'complex', 'compositional', 'datasets', ':', 'DROP', 'and', 'Quoref', '.']","(30, 31)","(186, 190)",0.0
213,"Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.",,,S,"['ADVP', ',', 'NP', 'VP', '.']",these annotations,0.0,9.0,26.0,9.0,26.0,"[(9, 14), (15, 26)]",encourage,0.0,27.0,36.0,27.0,36.0,"[(27, 36)]","['these', 'annotations']","['encourage', 'learn', 'helping']",,,,,,,,,,encourage,encourage,"[', these annotations']",['the model'],0,27,37,27,37,,,,,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Abstract,5,"Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.","['Second,', 'these', 'annotations', 'encourage', 'the', 'model', 'to', 'learn', 'the', 'correct', 'latent', 'reasoning', 'steps,', 'helping', 'combat', 'some', 'of', 'the', 'biases', 'introduced', 'during', 'the', 'data', 'collection', 'process.']",,,
214,Benefits of Intermediate Annotations in Reading Comprehension.,,,NP,"['NP', 'PP', '.']",Benefits,0.0,0.0,8.0,0.0,8.0,"[(0, 8)]",,,,,,,[],['Benefits'],[],,,,,,,,,,in,in,[],"['Reading', 'Benefits of Intermediate Annotations Comprehension .']",0,37,40,37,40,,,,,https://www.semanticscholar.org/paper/Benefits-of-Intermediate-Annotations-in-Reading-Dua-Singh/f4874bd968b785cb9fceeccf26c333567a2b8dca,26,Title,0,Benefits of Intermediate Annotations in Reading Comprehension.,"['Benefits', 'of', 'Intermediate', 'Annotations', 'in', 'Reading', 'Comprehension.']",,,
215,Complex question answering over knowledge base (Complex KBQA) is challenging because it requires the compositional reasoning capability.,,,S,"['NP', 'VP', '.']",Complex question,0.0,0.0,16.0,0.0,16.0,"[(0, 7), (8, 16)]",is,0.0,64.0,66.0,64.0,66.0,"[(64, 66)]","['Complex', 'question']",['is'],,,,,,,,,,challenging,is challenging,['Complex question answering over knowledge base ( Complex KBQA )'],[],0,64,79,64,79,,,,,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,0,Complex question answering over knowledge base (Complex KBQA) is challenging because it requires the compositional reasoning capability.,"['Complex', 'question', 'answering', 'over', 'knowledge', 'base', '(Complex', 'KBQA)', 'is', 'challenging', 'because', 'it', 'requires', 'the', 'compositional', 'reasoning', 'capability.']",,,
216,"Existing benchmarks have three shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are either generated by templates, leading to poor diversity, or on a small scale; and 3) they mostly only consider the relations among entities but not attributes.",,,S,"['S', ':', 'S', '.']",,,,,,,[],,,,,,,[],"['benchmarks', 'they', 'questions', 'they']","['have', 'provide', 'are', 'generated', 'leading', 'consider']",,,,,,,,,,have,have,"['Existing benchmarks three', 'shortcomings that limit the development of Complex KBQA : 1 ) they only provide QA pairs without explicit reasoning processes ; 2 ) questions are either generated by templates , leading to poor diversity , or on a small scale ; and 3 ) they mostly only consider the relations among entities but not attributes .']",[],0,20,25,20,25,,,,,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,1,"Existing benchmarks have three shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are either generated by templates, leading to poor diversity, or on a small scale; and 3) they mostly only consider the relations among entities but not attributes.","['Existing', 'benchmarks', 'have', 'three', 'shortcomings', 'that', 'limit', 'the', 'development', 'of', 'Complex', 'KBQA:', '1)', 'they', 'only', 'provide', 'QA', 'pairs', 'without', 'explicit', 'reasoning', 'processes;', '2)', 'questions', 'are', 'either', 'generated', 'by', 'templates,', 'leading', 'to', 'poor', 'diversity,', 'or', 'on', 'a', 'small', 'scale;', 'and', '3)', 'they', 'mostly', 'only', 'consider', 'the', 'relations', 'among', 'entities', 'but', 'not', 'attributes.']",,,
217,"To this end, we introduce KQA Pro, a large-scale dataset for Complex KBQA.",,,S,"['PP', ',', 'NP', 'VP', '.']",we,0.0,14.0,16.0,14.0,16.0,"[(14, 16)]",introduce,0.0,17.0,26.0,17.0,26.0,"[(17, 26)]",['we'],['introduce'],,,,,,,,,,introduce,introduce,"['we', 'this end']","['KQA Pro , a large - scale dataset for Complex KBQA']",0,17,27,17,27,,,,,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,2,"To this end, we introduce KQA Pro, a large-scale dataset for Complex KBQA.","['To', 'this', 'end,', 'we', 'introduce', 'KQA', 'Pro,', 'a', 'large-scale', 'dataset', 'for', 'Complex', 'KBQA.']",,,
218,"We generate questions, SPARQLs, and functional programs with recursive templates and then paraphrase the questions by crowdsourcing, giving rise to around 120K diverse instances.",,,S,"['NP', 'VP', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",generate and paraphrase,0.0,3.0,102.0,3.0,102.0,"[(3, 11), (83, 86), (92, 102)]",['We'],"['generate', 'paraphrase', 'crowdsourcing', 'giving']",,,,,,,,,,generate,generate,['We'],"['questions , SPARQLs , and functional programs with recursive templates and']",0,3,12,3,12,,,,,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,3,"We generate questions, SPARQLs, and functional programs with recursive templates and then paraphrase the questions by crowdsourcing, giving rise to around 120K diverse instances.","['We', 'generate', 'questions,', 'SPARQLs,', 'and', 'functional', 'programs', 'with', 'recursive', 'templates', 'and', 'then', 'paraphrase', 'the', 'questions', 'by', 'crowdsourcing,', 'giving', 'rise', 'to', 'around', '120K', 'diverse', 'instances.']",,,
219,"The SPARQLs and programs depict the reasoning processes in various manners, which can benefit a large spectrum of QA methods.",,,S,"['NP', 'VP', '.']",The SPARQLs programs,0.0,0.0,24.0,0.0,24.0,"[(0, 3), (4, 11), (16, 24)]",depict,0.0,25.0,31.0,25.0,31.0,"[(25, 31)]","['The', 'SPARQLs', 'programs']",['depict'],,,,,,,,,,depict,depict,['The SPARQLs and programs'],"['the reasoning processes in various manners , which can benefit a large spectrum of QA methods', '.']",0,25,32,25,32,,,,,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,4,"The SPARQLs and programs depict the reasoning processes in various manners, which can benefit a large spectrum of QA methods.","['The', 'SPARQLs', 'and', 'programs', 'depict', 'the', 'reasoning', 'processes', 'in', 'various', 'manners,', 'which', 'can', 'benefit', 'a', 'large', 'spectrum', 'of', 'QA', 'methods.']",,,
220,"We contribute a unified codebase and conduct extensive evaluations for baselines and state-of-the-arts: a blind GRU obtains 31.58%, the best model achieves only 35.15%, and humans top at 97.5%, which offers great research potential to fill the gap.",,,S,"['NP', 'VP', ':', 'S', ',', 'S', ',', 'CC', 'S', '.']",We,0.0,0.0,2.0,0.0,2.0,"[(0, 2)]",contribute and conduct and,0.0,3.0,183.0,3.0,183.0,"[(3, 13), (33, 36), (37, 44), (180, 183)]","['We', 'a', 'blind', 'GRU', 'the', 'best', 'model', 'humans']","['contribute', 'conduct', 'obtains', 'achieves', 'top']",,,,,,,,,,fill,fill,"['top at 97.5 % , which offers great research', 'potential', 'to']","['We contribute a unified codebase and conduct extensive evaluations for baselines and state - of - the - arts : a blind GRU obtains 31.58 % , the best model achieves only 35.15 % , and humans the gap .']",3,248,253,-1,4,,,,,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Abstract,5,"We contribute a unified codebase and conduct extensive evaluations for baselines and state-of-the-arts: a blind GRU obtains 31.58%, the best model achieves only 35.15%, and humans top at 97.5%, which offers great research potential to fill the gap.","['We', 'contribute', 'a', 'unified', 'codebase', 'and', 'conduct', 'extensive', 'evaluations', 'for', 'baselines', 'and', 'state-of-the-arts:', 'a', 'blind', 'GRU', 'obtains', '31.58%,', 'the', 'best', 'model', 'achieves', 'only', '35.15%,', 'and', 'humans', 'top', 'at', '97.5%,', 'which', 'offers', 'great', 'research', 'potential', 'to', 'fill', 'the', 'gap.']",,,
221,KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base.,,,NP,"['NP', ':', 'NP', '.']",KQA Pro A Large Diagnostic,0.0,0.0,28.0,0.0,28.0,"[(0, 3), (4, 7), (10, 11), (12, 17), (18, 28)]",,,,,,,[],"['KQA', 'Pro', 'A', 'Large', 'Diagnostic']",[],,,,,,,,,,KQA,KQA,[],"['Pro A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base', ':']",0,0,4,0,4,,,,,https://www.semanticscholar.org/paper/KQA-Pro%3A-A-Large-Diagnostic-Dataset-for-Complex-Shi-Cao/94312d7df647ba62bb542a8a19c69f3e5c4a32bb,27,Title,0,KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base.,"['KQA', 'Pro:', 'A', 'Large', 'Diagnostic', 'Dataset', 'for', 'Complex', 'Question', 'Answering', 'over', 'Knowledge', 'Base.']",,,
