{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do one-time large imports\n",
    "\n",
    "# For sentence tokenization\n",
    "from nltk import tokenize\n",
    "\n",
    "# For coreference resolution\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "coref_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\"\n",
    ")\n",
    "\n",
    "# For part-of-speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# For dependency parsing\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "dependency_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\"\n",
    ")\n",
    "\n",
    "# For constituency parsing\n",
    "constituency_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "### Basic input flags for the notebook / pipeline\n",
    "\n",
    "# Identify the term we are splitting on (the \"anchor\")\n",
    "# This also serves as the name for this entire cluster of results / practical pipeline run name\n",
    "search_word = 'BERT'\n",
    "\n",
    "# Other common names for this term that we should also consider as anchors\n",
    "# e.g. [search_word, 'GPT', 'GPT-2', 'GPT-3']\n",
    "# e.g. [search_word, 'Transformers', 'Transformer', 'transfer learning', 'transfer']\n",
    "anchor_synonyms = [search_word]\n",
    "\n",
    "# Flags\n",
    "flag_rerun_coreference = False or (not os.path.isfile(f'outputs/{search_word}/partial-coreference.hdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Read in the dataframe containing entire paper abstracts (NOT pre-split into sentences)\n",
    "df = pd.read_csv(f'data/nlp-align_{search_word}.csv')\n",
    "\n",
    "# Create the outputs directory for this search word\n",
    "Path(f\"outputs/{search_word}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split apart the 'Title' and 'Abstract' columns, add period to 'Title' if not present\n",
    "def separate_title_abstract(group):\n",
    "    row = group.loc[0]\n",
    "    abs_text = tokenize.sent_tokenize(row['Abstract'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * 2,\n",
    "        'ID': [row['ID']] * 2,\n",
    "        'Type': ['Title', 'Abstract'],\n",
    "        'Text': [row['Title']+'.' if not row['Title'].endswith('.') else row['Title'], \n",
    "                 row['Abstract']]\n",
    "    })\n",
    "\n",
    "# Restructure the dataframe to be more usable...\n",
    "df = df.groupby('ID', group_keys=False).apply(\n",
    "    lambda group: separate_title_abstract(group)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "def sentence_tokenize(group):\n",
    "    row = group.reset_index(drop=True).loc[0]\n",
    "    sentences = tokenize.sent_tokenize(row['Text'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * (len(sentences)),\n",
    "        'ID': [row['ID']] * (len(sentences)),\n",
    "        'Type': [row['Type']] * (len(sentences)),\n",
    "        'Index': list(range(len(sentences))),\n",
    "        'Text': sentences\n",
    "    })\n",
    "\n",
    "df_sentences = df.groupby(['ID', 'Type'], group_keys=False).apply(\n",
    "    lambda group: sentence_tokenize(group)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a test dataframe so we can run models without taking impractically long\n",
    "# # TODO: this is causing some type inconsistencies, fix those?\n",
    "\n",
    "# temp_df = pd.DataFrame.from_dict(\n",
    "#     {'URL': 'abc', \n",
    "#      'ID': '0', \n",
    "#      'Title': 'Paper Title',\n",
    "#      'Abstract': 'The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. Petroni et al. (2019) take this as evidence that it memorizes factual knowledge during pre-training. We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.'\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting functions\n",
    "\n",
    "Assume we have an input dataframe with some number of columns, at least one of which is titled `Text` and is the column containing each sentence of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_0 is the text literally preceding the anchor\n",
    "# split_1 is the text that the anchor consists of\n",
    "# split_2 is the text literally following the anchor\n",
    "# split_tokens is the list of tokens that split identifies\n",
    "# split_anchor_span is a tuple (anchor_first_token_index, anchor_last_token_index) or null if there is no anchor\n",
    "# split_anchor_indices is a tuple (anchor_start_char_index, anchor_end_char_index) or null if there is no anchor\n",
    "splitting_headers = ['split_0','split_1','split_2', \n",
    "                     'split_tokens', 'split_anchor_span', 'split_anchor_indices', \n",
    "                     'within_anchor_index']\n",
    "# Include ID, Type, Index in the split output to be able to join with df_sentences\n",
    "join_headers = ['ID', 'Type', 'Index']\n",
    "# The headers used for checking if rows should be eliminated as duplicate\n",
    "duplicate_check_headers = splitting_headers[:3]+join_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split on the search word, regardless of whitespace (if search word is A and we have word CAR, it slices it up)\n",
    "# Splits on ALL instances of the search word\n",
    "def split_term_literal(group, search_word, anchor_synonyms):\n",
    "    row = group.iloc[0]\n",
    "    output = []\n",
    "    anchors = [re.search(f'({a})', row['Text'], flags=re.IGNORECASE) for a in anchor_synonyms]\n",
    "    anchors = [a.group(1) for a in anchors if (a is not None)]\n",
    "    for anchor in anchors:\n",
    "        splits = row['Text'].split(anchor)\n",
    "        for i in range(len(splits) - 1):\n",
    "            output_i = [anchor.join(splits[:i+1]), anchor.strip(), anchor.join(splits[i+1:])]\n",
    "            output_i = [i.strip() for i in output_i]\n",
    "            # write tokens list and revise split to account for tokenization\n",
    "            pre_split = nltk.word_tokenize(output_i[0])\n",
    "            output_i[0] = ' '.join(pre_split)\n",
    "            mid_split = nltk.word_tokenize(anchor)\n",
    "            output_i[1] = ' '.join(mid_split)\n",
    "            post_split = nltk.word_tokenize(output_i[2])\n",
    "            output_i[2] = ' '.join(post_split)\n",
    "            # split_tokens\n",
    "            output_i.append(pre_split + mid_split + post_split)\n",
    "            # split_anchor_span\n",
    "            output_i.append((len(pre_split), len(pre_split)+len(mid_split)))\n",
    "            # split_anchor_indices\n",
    "            output_i.append((len(output_i[0]), len(output_i[0])+len(output_i[1])))\n",
    "            # within_anchor_index\n",
    "            # for split_term_literal, the split term is ALWAYS the entire anchor\n",
    "            output_i.append(0)\n",
    "            output_i += list(row[join_headers])\n",
    "            output.append(output_i)\n",
    "    if output == []:\n",
    "        output = [[row['Text'].strip(),'','',\n",
    "                   row['Text'].strip().split(' '),None,None,None]\n",
    "                  +list(row[join_headers])]\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    return pd.DataFrame(\n",
    "        dict(zip(splitting_headers+join_headers,output_t))\n",
    "    ).drop_duplicates(duplicate_check_headers)\n",
    "\n",
    "literal_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "    lambda group: split_term_literal(group, search_word, anchor_synonyms)).reset_index(drop=True)\n",
    "\n",
    "literal_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split on any token (whitespace delineated, or token-delineated) that contains an instance of the search word\n",
    "# Splits on ALL instances of the search word\n",
    "# a little bit misleadingly named, sorry. the intention is for it to be a simple split\n",
    "def split_term_whitespace(group, search_word, anchor_synonyms):\n",
    "    row = group.iloc[0]\n",
    "    output = []\n",
    "    anchors = [re.search(rf'(^|\\W)(\\w*{a}\\w*)($|\\W)', row['Text'], flags=re.IGNORECASE) for a in anchor_synonyms]\n",
    "    anchors = [a.group(2) for a in anchors if (a is not None)]\n",
    "    for anchor in anchors:\n",
    "        splits = row['Text'].split(anchor)\n",
    "        for i in range(len(splits) - 1):\n",
    "            output_i = [anchor.join(splits[:i+1]), anchor.strip(), anchor.join(splits[i+1:])]\n",
    "            output_i = [i.strip() for i in output_i]\n",
    "            # write tokens list and revise split to account for tokenization\n",
    "            pre_split = nltk.word_tokenize(output_i[0])\n",
    "            output_i[0] = ' '.join(pre_split)\n",
    "            mid_split = nltk.word_tokenize(anchor)\n",
    "            output_i[1] = ' '.join(mid_split)\n",
    "            post_split = nltk.word_tokenize(output_i[2])\n",
    "            output_i[2] = ' '.join(post_split)\n",
    "            # split_tokens\n",
    "            output_i.append(pre_split + mid_split + post_split)\n",
    "            # split_anchor_span\n",
    "            output_i.append((len(pre_split), len(pre_split)+len(mid_split)))\n",
    "            # split_anchor_indices\n",
    "            output_i.append((len(output_i[0]), len(output_i[0])+len(output_i[1])))\n",
    "            # within_anchor_index\n",
    "            anchorsearch = [re.search(rf'{a}', anchor, flags=re.IGNORECASE) for a in anchor_synonyms]\n",
    "            output_i.append([a.start() for a in anchorsearch if (a is not None)][0])\n",
    "            output_i += list(row[join_headers])\n",
    "            output.append(output_i)\n",
    "    if output == []:\n",
    "        output = [[row['Text'].strip(),'','',\n",
    "                   row['Text'].strip().split(' '),None,None,None]\n",
    "                  +list(row[join_headers])]\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    return pd.DataFrame(\n",
    "        dict(zip(splitting_headers+join_headers,output_t))\n",
    "    ).drop_duplicates(duplicate_check_headers)\n",
    "\n",
    "whitespace_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "    lambda group: split_term_whitespace(group, search_word, anchor_synonyms)).reset_index(drop=True)\n",
    "\n",
    "whitespace_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run coreference resolution over the entire abstract, not individual sentences\n",
    "if flag_rerun_coreference:\n",
    "    output = df.apply(\n",
    "        lambda row: coref_predictor.predict(row['Text']), axis=1, result_type='expand')\n",
    "    df_merged = df.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# transform the output of coreference resolution into something that is more easily manipulated\n",
    "# split it across multiple sentences so each indiv sentence row can still work\n",
    "def reinterpret_coref_clusters(row, search_word, anchor_synonyms, sentences):\n",
    "    # Create dicts to map full-document to indiv sentence data\n",
    "    src = sentences.loc[sentences['ID'] == row['ID']].loc[sentences['Type'] == row['Type']]['Text']\n",
    "    curr_sentence = 0\n",
    "    consumed = 0\n",
    "    sent_mapping = {}\n",
    "    sent_content = {}\n",
    "    last_sent_end = 0\n",
    "    doct_mapping = {}\n",
    "    doct_split = []\n",
    "    for i in range(len(row['document'])):\n",
    "        if row['document'][i].strip() != '':\n",
    "            if row['document'][i] not in src.iloc[curr_sentence][consumed:]:\n",
    "                doct_split.append(row['document'][last_sent_end:i])\n",
    "                last_sent_end = i\n",
    "                curr_sentence += 1\n",
    "                consumed = 0\n",
    "            offset = src.iloc[curr_sentence][consumed:].index(row['document'][i])\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            if curr_sentence not in sent_content:\n",
    "                sent_content[curr_sentence] = []\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "            consumed += offset + len(row['document'][i])\n",
    "        else:\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "        doct_split.append(row['document'][last_sent_end:])\n",
    "    # Select the clusters that contain search words\n",
    "    selcluster_idxs = set()\n",
    "    for i in range(len(row['clusters'])):\n",
    "        currcluster_ct = 0\n",
    "        for c in row['clusters'][i]:\n",
    "            for anchor in anchor_synonyms:\n",
    "                # TODO this does overcounting if an anchor synonym is contained within another\n",
    "                currcluster_ct += len(\n",
    "                    re.findall(f'{anchor}', ''.join(row['document'][c[0]:c[1]+1]), flags=re.IGNORECASE)\n",
    "                )\n",
    "        if currcluster_ct > 0:\n",
    "            selcluster_idxs.add(i)\n",
    "    # Build the output row\n",
    "    output = [sent_mapping, sent_content, doct_mapping, list(selcluster_idxs)]\n",
    "    return dict(zip(['sent_mapping', 'sent_content', 'doct_mapping', 'selcluster_idxs'],output))\n",
    "\n",
    "if flag_rerun_coreference:\n",
    "    output = df_merged.apply(\n",
    "        lambda row: reinterpret_coref_clusters(row, search_word, anchor_synonyms, df_sentences), \n",
    "        axis=1, result_type='expand')\n",
    "    df_merged = df_merged.join(output)\n",
    "    \n",
    "    df_merged.to_pickle(f'outputs/{search_word}/partial-coreference.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_pickle(f'outputs/{search_word}/partial-coreference.pkl')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split based on co-references to any phrase containing search term, using allennlp coreference resolution\n",
    "# This does NOT preserve the original sentence spacing\n",
    "# REQUIRES THAT WE ALREADY RAN THE COREFERENCE PREDICTOR - this func does NOT do all of the work!\n",
    "# Splits on ALL instances of references to the search word,\n",
    "# including sub-references (e.g. \"the accuracy of RoBERTa\")\n",
    "def split_term_coreference(group, search_word, anchor_synonyms, lookup, fallback):\n",
    "    row = group.iloc[0]\n",
    "    # there's probably a cleaner way to do this...\n",
    "    lookup_row = lookup.loc[lookup['ID']==row['ID']].loc[lookup['Type']==row['Type']].to_dict(orient='records')[0]\n",
    "    if len(lookup_row['selcluster_idxs']) == 0:\n",
    "        # if we didn't identify any clusters that match the search term, use our fallback method\n",
    "        return fallback(group, search_word, anchor_synonyms)\n",
    "    output = []\n",
    "    for cluster_id in lookup_row['selcluster_idxs']:\n",
    "        split_clusters = lookup_row['clusters'][cluster_id]\n",
    "        for i in range(len(split_clusters)):\n",
    "            c = split_clusters[i]\n",
    "            if lookup_row['sent_mapping'][c[0]] == row['Index']:\n",
    "                sentence_start = lookup_row['sent_content'][row['Index']][0]\n",
    "                sentence_end = lookup_row['sent_content'][row['Index']][-1]\n",
    "                pre_split = lookup_row['document'][sentence_start:c[0]]\n",
    "                anchor = lookup_row['document'][c[0]:c[1]+1]\n",
    "                post_split = lookup_row['document'][c[1]+1:sentence_end+1]\n",
    "                output_i=[' '.join(pre_split),\n",
    "                        ' '.join(anchor),\n",
    "                        ' '.join(post_split)]\n",
    "                # split_tokens\n",
    "                output_i.append(lookup_row['document'][sentence_start:sentence_end+1])\n",
    "                # split_anchor_span\n",
    "                output_i.append((len(pre_split), len(pre_split)+len(anchor)))\n",
    "                # split_anchor_indices\n",
    "                output_i.append((len(output_i[0]), len(output_i[0])+len(output_i[1])))\n",
    "                # within_anchor_index\n",
    "                anchorsearch = [re.search(rf'{a}', ' '.join(anchor), flags=re.IGNORECASE) for a in anchor_synonyms]\n",
    "                anchorsearch = [a.start() for a in anchorsearch if (a is not None)]\n",
    "                output_i.append(anchorsearch[0] if len(anchorsearch) > 0 else -1)\n",
    "                output_i += list(row[join_headers])\n",
    "                output.append(output_i)\n",
    "    if output == []:\n",
    "        # if there wasn't any reference in the sentence found, use our fallback method\n",
    "        return fallback(group, search_word, anchor_synonyms)\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    return pd.DataFrame(dict(zip(splitting_headers+join_headers,output_t)))\n",
    "\n",
    "coreference_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "    lambda group: split_term_coreference(group, search_word, anchor_synonyms, df_merged, split_term_whitespace)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "coreference_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping functions\n",
    "\n",
    "Assume we have an input dataframe with column headers `['split_0','split_1','split_2', 'split_tokens', 'split_anchor_span']`\n",
    "\n",
    "`'split_1'` is the column that contains our search term / anchor point\n",
    "\n",
    "`'split_0'` and `'split_2'` are the columns that contain text before and after the search terms respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group is the text uniquely identifying a group\n",
    "grouping_headers = [\n",
    "    'group', 'group2', 'group3', 'group4', 'group5', \n",
    "    'group6', 'group7', 'group8', 'group9', 'group10', \n",
    "    'group11', 'group12', 'group13', 'group14', 'group15', \n",
    "    'group16', 'group17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = df_sentences.merge(\n",
    "    coreference_output,\n",
    "    how='outer',\n",
    "    left_on=join_headers,\n",
    "    right_on=join_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the literal first word that comes after the anchor point\n",
    "# skips things that are punctuation\n",
    "def group_first_word(row):\n",
    "    output = None\n",
    "    if row['split_anchor_span'] is not None:\n",
    "        index = row['split_anchor_span'][1]\n",
    "        while (index < len(row['split_tokens'])) and (output is None):\n",
    "            next_token = row['split_tokens'][index]\n",
    "            next_token_r = re.search(rf'^[.,():-]*(\\w+(.+\\w+)*)[.,():-]*$', next_token, flags=re.IGNORECASE)\n",
    "            if next_token_r is not None:\n",
    "                output = [next_token_r.group(1)]\n",
    "            else:\n",
    "                index += 1\n",
    "    if output is None:\n",
    "        output = ['']\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_first_word(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the first verb that comes after the anchor point, using NLTK part-of-speech tagging\n",
    "def group_first_verb(row):\n",
    "    tokens = [nltk.word_tokenize(row['split_0']), \n",
    "              nltk.word_tokenize(row['split_1']),\n",
    "              nltk.word_tokenize(row['split_2'])]\n",
    "    tokens_pos = nltk.pos_tag([item for sublist in tokens for item in sublist])\n",
    "    verb = ''\n",
    "    for i in range(len(tokens[0])+len(tokens[1]), len(tokens_pos)):\n",
    "        if tokens_pos[i][1].startswith('V'):\n",
    "            verb = tokens_pos[i][0]\n",
    "            break\n",
    "    output = [verb]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_first_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guide on dependency parses:\n",
    "# https://web.stanford.edu/~jurafsky/slp3/15.pdf\n",
    "\n",
    "# POS tag info:\n",
    "# https://universaldependencies.org/u/pos/\n",
    "# https://cs.nyu.edu/grishman/jet/guide/PennPOS.html\n",
    "\n",
    "# Do dependency parsing once for the entire sample_input to save processing time\n",
    "# for groupings that require dependency parsing later\n",
    "def parse_dependency(row):\n",
    "    p = dependency_predictor.predict(\n",
    "        sentence=' '.join(row['split_tokens']).strip()\n",
    "    )\n",
    "    return dict(zip(['dependency_parse'], [p]))\n",
    "\n",
    "sample_input_dep = sample_input.apply(\n",
    "    lambda row: parse_dependency(row), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "sample_input_dep.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Guide on constituency parses:\n",
    "# https://web.stanford.edu/~jurafsky/slp3/13.pdf\n",
    "\n",
    "# POS tag info:\n",
    "# https://universaldependencies.org/u/pos/\n",
    "# https://cs.nyu.edu/grishman/jet/guide/PennPOS.html\n",
    "# http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html\n",
    "\n",
    "# Side effect: modifies the given node to add a 'spans' field\n",
    "def add_constituency_span_recursive(node):\n",
    "    curr_span_start = node['spans'][0]['start']\n",
    "    curr_word = node['word']\n",
    "    for c in (node['children'] if ('children' in node) else []):\n",
    "        c_start = curr_span_start + curr_word.index(c['word'])\n",
    "        c_end = c_start + len(c['word'])\n",
    "        c['spans'] = [{\n",
    "            'start': c_start,\n",
    "            'end': c_end\n",
    "        }]\n",
    "        add_constituency_span_recursive(c)\n",
    "        curr_span_start += len(c['word'])+1\n",
    "        curr_word = curr_word[len(c['word'])+1:]\n",
    "    return\n",
    "\n",
    "# Edit the constituency parse dict to contain a 'spans' field in the hierplane_tree\n",
    "def add_constituency_span(parse):\n",
    "    parse = copy.deepcopy(parse)\n",
    "    parse['hierplane_tree']['root']['spans'] = [{\n",
    "        'start': 0, \n",
    "        'end': len(parse['hierplane_tree']['root']['word'])\n",
    "    }]\n",
    "    add_constituency_span_recursive(parse['hierplane_tree']['root'])\n",
    "    return parse\n",
    "\n",
    "# Do constituency parsing once for the entire sample_input to save processing time\n",
    "# for groupings that require constituency parsing later\n",
    "# Also add a span field to the parse dict so that it's easier to process later too\n",
    "def parse_constituency(row):\n",
    "    p = constituency_predictor.predict(\n",
    "        sentence=' '.join(row['split_tokens']).strip()\n",
    "    )\n",
    "    p = add_constituency_span(p)\n",
    "    return dict(zip(['constituency_parse'], [p]))\n",
    "\n",
    "sample_input_con = sample_input.apply(\n",
    "    lambda row: parse_constituency(row), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "sample_input_con.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on sentence-level details, using allennlp constituency parsing\n",
    "def group_type_sentence_con(row):\n",
    "    p = row['constituency_parse']\n",
    "    sentence = p['hierplane_tree']['root']\n",
    "    tl_fillers = [] # [',', '.', ':', 'HYPH']\n",
    "    tl_breakdown = [child['nodeType'] for child in sentence['children'] \n",
    "                    if child['nodeType'] not in tl_fillers]\n",
    "    output = [sentence['nodeType'], tl_breakdown]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.join(sample_input_con).apply(\n",
    "    lambda row: group_type_sentence_con(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns n if the span is within split_n\n",
    "def split_span_descriptor(span, split):\n",
    "    # span = (whole_sentence_begin_index, whole_sentence_end_index)\n",
    "    # split = [str(split_0), str(split_1), str(split_2), maybe more ...]\n",
    "    length_sum = 0\n",
    "    spillover = span[0]\n",
    "    for i in range(len(split)):\n",
    "        length_sum += len(split[i])\n",
    "        if span[0] < length_sum:\n",
    "            return i, (spillover, spillover + span[1] - span[0])\n",
    "        spillover -= len(split[i]) + (1 if len(split[i]) > 0 else 0)\n",
    "    return len(split), (spillover, spillover + span[1] - span[0])\n",
    "\n",
    "print(split_span_descriptor((0, 3), ['', 'abc', 'def ghi']), \n",
    "      split_span_descriptor((4, 7), ['', 'abc', 'def ghi']), \n",
    "      split_span_descriptor((8, 11), ['', 'abc', 'def ghi']))\n",
    "print(split_span_descriptor((0, 3), ['abc', '', 'def ghi']), \n",
    "      split_span_descriptor((4, 7), ['abc', '', 'def ghi']), \n",
    "      split_span_descriptor((8, 11), ['abc', '', 'def ghi']))\n",
    "print(split_span_descriptor((0, 3), ['abc', 'def', 'ghi']), \n",
    "      split_span_descriptor((4, 7), ['abc', 'def', 'ghi']), \n",
    "      split_span_descriptor((8, 11), ['abc', 'def', 'ghi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group on the main verb in the sentence, using allennlp dependency parsing\n",
    "def group_main_verb_dep(row):\n",
    "    p = row['dependency_parse']\n",
    "    main_verb = p['hierplane_tree']['root']\n",
    "    main_verb_span = (main_verb['spans'][0]['start'], main_verb['spans'][0]['end'])\n",
    "    split_loc, cspan = split_span_descriptor(main_verb_span, [row['split_0'], row['split_1'], row['split_2']])\n",
    "    output = [main_verb['word'], split_loc, \n",
    "              main_verb_span[0], main_verb_span[1], cspan[0], cspan[1]]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.join(sample_input_dep).apply(\n",
    "    lambda row: group_main_verb_dep(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlaps two strings, filling in any whitespace characters in s1 with the non-whitespace char in s2\n",
    "# If len(s2) > len(s1), the extra is added to the end\n",
    "def string_union(s1, s2):\n",
    "    output = list(s1)\n",
    "    for i in range(min(len(s1), len(s2))):\n",
    "        if s1[i].isspace():\n",
    "            output[i] = s2[i]\n",
    "    # add on the extra if s2 is longer\n",
    "    if len(s2) > len(s1):\n",
    "        output += list(s2[len(s1):])\n",
    "    return ''.join(output)\n",
    "\n",
    "# Overlaps two spans, producing their union (if there is a gap in between, it fills the gaps)\n",
    "def span_union(s1, s2):\n",
    "    return (min(s1[0], s2[0]), max(s1[1], s2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "# Returns the entire phrase that comprises a dependency tree node and its children\n",
    "def unroll_dependency_node(node, allowed_links=None, allowed_types=None):\n",
    "    node_str = ' '*node['spans'][0]['start'] + node['word']\n",
    "    node_span = (node['spans'][0]['start'], node['spans'][0]['end'])\n",
    "    if 'children' in node:\n",
    "        for i in range(len(node['children'])):\n",
    "            if (allowed_links is None) or (node['children'][i]['link'] in allowed_links):\n",
    "                if (allowed_types is None) or (node['children'][i]['attributes'][0] in allowed_types):\n",
    "                    child_str, child_span = unroll_dependency_node(\n",
    "                        node['children'][i],\n",
    "                        allowed_links=allowed_links,\n",
    "                        allowed_types=allowed_types)\n",
    "                    node_str = string_union(node_str, child_str)\n",
    "                    node_span = span_union(node_span, child_span)\n",
    "    return node_str, node_span\n",
    "\n",
    "# print(unroll_dependency_node(sample_input_dep.iloc[0][0]['hierplane_tree']['root']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_from_index(p, i):\n",
    "    pathway = []\n",
    "    while p['predicted_heads'][i] != 0:\n",
    "        # count what index of its parent this node is\n",
    "        parent_idx = 0\n",
    "        for test_i in range(i):\n",
    "            if p['predicted_heads'][i] == p['predicted_heads'][test_i]:\n",
    "                parent_idx += 1\n",
    "        pathway.insert(0, parent_idx)\n",
    "        i = p['predicted_heads'][i] - 1\n",
    "    curr_node = p['hierplane_tree']['root']\n",
    "    for child_i in pathway:\n",
    "        curr_node = curr_node['children'][child_i]\n",
    "    return curr_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_children(node, types):\n",
    "    child_nodes = node['children'] if ('children' in node) else []\n",
    "    matches = []\n",
    "    for i in range(len(child_nodes)):\n",
    "        if child_nodes[i]['nodeType'] in types:\n",
    "            matches.append(child_nodes[i])\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the main verb in the sentence, using allennlp dependency parsing\n",
    "# Expanded to include subject, object, expand upon verb form\n",
    "def group_main_verb_expanded_dep(row):\n",
    "    p = row['dependency_parse']\n",
    "    root_node = p['hierplane_tree']['root']\n",
    "    root_phrase, root_span = unroll_dependency_node(\n",
    "        root_node,\n",
    "        allowed_links=['aux', 'auxpass', 'cop'],\n",
    "        allowed_types=['VERB', 'AUX', 'PART']\n",
    "    )\n",
    "    root_phrase = ' '.join(root_phrase.strip().split())\n",
    "    split_loc, cspan = split_span_descriptor(root_span, [row['split_0'], row['split_1'], row['split_2']])\n",
    "    subjects = get_node_children(root_node, ['subj', 'nsubj', 'nsubjpass', 'csubj', 'csubjpass'])\n",
    "    subjects = [' '.join(unroll_dependency_node(n)[0].strip().split()) for n in subjects]\n",
    "    objects = get_node_children(root_node, ['obj', 'dobj', 'iobj', 'pobj'])\n",
    "    objects = [' '.join(unroll_dependency_node(n)[0].strip().split()) for n in objects]\n",
    "    # Pick out dependencies that come before/after the verb...\n",
    "    # this targets dependencies that weren't clearly identified as subj/obj by the parser\n",
    "    unsure_relation = get_node_children(root_node, ['dep'])\n",
    "    unsure_relation = [(n['spans'][0]['start'],\n",
    "                        ' '.join(unroll_dependency_node(n)[0].strip().split()))\n",
    "                       for n in unsure_relation]\n",
    "    # TODO improve this possibly\n",
    "    # For now, assume a dependent phrase before the averb is more likely to be subject-like\n",
    "    # and a dependent phrase after the averb is more likely to be object-like\n",
    "    subjects += [n[1] for n in unsure_relation if n[0] < root_node['spans'][0]['start']]\n",
    "    objects += [n[1] for n in unsure_relation if n[0] > root_node['spans'][0]['start']]\n",
    "    output = [root_phrase, subjects, objects, \n",
    "              split_loc, root_span[0], root_span[1], cspan[0], cspan[1]]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.join(sample_input_dep).apply(\n",
    "    lambda row: group_main_verb_expanded_dep(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement some kind of clustering for level-1 sentence structure\n",
    "# (order of NP, VP, PP, commas or periods etc in the sentence)\n",
    "# can start by making a distance metric between different structure lists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "# Returns the entire phrase that comprises a constituency tree node and its children\n",
    "# If there's link or type restrictions, then enforces those.\n",
    "def unroll_constituency_node(node, allowed_links=None, allowed_types=None):\n",
    "    node_str = []\n",
    "    node_span = []\n",
    "    if 'children' in node:\n",
    "        for i in range(len(node['children'])):\n",
    "            if (allowed_links is None) or (node['children'][i]['link'] in allowed_links):\n",
    "                if (allowed_types is None) or (node['children'][i]['attributes'][0] in allowed_types):\n",
    "                    child_str, child_span = unroll_constituency_node(\n",
    "                        node['children'][i],\n",
    "                        allowed_links=allowed_links,\n",
    "                        allowed_types=allowed_types)\n",
    "                    node_str += child_str\n",
    "                    node_span += child_span\n",
    "    else:\n",
    "        node_str = [node['word']]\n",
    "        node_span = [(node['spans'][0]['start'], node['spans'][0]['end'])]\n",
    "    return node_str, node_span\n",
    "\n",
    "# print(unroll_dependency_node(sample_input_dep.iloc[0][0]['hierplane_tree']['root']))\n",
    "for num in range(100):\n",
    "    print(sample_input_con.iloc[num][0]['hierplane_tree']['root']['word'])\n",
    "    print('=====')\n",
    "    print(unroll_constituency_node(\n",
    "        sample_input_con.iloc[num][0]['hierplane_tree']['root'],\n",
    "        allowed_links=['DT','JJ','JJR','JJS','NP','PRP','PRPS','NN','NNS','NNP','NNPS','POS']))\n",
    "    print('=====')\n",
    "    print(unroll_constituency_node(\n",
    "        sample_input_con.iloc[num][0]['hierplane_tree']['root'],\n",
    "        allowed_links=['S','DT','JJ','JJR','JJS','NP','PRP','PRPS','NN','NNS','NNP','NNPS']))\n",
    "    print('=====')\n",
    "    print(unroll_constituency_node(\n",
    "        sample_input_con.iloc[num][0]['hierplane_tree']['root'],\n",
    "        allowed_links=['VP','VB','VBD','VBG','VBN','VBP','VBZ','MD','CC']))\n",
    "    print('=====')\n",
    "    print(unroll_constituency_node(\n",
    "        sample_input_con.iloc[num][0]['hierplane_tree']['root'],\n",
    "        allowed_links=['S','VP','VB','VBD','VBG','VBN','VBP','VBZ','MD']))\n",
    "    print('===============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Group on the main subject and verb in the sentence, using allennlp constituency parsing\n",
    "# Also include a misc collection of subject-like words and verb-like words\n",
    "def group_main_verb_con(row):\n",
    "    p = row['constituency_parse']\n",
    "    sentence = p['hierplane_tree']['root']\n",
    "    tl_subj, tl_subj_span = unroll_constituency_node(\n",
    "        sentence,\n",
    "        allowed_links=['DT','JJ','JJR','JJS','NP','PRP','PRPS','NN','NNS','NNP','NNPS','POS'])\n",
    "    if len(tl_subj_span) > 0:\n",
    "        tl_subj_span_unified = functools.reduce(span_union, tl_subj_span)\n",
    "        tl_subj_split_loc, tl_subj_cspan = split_span_descriptor(\n",
    "            tl_subj_span_unified, [row['split_0'], row['split_1'], row['split_2']])\n",
    "    else:\n",
    "        tl_subj_span_unified = ('', '')\n",
    "        tl_subj_split_loc = ''\n",
    "        tl_subj_cspan = ('', '')\n",
    "    all_subj, all_subj_span = unroll_constituency_node(\n",
    "        sentence,\n",
    "        allowed_links=['S','DT','JJ','JJR','JJS','NP','PRP','PRPS','NN','NNS','NNP','NNPS'])\n",
    "    tl_verb, tl_verb_span = unroll_constituency_node(\n",
    "        sentence,\n",
    "        allowed_links=['VP','VB','VBD','VBG','VBN','VBP','VBZ','MD','CC'])\n",
    "    if len(tl_verb_span) > 0:\n",
    "        tl_verb_span_unified = functools.reduce(span_union, tl_verb_span)\n",
    "        tl_verb_split_loc, tl_verb_cspan = split_span_descriptor(\n",
    "            tl_verb_span_unified, [row['split_0'], row['split_1'], row['split_2']])\n",
    "    else:\n",
    "        tl_verb_span_unified = ('', '')\n",
    "        tl_verb_split_loc = ''\n",
    "        tl_verb_cspan = ('', '')\n",
    "    all_verb, all_verb_span = unroll_constituency_node(\n",
    "        sentence,\n",
    "        allowed_links=['S','VP','VB','VBD','VBG','VBN','VBP','VBZ','MD'])\n",
    "    output = [' '.join(tl_subj), tl_subj_split_loc, \n",
    "              tl_subj_span_unified[0], tl_subj_span_unified[1], \n",
    "              tl_subj_cspan[0], tl_subj_cspan[1],\n",
    "              tl_subj_span,\n",
    "              ' '.join(tl_verb), tl_verb_split_loc, \n",
    "              tl_verb_span_unified[0], tl_verb_span_unified[1], \n",
    "              tl_verb_cspan[0], tl_verb_cspan[1],\n",
    "              tl_verb_span,\n",
    "              all_subj, all_verb]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.join(sample_input_con).apply(\n",
    "    lambda row: group_main_verb_con(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for phrase POS\n",
    "# Return the match bounds of the sequence of elements of given sizes starting at list1[i1] and list2[i2] \n",
    "# that match\n",
    "# If no given size is returned, returns max matching sequence length\n",
    "# (ratio of element matches must be 1:some or some:1 between l1 and l2)\n",
    "# Returns [(l1 bounds), (l2 bounds)] or None if they do not match\n",
    "def list_elements_match(list1, list2, i1, i2, size1=None, size2=None):\n",
    "    matchlen = 0\n",
    "    if size1 is not None and size2 is not None:\n",
    "        # check for exact text match\n",
    "        matchlen = len(''.join(list1[i1:i1+size1]))\n",
    "        if ''.join(list1[i1:i1+size1]) != ''.join(list2[i2:i2+size2]):\n",
    "            return None\n",
    "    elif size1 is not None:\n",
    "        # and size2 is none\n",
    "        matchlen = len(''.join(list1[i1:i1+size1]))\n",
    "        if ''.join(list1[i1:i1+size1]) != ''.join(list2[i2:])[:matchlen]:\n",
    "            return None\n",
    "    elif size2 is not None:\n",
    "        # and size1 is none\n",
    "        matchlen = len(''.join(list2[i2:i2+size2]))\n",
    "        if ''.join(list2[i2:i2+size2]) != ''.join(list1[i1:])[:matchlen]:\n",
    "            return None\n",
    "    else:\n",
    "        # both are none; just calculate the match length\n",
    "        matchlen = 0\n",
    "        while l1concat[matching] == l2concat[matching]:\n",
    "            matchlen += 1\n",
    "    matchphrase = ''.join(list1[i1:])[:matchlen]\n",
    "    # get the exact bounds for list1\n",
    "    bound1 = 1\n",
    "    for i in range(len(list1)-i1+1):\n",
    "        if ''.join(list1[i1:i1+i]) == matchphrase:\n",
    "            bound1 = i\n",
    "            break\n",
    "    # get the exact bounds for list2\n",
    "    bound2 = 1\n",
    "    for i in range(len(list2)-i2+1):\n",
    "        if ''.join(list2[i2:i2+i]) == matchphrase:\n",
    "            bound2 = i\n",
    "            break\n",
    "    return [(i1, i1+bound1), (i2, i2+bound2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Group on the verb closest to the anchor point, using allennlp dependency parsing\n",
    "# Include SVO information and expanded verbs\n",
    "def group_anchor_verb_dep(row):\n",
    "    if row['split_anchor_span'] is None:\n",
    "        return dict(zip(grouping_headers, ['']))\n",
    "    p = row['dependency_parse']\n",
    "    # build out a more usable version of the dependency tree with information about tree level!\n",
    "    # tree_array[n] = (parent, level) so the root node is (0, 0)\n",
    "    tree_array = [(h, -1) for h in p['predicted_heads']]\n",
    "    need_connection = [0]\n",
    "    level = 0\n",
    "    while len(need_connection) > 0:\n",
    "        need_connection_update = []\n",
    "        for i in range(len(tree_array)):\n",
    "            if tree_array[i][0] in need_connection:\n",
    "                tree_array[i] = (tree_array[i][0], level)\n",
    "                need_connection_update.append(i+1)\n",
    "        need_connection = need_connection_update\n",
    "        level += 1\n",
    "    # Figure out what indexes our anchor fits into\n",
    "    # Assume that the anchor is contiguous text\n",
    "    # TODO: is this always true?\n",
    "    for i in range(len(p['words'])):\n",
    "        if len(''.join(p['words'][:i])) >= len(''.join(row['split_tokens'][:row['split_anchor_span'][0]])):\n",
    "            match = list_elements_match(\n",
    "                p['words'], row['split_tokens'], i, row['split_anchor_span'][0], \n",
    "                size2=row['split_anchor_span'][1]-row['split_anchor_span'][0])\n",
    "            if match is not None:\n",
    "                break\n",
    "    # Find the smallest containing dependency node\n",
    "    matching_nodes = [(i, tree_array[i][0], tree_array[i][1]) for i in range(match[0][0], match[0][1])]\n",
    "    matching_nodes = list(set(matching_nodes))\n",
    "    # Remove punctuation from the set of nodes that consist the anchor\n",
    "    # This avoids odd dependency structures\n",
    "    matching_nodes = [node for node in matching_nodes if p['pos'][node[0]] != 'PUNCT']\n",
    "    # check if there's no non-punctuation anchor ...?\n",
    "    if len(matching_nodes) == 0:\n",
    "        return dict(zip(grouping_headers, ['']))\n",
    "    while len(matching_nodes) > 1:\n",
    "        matching_nodes.sort(key=lambda x: x[2])\n",
    "        parent = matching_nodes.pop()[1]\n",
    "        matching_nodes.append((parent-1, tree_array[parent-1][0], tree_array[parent-1][1]))\n",
    "        matching_nodes = list(set(matching_nodes))\n",
    "    # this is the anchor node\n",
    "    node = matching_nodes[0]\n",
    "    anchor_node = matching_nodes[0]\n",
    "    # Find the closest parent that is a verb\n",
    "    parent = node[1]\n",
    "    while p['pos'][node[0]] not in ['VERB', 'AUX'] and parent != 0:\n",
    "        node = (parent-1, tree_array[parent-1][0], tree_array[parent-1][1])\n",
    "        parent = node[1]\n",
    "    if p['pos'][node[0]] not in ['VERB', 'AUX']:\n",
    "        # we've landed in a root node that isn't a verb, wheeeeee\n",
    "        return dict(zip(grouping_headers, ['']))\n",
    "    # node is definitely in the anchor verb (type 'VERB' or 'AUX') now\n",
    "    immediate_verb = p['words'][node[0]]\n",
    "    relevant_verbs = ['']*len(p['words'])\n",
    "    relevant_verbs[node[0]] = p['words'][node[0]]\n",
    "    # climb to the top of the anchor verb blob\n",
    "    if p['pos'][node[0]] in ['AUX']:\n",
    "        parent = node[1]\n",
    "        while parent != 0 and p['pos'][node[0]] in ['VERB']:\n",
    "            node = (parent-1, tree_array[parent-1][0], tree_array[parent-1][1])\n",
    "            if p['pos'][node[0]] in ['VERB']:\n",
    "                relevant_verbs.append(p['words'][node[0]])\n",
    "            parent = node[1]\n",
    "    # node should now be the top anchorverb\n",
    "    averb_node = node\n",
    "    node = get_node_from_index(row['dependency_parse'], node[0])\n",
    "    averb_string, averb_span = unroll_dependency_node(\n",
    "        node,\n",
    "        allowed_links=['aux', 'auxpass', 'cop'],\n",
    "        allowed_types=['VERB', 'AUX', 'PART']\n",
    "    )\n",
    "    averb_string = ' '.join(averb_string.strip().split())\n",
    "    split_loc, cspan = split_span_descriptor(averb_span, [row['split_0'], row['split_1'], row['split_2']])\n",
    "    subjects = get_node_children(node, ['subj', 'nsubj', 'nsubjpass', 'csubj', 'csubjpass'])\n",
    "    subjects = [' '.join(unroll_dependency_node(n)[0].strip().split()) for n in subjects]\n",
    "    objects = get_node_children(node, ['obj', 'dobj', 'iobj', 'pobj'])\n",
    "    objects = [' '.join(unroll_dependency_node(n)[0].strip().split()) for n in objects]\n",
    "    # Pick out dependencies that come before/after the verb...\n",
    "    # this targets dependencies that weren't clearly identified as subj/obj by the parser\n",
    "    unsure_relation = get_node_children(node, ['dep'])\n",
    "    unsure_relation = [(n['spans'][0]['start'],\n",
    "                        ' '.join(unroll_dependency_node(n)[0].strip().split()))\n",
    "                       for n in unsure_relation]\n",
    "    # TODO improve this possibly\n",
    "    # For now, assume a dependent phrase before the averb is more likely to be subject-like\n",
    "    # and a dependent phrase after the averb is more likely to be object-like\n",
    "    subjects += [n[1] for n in unsure_relation if n[0] < node['spans'][0]['start']]\n",
    "    objects += [n[1] for n in unsure_relation if n[0] > node['spans'][0]['start']]\n",
    "    # determine if the anchor is in the subj or obj part\n",
    "    # TODO this could be improved by adding linguistics knowledge to algo\n",
    "    # For now, just mark if the anchor comes before or after the top-averb\n",
    "    # 1 if anchor is subj (before), 0 if anchor=verb (same index), -1 if anchor is obj (after)\n",
    "    # TODO change this to use the anchor span/index and the averb span, NOT the node number\n",
    "    relation = averb_node[0] - anchor_node[0]\n",
    "    relation = int(math.copysign(1, relation)) if relation != 0 else 0\n",
    "    output = [averb_string, subjects, objects, relation, \n",
    "              split_loc, averb_span[0], averb_span[1], cspan[0], cspan[1]]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.join(sample_input_dep).apply(\n",
    "    lambda row: group_anchor_verb_dep(row), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group on the POS of the anchor point, using allennlp dependency parsing\n",
    "# I'm defining the \"POS of a phrase\" as the POS of the lowest node that contains the entire phrase\n",
    "def group_anchor_pos_dep(row, context=1):\n",
    "    if row['split_anchor_span'] is None:\n",
    "        return dict(zip(grouping_headers, ['']))\n",
    "    p = row['dependency_parse']\n",
    "    # build out a more usable version of the dependency tree with information about tree level!\n",
    "    # tree_array[n] = (parent, level) so the root node is (0, 0)\n",
    "    tree_array = [(h, -1) for h in p['predicted_heads']]\n",
    "    need_connection = [0]\n",
    "    level = 0\n",
    "    while len(need_connection) > 0:\n",
    "        need_connection_update = []\n",
    "        for i in range(len(tree_array)):\n",
    "            if tree_array[i][0] in need_connection:\n",
    "                tree_array[i] = (tree_array[i][0], level)\n",
    "                need_connection_update.append(i+1)\n",
    "        need_connection = need_connection_update\n",
    "        level += 1\n",
    "    # Figure out what indexes our anchor fits into\n",
    "    # Assume that the anchor is contiguous text\n",
    "    # TODO: is this always true?\n",
    "    for i in range(len(p['words'])):\n",
    "        if len(''.join(p['words'][:i])) >= len(''.join(row['split_tokens'][:row['split_anchor_span'][0]])):\n",
    "            match = list_elements_match(\n",
    "                p['words'], row['split_tokens'], i, row['split_anchor_span'][0], \n",
    "                size2=row['split_anchor_span'][1]-row['split_anchor_span'][0])\n",
    "            if match is not None:\n",
    "                break\n",
    "    # Find the smallest containing dependency node\n",
    "    matching_nodes = [(i, tree_array[i][0], tree_array[i][1]) for i in range(match[0][0], match[0][1])]\n",
    "    matching_nodes = list(set(matching_nodes))\n",
    "    # Remove punctuation from the set of nodes that consist the anchor\n",
    "    # This avoids odd dependency structures\n",
    "    matching_nodes = [node for node in matching_nodes if p['pos'][node[0]] != 'PUNCT']\n",
    "    # check if there's no non-punctuation anchor ...?\n",
    "    if len(matching_nodes) == 0:\n",
    "        return dict(zip(grouping_headers, [[], []]))\n",
    "    while len(matching_nodes) > 1:\n",
    "        matching_nodes.sort(key=lambda x: x[2])\n",
    "        parent = matching_nodes.pop()[1]\n",
    "        matching_nodes.append((parent-1, tree_array[parent-1][0], tree_array[parent-1][1]))\n",
    "        matching_nodes = list(set(matching_nodes))\n",
    "    # this is the anchor node\n",
    "    node = matching_nodes[0]\n",
    "    # And get the POS and words corresponding to that node and its {context} parents\n",
    "    labeltiers = []\n",
    "    labelwords = []\n",
    "    while len(labeltiers) < context:\n",
    "        labeltiers.append(p['pos'][node[0]])\n",
    "        labelwords.append(p['words'][node[0]])\n",
    "        parent = node[1]\n",
    "        if parent == 0:\n",
    "            break\n",
    "        node = (parent-1, tree_array[parent-1][0], tree_array[parent-1][1])\n",
    "    return dict(zip(grouping_headers, [labeltiers, labelwords]))\n",
    "\n",
    "output = sample_input.join(sample_input_dep).apply(\n",
    "    lambda row: group_anchor_pos_dep(row, context=3), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSVs\n",
    "\n",
    "This section should be able to be run as a sequence by itself, assuming all functions are defined and the large imports have been performed already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_outputs(search_word, anchor_type, try_rerun, split_data, df_sentences):\n",
    "    splitted_sentences = df_sentences.merge(\n",
    "        split_data,\n",
    "        how='outer',\n",
    "        left_on=join_headers,\n",
    "        right_on=join_headers)\n",
    "    \n",
    "    # Retrieve dependency parse\n",
    "    if try_rerun or (not os.path.isfile(f'outputs/{search_word}/partial-dependency-{anchor_type}.pkl')):\n",
    "        splitted_sentences_dep = splitted_sentences.apply(\n",
    "            lambda row: parse_dependency(row), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences_dep.to_pickle(f'outputs/{search_word}/partial-dependency-{anchor_type}.pkl')\n",
    "    splitted_sentences_dep = pd.read_pickle(f'outputs/{search_word}/partial-dependency-{anchor_type}.pkl')\n",
    "    \n",
    "    # Retrieve constituency parse\n",
    "    if try_rerun or (not os.path.isfile(f'outputs/{search_word}/partial-constituency-{anchor_type}.pkl')):\n",
    "        splitted_sentences_con = splitted_sentences.apply(\n",
    "            lambda row: parse_constituency(row), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences_con.to_pickle(f'outputs/{search_word}/partial-constituency-{anchor_type}.pkl')\n",
    "    splitted_sentences_con = pd.read_pickle(f'outputs/{search_word}/partial-constituency-{anchor_type}.pkl')\n",
    "    \n",
    "    # Generate individual outputs\n",
    "    output = splitted_sentences.apply(\n",
    "        lambda row: group_first_word(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_firstword.csv')\n",
    "    \n",
    "    output = splitted_sentences.apply(\n",
    "        lambda row: group_first_verb(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_firstverb.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_con).apply(\n",
    "        lambda row: group_type_sentence_con(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_sentstruct_con.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_con).apply(\n",
    "        lambda row: group_main_verb_con(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo_con.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "        lambda row: group_main_verb_dep(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_mainverb_dep.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "        lambda row: group_main_verb_expanded_dep(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo_dep.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "        lambda row: group_anchor_pos_dep(row, context=3), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_anchorpos_dep.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "        lambda row: group_anchor_verb_dep(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_anchorverb_dep.csv')\n",
    "    \n",
    "    # Restructure the outputs into something that can be neatly imported into a spreadsheet to play with\n",
    "    group_anchorverb_dep = pd.read_csv(f'outputs/{search_word}/{anchor_type}_anchorverb_dep.csv')\n",
    "    group_anchorverb_dep = group_anchorverb_dep.rename(\n",
    "        columns={\n",
    "            \"group\": \"d_averb\", \"group2\": \"d_averb_s\", \"group3\": \"d_averb_o\", \"group4\": \"d_averb_relation\", \n",
    "            \"group5\": \"d_averb_split\", \"group6\": \"d_averb_span0\", \"group7\": \"d_averb_span1\", \n",
    "            \"group8\": \"d_averb_cspan0\", \"group9\": \"d_averb_cspan1\"})\n",
    "    group_mainverb_dep = pd.read_csv(f'outputs/{search_word}/{anchor_type}_mainverb_dep.csv')\n",
    "    group_mainverb_dep = group_mainverb_dep[\n",
    "        ['group']]\n",
    "    group_mainverb_dep = group_mainverb_dep.rename(\n",
    "        columns={\n",
    "            \"group\": \"d_root\"})\n",
    "    group_mainverb_svo_dep = pd.read_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo_dep.csv')\n",
    "    group_mainverb_svo_dep = group_mainverb_svo_dep[\n",
    "        ['group', 'group2', 'group3', 'group4', 'group5', \n",
    "         'group6', 'group7', 'group8']]\n",
    "    group_mainverb_svo_dep = group_mainverb_svo_dep.rename(\n",
    "        columns={\n",
    "            \"group\": \"d_root_full\", \"group2\": \"d_root_s\", \"group3\": \"d_root_o\", \n",
    "            \"group4\": \"d_root_split\", \"group5\": \"d_root_span0\", \"group6\": \"d_root_span1\", \n",
    "            \"group7\": \"d_root_cspan0\", \"group8\": \"d_root_cspan1\"})\n",
    "    group_firstverb = pd.read_csv(f'outputs/{search_word}/{anchor_type}_firstverb.csv')\n",
    "    group_firstverb = group_firstverb[\n",
    "        ['group']]\n",
    "    group_firstverb = group_firstverb.rename(\n",
    "        columns={\n",
    "            \"group\": \"fverb\"})\n",
    "    group_anchorpos_dep = pd.read_csv(f'outputs/{search_word}/{anchor_type}_anchorpos_dep.csv')\n",
    "    group_anchorpos_dep = group_anchorpos_dep[\n",
    "        ['group', 'group2']]\n",
    "    group_anchorpos_dep = group_anchorpos_dep.rename(\n",
    "        columns={\n",
    "            \"group\": \"d_apos\", \"group2\": \"d_apos_w\"})\n",
    "    group_firstword = pd.read_csv(f'outputs/{search_word}/{anchor_type}_firstword.csv')\n",
    "    group_firstword = group_firstword[\n",
    "        ['group']]\n",
    "    group_firstword = group_firstword.rename(\n",
    "        columns={\n",
    "            \"group\": \"fword\"})\n",
    "    group_sentstruct_con = pd.read_csv(f'outputs/{search_word}/{anchor_type}_sentstruct_con.csv')\n",
    "    group_sentstruct_con = group_sentstruct_con[\n",
    "        ['group', 'group2']]\n",
    "    group_sentstruct_con = group_sentstruct_con.rename(\n",
    "        columns={\n",
    "            \"group\": \"c_senttype\", \"group2\": \"c_sentparts\"})\n",
    "    group_mainverb_svo_con = pd.read_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo_con.csv')\n",
    "    group_mainverb_svo_con = group_mainverb_svo_con[\n",
    "        ['group', 'group2', 'group3', 'group4', 'group5', \n",
    "         'group6', 'group7', 'group8', 'group9', 'group10', \n",
    "         'group11', 'group12', 'group13', 'group14', 'group15', \n",
    "         'group16']]\n",
    "    group_mainverb_svo_con = group_mainverb_svo_con.rename(\n",
    "        columns={\n",
    "            \"group\": \"c_subj_full\", \"group2\": \"c_subj_split\", \"group3\": \"c_subj_span0\", \n",
    "            \"group4\": \"c_subj_span1\", \"group5\": \"c_subj_cspan0\", \"group6\": \"c_subj_cspan1\", \n",
    "            \"group7\": \"c_subj_allspans\", \"group8\": \"c_verb_full\", \"group9\": \"c_verb_split\", \n",
    "            \"group10\": \"c_verb_span0\", \"group11\": \"c_verb_span1\", \"group12\": \"c_verb_cspan0\", \n",
    "            \"group13\": \"c_verb_cspan1\", \"group14\": \"c_verb_allspans\", \"group15\": \"c_subj_list\", \n",
    "            \"group16\": \"c_verb_list\"})\n",
    "\n",
    "    outputs_merged = group_anchorverb_dep.join(\n",
    "        group_mainverb_dep\n",
    "    ).join(\n",
    "        group_mainverb_svo_dep\n",
    "    ).join(\n",
    "        group_firstverb\n",
    "    ).join(\n",
    "        group_anchorpos_dep\n",
    "    ).join(\n",
    "        group_firstword\n",
    "    ).join(\n",
    "        group_sentstruct_con\n",
    "    ).join(\n",
    "        group_mainverb_svo_con\n",
    "    )\n",
    "    outputs_merged = outputs_merged[[\n",
    "        'split_0', 'split_1', 'split_2', \n",
    "        'c_senttype', 'c_sentparts',\n",
    "        'c_subj_full', 'c_subj_split', \n",
    "        'c_subj_span0', 'c_subj_span1', 'c_subj_cspan0', 'c_subj_cspan1', 'c_subj_allspans',\n",
    "        'c_verb_full', 'c_verb_split', \n",
    "        'c_verb_span0', 'c_verb_span1', 'c_verb_cspan0', 'c_verb_cspan1', 'c_verb_allspans',\n",
    "        'c_subj_list', 'c_verb_list',\n",
    "        'd_averb', 'd_averb_s', 'd_averb_o', 'd_averb_relation', \n",
    "            'd_averb_split', 'd_averb_span0', 'd_averb_span1', 'd_averb_cspan0', 'd_averb_cspan1',\n",
    "        'd_root', 'd_root_full', 'd_root_s', 'd_root_o', \n",
    "            'd_root_split', 'd_root_span0', 'd_root_span1', 'd_root_cspan0', 'd_root_cspan1',\n",
    "        'fverb', 'fword', 'd_apos', 'd_apos_w',\n",
    "        'URL', 'ID', 'Type', 'Index', 'Text', \n",
    "        'split_tokens', 'split_anchor_span', 'split_anchor_indices', \n",
    "        'within_anchor_index'\n",
    "    ]]\n",
    "\n",
    "    outputs_merged.to_csv(f'outputs/{search_word}/{anchor_type}.csv')\n",
    "    return outputs_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "def pipeline(search_word, \n",
    "             anchor_synonyms, \n",
    "             try_rerun=False):\n",
    "    \"\"\"\n",
    "    Run the entire analysis pipeline for a given search phrase and set of synonyms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_word : str\n",
    "        Identify the term we are splitting on (the \"anchor\").\n",
    "        This also serves as the name for this entire cluster of results / practical pipeline run name.\n",
    "    anchor_synonyms : [str]\n",
    "        Other common names for this term that we should also consider as anchors.\n",
    "    try_rerun : Boolean\n",
    "        Forcibly recreate intermediate files such as coreference or dependency parses.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform basic df loading\n",
    "    \n",
    "    # Read in the dataframe containing entire paper abstracts (NOT pre-split into sentences)\n",
    "    df = pd.read_csv(f'data/nlp-align_{search_word}.csv')\n",
    "\n",
    "    # Create the outputs directory for this search word\n",
    "    Path(f\"outputs/{search_word}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Restructure the dataframe to be more usable\n",
    "    df = df.groupby('ID', group_keys=False).apply(\n",
    "        lambda row: separate_title_abstract(row)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "    df_sentences = df.groupby(['ID', 'Type'], group_keys=False).apply(\n",
    "        lambda row: sentence_tokenize(row)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Export whitespace-based split data\n",
    "    whitespace_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "        lambda group: split_term_whitespace(group, search_word, anchor_synonyms)).reset_index(drop=True)\n",
    "\n",
    "    # Export coreference-based split data\n",
    "    if try_rerun or (not os.path.isfile(f'outputs/{search_word}/partial-coreference.pkl')):\n",
    "        output = df.apply(\n",
    "            lambda row: coref_predictor.predict(row['Text']), axis=1, result_type='expand')\n",
    "        df_merged = df.join(output)\n",
    "\n",
    "        output = df_merged.apply(\n",
    "            lambda row: reinterpret_coref_clusters(row, search_word, anchor_synonyms, df_sentences), \n",
    "            axis=1, result_type='expand')\n",
    "        df_merged = df_merged.join(output)\n",
    "\n",
    "        df_merged.to_pickle(f'outputs/{search_word}/partial-coreference.pkl')\n",
    "\n",
    "    df_merged = pd.read_pickle(f'outputs/{search_word}/partial-coreference.pkl')\n",
    "    coreference_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "        lambda group: split_term_coreference(group, search_word, anchor_synonyms, df_merged, split_term_whitespace)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Whitespace outputs\n",
    "    generate_outputs(search_word, 'whitespace', try_rerun, whitespace_output, df_sentences)\n",
    "    \n",
    "    # Coreference outputs\n",
    "    generate_outputs(search_word, 'coreference', try_rerun, coreference_output, df_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = [\n",
    "    ('BERT', ['BERT']),\n",
    "    ('SQuAD', ['SQuAD']),\n",
    "    ('DROP', ['DROP']),\n",
    "    ('GPT-2', ['GPT', 'GPT-2', 'GPT-3']),\n",
    "    ('Transformers', ['Transformers', 'Transformer', 'transfer learning', 'transfer'])\n",
    "]\n",
    "\n",
    "for dataset in all_targets:\n",
    "    pipeline(dataset[0], dataset[1], try_rerun=False)\n",
    "    print(dataset[0], 'done running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = dependency_predictor.predict(\n",
    "#     sentence='the quick red fox jumped over the lazy dog.'\n",
    "# )\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of the pipeline to use EBM dataset\n",
    "# and temporary-generated exports of PIO extraction for the dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def generate_outputs(search_word, anchor_type, try_rerun, split_data, df_sentences):\n",
    "    # these are EBM-specific join headers\n",
    "    join_headers = ['ID', 'Type', 'Index', 'ann_section', 'ann_source']\n",
    "    \n",
    "    # EBM uses dep and const parse on each sentence individually NOT per-split, to save memory\n",
    "    # Since the tokenization is exactly the same for each split\n",
    "    # (unlike whitespace, const parsing splits .... TODO, check if this is true)\n",
    "    \n",
    "    # Retrieve dependency parse\n",
    "    if try_rerun or (not os.path.isfile(f'outputs/{search_word}/partial-dependency-{anchor_type}.pkl')):\n",
    "        df_sentences_dep = df_sentences.apply(\n",
    "            lambda row: parse_dependency(row), \n",
    "            axis=1, result_type='expand')\n",
    "        df_sentences_dep.to_pickle(f'outputs/{search_word}/partial-dependency-{anchor_type}.pkl')\n",
    "    df_sentences_dep = pd.read_pickle(f'outputs/{search_word}/partial-dependency-{anchor_type}.pkl')\n",
    "    \n",
    "    # Retrieve constituency parse\n",
    "    if try_rerun or (not os.path.isfile(f'outputs/{search_word}/partial-constituency-{anchor_type}.pkl')):\n",
    "        df_sentences_con = df_sentences.apply(\n",
    "            lambda row: parse_constituency(row), \n",
    "            axis=1, result_type='expand')\n",
    "        df_sentences_con.to_pickle(f'outputs/{search_word}/partial-constituency-{anchor_type}.pkl')\n",
    "    df_sentences_con = pd.read_pickle(f'outputs/{search_word}/partial-constituency-{anchor_type}.pkl')\n",
    "    \n",
    "    # Attach const and dep parses to df_sentences\n",
    "    df_sentences = df_sentences.join(df_sentences_dep)\n",
    "    df_sentences = df_sentences.join(df_sentences_con)\n",
    "    \n",
    "    splitted_sentences = df_sentences.merge(\n",
    "        split_data,\n",
    "        how='outer',\n",
    "        left_on=join_headers,\n",
    "        right_on=join_headers,\n",
    "        suffixes=[None, '_y'])\n",
    "    \n",
    "    # Generate individual outputs\n",
    "    if not os.path.isfile(f'outputs/{search_word}/{anchor_type}_firstword.csv'):\n",
    "        output = splitted_sentences.apply(\n",
    "            lambda row: group_first_word(row), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_firstword.csv')\n",
    "    \n",
    "    if not os.path.isfile(f'outputs/{search_word}/{anchor_type}_firstverb.csv'):\n",
    "        output = splitted_sentences.apply(\n",
    "            lambda row: group_first_verb(row), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_firstverb.csv')\n",
    "\n",
    "    if not os.path.isfile(f'outputs/{search_word}/{anchor_type}_sentstruct_con.csv'):\n",
    "        output = splitted_sentences.apply(\n",
    "            lambda row: group_type_sentence_con(row), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_sentstruct_con.csv')\n",
    "\n",
    "    if not os.path.isfile(f'outputs/{search_word}/{anchor_type}_mainverb_svo_con.csv'):\n",
    "        output = splitted_sentences.apply(\n",
    "            lambda row: group_main_verb_con(row), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo_con.csv')\n",
    "\n",
    "    if not os.path.isfile(f'outputs/{search_word}/{anchor_type}_mainverb_dep.csv'):\n",
    "        output = splitted_sentences.apply(\n",
    "            lambda row: group_main_verb_dep(row), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_mainverb_dep.csv')\n",
    "\n",
    "    if not os.path.isfile(f'outputs/{search_word}/{anchor_type}_mainverb_svo_dep.csv'):\n",
    "        output = splitted_sentences.apply(\n",
    "            lambda row: group_main_verb_expanded_dep(row), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo_dep.csv')\n",
    "\n",
    "    if not os.path.isfile(f'outputs/{search_word}/{anchor_type}_anchorpos_dep.csv'):\n",
    "        output = splitted_sentences.apply(\n",
    "            lambda row: group_anchor_pos_dep(row, context=3), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_anchorpos_dep.csv')\n",
    "\n",
    "    if not os.path.isfile(f'outputs/{search_word}/{anchor_type}_anchorverb_dep.csv'):\n",
    "        output = splitted_sentences.apply(\n",
    "            lambda row: group_anchor_verb_dep(row), \n",
    "            axis=1, result_type='expand')\n",
    "        splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_anchorverb_dep.csv')\n",
    "    \n",
    "    # Restructure the outputs into something that can be neatly imported into a spreadsheet to play with\n",
    "    group_anchorverb_dep = pd.read_csv(f'outputs/{search_word}/{anchor_type}_anchorverb_dep.csv')\n",
    "    group_anchorverb_dep = group_anchorverb_dep.rename(\n",
    "        columns={\n",
    "            \"group\": \"d_averb\", \"group2\": \"d_averb_s\", \"group3\": \"d_averb_o\", \"group4\": \"d_averb_relation\", \n",
    "            \"group5\": \"d_averb_split\", \"group6\": \"d_averb_span0\", \"group7\": \"d_averb_span1\", \n",
    "            \"group8\": \"d_averb_cspan0\", \"group9\": \"d_averb_cspan1\"})\n",
    "    group_mainverb_dep = pd.read_csv(f'outputs/{search_word}/{anchor_type}_mainverb_dep.csv')\n",
    "    group_mainverb_dep = group_mainverb_dep[\n",
    "        ['group']]\n",
    "    group_mainverb_dep = group_mainverb_dep.rename(\n",
    "        columns={\n",
    "            \"group\": \"d_root\"})\n",
    "    group_mainverb_svo_dep = pd.read_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo_dep.csv')\n",
    "    group_mainverb_svo_dep = group_mainverb_svo_dep[\n",
    "        ['group', 'group2', 'group3', 'group4', 'group5', \n",
    "         'group6', 'group7', 'group8']]\n",
    "    group_mainverb_svo_dep = group_mainverb_svo_dep.rename(\n",
    "        columns={\n",
    "            \"group\": \"d_root_full\", \"group2\": \"d_root_s\", \"group3\": \"d_root_o\", \n",
    "            \"group4\": \"d_root_split\", \"group5\": \"d_root_span0\", \"group6\": \"d_root_span1\", \n",
    "            \"group7\": \"d_root_cspan0\", \"group8\": \"d_root_cspan1\"})\n",
    "    group_firstverb = pd.read_csv(f'outputs/{search_word}/{anchor_type}_firstverb.csv')\n",
    "    group_firstverb = group_firstverb[\n",
    "        ['group']]\n",
    "    group_firstverb = group_firstverb.rename(\n",
    "        columns={\n",
    "            \"group\": \"fverb\"})\n",
    "    group_anchorpos_dep = pd.read_csv(f'outputs/{search_word}/{anchor_type}_anchorpos_dep.csv')\n",
    "    group_anchorpos_dep = group_anchorpos_dep[\n",
    "        ['group', 'group2']]\n",
    "    group_anchorpos_dep = group_anchorpos_dep.rename(\n",
    "        columns={\n",
    "            \"group\": \"d_apos\", \"group2\": \"d_apos_w\"})\n",
    "    group_firstword = pd.read_csv(f'outputs/{search_word}/{anchor_type}_firstword.csv')\n",
    "    group_firstword = group_firstword[\n",
    "        ['group']]\n",
    "    group_firstword = group_firstword.rename(\n",
    "        columns={\n",
    "            \"group\": \"fword\"})\n",
    "    group_sentstruct_con = pd.read_csv(f'outputs/{search_word}/{anchor_type}_sentstruct_con.csv')\n",
    "    group_sentstruct_con = group_sentstruct_con[\n",
    "        ['group', 'group2']]\n",
    "    group_sentstruct_con = group_sentstruct_con.rename(\n",
    "        columns={\n",
    "            \"group\": \"c_senttype\", \"group2\": \"c_sentparts\"})\n",
    "    group_mainverb_svo_con = pd.read_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo_con.csv')\n",
    "    group_mainverb_svo_con = group_mainverb_svo_con[\n",
    "        ['group', 'group2', 'group3', 'group4', 'group5', \n",
    "         'group6', 'group7', 'group8', 'group9', 'group10', \n",
    "         'group11', 'group12', 'group13', 'group14', 'group15', \n",
    "         'group16']]\n",
    "    group_mainverb_svo_con = group_mainverb_svo_con.rename(\n",
    "        columns={\n",
    "            \"group\": \"c_subj_full\", \"group2\": \"c_subj_split\", \"group3\": \"c_subj_span0\", \n",
    "            \"group4\": \"c_subj_span1\", \"group5\": \"c_subj_cspan0\", \"group6\": \"c_subj_cspan1\", \n",
    "            \"group7\": \"c_subj_allspans\", \"group8\": \"c_verb_full\", \"group9\": \"c_verb_split\", \n",
    "            \"group10\": \"c_verb_span0\", \"group11\": \"c_verb_span1\", \"group12\": \"c_verb_cspan0\", \n",
    "            \"group13\": \"c_verb_cspan1\", \"group14\": \"c_verb_allspans\", \"group15\": \"c_subj_list\", \n",
    "            \"group16\": \"c_verb_list\"})\n",
    "\n",
    "    outputs_merged = group_anchorverb_dep.join(\n",
    "        group_mainverb_dep\n",
    "    ).join(\n",
    "        group_mainverb_svo_dep\n",
    "    ).join(\n",
    "        group_firstverb\n",
    "    ).join(\n",
    "        group_anchorpos_dep\n",
    "    ).join(\n",
    "        group_firstword\n",
    "    ).join(\n",
    "        group_sentstruct_con\n",
    "    ).join(\n",
    "        group_mainverb_svo_con\n",
    "    )\n",
    "    outputs_merged = outputs_merged[[\n",
    "        'split_0', 'split_1', 'split_2', \n",
    "        'c_senttype', 'c_sentparts',\n",
    "        'c_subj_full', 'c_subj_split', \n",
    "        'c_subj_span0', 'c_subj_span1', 'c_subj_cspan0', 'c_subj_cspan1', 'c_subj_allspans',\n",
    "        'c_verb_full', 'c_verb_split', \n",
    "        'c_verb_span0', 'c_verb_span1', 'c_verb_cspan0', 'c_verb_cspan1', 'c_verb_allspans',\n",
    "        'c_subj_list', 'c_verb_list',\n",
    "        'd_averb', 'd_averb_s', 'd_averb_o', 'd_averb_relation', \n",
    "            'd_averb_split', 'd_averb_span0', 'd_averb_span1', 'd_averb_cspan0', 'd_averb_cspan1',\n",
    "        'd_root', 'd_root_full', 'd_root_s', 'd_root_o', \n",
    "            'd_root_split', 'd_root_span0', 'd_root_span1', 'd_root_cspan0', 'd_root_cspan1',\n",
    "        'fverb', 'fword', 'd_apos', 'd_apos_w',\n",
    "        'URL', 'ID', 'Type', 'Index', 'Text', \n",
    "        'split_tokens', 'split_anchor_span', 'split_anchor_indices', \n",
    "        'within_anchor_index', 'split_anchor_type'\n",
    "    ]]\n",
    "\n",
    "    outputs_merged.to_csv(f'outputs/{search_word}/{anchor_type}.csv')\n",
    "    return outputs_merged\n",
    "\n",
    "search_word = 'ebm'\n",
    "\n",
    "# Retrieve EBM sentence data...\n",
    "pio_df_sentences = pd.read_pickle(f'temp/ebm-df_sentences.pkl')\n",
    "\n",
    "# Add tokenization to the EBM sentence data\n",
    "pio_df_tokenized = pd.read_pickle(f'temp/ebm-df_tokenized.pkl')\n",
    "    \n",
    "# Retrieve PIO data\n",
    "pio_output = pd.read_pickle(f'temp/ebm-pio_output.pkl')\n",
    "\n",
    "# Generate outputs for grouping...\n",
    "# TODO refine the join column lists into something thats actually reusable ??? not a super high priority\n",
    "outputs_merged = generate_outputs(\n",
    "    search_word, \n",
    "    'pio', \n",
    "    False, \n",
    "    pio_output, \n",
    "    pio_df_sentences.join(\n",
    "        pio_df_tokenized.set_index(['ID', 'Type', 'Index', 'ann_section', 'ann_source']), \n",
    "        on=['ID', 'Type', 'Index', 'ann_section', 'ann_source']))\n",
    "outputs_merged.iloc[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual element alignment / analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model source: https://code.google.com/archive/p/word2vec/\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'model/GoogleNews-vectors-negative300.bin', \n",
    "    binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word2vec embedding of a phrase\n",
    "def get_phrase_embed_word2vec(word2vec, phrase):\n",
    "    try:\n",
    "        phraseS = phrase.split()\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    unknowns = []\n",
    "    emb = []\n",
    "    for w in phraseS:\n",
    "        try:\n",
    "            emb.append(word2vec[w])\n",
    "        except:\n",
    "            unknowns.append(w)\n",
    "    if len(emb) == 0:\n",
    "        return pd.DataFrame()\n",
    "    emb_sum = pd.DataFrame(emb).sum() / len(phraseS)\n",
    "    emb_sum['word'] = phrase\n",
    "    return pd.DataFrame([emb_sum])\n",
    "\n",
    "v = get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'test sentence')\n",
    "sent_v = get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'This is a test sentence !')\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio_extracted = pd.read_hdf(f'temp/ebm-pio_extracted.hdf', 'mydata')\n",
    "# pio_extracted.iloc[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency_predictor.predict(\n",
    "#     sentence='hypertensive patients receiving drug treatment'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do dependency parsing once for the entire input to save processing time\n",
    "# for groupings that require dependency parsing later\n",
    "depparse = pd.DataFrame()\n",
    "depparse['dependency_parse'] = pio_extracted.apply(\n",
    "    lambda row: dependency_predictor.predict(\n",
    "        sentence=row['text'].strip()\n",
    "    ), axis=1)\n",
    "depparse.to_hdf(f'temp/ebm-pio_extracted_depparsed.hdf', 'mydata', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depparse = pd.read_hdf(f'temp/ebm-pio_extracted_depparsed.hdf', 'mydata')\n",
    "# depparse.iloc[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab only the hierplane_tree segment to save space\n",
    "# # TODO rework previous code to also use hierplane_tree ... ? ????\n",
    "# # ... this is not a priority, it's just a code cleanliness thing\n",
    "# # do we even need this? / can we even use this?\n",
    "# # TODO also this needs to be updated to use hdf reading instead of pkl, no pkl export anymore\n",
    "# pio_extracted = pd.read_pickle(f'temp/ebm-pio_extracted_parsed.pkl')\n",
    "# pio_extracted['dependency_parse'] = pio_extracted.apply(\n",
    "#     lambda row: row['dependency_parse']['hierplane_tree'], axis=1)\n",
    "# pio_extracted.to_pickle(f'temp/ebm-pio_extracted_parsed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcon = constituency_predictor.predict(\n",
    "#     sentence='hypertensive patients receiving drug treatment'\n",
    "# )\n",
    "# {i:tcon[i] for i in tcon if i!='class_probabilities'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inplace_constparse(row):\n",
    "    parse = constituency_predictor.predict(\n",
    "        sentence=row['text'].strip()\n",
    "    )\n",
    "    return {i:parse[i] for i in parse if (i not in ['class_probabilities'])}\n",
    "\n",
    "# Do constituency parsing once for the entire input to save processing time\n",
    "# for groupings that require constituency parsing later\n",
    "conparse = pd.DataFrame()\n",
    "conparse['constituency_parse'] = pio_extracted.apply(\n",
    "    lambda row: inplace_constparse(row), axis=1)\n",
    "conparse.to_hdf(f'temp/ebm-pio_extracted_conparsed.hdf', 'mydata', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conparse = pd.read_hdf(f'temp/ebm-pio_extracted_conparsed.hdf', 'mydata')\n",
    "# conparse.iloc[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depparse = pd.read_hdf(f'temp/ebm-pio_extracted_depparsed.hdf','mydata')\n",
    "conparse = pd.read_hdf(f'temp/ebm-pio_extracted_conparsed.hdf','mydata')\n",
    "pio_extracted['dependency_parse'] = depparse['dependency_parse']\n",
    "pio_extracted['constituency_parse'] = conparse['constituency_parse']\n",
    "pio_extracted.to_hdf(f'temp/ebm-pio_extracted_parsed.hdf', 'mydata', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start running here if already generated dep and con parses!\n",
    "pio_ex_depparse = pd.read_hdf(f'temp/ebm-pio_extracted_parsed.hdf','mydata')\n",
    "# pio_ex_depparse.loc[pio_ex_depparse['class'] == 'p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_input = pio_ex_depparse.loc[pio_ex_depparse['class'] == 'p']\n",
    "# p_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_input = pio_ex_depparse.loc[pio_ex_depparse['class'] == 'p']\n",
    "\n",
    "dep_root = pd.DataFrame()\n",
    "dep_root['word'] = p_input.apply(\n",
    "    lambda row: row['dependency_parse']['hierplane_tree']['root']['word'],\n",
    "    axis=1\n",
    ")\n",
    "dep_root['attr'] = p_input.apply(\n",
    "    lambda row: row['dependency_parse']['hierplane_tree']['root']['attributes'][0],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# dep_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio_ex_conparse = pd.read_hdf(f'temp/ebm-pio_extracted_parsed.hdf','mydata')\n",
    "# pio_ex_conparse.loc[pio_ex_conparse['class'] == 'p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_input = pio_ex_conparse.loc[pio_ex_conparse['class'] == 'p']\n",
    "# p_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_input = pio_ex_conparse.loc[pio_ex_conparse['class'] == 'p']\n",
    "\n",
    "# pick out the phrase type of the entire P text chunk\n",
    "con_root = pd.DataFrame()\n",
    "con_root['src'] = p_input['text']\n",
    "con_root['attr'] = p_input.apply(\n",
    "    lambda row: row['constituency_parse']['hierplane_tree']['root']['nodeType'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# con_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_input = pio_ex_conparse.loc[pio_ex_conparse['class'] == 'p']\n",
    "\n",
    "# pick out the \"main subject\" using constituency parsing\n",
    "def ebmnlp_const_main_subj(row):\n",
    "    output = []\n",
    "    output_types = []\n",
    "    output_levels = []\n",
    "    active_nodes = [row['constituency_parse']['hierplane_tree']['root']]\n",
    "    active_child_attrs = [[n['nodeType'] for n in active_nodes[0]['children']]]\n",
    "    level = 0\n",
    "    while len(active_nodes) > 0:\n",
    "        new_active_nodes = []\n",
    "        new_active_child_attrs = []\n",
    "        for node_i in range(len(active_nodes)):\n",
    "            node = active_nodes[node_i]\n",
    "            child_attr = active_child_attrs[node_i]\n",
    "            child_np_i = [i for i, x in enumerate(child_attr) if x == 'NP']\n",
    "            if len(child_np_i) == 0:\n",
    "                # we've landed on a node with no child NPs\n",
    "                endpoint_children = [n for n in node['children'] if ('children' not in n)]\n",
    "                output.append([n['word'] for n in endpoint_children])\n",
    "                output_types.append([n['nodeType'] for n in endpoint_children])\n",
    "                output_levels.append(level)\n",
    "            else:\n",
    "                # this node has child NPs, add those to the list of nodes to investigate\n",
    "                for i in child_np_i:\n",
    "                    child_node = node['children'][i]\n",
    "                    new_active_nodes.append(child_node)\n",
    "                    new_active_child_attrs.append([n['nodeType'] for n in child_node['children']])\n",
    "        active_nodes = new_active_nodes\n",
    "        active_child_attrs = new_active_child_attrs\n",
    "        level += 1\n",
    "    output = [' '.join(phrase) for phrase in output]\n",
    "    return {'mainsubj':output, 'mainsubj_type':output_types, 'mainsubj_levels':output_levels}\n",
    "\n",
    "con_mainsubj = pd.DataFrame()\n",
    "con_mainsubj['src'] = p_input['text']\n",
    "con_mainsubj = con_mainsubj.join(\n",
    "    p_input.apply(\n",
    "        lambda row: ebmnlp_const_main_subj(row),\n",
    "        axis=1, result_type='expand'\n",
    "    )\n",
    ")\n",
    "\n",
    "# con_mainsubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_input = pio_ex_conparse.loc[pio_ex_conparse['class'] == 'p']\n",
    "\n",
    "# pick out the \"segments\" of the phrase using constituency parsing\n",
    "def const_node_align_segments(node):\n",
    "    punct_types = [] #['.', ':', 'HYPH', '-LRB-', '-RRB-']\n",
    "    # ... ignore punct splits for now, these are mostly visual decoration anyways\n",
    "    output = []\n",
    "    output_types = []\n",
    "    output_child_types = []\n",
    "    children = node['children']\n",
    "    active_curr_text = None\n",
    "    active_curr_types = None\n",
    "    active_curr_i = None\n",
    "    for c_i in range(len(children)):\n",
    "        if 'children' in children[c_i]:\n",
    "            # if this child node has children\n",
    "            # first save any previous non-children blobs\n",
    "            if active_curr_text is not None:\n",
    "                output.append(active_curr_text)\n",
    "                output_types.append(node['nodeType'])\n",
    "                output_child_types.append(active_curr_types)\n",
    "                active_curr_text = None\n",
    "            # break child node down and get its segments\n",
    "            child_output = const_node_align_segments(children[c_i])\n",
    "            output += child_output[0]\n",
    "            output_types += child_output[1]\n",
    "            output_child_types += child_output[2]\n",
    "        else:\n",
    "            # otherwise, this child node has no children\n",
    "            # so we put it into the direct output of this node\n",
    "            if children[c_i]['nodeType'] in punct_types:\n",
    "                # if the text we're adding is punctuation, split it into a separate punctuation group!\n",
    "                # put the previous group in output\n",
    "                if active_curr_text is not None:\n",
    "                    output.append(active_curr_text)\n",
    "                    output_types.append(node['nodeType'])\n",
    "                    output_child_types.append(active_curr_types)\n",
    "                # put the punctuation into output\n",
    "                output.append(children[c_i]['word'])\n",
    "                output_types.append(children[c_i]['nodeType'])\n",
    "                output_child_types.append([children[c_i]['nodeType']])\n",
    "                # reset groups\n",
    "                active_curr_text = None\n",
    "            elif active_curr_text is None:\n",
    "                # empty blob, initialize it\n",
    "                active_curr_text = children[c_i]['word']\n",
    "                active_curr_types = [children[c_i]['nodeType']]\n",
    "                active_curr_i = c_i\n",
    "            else:\n",
    "                # we extend the already-existing blob\n",
    "                active_curr_text += ' ' + children[c_i]['word']\n",
    "                active_curr_types.append(children[c_i]['nodeType'])\n",
    "                active_curr_i = c_i\n",
    "    # and finish off the last blob we started\n",
    "    if active_curr_text is not None:\n",
    "        output.append(active_curr_text)\n",
    "        output_types.append(node['nodeType'])\n",
    "        output_child_types.append(active_curr_types)\n",
    "    return output, output_types, output_child_types\n",
    "\n",
    "# print(const_node_align_segments(tcon['hierplane_tree']['root']))\n",
    "\n",
    "con_segments = pd.DataFrame()\n",
    "con_segments['src'] = p_input['text']\n",
    "con_segments['aligntup'] = p_input.apply(\n",
    "    lambda row: const_node_align_segments(row['constituency_parse']['hierplane_tree']['root']),\n",
    "    axis=1\n",
    ")\n",
    "con_segments['alignsegments'] = con_segments.apply(\n",
    "    lambda row: row['aligntup'][0],\n",
    "    axis=1\n",
    ")\n",
    "con_segments['aligntypes'] = con_segments.apply(\n",
    "    lambda row: row['aligntup'][1],\n",
    "    axis=1\n",
    ")\n",
    "con_segments['alignctypes'] = con_segments.apply(\n",
    "    lambda row: row['aligntup'][2],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# con_segments.to_csv(f'temp/ebm-pio_consegments.csv')\n",
    "con_segments.to_hdf(f'temp/ebm-pio_consegments.hdf', 'mydata', mode='w')\n",
    "# con_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con_segments = pd.read_hdf(f'temp/ebm-pio_consegments.hdf','mydata')\n",
    "# con_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_segments.apply(\n",
    "    lambda row: list(zip(row['alignsegments'], row['aligntypes'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple 'main subject' alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = con_segments.join(con_mainsubj['mainsubj'])\n",
    "\n",
    "# get simple align spacing information\n",
    "# for NOW, only use the FIRST mainsubj identified that's not an empty string as the anchor...\n",
    "num_before = 0\n",
    "num_after = 0\n",
    "for index, row in temp_df.iterrows():\n",
    "    anchors = [e for e in row['mainsubj'] if e != '']\n",
    "    # find the index of the first 'main subj element'\n",
    "    if len(anchors) > 0:\n",
    "        anchorfound = 0\n",
    "        while anchorfound == 0:\n",
    "            try:\n",
    "                temp_df.at[index,'align_mainsubjsimple'] = row['alignsegments'].index(anchors[0])\n",
    "                anchorfound = 1\n",
    "            except:\n",
    "                anchors[0] = ' '.join(anchors[0].split(' ')[:-1])\n",
    "    else:\n",
    "        temp_df.at[index,'align_mainsubjsimple'] = -1\n",
    "    # calcuate how much space we need before and after the 'main subj element'\n",
    "    num_before = max(num_before, temp_df.at[index,'align_mainsubjsimple'])\n",
    "    num_after = max(num_after, len(row['alignsegments']) - temp_df.at[index,'align_mainsubjsimple'] - 1)\n",
    "    \n",
    "def alignRowMainSubjSimple(row, num_before, num_after):\n",
    "    output = ['']*int(num_before-row['align_mainsubjsimple'])\n",
    "    output += row['alignsegments']\n",
    "    output += ['']*int(num_after-len(row['alignsegments'])+row['align_mainsubjsimple']+1)\n",
    "    return dict(zip(range(len(output)), output))\n",
    "\n",
    "temp_df = temp_df.apply(\n",
    "    lambda row: alignRowMainSubjSimple(row, num_before, num_after), \n",
    "    axis=1, result_type='expand')\n",
    "temp_df.to_csv(f'temp/ebm-pio_alignRowMainSubjSimple.csv')\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
