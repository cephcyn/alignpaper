{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do one-time large imports\n",
    "\n",
    "# For sentence tokenization\n",
    "from nltk import tokenize\n",
    "\n",
    "# For coreference resolution\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "coref_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n",
    ")\n",
    "\n",
    "# For part-of-speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# For dependency parsing\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "dependency_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "### Basic input flags for the notebook / pipeline\n",
    "\n",
    "# Identify the term we are splitting on (the \"anchor\")\n",
    "# This also serves as the name for this entire cluster of results / practical pipeline run name\n",
    "search_word = 'BERT'\n",
    "\n",
    "# Other common names for this term that we should also consider as anchors\n",
    "# e.g. [search_word, 'GPT', 'GPT-2', 'GPT-3']\n",
    "# e.g. [search_word, 'Transformers', 'Transformer', 'transfer learning', 'transfer']\n",
    "anchor_synonyms = [search_word]\n",
    "\n",
    "# Flags\n",
    "# rerun per-text coreference\n",
    "flag_rerun_coreference = False or (not os.path.isfile(f'outputs/{search_word}/partial-coreference.pkl'))\n",
    "# rerun merged coreference (squish all the abstracts together!)\n",
    "flag_rerun_mcoreference = False or (not os.path.isfile(f'outputs/{search_word}/partial-mcoreference.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Read in the dataframe containing entire paper abstracts (NOT pre-split into sentences)\n",
    "df = pd.read_csv(f'data/nlp-align_{search_word}.csv')\n",
    "\n",
    "# Create the outputs directory for this search word\n",
    "Path(f\"outputs/{search_word}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split apart the 'Title' and 'Abstract' columns, add period to the end of text if not present\n",
    "def separate_title_abstract(group):\n",
    "    row = group.loc[0]\n",
    "    abs_text = tokenize.sent_tokenize(row['Abstract'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * 2,\n",
    "        'ID': [row['ID']] * 2,\n",
    "        'Type': ['Title', 'Abstract'],\n",
    "        'Text': [row['Title']+'.' if not row['Title'].endswith('.') else row['Title'], \n",
    "                 row['Abstract']+'.' if not row['Abstract'].endswith('.') else row['Abstract']]\n",
    "    })\n",
    "\n",
    "# Restructure the dataframe to be more usable...\n",
    "df = df.groupby('ID', group_keys=False).apply(\n",
    "    lambda row: separate_title_abstract(row)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "def sentence_tokenize(group):\n",
    "    row = group.reset_index(drop=True).loc[0]\n",
    "    sentences = tokenize.sent_tokenize(row['Text'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * (len(sentences)),\n",
    "        'ID': [row['ID']] * (len(sentences)),\n",
    "        'Type': [row['Type']] * (len(sentences)),\n",
    "        'Index': list(range(len(sentences))),\n",
    "        'Text': sentences\n",
    "    })\n",
    "\n",
    "df_sentences = df.groupby(['ID', 'Type'], group_keys=False).apply(\n",
    "    lambda row: sentence_tokenize(row)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a test dataframe so we can run models without taking impractically long\n",
    "# # TODO: this test dataframe has type inconsistencies with actual input data now, update if needed for use\n",
    "\n",
    "# temp_df = pd.DataFrame.from_dict(\n",
    "#     {'URL': 'abc', \n",
    "#      'ID': '0', \n",
    "#      'Title': 'Paper Title',\n",
    "#      'Abstract': 'The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. Petroni et al. (2019) take this as evidence that it memorizes factual knowledge during pre-training. We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.'\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting functions\n",
    "\n",
    "Assume we have an input dataframe with some number of columns, at least one of which is titled `Text` and is the column containing each sentence of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_0 is the text literally preceding the anchor\n",
    "# split_1 is the text that the anchor consists of\n",
    "# split_2 is the text literally following the anchor\n",
    "# split_tokens is the list of tokens that split identifies\n",
    "# split_anchor_span is a tuple (anchor_first_token_index, anchor_last_token_index) or null if there is no anchor\n",
    "# split_anchor_indices is a tuple (anchor_start_char_index, anchor_end_char_index) or null if there is no anchor\n",
    "splitting_headers = ['split_0','split_1','split_2', \n",
    "                     'split_tokens', 'split_anchor_span', 'split_anchor_indices', \n",
    "                     'within_anchor_index']\n",
    "# Include ID, Type, Index in the split output to be able to join with df_sentences\n",
    "join_headers = ['ID', 'Type', 'Index']\n",
    "# The headers used for checking if rows should be eliminated as duplicate\n",
    "duplicate_check_headers = splitting_headers[:3]+join_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split on the search word, regardless of whitespace (if search word is A and we have word CAR, it slices it up)\n",
    "# Splits on ALL instances of the search word\n",
    "def split_term_literal(group, search_word, anchor_synonyms):\n",
    "    row = group.iloc[0]\n",
    "    output = []\n",
    "    anchors = [re.search(f'({a})', row['Text'], flags=re.IGNORECASE) for a in anchor_synonyms]\n",
    "    anchors = [a.group(1) for a in anchors if (a is not None)]\n",
    "    for anchor in anchors:\n",
    "        splits = row['Text'].split(anchor)\n",
    "        for i in range(len(splits) - 1):\n",
    "            output_i = [anchor.join(splits[:i+1]), anchor.strip(), anchor.join(splits[i+1:])]\n",
    "            output_i = [i.strip() for i in output_i]\n",
    "            # write tokens list and revise split to account for tokenization\n",
    "            pre_split = nltk.word_tokenize(output_i[0])\n",
    "            output_i[0] = ' '.join(pre_split)\n",
    "            mid_split = nltk.word_tokenize(anchor)\n",
    "            output_i[1] = ' '.join(mid_split)\n",
    "            post_split = nltk.word_tokenize(output_i[2])\n",
    "            output_i[2] = ' '.join(post_split)\n",
    "            # split_tokens\n",
    "            output_i.append(pre_split + mid_split + post_split)\n",
    "            # split_anchor_span\n",
    "            output_i.append((len(pre_split), len(pre_split)+len(mid_split)))\n",
    "            # split_anchor_indices\n",
    "            output_i.append((len(output_i[0]), len(output_i[0])+len(output_i[1])))\n",
    "            # within_anchor_index\n",
    "            # for split_term_literal, the split term is ALWAYS the entire anchor\n",
    "            output_i.append(0)\n",
    "            output_i += list(row[join_headers])\n",
    "            output.append(output_i)\n",
    "    if output == []:\n",
    "        output = [[row['Text'].strip(),'','',\n",
    "                   row['Text'].strip().split(' '),None,None,None]\n",
    "                  +list(row[join_headers])]\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    return pd.DataFrame(\n",
    "        dict(zip(splitting_headers+join_headers,output_t))\n",
    "    ).drop_duplicates(duplicate_check_headers)\n",
    "\n",
    "literal_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "    lambda group: split_term_literal(group, search_word, anchor_synonyms)).reset_index(drop=True)\n",
    "\n",
    "literal_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split on any token (whitespace delineated, or token-delineated) that contains an instance of the search word\n",
    "# Splits on ALL instances of the search word\n",
    "# a little bit misleadingly named, sorry. the intention is for it to be a simple split\n",
    "def split_term_whitespace(group, search_word, anchor_synonyms):\n",
    "    row = group.iloc[0]\n",
    "    output = []\n",
    "    anchors = [re.search(rf'(^|\\W)(\\w*{a}\\w*)($|\\W)', row['Text'], flags=re.IGNORECASE) for a in anchor_synonyms]\n",
    "    anchors = [a.group(2) for a in anchors if (a is not None)]\n",
    "    for anchor in anchors:\n",
    "        splits = row['Text'].split(anchor)\n",
    "        for i in range(len(splits) - 1):\n",
    "            output_i = [anchor.join(splits[:i+1]), anchor.strip(), anchor.join(splits[i+1:])]\n",
    "            output_i = [i.strip() for i in output_i]\n",
    "            # write tokens list and revise split to account for tokenization\n",
    "            pre_split = nltk.word_tokenize(output_i[0])\n",
    "            output_i[0] = ' '.join(pre_split)\n",
    "            mid_split = nltk.word_tokenize(anchor)\n",
    "            output_i[1] = ' '.join(mid_split)\n",
    "            post_split = nltk.word_tokenize(output_i[2])\n",
    "            output_i[2] = ' '.join(post_split)\n",
    "            # split_tokens\n",
    "            output_i.append(pre_split + mid_split + post_split)\n",
    "            # split_anchor_span\n",
    "            output_i.append((len(pre_split), len(pre_split)+len(mid_split)))\n",
    "            # split_anchor_indices\n",
    "            output_i.append((len(output_i[0]), len(output_i[0])+len(output_i[1])))\n",
    "            # within_anchor_index\n",
    "            anchorsearch = [re.search(rf'{a}', anchor, flags=re.IGNORECASE) for a in anchor_synonyms]\n",
    "            output_i.append([a.start() for a in anchorsearch if (a is not None)][0])\n",
    "            output_i += list(row[join_headers])\n",
    "            output.append(output_i)\n",
    "    if output == []:\n",
    "        output = [[row['Text'].strip(),'','',\n",
    "                   row['Text'].strip().split(' '),None,None,None]\n",
    "                  +list(row[join_headers])]\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    return pd.DataFrame(\n",
    "        dict(zip(splitting_headers+join_headers,output_t))\n",
    "    ).drop_duplicates(duplicate_check_headers)\n",
    "\n",
    "whitespace_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "    lambda group: split_term_whitespace(group, search_word, anchor_synonyms)).reset_index(drop=True)\n",
    "\n",
    "whitespace_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run coreference resolution over the entire abstract, not individual sentences\n",
    "if flag_rerun_coreference:\n",
    "    output = df.apply(\n",
    "        lambda row: coref_predictor.predict(row['Text']), axis=1, result_type='expand')\n",
    "    df_merged = df.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# transform the output of coreference resolution into something that is more easily manipulated\n",
    "# split it across multiple sentences so each indiv sentence row can still work\n",
    "def reinterpret_coref_clusters(row, search_word, anchor_synonyms, sentences):\n",
    "    # Create dicts to map full-document to indiv sentence data\n",
    "    src = sentences.loc[sentences['ID'] == row['ID']].loc[sentences['Type'] == row['Type']]['Text']\n",
    "    curr_sentence = 0\n",
    "    consumed = 0\n",
    "    sent_mapping = {}\n",
    "    sent_content = {}\n",
    "    last_sent_end = 0\n",
    "    doct_mapping = {}\n",
    "    doct_split = []\n",
    "    for i in range(len(row['document'])):\n",
    "        if row['document'][i].strip() != '':\n",
    "            if row['document'][i] not in src.iloc[curr_sentence][consumed:]:\n",
    "                doct_split.append(row['document'][last_sent_end:i])\n",
    "                last_sent_end = i\n",
    "                curr_sentence += 1\n",
    "                consumed = 0\n",
    "            offset = src.iloc[curr_sentence][consumed:].index(row['document'][i])\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            if curr_sentence not in sent_content:\n",
    "                sent_content[curr_sentence] = []\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "            consumed += offset + len(row['document'][i])\n",
    "        else:\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "        doct_split.append(row['document'][last_sent_end:])\n",
    "    # Select the clusters that contain search words\n",
    "    selcluster_idxs = set()\n",
    "    for i in range(len(row['clusters'])):\n",
    "        currcluster_ct = 0\n",
    "        for c in row['clusters'][i]:\n",
    "            for anchor in anchor_synonyms:\n",
    "                # TODO this does overcounting if an anchor synonym is contained within another\n",
    "                currcluster_ct += len(\n",
    "                    re.findall(f'{anchor}', ''.join(row['document'][c[0]:c[1]+1]), flags=re.IGNORECASE)\n",
    "                )\n",
    "        if currcluster_ct > 0:\n",
    "            selcluster_idxs.add(i)\n",
    "    # Build the output row\n",
    "    output = [sent_mapping, sent_content, doct_mapping, list(selcluster_idxs)]\n",
    "    return dict(zip(['sent_mapping', 'sent_content', 'doct_mapping', 'selcluster_idxs'],output))\n",
    "\n",
    "if flag_rerun_coreference:\n",
    "    output = df_merged.apply(\n",
    "        lambda row: reinterpret_coref_clusters(row, search_word, anchor_synonyms, df_sentences), \n",
    "        axis=1, result_type='expand')\n",
    "    df_merged = df_merged.join(output)\n",
    "    \n",
    "    df_merged.to_pickle(f'outputs/{search_word}/partial-coreference.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_pickle(f'outputs/{search_word}/partial-coreference.pkl')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Run coreference resolution over all of the abstracts (and their titles) together, not individual abstracts\n",
    "# if flag_rerun_mcoreference:\n",
    "#     output = df.apply(\n",
    "#         lambda row: coref_predictor.predict(row['Text']), axis=1, result_type='expand')\n",
    "#     df_merged = df.join(output)\n",
    "coref_output = coref_predictor.predict('Mary had a little lamb.\\nIt was pink and fluffy.\\nIt ran away.')\n",
    "pickle.dump(coref_output, open(f'temp/mini_mergedcoreference.pkl', 'wb'))\n",
    "coref_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_value_split(array, value):\n",
    "    output = []\n",
    "    current = []\n",
    "    for i in range(len(array)):\n",
    "        if array[i] != value:\n",
    "            current.append(array[i])\n",
    "        else:\n",
    "            output.append(current)\n",
    "            current = []\n",
    "    output.append(current)\n",
    "    return output\n",
    "\n",
    "test_array = ['Mary', 'had', 'a', 'little', 'lamb', '.', '\\n', \n",
    "              'It', 'was', 'pink', 'and', 'fluffy', '.', '\\n', \n",
    "              'It', 'ran', 'away', '.']\n",
    "print(test_array)\n",
    "array_value_split(test_array, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text = '\\n'.join(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_merged_text = coref_predictor.predict(merged_text)\n",
    "coref_merged_text.to_pickle(f'temp/{search_word}_mergedcoreference.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO figure out how we could convert a merged coreference back into a split coreference?\n",
    "# Keep IDs of each \"entity\" tied to the split as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split based on co-references to any phrase containing search term, using allennlp coreference resolution\n",
    "# This does NOT preserve the original sentence spacing\n",
    "# REQUIRES THAT WE ALREADY RAN THE COREFERENCE PREDICTOR - this func does NOT do all of the work!\n",
    "# Splits on ALL instances of references to the search word,\n",
    "# including sub-references (e.g. \"the accuracy of RoBERTa\")\n",
    "def split_term_coreference(group, search_word, anchor_synonyms, lookup, fallback):\n",
    "    row = group.iloc[0]\n",
    "    # there's probably a cleaner way to do this...\n",
    "    lookup_row = lookup.loc[lookup['ID']==row['ID']].loc[lookup['Type']==row['Type']].to_dict(orient='records')[0]\n",
    "    if len(lookup_row['selcluster_idxs']) == 0:\n",
    "        # if we didn't identify any clusters that match the search term, use our fallback method\n",
    "        return fallback(group, search_word, anchor_synonyms)\n",
    "    output = []\n",
    "    for cluster_id in lookup_row['selcluster_idxs']:\n",
    "        split_clusters = lookup_row['clusters'][cluster_id]\n",
    "        for i in range(len(split_clusters)):\n",
    "            c = split_clusters[i]\n",
    "            if lookup_row['sent_mapping'][c[0]] == row['Index']:\n",
    "                sentence_start = lookup_row['sent_content'][row['Index']][0]\n",
    "                sentence_end = lookup_row['sent_content'][row['Index']][-1]\n",
    "                pre_split = lookup_row['document'][sentence_start:c[0]]\n",
    "                anchor = lookup_row['document'][c[0]:c[1]+1]\n",
    "                post_split = lookup_row['document'][c[1]+1:sentence_end+1]\n",
    "                output_i=[' '.join(pre_split),\n",
    "                        ' '.join(anchor),\n",
    "                        ' '.join(post_split)]\n",
    "                # split_tokens\n",
    "                output_i.append(lookup_row['document'][sentence_start:sentence_end+1])\n",
    "                # split_anchor_span\n",
    "                output_i.append((len(pre_split), len(pre_split)+len(anchor)))\n",
    "                # split_anchor_indices\n",
    "                output_i.append((len(output_i[0]), len(output_i[0])+len(output_i[1])))\n",
    "                # within_anchor_index\n",
    "                anchorsearch = [re.search(rf'{a}', ' '.join(anchor), flags=re.IGNORECASE) for a in anchor_synonyms]\n",
    "                anchorsearch = [a.start() for a in anchorsearch if (a is not None)]\n",
    "                output_i.append(anchorsearch[0] if len(anchorsearch) > 0 else -1)\n",
    "                output_i += list(row[join_headers])\n",
    "                output.append(output_i)\n",
    "    if output == []:\n",
    "        # if there wasn't any reference in the sentence found, use our fallback method\n",
    "        return fallback(group, search_word, anchor_synonyms)\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    return pd.DataFrame(dict(zip(splitting_headers+join_headers,output_t)))\n",
    "\n",
    "coreference_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "    lambda group: split_term_coreference(group, search_word, anchor_synonyms, df_merged, split_term_whitespace)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "coreference_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping functions\n",
    "\n",
    "Assume we have an input dataframe with column headers `['split_0','split_1','split_2', 'split_tokens', 'split_anchor_span']`\n",
    "\n",
    "`'split_1'` is the column that contains our search term / anchor point\n",
    "\n",
    "`'split_0'` and `'split_2'` are the columns that contain text before and after the search terms respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group is the text uniquely identifying a group\n",
    "grouping_headers = ['group', 'group2', 'group3', 'group4', 'group5', \n",
    "                    'group6', 'group7', 'group8', 'group9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = df_sentences.merge(coreference_output,\n",
    "                                  how='outer',\n",
    "                                  left_on=join_headers,\n",
    "                                  right_on=join_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the literal first word that comes after the anchor point\n",
    "# skips things that are punctuation\n",
    "def group_first_word(row):\n",
    "    output = None\n",
    "    if row['split_anchor_span'] is not None:\n",
    "        index = row['split_anchor_span'][1]\n",
    "        while (index < len(row['split_tokens'])) and (output is None):\n",
    "            next_token = row['split_tokens'][index]\n",
    "            next_token_r = re.search(rf'^[.,():-]*(\\w+(.+\\w+)*)[.,():-]*$', next_token, flags=re.IGNORECASE)\n",
    "            if next_token_r is not None:\n",
    "                output = [next_token_r.group(1)]\n",
    "            else:\n",
    "                index += 1\n",
    "    if output is None:\n",
    "        output = ['']\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_first_word(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the first verb that comes after the anchor point, using NLTK part-of-speech tagging\n",
    "def group_first_verb(row):\n",
    "    tokens = [nltk.word_tokenize(row['split_0']), \n",
    "              nltk.word_tokenize(row['split_1']),\n",
    "              nltk.word_tokenize(row['split_2'])]\n",
    "    tokens_pos = nltk.pos_tag([item for sublist in tokens for item in sublist])\n",
    "    verb = ''\n",
    "    for i in range(len(tokens[0])+len(tokens[1]), len(tokens_pos)):\n",
    "        if tokens_pos[i][1].startswith('V'):\n",
    "            verb = tokens_pos[i][0]\n",
    "            break\n",
    "    output = [verb]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_first_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guides on dependency parses:\n",
    "# https://nlp.stanford.edu/software/dependencies_manual.pdf\n",
    "# https://web.stanford.edu/~jurafsky/slp3/15.pdf\n",
    "\n",
    "# Do dependency parsing once for the entire sample_input to save processing time\n",
    "# for groupings that require dependency parsing later\n",
    "def parse_dependency(row):\n",
    "    p = dependency_predictor.predict(\n",
    "        sentence=' '.join(row['split_tokens']).strip()\n",
    "    )\n",
    "    return dict(zip(['dependency_parse'], [p]))\n",
    "\n",
    "sample_input_dep = sample_input.apply(\n",
    "    lambda row: parse_dependency(row), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "sample_input_dep.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns n if the span is within split_n\n",
    "def split_span_descriptor(span, split):\n",
    "    # span = (whole_sentence_begin_index, whole_sentence_end_index)\n",
    "    # split = [str(split_0), str(split_1), str(split_2), maybe more ...]\n",
    "    length_sum = 0\n",
    "    spillover = span[0]\n",
    "    for i in range(len(split)):\n",
    "        length_sum += len(split[i])\n",
    "        if span[0] < length_sum:\n",
    "            return i, (spillover, spillover + span[1] - span[0])\n",
    "        spillover -= len(split[i]) + (1 if len(split[i]) > 0 else 0)\n",
    "    return len(split), (spillover, spillover + span[1] - span[0])\n",
    "\n",
    "print(split_span_descriptor((0, 3), ['', 'abc', 'def ghi']), \n",
    "      split_span_descriptor((4, 7), ['', 'abc', 'def ghi']), \n",
    "      split_span_descriptor((8, 11), ['', 'abc', 'def ghi']))\n",
    "print(split_span_descriptor((0, 3), ['abc', '', 'def ghi']), \n",
    "      split_span_descriptor((4, 7), ['abc', '', 'def ghi']), \n",
    "      split_span_descriptor((8, 11), ['abc', '', 'def ghi']))\n",
    "print(split_span_descriptor((0, 3), ['abc', 'def', 'ghi']), \n",
    "      split_span_descriptor((4, 7), ['abc', 'def', 'ghi']), \n",
    "      split_span_descriptor((8, 11), ['abc', 'def', 'ghi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group on the main verb in the sentence, using allennlp dependency parsing (based on demo code)\n",
    "def group_main_verb(row):\n",
    "    p = row['dependency_parse']\n",
    "    main_verb = p['hierplane_tree']['root']\n",
    "    main_verb_span = (main_verb['spans'][0]['start'], main_verb['spans'][0]['end'])\n",
    "    split_loc, cspan = split_span_descriptor(main_verb_span, [row['split_0'], row['split_1'], row['split_2']])\n",
    "    output = [main_verb['word'], split_loc, \n",
    "              main_verb_span[0], main_verb_span[1], cspan[0], cspan[1]]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.join(sample_input_dep).apply(\n",
    "    lambda row: group_main_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlaps two strings, filling in any whitespace characters in s1 with the non-whitespace char in s2\n",
    "# If len(s2) > len(s1), the extra is added to the end\n",
    "def string_union(s1, s2):\n",
    "    output = list(s1)\n",
    "    for i in range(min(len(s1), len(s2))):\n",
    "        if s1[i].isspace():\n",
    "            output[i] = s2[i]\n",
    "    # add on the extra if s2 is longer\n",
    "    if len(s2) > len(s1):\n",
    "        output += list(s2[len(s1):])\n",
    "    return ''.join(output)\n",
    "\n",
    "# Overlaps two spans, producing their union (if there is a gap in between, it fills the gaps)\n",
    "def span_union(s1, s2):\n",
    "    return (min(s1[0], s2[0]), max(s1[1], s2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "# Returns the entire phrase that comprises a dependency tree node and its children\n",
    "def unroll_dependency_node(node, allowed_links=None, allowed_types=None):\n",
    "    node_str = ' '*node['spans'][0]['start'] + node['word']\n",
    "    node_span = (node['spans'][0]['start'], node['spans'][0]['end'])\n",
    "    if 'children' in node:\n",
    "        for i in range(len(node['children'])):\n",
    "            if (allowed_links is None) or (node['children'][i]['link'] in allowed_links):\n",
    "                if (allowed_types is None) or (node['children'][i]['attributes'][0] in allowed_types):\n",
    "                    child_str, child_span = unroll_dependency_node(node['children'][i],\n",
    "                                                      allowed_links=allowed_links,\n",
    "                                                      allowed_types=allowed_types)\n",
    "                    node_str = string_union(node_str, child_str)\n",
    "                    node_span = span_union(node_span, child_span)\n",
    "    return node_str, node_span\n",
    "\n",
    "# print(unroll_dependency_node(sample_input_dep.iloc[0][0]['hierplane_tree']['root']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_from_index(p, i):\n",
    "    pathway = []\n",
    "    while p['predicted_heads'][i] != 0:\n",
    "        # count what index of its parent this node is\n",
    "        parent_idx = 0\n",
    "        for test_i in range(i):\n",
    "            if p['predicted_heads'][i] == p['predicted_heads'][test_i]:\n",
    "                parent_idx += 1\n",
    "        pathway.insert(0, parent_idx)\n",
    "        i = p['predicted_heads'][i] - 1\n",
    "    curr_node = p['hierplane_tree']['root']\n",
    "    for child_i in pathway:\n",
    "        curr_node = curr_node['children'][child_i]\n",
    "    return curr_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_children(node, types):\n",
    "    child_nodes = node['children'] if ('children' in node) else []\n",
    "    matches = []\n",
    "    for i in range(len(child_nodes)):\n",
    "        if child_nodes[i]['nodeType'] in types:\n",
    "            matches.append(child_nodes[i])\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the main verb in the sentence, using allennlp dependency parsing (based on demo code)\n",
    "# Expanded to include subject, object, expand upon verb form\n",
    "def group_main_verb_expanded(row):\n",
    "    p = row['dependency_parse']\n",
    "    root_node = p['hierplane_tree']['root']\n",
    "    root_phrase, root_span = unroll_dependency_node(\n",
    "        root_node,\n",
    "        allowed_links=['aux', 'auxpass', 'cop'],\n",
    "        allowed_types=['VERB', 'AUX', 'PART']\n",
    "    )\n",
    "    root_phrase = ' '.join(root_phrase.strip().split())\n",
    "    split_loc, cspan = split_span_descriptor(root_span, [row['split_0'], row['split_1'], row['split_2']])\n",
    "    subjects = get_node_children(root_node, ['subj', 'nsubj', 'nsubjpass', 'csubj', 'csubjpass'])\n",
    "    subjects = [' '.join(unroll_dependency_node(n)[0].strip().split()) for n in subjects]\n",
    "    objects = get_node_children(root_node, ['obj', 'dobj', 'iobj', 'pobj'])\n",
    "    objects = [' '.join(unroll_dependency_node(n)[0].strip().split()) for n in objects]\n",
    "    # Pick out dependencies that come before/after the verb...\n",
    "    # this targets dependencies that weren't clearly identified as subj/obj by the parser\n",
    "    unsure_relation = get_node_children(root_node, ['dep'])\n",
    "    unsure_relation = [(n['spans'][0]['start'],\n",
    "                        ' '.join(unroll_dependency_node(n)[0].strip().split()))\n",
    "                       for n in unsure_relation]\n",
    "    # TODO improve this possibly\n",
    "    # For now, assume a dependent phrase before the averb is more likely to be subject-like\n",
    "    # and a dependent phrase after the averb is more likely to be object-like\n",
    "    subjects += [n[1] for n in unsure_relation if n[0] < root_node['spans'][0]['start']]\n",
    "    objects += [n[1] for n in unsure_relation if n[0] > root_node['spans'][0]['start']]\n",
    "    output = [root_phrase, subjects, objects, \n",
    "              split_loc, root_span[0], root_span[1], cspan[0], cspan[1]]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.join(sample_input_dep).apply(\n",
    "    lambda row: group_main_verb_expanded(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for phrase POS\n",
    "# Return the match bounds of the sequence of elements of given sizes starting at list1[i1] and list2[i2] \n",
    "# that match\n",
    "# If no given size is returned, returns max matching sequence length\n",
    "# (ratio of element matches must be 1:some or some:1 between l1 and l2)\n",
    "# Returns [(l1 bounds), (l2 bounds)] or None if they do not match\n",
    "def list_elements_match(list1, list2, i1, i2, size1=None, size2=None):\n",
    "    matchlen = 0\n",
    "    if size1 is not None and size2 is not None:\n",
    "        # check for exact text match\n",
    "        matchlen = len(''.join(list1[i1:i1+size1]))\n",
    "        if ''.join(list1[i1:i1+size1]) != ''.join(list2[i2:i2+size2]):\n",
    "            return None\n",
    "    elif size1 is not None:\n",
    "        # and size2 is none\n",
    "        matchlen = len(''.join(list1[i1:i1+size1]))\n",
    "        if ''.join(list1[i1:i1+size1]) != ''.join(list2[i2:])[:matchlen]:\n",
    "            return None\n",
    "    elif size2 is not None:\n",
    "        # and size1 is none\n",
    "        matchlen = len(''.join(list2[i2:i2+size2]))\n",
    "        if ''.join(list2[i2:i2+size2]) != ''.join(list1[i1:])[:matchlen]:\n",
    "            return None\n",
    "    else:\n",
    "        # both are none; just calculate the match length\n",
    "        matchlen = 0\n",
    "        while l1concat[matching] == l2concat[matching]:\n",
    "            matchlen += 1\n",
    "    matchphrase = ''.join(list1[i1:])[:matchlen]\n",
    "    # get the exact bounds for list1\n",
    "    bound1 = 1\n",
    "    for i in range(len(list1)-i1+1):\n",
    "        if ''.join(list1[i1:i1+i]) == matchphrase:\n",
    "            bound1 = i\n",
    "            break\n",
    "    # get the exact bounds for list2\n",
    "    bound2 = 1\n",
    "    for i in range(len(list2)-i2+1):\n",
    "        if ''.join(list2[i2:i2+i]) == matchphrase:\n",
    "            bound2 = i\n",
    "            break\n",
    "    return [(i1, i1+bound1), (i2, i2+bound2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Group on the verb closest to the anchor point, using allennlp dependency parsing (based on demo code)\n",
    "# Include SVO information and expanded verbs\n",
    "def group_anchor_verb(row):\n",
    "    if row['split_anchor_span'] is None:\n",
    "        return dict(zip(grouping_headers, ['']))\n",
    "    p = row['dependency_parse']\n",
    "    # build out a more usable version of the dependency tree with information about tree level!\n",
    "    # tree_array[n] = (parent, level) so the root node is (0, 0)\n",
    "    tree_array = [(h, -1) for h in p['predicted_heads']]\n",
    "    need_connection = [0]\n",
    "    level = 0\n",
    "    while len(need_connection) > 0:\n",
    "        need_connection_update = []\n",
    "        for i in range(len(tree_array)):\n",
    "            if tree_array[i][0] in need_connection:\n",
    "                tree_array[i] = (tree_array[i][0], level)\n",
    "                need_connection_update.append(i+1)\n",
    "        need_connection = need_connection_update\n",
    "        level += 1\n",
    "    # Figure out what indexes our anchor fits into\n",
    "    # Assume that the anchor is contiguous text\n",
    "    # TODO: is this always true?\n",
    "    for i in range(len(p['words'])):\n",
    "        if len(''.join(p['words'][:i])) >= len(''.join(row['split_tokens'][:row['split_anchor_span'][0]])):\n",
    "            match = list_elements_match(\n",
    "                p['words'], row['split_tokens'], i, row['split_anchor_span'][0], \n",
    "                size2=row['split_anchor_span'][1]-row['split_anchor_span'][0])\n",
    "            if match is not None:\n",
    "                break\n",
    "    # Find the smallest containing dependency node\n",
    "    matching_nodes = [(i, tree_array[i][0], tree_array[i][1]) for i in range(match[0][0], match[0][1])]\n",
    "    matching_nodes = list(set(matching_nodes))\n",
    "    # Remove punctuation from the set of nodes that consist the anchor\n",
    "    # This avoids odd dependency structures\n",
    "    matching_nodes = [node for node in matching_nodes if p['pos'][node[0]] != 'PUNCT']\n",
    "    while len(matching_nodes) > 1:\n",
    "        matching_nodes.sort(key=lambda x: x[2])\n",
    "        parent = matching_nodes.pop()[1]\n",
    "        matching_nodes.append((parent-1, tree_array[parent-1][0], tree_array[parent-1][1]))\n",
    "        matching_nodes = list(set(matching_nodes))\n",
    "    # this is the anchor node\n",
    "    node = matching_nodes[0]\n",
    "    anchor_node = matching_nodes[0]\n",
    "    # Find the closest parent that is a verb\n",
    "    parent = node[1]\n",
    "    while p['pos'][node[0]] not in ['VERB', 'AUX'] and parent != 0:\n",
    "        node = (parent-1, tree_array[parent-1][0], tree_array[parent-1][1])\n",
    "        parent = node[1]\n",
    "    if p['pos'][node[0]] not in ['VERB', 'AUX']:\n",
    "        # we've landed in a root node that isn't a verb, wheeeeee\n",
    "        return dict(zip(grouping_headers, ['']))\n",
    "    # node is definitely in the anchor verb (type 'VERB' or 'AUX') now\n",
    "    immediate_verb = p['words'][node[0]]\n",
    "    relevant_verbs = ['']*len(p['words'])\n",
    "    relevant_verbs[node[0]] = p['words'][node[0]]\n",
    "    # climb to the top of the anchor verb blob\n",
    "    if p['pos'][node[0]] in ['AUX']:\n",
    "        parent = node[1]\n",
    "        while parent != 0 and p['pos'][node[0]] in ['VERB']:\n",
    "            node = (parent-1, tree_array[parent-1][0], tree_array[parent-1][1])\n",
    "            if p['pos'][node[0]] in ['VERB']:\n",
    "                relevant_verbs.append(p['words'][node[0]])\n",
    "            parent = node[1]\n",
    "    # node should now be the top anchorverb\n",
    "    averb_node = node\n",
    "    node = get_node_from_index(row['dependency_parse'], node[0])\n",
    "    averb_string, averb_span = unroll_dependency_node(\n",
    "        node,\n",
    "        allowed_links=['aux', 'auxpass', 'cop'],\n",
    "        allowed_types=['VERB', 'AUX', 'PART']\n",
    "    )\n",
    "    averb_string = ' '.join(averb_string.strip().split())\n",
    "    split_loc, cspan = split_span_descriptor(averb_span, [row['split_0'], row['split_1'], row['split_2']])\n",
    "    subjects = get_node_children(node, ['subj', 'nsubj', 'nsubjpass', 'csubj', 'csubjpass'])\n",
    "    subjects = [' '.join(unroll_dependency_node(n)[0].strip().split()) for n in subjects]\n",
    "    objects = get_node_children(node, ['obj', 'dobj', 'iobj', 'pobj'])\n",
    "    objects = [' '.join(unroll_dependency_node(n)[0].strip().split()) for n in objects]\n",
    "    # Pick out dependencies that come before/after the verb...\n",
    "    # this targets dependencies that weren't clearly identified as subj/obj by the parser\n",
    "    unsure_relation = get_node_children(node, ['dep'])\n",
    "    unsure_relation = [(n['spans'][0]['start'],\n",
    "                        ' '.join(unroll_dependency_node(n)[0].strip().split()))\n",
    "                       for n in unsure_relation]\n",
    "    # TODO improve this possibly\n",
    "    # For now, assume a dependent phrase before the averb is more likely to be subject-like\n",
    "    # and a dependent phrase after the averb is more likely to be object-like\n",
    "    subjects += [n[1] for n in unsure_relation if n[0] < node['spans'][0]['start']]\n",
    "    objects += [n[1] for n in unsure_relation if n[0] > node['spans'][0]['start']]\n",
    "    # determine if the anchor is in the subj or obj part\n",
    "    # TODO this could be improved by adding linguistics knowledge to algo\n",
    "    # For now, just mark if the anchor comes before or after the top-averb\n",
    "    # 1 if anchor is subj (before), 0 if anchor=verb (same index), -1 if anchor is obj (after)\n",
    "    # TODO change this to use the anchor span/index and the averb span, NOT the node number\n",
    "    relation = averb_node[0] - anchor_node[0]\n",
    "    relation = int(math.copysign(1, relation)) if relation != 0 else 0\n",
    "    output = [averb_string, subjects, objects, relation, \n",
    "              split_loc, averb_span[0], averb_span[1], cspan[0], cspan[1]]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.join(sample_input_dep).apply(\n",
    "    lambda row: group_anchor_verb(row), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group on the POS of the anchor point, using allennlp dependency parsing (based on demo code)\n",
    "# I'm defining the \"POS of a phrase\" as the POS of the lowest node that contains the entire phrase\n",
    "def group_anchor_pos(row, context=1):\n",
    "    if row['split_anchor_span'] is None:\n",
    "        return dict(zip(grouping_headers, ['']))\n",
    "    p = row['dependency_parse']\n",
    "    # build out a more usable version of the dependency tree with information about tree level!\n",
    "    # tree_array[n] = (parent, level) so the root node is (0, 0)\n",
    "    tree_array = [(h, -1) for h in p['predicted_heads']]\n",
    "    need_connection = [0]\n",
    "    level = 0\n",
    "    while len(need_connection) > 0:\n",
    "        need_connection_update = []\n",
    "        for i in range(len(tree_array)):\n",
    "            if tree_array[i][0] in need_connection:\n",
    "                tree_array[i] = (tree_array[i][0], level)\n",
    "                need_connection_update.append(i+1)\n",
    "        need_connection = need_connection_update\n",
    "        level += 1\n",
    "    # Figure out what indexes our anchor fits into\n",
    "    # Assume that the anchor is contiguous text\n",
    "    # TODO: is this always true?\n",
    "    for i in range(len(p['words'])):\n",
    "        if len(''.join(p['words'][:i])) >= len(''.join(row['split_tokens'][:row['split_anchor_span'][0]])):\n",
    "            match = list_elements_match(\n",
    "                p['words'], row['split_tokens'], i, row['split_anchor_span'][0], \n",
    "                size2=row['split_anchor_span'][1]-row['split_anchor_span'][0])\n",
    "            if match is not None:\n",
    "                break\n",
    "    # Find the smallest containing dependency node\n",
    "    matching_nodes = [(i, tree_array[i][0], tree_array[i][1]) for i in range(match[0][0], match[0][1])]\n",
    "    matching_nodes = list(set(matching_nodes))\n",
    "    # Remove punctuation from the set of nodes that consist the anchor\n",
    "    # This avoids odd dependency structures\n",
    "    matching_nodes = [node for node in matching_nodes if p['pos'][node[0]] != 'PUNCT']\n",
    "    while len(matching_nodes) > 1:\n",
    "        matching_nodes.sort(key=lambda x: x[2])\n",
    "        parent = matching_nodes.pop()[1]\n",
    "        matching_nodes.append((parent-1, tree_array[parent-1][0], tree_array[parent-1][1]))\n",
    "        matching_nodes = list(set(matching_nodes))\n",
    "    # this is the anchor node\n",
    "    node = matching_nodes[0]\n",
    "    # And get the POS and words corresponding to that node and its {context} parents\n",
    "    labeltiers = []\n",
    "    labelwords = []\n",
    "    while len(labeltiers) < context:\n",
    "        labeltiers.append(p['pos'][node[0]])\n",
    "        labelwords.append(p['words'][node[0]])\n",
    "        parent = node[1]\n",
    "        if parent == 0:\n",
    "            break\n",
    "        node = (parent-1, tree_array[parent-1][0], tree_array[parent-1][1])\n",
    "    return dict(zip(grouping_headers, [labeltiers, labelwords]))\n",
    "\n",
    "output = sample_input.join(sample_input_dep).apply(\n",
    "    lambda row: group_anchor_pos(row, context=3), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSVs\n",
    "\n",
    "This section should be able to be run as a sequence by itself, assuming all functions are defined and the large imports have been performed already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_outputs(search_word, anchor_type, try_rerun, split_data, df_sentences):\n",
    "    splitted_sentences = df_sentences.merge(split_data,\n",
    "                                            how='outer',\n",
    "                                            left_on=join_headers,\n",
    "                                            right_on=join_headers)\n",
    "    \n",
    "    # Retrieve dependencies\n",
    "    if try_rerun or (not os.path.isfile(f'outputs/{search_word}/partial-dependency-{anchor_type}.pkl')):\n",
    "        splitted_sentences_dep = splitted_sentences.apply(\n",
    "            lambda row: parse_dependency(row), \n",
    "            axis=1, result_type='expand')\n",
    "\n",
    "        splitted_sentences_dep.to_pickle(f'outputs/{search_word}/partial-dependency-{anchor_type}.pkl')\n",
    "\n",
    "    splitted_sentences_dep = pd.read_pickle(f'outputs/{search_word}/partial-dependency-{anchor_type}.pkl')\n",
    "    \n",
    "    # Generate individual outputs\n",
    "    output = splitted_sentences.apply(\n",
    "        lambda row: group_first_word(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_firstword.csv')\n",
    "    \n",
    "    output = splitted_sentences.apply(\n",
    "        lambda row: group_first_verb(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_firstverb.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "        lambda row: group_main_verb(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_mainverb.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "        lambda row: group_main_verb_expanded(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "        lambda row: group_anchor_pos(row, context=3), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_anchorpos.csv')\n",
    "\n",
    "    output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "        lambda row: group_anchor_verb(row), \n",
    "        axis=1, result_type='expand')\n",
    "    splitted_sentences.join(output).to_csv(f'outputs/{search_word}/{anchor_type}_anchorverb.csv')\n",
    "    \n",
    "    # Restructure the outputs into something that can be neatly imported into a spreadsheet to play with\n",
    "    coreference_anchorverb = pd.read_csv(f'outputs/{search_word}/{anchor_type}_anchorverb.csv')\n",
    "    coreference_anchorverb = coreference_anchorverb.rename(\n",
    "        columns={\"group\": \"averb\", \"group2\": \"averb_s\", \"group3\": \"averb_o\", \"group4\": \"averb_relation\", \n",
    "                 \"group5\": \"averb_split\", \"group6\": \"averb_span0\", \"group7\": \"averb_span1\", \n",
    "                 \"group8\": \"averb_cspan0\", \"group9\": \"averb_cspan1\"})\n",
    "    coreference_mainverb = pd.read_csv(f'outputs/{search_word}/{anchor_type}_mainverb.csv')\n",
    "    coreference_mainverb = coreference_mainverb[['group']]\n",
    "    coreference_mainverb = coreference_mainverb.rename(\n",
    "        columns={\"group\": \"root\"})\n",
    "    coreference_mainverb_svo = pd.read_csv(f'outputs/{search_word}/{anchor_type}_mainverb_svo.csv')\n",
    "    coreference_mainverb_svo = coreference_mainverb_svo[['group', 'group2', 'group3', \n",
    "                                                         'group4', 'group5', 'group6', \n",
    "                                                         'group7', 'group8']]\n",
    "    coreference_mainverb_svo = coreference_mainverb_svo.rename(\n",
    "        columns={\"group\": \"root_full\", \"group2\": \"root_s\", \"group3\": \"root_o\", \n",
    "                 \"group4\": \"root_split\", \"group5\": \"root_span0\", \"group6\": \"root_span1\", \n",
    "                 \"group7\": \"root_cspan0\", \"group8\": \"root_cspan1\"})\n",
    "    coreference_firstverb = pd.read_csv(f'outputs/{search_word}/{anchor_type}_firstverb.csv')\n",
    "    coreference_firstverb = coreference_firstverb[['group']]\n",
    "    coreference_firstverb = coreference_firstverb.rename(\n",
    "        columns={\"group\": \"fverb\"})\n",
    "    coreference_anchorpos = pd.read_csv(f'outputs/{search_word}/{anchor_type}_anchorpos.csv')\n",
    "    coreference_anchorpos = coreference_anchorpos[['group', 'group2']]\n",
    "    coreference_anchorpos = coreference_anchorpos.rename(\n",
    "        columns={\"group\": \"apos\", \"group2\": \"apos_w\"})\n",
    "    coreference_firstword = pd.read_csv(f'outputs/{search_word}/{anchor_type}_firstword.csv')\n",
    "    coreference_firstword = coreference_firstword[['group']]\n",
    "    coreference_firstword = coreference_firstword.rename(\n",
    "        columns={\"group\": \"fword\"})\n",
    "\n",
    "    outputs_merged = coreference_anchorverb.join(\n",
    "        coreference_mainverb\n",
    "    ).join(\n",
    "        coreference_mainverb_svo\n",
    "    ).join(\n",
    "        coreference_firstverb\n",
    "    ).join(\n",
    "        coreference_anchorpos\n",
    "    ).join(\n",
    "        coreference_firstword\n",
    "    )\n",
    "    outputs_merged = outputs_merged[[\n",
    "        'split_0', 'split_1', 'split_2',\n",
    "        'averb', 'averb_s', 'averb_o', 'averb_relation', \n",
    "            'averb_split', 'averb_span0', 'averb_span1', 'averb_cspan0', 'averb_cspan1',\n",
    "        'root', 'root_full', 'root_s', 'root_o', \n",
    "            'root_split', 'root_span0', 'root_span1', 'root_cspan0', 'root_cspan1',\n",
    "        'fverb', 'fword', 'apos', 'apos_w',\n",
    "        'URL', 'ID', 'Type', 'Index', 'Text', \n",
    "        'split_tokens', 'split_anchor_span', 'split_anchor_indices', \n",
    "        'within_anchor_index'\n",
    "    ]]\n",
    "\n",
    "    outputs_merged.to_csv(f'outputs/{search_word}/{anchor_type}.csv')\n",
    "    return outputs_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "def pipeline(search_word, \n",
    "             anchor_synonyms, \n",
    "             try_rerun=False):\n",
    "    \"\"\"\n",
    "    Run the entire analysis pipeline for a given search phrase and set of synonyms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_word : str\n",
    "        Identify the term we are splitting on (the \"anchor\").\n",
    "        This also serves as the name for this entire cluster of results / practical pipeline run name.\n",
    "    anchor_synonyms : [str]\n",
    "        Other common names for this term that we should also consider as anchors.\n",
    "    try_rerun : Boolean\n",
    "        Forcibly recreate intermediate files such as coreference or dependency parses.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform basic df loading\n",
    "    \n",
    "    # Read in the dataframe containing entire paper abstracts (NOT pre-split into sentences)\n",
    "    df = pd.read_csv(f'data/nlp-align_{search_word}.csv')\n",
    "\n",
    "    # Create the outputs directory for this search word\n",
    "    Path(f\"outputs/{search_word}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Restructure the dataframe to be more usable\n",
    "    df = df.groupby('ID', group_keys=False).apply(\n",
    "        lambda row: separate_title_abstract(row)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "    df_sentences = df.groupby(['ID', 'Type'], group_keys=False).apply(\n",
    "        lambda row: sentence_tokenize(row)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Export whitespace-based split data\n",
    "    whitespace_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "        lambda group: split_term_whitespace(group, search_word, anchor_synonyms)).reset_index(drop=True)\n",
    "\n",
    "    # Export coreference-based split data\n",
    "    if try_rerun or (not os.path.isfile(f'outputs/{search_word}/partial-coreference.pkl')):\n",
    "        output = df.apply(\n",
    "            lambda row: coref_predictor.predict(row['Text']), axis=1, result_type='expand')\n",
    "        df_merged = df.join(output)\n",
    "\n",
    "        output = df_merged.apply(\n",
    "            lambda row: reinterpret_coref_clusters(row, search_word, anchor_synonyms, df_sentences), \n",
    "            axis=1, result_type='expand')\n",
    "        df_merged = df_merged.join(output)\n",
    "\n",
    "        df_merged.to_pickle(f'outputs/{search_word}/partial-coreference.pkl')\n",
    "\n",
    "    df_merged = pd.read_pickle(f'outputs/{search_word}/partial-coreference.pkl')\n",
    "    coreference_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "        lambda group: split_term_coreference(group, search_word, anchor_synonyms, df_merged, split_term_whitespace)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Whitespace outputs\n",
    "    generate_outputs(search_word, 'whitespace', try_rerun, whitespace_output, df_sentences)\n",
    "    \n",
    "    # Coreference outputs\n",
    "    generate_outputs(search_word, 'coreference', try_rerun, coreference_output, df_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = [\n",
    "    ('BERT', ['BERT']),\n",
    "    ('SQuAD', ['SQuAD']),\n",
    "    ('DROP', ['DROP']),\n",
    "    ('GPT-2', ['GPT', 'GPT-2', 'GPT-3']),\n",
    "    ('Transformers', ['Transformers', 'Transformer', 'transfer learning', 'transfer'])\n",
    "]\n",
    "\n",
    "for dataset in all_targets:\n",
    "    pipeline(dataset[0], dataset[1], try_rerun=False)\n",
    "    print(dataset[0], 'done running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = dependency_predictor.predict(\n",
    "#     sentence='the quick red fox jumped over the lazy dog.'\n",
    "# )\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
