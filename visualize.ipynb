{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "# https://altair-viz.github.io/gallery/simple_bar_chart.html\n",
    "# source = pd.DataFrame({\n",
    "#     'a': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],\n",
    "#     'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]\n",
    "# })\n",
    "\n",
    "# alt.Chart(source).mark_bar().encode(\n",
    "#     x='a',\n",
    "#     y='b'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inheriting usage notes from my other code for now (not self-plagiarization)\n",
    "# https://github.com/cephcyn/cse517project/blob/master/embed_w2v.py\n",
    "# for word2vec code; see also word2vec documentation\n",
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model source: https://code.google.com/archive/p/word2vec/\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'model/GoogleNews-vectors-negative300.bin', \n",
    "    binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word2vec embedding of a phrase\n",
    "def get_phrase_embed_word2vec(word2vec, phrase):\n",
    "    try:\n",
    "        phraseS = phrase.split()\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    unknowns = []\n",
    "    emb = []\n",
    "    for w in phraseS:\n",
    "        try:\n",
    "            emb.append(word2vec[w])\n",
    "        except:\n",
    "            unknowns.append(w)\n",
    "    if len(emb) == 0:\n",
    "        return pd.DataFrame()\n",
    "    emb_sum = pd.DataFrame(emb).sum()\n",
    "    emb_sum['word'] = phrase\n",
    "    return pd.DataFrame([emb_sum])\n",
    "\n",
    "v = get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'test sentence')\n",
    "sent_v = get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'This is a test sentence !')\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_subtract(v, sent_v, dim):\n",
    "    try:\n",
    "        inverse_v = sent_v.iloc[:, 0:dim].subtract(v.iloc[:, 0:dim])\n",
    "        inverse_v['word'] = v['word']\n",
    "        inverse_v['sentence'] = sent_v['word']\n",
    "        return inverse_v\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "embed_subtract(v, sent_v, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "\n",
    "# OPTIONAL - to disable outputs from Tensorflow\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "# This line only needs to be a one-time download\n",
    "# !python -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load ELMo model\n",
    "url = \"https://tfhub.dev/google/elmo/2\"\n",
    "elmo = hub.Module(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ELMo embedding of a phrase (with given span limits)\n",
    "def get_phrase_embed_elmo(elmo, sentence, span0, span1, phrase, pregenerated=None):\n",
    "    try:\n",
    "        # TODO make the NaN / none case check neater?\n",
    "        phraseS = phrase.split()\n",
    "        span0 = int(span0)\n",
    "        span1 = int(span1)\n",
    "    except:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    span0 = len(sentence[:span0].split(' ')) - 1\n",
    "    span1 = len(sentence[:span1].split(' ')) - 1\n",
    "    if pregenerated is None:\n",
    "        # If we don't have a handy pre-generated dict of sentence:vector already...\n",
    "        embeddings = elmo(\n",
    "            [sentence], \n",
    "            signature='default', \n",
    "            as_dict=True)['elmo']\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.tables_initializer())\n",
    "            x = sess.run(embeddings)\n",
    "            x = x[0]\n",
    "    else:\n",
    "        # otherwise just grab the vector\n",
    "        x = pregenerated[sentence]\n",
    "    emb_sum = pd.DataFrame(x[span0:span1]).sum()\n",
    "    emb_sum['word'] = phrase\n",
    "    sentence_sum = pd.DataFrame(x).sum()\n",
    "    sentence_sum['word'] = sentence\n",
    "    return pd.DataFrame([emb_sum]), pd.DataFrame([sentence_sum])\n",
    "\n",
    "# v, sent_v = get_phrase_embed_elmo(\n",
    "#     elmo, \n",
    "#     'This is a test sentence !',\n",
    "#     10, 24, 'test sentence')\n",
    "# v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_subtract(v, sent_v, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word = 'BERT'\n",
    "anchor_type = 'coreference'\n",
    "\n",
    "csv = pd.read_csv(\n",
    "    f'outputs/{search_word}/{anchor_type}.csv', \n",
    "    index_col='Unnamed: 0')\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_w2v = csv.groupby(\n",
    "    csv.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: get_phrase_embed_word2vec(\n",
    "        word2vec,\n",
    "        group.iloc[0]['d_averb']\n",
    "    )\n",
    ").reset_index(level=1, drop=True)\n",
    "output_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_w2v_inv = csv.groupby(\n",
    "    csv.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: embed_subtract(\n",
    "        get_phrase_embed_word2vec(\n",
    "            word2vec,\n",
    "            group.iloc[0]['d_averb']\n",
    "        ), \n",
    "        get_phrase_embed_word2vec(\n",
    "            word2vec,\n",
    "            ' '.join(group.iloc[0]['split_tokens'])\n",
    "        ), 300)\n",
    ").reset_index(level=1, drop=True)\n",
    "output_w2v_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid is the arithmetic mean position of all points in the figure\n",
    "output_w2v_c = pd.DataFrame(\n",
    "    [np.mean(output_w2v.iloc[:, 0:300])])\n",
    "output_w2v_c['word'] = \"[CENTROID]\"\n",
    "output_w2v_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import ast\n",
    "\n",
    "# save ELMo outputs so rerunning doesn't take forever...\n",
    "# don't rerun this if we don't need to regenerate the ELMo outputs :|\n",
    "all_sentences = [' '.join(ast.literal_eval(r)) for r in csv['split_tokens']]\n",
    "embeddings = elmo(\n",
    "    all_sentences, \n",
    "    signature='default', \n",
    "    as_dict=True)['elmo']\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    x = sess.run(embeddings)\n",
    "\n",
    "x = dict(zip(all_sentences, x))\n",
    "pickle.dump(x, open(\"temp/BERT_ELMo.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pickle\n",
    "\n",
    "x = pickle.load(open(\"temp/BERT_ELMo.pkl\", \"rb\"))\n",
    "output_elmo = csv.groupby(\n",
    "    csv.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: get_phrase_embed_elmo(\n",
    "        elmo,\n",
    "        ' '.join(ast.literal_eval(group.iloc[0]['split_tokens'])),\n",
    "        group.iloc[0]['averb_span0'],\n",
    "        group.iloc[0]['averb_span1'],\n",
    "        group.iloc[0]['averb'], pregenerated=x\n",
    "    )[0]\n",
    ").reset_index(level=1, drop=True)\n",
    "\n",
    "pickle.dump(output_elmo, open(f'temp/BERT_anchorverb_elmo.pkl', \"wb\"))\n",
    "output_elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid is the arithmetic mean position of all points in the figure\n",
    "output_elmo_c = pd.DataFrame(\n",
    "    [np.mean(output_elmo.iloc[:, 0:300])])\n",
    "output_elmo_c['word'] = \"[CENTROID]\"\n",
    "output_elmo_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def elmo_inv(row, pregenerated=None):\n",
    "    v, sent_v = get_phrase_embed_elmo(\n",
    "        elmo,\n",
    "        ' '.join(ast.literal_eval(row['split_tokens'])),\n",
    "        row['averb_span0'],\n",
    "        row['averb_span1'],\n",
    "        row['averb'], pregenerated=pregenerated\n",
    "    )\n",
    "    sent_v_inv = embed_subtract(v, sent_v, 1024)\n",
    "    return sent_v_inv\n",
    "\n",
    "x = pickle.load(open(\"temp/BERT_ELMo.pkl\", \"rb\"))\n",
    "output_elmo_inv = csv.groupby(\n",
    "    csv.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: elmo_inv(group.iloc[0], pregenerated=x)\n",
    ").reset_index(level=1, drop=True)\n",
    "\n",
    "output_elmo_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the vectors\n",
    "# loaned from :\n",
    "# https://github.com/cephcyn/ChatlogGrapher/blob/master/data_processing.ipynb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import altair as alt\n",
    "alt.renderers.enable('default')\n",
    "\n",
    "def visualize_embeds(data, reference, color=None, tooltip=['word']):\n",
    "    x = data.iloc[:, 0:300]\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principalDf = pd.DataFrame(\n",
    "        data=principalComponents,\n",
    "        columns=['pc1', 'pc2'])\n",
    "\n",
    "    finalDf = principalDf\n",
    "    finalDf = finalDf.set_index(data.index)\n",
    "    finalDf['word'] = data['word']\n",
    "    finalDf = finalDf.join(\n",
    "        reference, \n",
    "        how='inner',\n",
    "        lsuffix='_embed', \n",
    "        rsuffix='_ref'\n",
    "    )\n",
    "\n",
    "    chart = alt.Chart(finalDf).mark_circle(size=60)\n",
    "    # should figure out a more pythonic way to do this :/\n",
    "    if color is None:\n",
    "        chart = chart.encode(\n",
    "            x='pc1',\n",
    "            y='pc2',\n",
    "            tooltip=tooltip\n",
    "        )\n",
    "    else:\n",
    "        chart = chart.encode(\n",
    "            x='pc1',\n",
    "            y='pc2',\n",
    "            color=color,\n",
    "            tooltip=tooltip\n",
    "        )\n",
    "    return chart.interactive()\n",
    "\n",
    "# visualize_embeds(pd.concat([output_w2v, output_w2v_c]).reset_index())\n",
    "visualize_embeds(output_w2v, csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeds(output_w2v_inv, csv, tooltip=['word', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeds(output_elmo, csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeds(output_elmo_inv, csv, tooltip=['word', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this is the automatically-identified \"root words\"\n",
    "output_manualgroups = pd.DataFrame([['[introduce]', 'stands'], ['[introduce]', 'role'],\n",
    "                                    ['[introduce]', 'propose'], ['[success]', 'Encoder'],\n",
    "                                    ['[success]', 'Recently'], ['[success]', 'ground'],\n",
    "                                    ['[success]', 'achieved'], ['[success]', 'successfully'],\n",
    "                                    ['[success]', 'improved'], ['[success]', 'found'],\n",
    "                                    ['[success]', 'good'], ['[success]', 'advanced'],\n",
    "                                    ['[success]', 'models'], ['[success]', 'proven'],\n",
    "                                    ['[success]', 'yielded'], ['[successbut]', 'explore'],\n",
    "                                    ['[successbut]', 'success'], ['[successbut]', 'currently'],\n",
    "                                    ['[successbut]', 'currently'], ['[successbut]', 'success'],\n",
    "                                    ['[successbut]', 'success'], ['[successbut]', 'prevalent'],\n",
    "                                    ['[successbut]', 'approach'], ['[successbut]', 'led'],\n",
    "                                    ['[stateproof]', 'show'], ['[introtask]', 'analysis'],\n",
    "                                    ['[introtask]', 'are'], ['[introtask]', 'is'],\n",
    "                                    ['[introtask]', 'Question'], ['[assessment]', 'studies'],\n",
    "                                    ['[assessment]', ')'], ['[known]', 'allows'],\n",
    "                                    ['[known]', 'are']])\n",
    "# this is my manually-identified \"root words / main verbs\"\n",
    "# output_manualgroups = pd.DataFrame([['[introduce]', 'introduce'], ['[introduce]', 'present'],\n",
    "#                                     ['[introduce]', 'propose'], ['[success]', 'has shown'],\n",
    "#                                     ['[success]', 'have achieved'], ['[success]', 'has achieved'],\n",
    "#                                     ['[success]', 'has achieved'], ['[success]', 'has been applied'],\n",
    "#                                     ['[success]', 'has improved'], ['[success]', 'was found'],\n",
    "#                                     ['[success]', 'is'], ['[success]', 'have advanced'],\n",
    "#                                     ['[success]', 'have pushed forward'], ['[success]', 'has proven'],\n",
    "#                                     ['[success]', 'has yielded'], ['[successbut]', 'explore'],\n",
    "#                                     ['[successbut]', 'have had success'], ['[successbut]', 'give'],\n",
    "#                                     ['[successbut]', 'give'], ['[successbut]', 'have had success'],\n",
    "#                                     ['[successbut]', 'have had success'], ['[successbut]', 'remains'],\n",
    "#                                     ['[successbut]', 'has become'], ['[successbut]', 'has led'],\n",
    "#                                     ['[stateproof]', 'show'], ['[introtask]', 'is'],\n",
    "#                                     ['[introtask]', 'are'], ['[introtask]', 'is'],\n",
    "#                                     ['[introtask]', 'plays'], ['[assessment]', 'studies'],\n",
    "#                                     ['[assessment]', 'assess'], ['[known]', 'allows'],\n",
    "#                                     ['[known]', 'are applied']])\n",
    "# this is automaticaly-identified \"anchor verbs\" (excluding sentences where no anchor/anchorverb was IDed)\n",
    "# output_manualgroups = pd.DataFrame([['[successbut]', 'applying'], ['[assessment]', 'assess'],\n",
    "#                                     ['[successbut]', 'contribute'], ['[success]', 'has achieved'],\n",
    "#                                     ['[successbut]', 'have had'], ['[successbut]', 'have had'],\n",
    "#                                     ['[successbut]', 'motivating'], ['[success]', 'pre'],\n",
    "#                                     ['[success]', 'pre'], ['[stateproof]', 'show'],\n",
    "#                                     ['[introduce]', 'stands'], ['[assessment]', 'studies'],\n",
    "#                                     ['[success]', 'using']])\n",
    "# Using the entire sentence\n",
    "output_manualgroups = pd.DataFrame([\n",
    "     ['[successbut]', 'Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval.'],\n",
    "     ['[assessment]', 'I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) \"coloreless green ideas\" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.'],\n",
    "     ['[successbut]', 'BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success.'],\n",
    "     ['[success]', 'BERT, a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks.'],\n",
    "     ['[successbut]', 'Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.'],\n",
    "     ['[successbut]', 'Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.'],\n",
    "     ['[successbut]', 'Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.'],\n",
    "     ['[success]', 'Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks.'],\n",
    "     ['[success]', 'Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks.'],\n",
    "     ['[stateproof]', 'We show that BERT (Devlin et al., 2018) is a Markov random field language model.'],\n",
    "     ['[introduce]', 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.'],\n",
    "     ['[assessment]', 'This paper studies the performances and behaviors of BERT in ranking tasks.'],\n",
    "     ['[success]', 'Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset.'],\n",
    "     ['[introduce]', 'We present simple BERT-based models for relation extraction and semantic role labeling.'],\n",
    "     ['[introduce]', 'We propose a practical scheme to train a single multilingual sequence labeling model that yields state of the art results and is small and fast enough to run on a single CPU.'],\n",
    "     ['[success]', 'Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks.'],\n",
    "     ['[success]', 'Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference.'],\n",
    "     ['[success]', 'BERT model has been successfully applied to open-domain QA tasks.'],\n",
    "     ['[success]', 'The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts.'],\n",
    "     ['[success]', 'Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks.'],\n",
    "     ['[success]', 'Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks.'],\n",
    "     ['[success]', 'Language model pre-training has proven to be useful in learning universal language representations.'],\n",
    "     ['[success]', 'Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks.'],\n",
    "     ['[successbut]', 'BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success.'],\n",
    "     ['[successbut]', 'As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging.'],\n",
    "     ['[successbut]', 'Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models.'],\n",
    "     ['[successbut]', 'Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging.'],\n",
    "     ['[introtask]', 'Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA).'],\n",
    "     ['[introtask]', 'Intent classification and slot filling are two essential tasks for natural language understanding.'],\n",
    "     ['[introtask]', 'Conversational search is an emerging topic in the information retrieval community.'],\n",
    "     ['[introtask]', 'Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making.'],\n",
    "     ['[known]', 'Multi-task learning allows the sharing of useful information between multiple related tasks.'],\n",
    "     ['[known]', 'Data augmentation methods are often applied to prevent overfitting and improve generalization of deep neural network models.']])\n",
    "output_manualgroups = output_manualgroups.rename(columns={0: \"groupname\", 1: \"rootword\"})\n",
    "\n",
    "output_mg_v = output_manualgroups.groupby(\n",
    "    output_manualgroups.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: get_phrase_embed_word2vec(\n",
    "        word2vec,\n",
    "        group.iloc[0]['rootword']\n",
    "    )\n",
    ").reset_index(level=1, drop=True)\n",
    "\n",
    "visualize_embeds(output_mg_v, output_manualgroups, color='groupname', tooltip=['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from\n",
    "# https://gmarti.gitlab.io/ml/2017/09/07/how-to-sort-distance-matrix.html\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "N = len(output_mg_v)\n",
    "data = output_mg_v.iloc[:, 0:300]\n",
    "# data = output_elmo.iloc[:, 0:1024]\n",
    "# Shuffle data for extra comparison\n",
    "# data = data.sample(frac=1)\n",
    "dist_mat = squareform(pdist(data))\n",
    "\n",
    "plt.pcolormesh(dist_mat)\n",
    "plt.colorbar()\n",
    "plt.xlim([0,N])\n",
    "plt.ylim([0,N])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from\n",
    "# https://gmarti.gitlab.io/ml/2017/09/07/how-to-sort-distance-matrix.html\n",
    "\n",
    "from fastcluster import linkage\n",
    "\n",
    "def seriation(Z,N,cur_index):\n",
    "    '''\n",
    "        input:\n",
    "            - Z is a hierarchical tree (dendrogram)\n",
    "            - N is the number of points given to the clustering process\n",
    "            - cur_index is the position in the tree for the recursive traversal\n",
    "        output:\n",
    "            - order implied by the hierarchical tree Z\n",
    "            \n",
    "        seriation computes the order implied by a hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    if cur_index < N:\n",
    "        return [cur_index]\n",
    "    else:\n",
    "        left = int(Z[cur_index-N,0])\n",
    "        right = int(Z[cur_index-N,1])\n",
    "        return (seriation(Z,N,left) + seriation(Z,N,right))\n",
    "    \n",
    "def compute_serial_matrix(dist_mat,method=\"ward\"):\n",
    "    '''\n",
    "        input:\n",
    "            - dist_mat is a distance matrix\n",
    "            - method = [\"ward\",\"single\",\"average\",\"complete\"]\n",
    "        output:\n",
    "            - seriated_dist is the input dist_mat,\n",
    "              but with re-ordered rows and columns\n",
    "              according to the seriation, i.e. the\n",
    "              order implied by the hierarchical tree\n",
    "            - res_order is the order implied by\n",
    "              the hierarchical tree\n",
    "            - res_linkage is the hierarchical tree (dendrogram)\n",
    "        \n",
    "        compute_serial_matrix transforms a distance matrix into \n",
    "        a sorted distance matrix according to the order implied \n",
    "        by the hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    N = len(dist_mat)\n",
    "    flat_dist_mat = squareform(dist_mat)\n",
    "    res_linkage = linkage(flat_dist_mat, method=method,preserve_input=True)\n",
    "    res_order = seriation(res_linkage, N, N + N-2)\n",
    "    seriated_dist = np.zeros((N,N))\n",
    "    a,b = np.triu_indices(N,k=1)\n",
    "    seriated_dist[a,b] = dist_mat[ [res_order[i] for i in a], [res_order[j] for j in b]]\n",
    "    seriated_dist[b,a] = seriated_dist[a,b]\n",
    "    \n",
    "    return seriated_dist, res_order, res_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes on the different clustering linkage methods: \n",
    "# https://en.wikipedia.org/wiki/Complete-linkage_clustering\n",
    "# https://en.wikipedia.org/wiki/Hierarchical_clustering\n",
    "# (The main difference is in how distances between clusters are calculated)\n",
    "methods = [\"ward\",\"single\",\"average\",\"complete\"]\n",
    "for method in methods:\n",
    "    print(\"Method:\\t\",method)\n",
    "    \n",
    "    ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(dist_mat,method)\n",
    "    \n",
    "    plt.pcolormesh(ordered_dist_mat)\n",
    "    plt.xlim([0,N])\n",
    "    plt.ylim([0,N])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = output_elmo.join(\n",
    "    csv, \n",
    "    how='inner',\n",
    "    lsuffix='_embed', \n",
    "    rsuffix='_ref'\n",
    ")\n",
    "data = data.loc[csv['averb_relation'] == -1]\n",
    "# Shuffle data for extra comparison\n",
    "# data = data.sample(frac=1)\n",
    "dist_mat = squareform(pdist(data.iloc[:, 0:1024]))\n",
    "\n",
    "ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(\n",
    "    dist_mat,\n",
    "    'ward')\n",
    "reordered_data = data\n",
    "reordered_data['temp_index'] = data.index\n",
    "reordered_data = reordered_data.reset_index(drop=True)\n",
    "reordered_data = reordered_data.iloc[res_order].reset_index(drop=True)\n",
    "reordered_data['order_cluster'] = reordered_data.index\n",
    "reordered_data = reordered_data.set_index('temp_index')\n",
    "\n",
    "# reordered_data = csv.join(\n",
    "#     reordered_data, \n",
    "#     how='inner',\n",
    "#     lsuffix='_embed', \n",
    "#     rsuffix='_ref'\n",
    "# )\n",
    "# reordered_data.to_csv(f'temp/BERT_anchorverb_elmo_order_cluster.csv')\n",
    "# reordered_data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cephcyn/ChatlogGrapher/blob/master/data_processing.ipynb\n",
    "# for cosine similarity; see also sklearn documentation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute a distance metric column from some origin phrase and given some column name to calculate diff of\n",
    "def distance_word2vec(row, origin, colname):\n",
    "    try:\n",
    "        sim = cosine_similarity(get_phrase_vector(origin), get_phrase_vector(row[colname]))[0][0]\n",
    "    except:\n",
    "        sim = -1\n",
    "    return dict(zip(['distance'], [sim]))\n",
    "\n",
    "base_averb = 'encodes'\n",
    "output = csv.apply(\n",
    "    lambda row: distance_word2vec(\n",
    "        row, \n",
    "        base_averb, \n",
    "        'averb'\n",
    "    ), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output = csv.join(output).sort_values(by=['distance'], ascending=False)\n",
    "output.to_csv(f'temp/BERT_similarity_encodes.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
