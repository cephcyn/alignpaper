{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do one-time large imports\n",
    "\n",
    "# For sentence tokenization\n",
    "from nltk import tokenize\n",
    "\n",
    "# For coreference resolution\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "coref_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n",
    ")\n",
    "\n",
    "# For part-of-speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# For dependency parsing\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "dependency_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Identify the term we are splitting on\n",
    "search_word = \"BERT\"\n",
    "\n",
    "# Read in the dataframe containing entire paper abstracts (NOT pre-split into sentences)\n",
    "df = pd.read_csv('data/nlp-align_BERT.csv')\n",
    "\n",
    "# Restructure the dataframe to be more usable...\n",
    "df = df.groupby('ID', group_keys=False).apply(\n",
    "    lambda row: separate_title_abstract(row)\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "def sentence_tokenize(group):\n",
    "    row = group.reset_index(drop=True).loc[0]\n",
    "    sentences = tokenize.sent_tokenize(row['Text'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * (len(sentences)),\n",
    "        'ID': [row['ID']] * (len(sentences)),\n",
    "        'Type': [row['Type']] * (len(sentences)),\n",
    "        'Index': list(range(len(sentences))),\n",
    "        'Text': sentences\n",
    "    })\n",
    "\n",
    "df_sentences = df.groupby(['ID', 'Type'], group_keys=False).apply(\n",
    "    lambda row: sentence_tokenize(row)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a test dataframe so we can run models without taking impractically long\n",
    "# # TODO: this is causing some type inconsistencies, fix those?\n",
    "\n",
    "# temp_df = pd.DataFrame.from_dict(\n",
    "#     {'URL': 'abc', \n",
    "#      'ID': '0', \n",
    "#      'Title': 'Paper Title',\n",
    "#      'Abstract': 'The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. Petroni et al. (2019) take this as evidence that it memorizes factual knowledge during pre-training. We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.'\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting functions\n",
    "\n",
    "Assume we have an input dataframe with some number of columns, at least one of which is titled `Text` and is the column containing each sentence of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_headers = ['text_split_0','text_split_1','text_split_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, regardless of whitespace (if search word is A and we have word CAR, it slices it up)\n",
    "def split_search_term_literal(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        output = row['Text'].split(search_word, maxsplit=1)\n",
    "        output.insert(1, search_word)\n",
    "    else:\n",
    "        output = [row['Text'],'','']\n",
    "    return dict(zip(splitting_headers,output))\n",
    "\n",
    "output = df_sentences.apply(\n",
    "    lambda row: split_search_term_literal(row, search_word), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, taking care to only split on whitespace\n",
    "def split_search_term_whitespace(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        tokens = row['Text'].split(' ')\n",
    "        for i in range(len(tokens)):\n",
    "            if search_word in tokens[i]:\n",
    "                output=[' '.join(tokens[:i]), tokens[i], ' '.join(tokens[i+1:])]\n",
    "                break\n",
    "    else:\n",
    "        output = [row['Text'],'','']\n",
    "    return dict(zip(splitting_headers,output))\n",
    "\n",
    "output = df_sentences.apply(\n",
    "    lambda row: split_search_term_whitespace(row, search_word), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split apart the 'Title' and 'Abstract' columns\n",
    "def separate_title_abstract(group):\n",
    "    row = group.loc[0]\n",
    "    abs_text = tokenize.sent_tokenize(row['Abstract'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * 2,\n",
    "        'ID': [row['ID']] * 2,\n",
    "        'Type': ['Title', 'Abstract'],\n",
    "        'Text': [row['Title'], row['Abstract']]\n",
    "    })\n",
    "\n",
    "# transform the output of coreference resolution into something that is more easily manipulated\n",
    "# split it across multiple sentences so each indiv sentence row can still work\n",
    "def reinterpret_coref_clusters(row, search_word, sentences):\n",
    "    # Create dicts to map full-document to indiv sentence data\n",
    "    src = sentences.loc[sentences['ID'] == row['ID']].loc[sentences['Type'] == row['Type']]['Text']\n",
    "    curr_sentence = 0\n",
    "    consumed = 0\n",
    "    sent_mapping = {}\n",
    "    sent_content = {}\n",
    "    last_sent_end = 0\n",
    "    doct_mapping = {}\n",
    "    doct_split = []\n",
    "    for i in range(len(row['document'])):\n",
    "        if row['document'][i].strip() != '':\n",
    "            if row['document'][i] not in src.iloc[curr_sentence][consumed:]:\n",
    "                doct_split.append(row['document'][last_sent_end:i])\n",
    "                last_sent_end = i\n",
    "                curr_sentence += 1\n",
    "                consumed = 0\n",
    "            offset = src.iloc[curr_sentence][consumed:].index(row['document'][i])\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            if curr_sentence not in sent_content:\n",
    "                sent_content[curr_sentence] = []\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "            consumed += offset + len(row['document'][i])\n",
    "        else:\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "        doct_split.append(row['document'][last_sent_end:])\n",
    "    # Select the cluster that best matches the search word\n",
    "    selcluster_idx = -1\n",
    "    selcluster_ct = 0\n",
    "    for i in range(len(row['clusters'])):\n",
    "        currcluster_ct = 0\n",
    "        for c in row['clusters'][i]:\n",
    "            currcluster_ct += len(re.findall(f'{search_word}', ' '.join(row['document'][c[0]:c[1]+1])))\n",
    "        if currcluster_ct > selcluster_ct:\n",
    "            selcluster_idx = i\n",
    "            selcluster_ct = currcluster_ct\n",
    "    # Build the output row\n",
    "    output = [sent_mapping, sent_content, doct_mapping, selcluster_idx]\n",
    "    return dict(zip(['sent_mapping', 'sent_content', 'doct_mapping', 'selcluster_idx'],output))\n",
    "\n",
    "# Run coreference resolution over the entire abstract, not individual sentences\n",
    "output = df.apply(\n",
    "    lambda row: coref_predictor.predict(row['Text']), axis=1, result_type='expand')\n",
    "df_merged = df.join(output)\n",
    "\n",
    "output = df_merged.apply(\n",
    "    lambda row: reinterpret_coref_clusters(row, search_word, df_sentences), \n",
    "    axis=1, result_type='expand')\n",
    "# df_merged = df_merged.join(output)\n",
    "\n",
    "df_merged.join(output).to_csv(f'outputs/coreference-partial.csv')\n",
    "df_merged.join(output)\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split based on co-references to any phrase containing BERT, using allennlp coreference resolution\n",
    "# This does NOT preserve the original sentence spacing\n",
    "def split_search_term_coreference(row, search_word, lookup, fallback):\n",
    "    # Splits on first coref instance ONLY\n",
    "    # there's probably a cleaner way to do this...\n",
    "    lookup_row = lookup.loc[lookup['ID']==row['ID']].loc[lookup['Type']==row['Type']].to_dict(orient='records')[0]\n",
    "    if lookup_row['selcluster_idx'] == -1:\n",
    "        # if we didn't identify any clusters that match the search term, use our fallback method\n",
    "        return fallback(row, search_word)\n",
    "    split_clusters = lookup_row['clusters'][lookup_row['selcluster_idx']]\n",
    "    output = []\n",
    "    for i in range(len(split_clusters)):\n",
    "        c = split_clusters[i]\n",
    "        if lookup_row['sent_mapping'][c[0]] == lookup_row['sent_mapping'][c[0]] == row['Index']:\n",
    "            sentence_start = lookup_row['sent_content'][row['Index']][0]\n",
    "            sentence_end = lookup_row['sent_content'][row['Index']][-1]\n",
    "            output=[' '.join(lookup_row['document'][sentence_start:c[0]]),\n",
    "                    ' '.join(lookup_row['document'][c[0]:c[1]+1]),\n",
    "                    ' '.join(lookup_row['document'][c[1]+1:sentence_end])]\n",
    "            break\n",
    "    if output == []:\n",
    "        # if there wasn't any reference in the sentence found, use our fallback method\n",
    "        return fallback(row, search_word)\n",
    "    return dict(zip(splitting_headers,output))\n",
    "\n",
    "metadata = df_merged.apply(\n",
    "    lambda row: reinterpret_coref_clusters(row, search_word, df_sentences), \n",
    "    axis=1, result_type='expand')\n",
    "output = df_sentences.apply(\n",
    "    lambda row: split_search_term_coreference(row, search_word, df_merged.join(metadata), split_search_term_whitespace), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping functions\n",
    "\n",
    "Assume we have an input dataframe with column headers `['text_split_0','text_split_1','text_split_2']`\n",
    "\n",
    "`'text_split_1'` is the column that contains our search term / anchor point\n",
    "\n",
    "`'text_split_0'` and `'text_split_2'` are the columns that contain text before and after the search terms respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_input = df.apply(lambda row: split_search_term_whitespace(row, search_word), axis=1, result_type='expand')\n",
    "\n",
    "sample_input = df.join(output)\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the first word (regardless of part of speech) that comes after the anchor point\n",
    "def group_first_word(row):\n",
    "    output = [row['text_split_2'].split(' ')[0]]\n",
    "    return dict(zip(['group'], output))\n",
    "\n",
    "output = sample_input.apply(lambda row: group_first_word(row), axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output.to_csv(f'outputs/coreference_firstword.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the first verb that comes after the anchor point, using NLTK part-of-speech tagging\n",
    "def group_first_verb(row):\n",
    "    tokens = [nltk.word_tokenize(row['text_split_0']), \n",
    "              nltk.word_tokenize(row['text_split_1']),\n",
    "              nltk.word_tokenize(row['text_split_2'])]\n",
    "    tokens_pos = nltk.pos_tag([item for sublist in tokens for item in sublist])\n",
    "    verb = ''\n",
    "    for i in range(len(tokens[0])+len(tokens[1]), len(tokens_pos)):\n",
    "        if tokens_pos[i][1].startswith('V'):\n",
    "            verb = tokens_pos[i][0]\n",
    "            break\n",
    "    output = [verb]\n",
    "    return dict(zip(['group'], output))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_first_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output.to_csv(f'outputs/coreference_firstverb.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the main verb in the sentence, using allennlp dependency parsing (based on demo code)\n",
    "def group_main_verb(row):\n",
    "    p = dependency_predictor.predict(\n",
    "        sentence=' '.join([row['text_split_0'], row['text_split_1'], row['text_split_2']]).strip()\n",
    "    )\n",
    "    output = [p['hierplane_tree']['root']['word']]\n",
    "    return dict(zip(['group'], output))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_main_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output.to_csv(f'outputs/coreference_mainverb.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "pd.read_csv(f'outputs/coreference_mainverb.csv').sort_values(by=['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
