{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "search_word = \"BERT\"\n",
    "df = pd.read_csv('data/nlp-align_BERT.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting functions\n",
    "\n",
    "Assume we have an input dataframe with some number of columns, at least one of which is titled `Text` and is the column containing each sentence of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_headers = ['text_split_0','text_split_1','text_split_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, regardless of whitespace (if search word is A and we have word CAR, it slices it up)\n",
    "def split_search_term_literal(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    if search_word in row['Text']:\n",
    "        splitted = row['Text'].split(search_word, maxsplit=1)\n",
    "        splitted.insert(1, search_word)\n",
    "    else:\n",
    "        splitted = [row['Text'],'','']\n",
    "    return dict(zip(splitting_headers,splitted))\n",
    "\n",
    "# df = df.join()\n",
    "df.apply(lambda row: split_search_term_literal(row, search_word), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, taking care to only split on whitespace\n",
    "def split_search_term_whitespace(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    if search_word in row['Text']:\n",
    "        tokens = row['Text'].split(' ')\n",
    "        for i in range(len(tokens)):\n",
    "            if search_word in tokens[i]:\n",
    "                splitted=[' '.join(tokens[:i]), tokens[i], ' '.join(tokens[i+1:])]\n",
    "                break\n",
    "    else:\n",
    "        splitted = [row['Text'],'','']\n",
    "    return dict(zip(splitting_headers,splitted))\n",
    "\n",
    "df.apply(lambda row: split_search_term_whitespace(row, search_word), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "p = predictor.predict(\n",
    "  document=\"The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. \\\n",
    "  Petroni et al. (2019) take this as evidence that BERT memorizes factual knowledge during pre-training. \\\n",
    "  We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_src=[\"The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts.\",\n",
    "  \"Petroni et al. (2019) take this as evidence that BERT memorizes factual knowledge during pre-training.\",\n",
    "  \"We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.\"]\n",
    "\n",
    "current_sent_counter = 0\n",
    "consumed_len = 0\n",
    "mapping = {}\n",
    "sent_mapping = {}\n",
    "for di in range(len(p['document'])):\n",
    "    if p['document'][di].strip() != '':\n",
    "        if p['document'][di] not in p_src[current_sent_counter]:\n",
    "            current_sent_counter += 1\n",
    "            consumed_len = 0\n",
    "        remaining_index = p_src[current_sent_counter][consumed_len:].index(p['document'][di])\n",
    "        mapping[di] = consumed_len + remaining_index\n",
    "        sent_mapping[di] = current_sent_counter\n",
    "        consumed_len += remaining_index + len(p['document'][di])\n",
    "    else:\n",
    "        mapping[di] = consumed_len\n",
    "        sent_mapping[di] = current_sent_counter\n",
    "print(mapping)\n",
    "print(sent_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO convert the full list of cluster spans into something that can be interpreted across multiple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "spans = []\n",
    "# for ts in p['top_spans']:\n",
    "#     spans.append(p['document'][ts[0]:ts[1]+1])\n",
    "# print(spans)\n",
    "# for i in range(len(p['predicted_antecedents'])):\n",
    "#     if p['predicted_antecedents'][i] is not -1:\n",
    "#         print(spans[i], spans[p['predicted_antecedents'][i]])\n",
    "print(p['clusters'])\n",
    "selected_index = 0\n",
    "selected_count = 0\n",
    "for i in range(len(p['clusters'])):\n",
    "    curr_count = 0\n",
    "    for c in p['clusters'][i]:\n",
    "        print(' '.join(p['document'][c[0]:c[1]+1]))\n",
    "        curr_count += len(re.findall(f'{search_word}', ' '.join(p['document'][c[0]:c[1]+1])))\n",
    "#         print(p['document'][c[0]:c[1]+1])\n",
    "    if curr_count > selected_count:\n",
    "        selected_index = i\n",
    "        selected_count = curr_count\n",
    "    print()\n",
    "print(selected_index, selected_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "\n",
    "# Create dataframe with merged abstracts\n",
    "# we have to build the coref-resolved table first since I want to do coreference resolution across \n",
    "# the entire abstract, not individual sentences\n",
    "df_merged = df.sort_values(by=['Index']).groupby(['ID', 'Type'])['Text'].apply(' '.join).reset_index()\n",
    "df_merged = df_merged.join(df_merged.apply(lambda row: predictor.predict(row['Text']), axis=1, result_type='expand'))\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Index']).groupby(['ID', 'Type'])['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split based on co-references to any phrase containing BERT, using allennlp coreference resolution\n",
    "# TODO should this preserve the original sentence spacing?\n",
    "def split_search_term_coreference(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    if search_word in row['Text']:\n",
    "        tokens = row['Text'].split(' ')\n",
    "        for i in range(len(tokens)):\n",
    "            if search_word in tokens[i]:\n",
    "                splitted=[' '.join(tokens[:i]), tokens[i], ' '.join(tokens[i+1:])]\n",
    "                break\n",
    "    else:\n",
    "        splitted = [row['Text'],'','']\n",
    "    return dict(zip(splitting_headers,splitted))\n",
    "\n",
    "df.apply(lambda row: split_search_term_coreference(row, search_word), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping functions\n",
    "\n",
    "Assume we have an input dataframe with column headers `['text_split_0','text_split_1','text_split_2']`\n",
    "\n",
    "`'text_split_1'` is the column that contains our search term / anchor point\n",
    "\n",
    "`'text_split_0'` and `'text_split_2'` are the columns that contain text before and after the search terms respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = df.apply(lambda row: split_search_term_whitespace(row, search_word), axis=1, result_type='expand')\n",
    "\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the first word (regardless of part of speech) that comes after the anchor point\n",
    "def group_first_word(row):\n",
    "    grouped = [row['text_split_2'].split(' ')[0]]\n",
    "    return dict(zip(['group'], grouped))\n",
    "\n",
    "output = sample_input.apply(lambda row: group_first_word(row), axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "sample_input = sample_input.join(output)\n",
    "sample_input\n",
    "\n",
    "sample_input.to_csv(f'outputs/whitespace_firstword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Group on the first verb that comes after the anchor point, using NLTK part-of-speech tagging\n",
    "def group_first_verb(row):\n",
    "    tokens = [nltk.word_tokenize(row['text_split_0']), \n",
    "              nltk.word_tokenize(row['text_split_1']),\n",
    "              nltk.word_tokenize(row['text_split_2'])]\n",
    "    tokens_pos = nltk.pos_tag([item for sublist in tokens for item in sublist])\n",
    "    verb = ''\n",
    "    for i in range(len(tokens[0])+len(tokens[1]), len(tokens_pos)):\n",
    "        if tokens_pos[i][1].startswith('V'):\n",
    "            verb = tokens_pos[i][0]\n",
    "            break\n",
    "    grouped = [verb]\n",
    "    return dict(zip(['group'], grouped))\n",
    "\n",
    "output = sample_input.apply(lambda row: group_first_verb(row), axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "\n",
    "sample_input = sample_input.join(output)\n",
    "sample_input\n",
    "\n",
    "sample_input.to_csv(f'outputs/whitespace_firstverb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\")\n",
    "\n",
    "# Group on the main verb in the sentence, using allennlp dependency parsing (based on demo code)\n",
    "def group_main_verb(row):\n",
    "    p = predictor.predict(\n",
    "        sentence=' '.join([row['text_split_0'], row['text_split_1'], row['text_split_2']]).strip()\n",
    "    )\n",
    "    grouped = [p['hierplane_tree']['root']['word']]\n",
    "    return dict(zip(['group'], grouped))\n",
    "\n",
    "output = sample_input.apply(lambda row: group_main_verb(row), axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "sample_input = sample_input.join(output)\n",
    "sample_input\n",
    "\n",
    "sample_input.to_csv(f'outputs/whitespace_mainverb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
