{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Unlike recent language representation models, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a result, the pre-trained BERT model can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT is conceptually simple and empirically po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We present a replication study of BERT pretrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We release our models and code.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We focus on one such model, BERT, and aim to q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What Does BERT Look At? An Analysis of BERT's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT's attention heads exhibit patterns such a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>The BERT model performs remarkably well on all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>A new release of BERT (Devlin, 2018) includes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We compare mBERT with the best-published metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>TinyBERT is also significantly better than sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe BERTSUM, a simple v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We fine-tune the pre-trained model from BERT a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, an upgraded version of BERT has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, previous work trains BERT by viewing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To tackle this issue, we propose a multi-passa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently a new language representation model, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>By using PALs in parallel with BERT layers, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Third, the lower layers of BERT are more invar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>In our paper, conditional masked language mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>The well trained conditional BERT can be appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Starting from a public multilingual BERT check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Petroni et al. (2019) take this as evidence th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>More specifically, we show that BERT's precisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>As a remedy, we propose E-BERT, an extension o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>E-BERT outperforms both BERT and ERNIE (Zhang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In all layers of ELMo, BERT, and GPT-2, on ave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  ID      Type  Index  \\\n",
       "0    https://www.semanticscholar.org/paper/BERT%3A-...   0     Title      0   \n",
       "1    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      0   \n",
       "2    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      1   \n",
       "3    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      2   \n",
       "4    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      3   \n",
       "5    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      4   \n",
       "6    https://www.semanticscholar.org/paper/RoBERTa%...   1     Title      0   \n",
       "7    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      0   \n",
       "8    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      1   \n",
       "9    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      2   \n",
       "10   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      3   \n",
       "11   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      4   \n",
       "12   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      5   \n",
       "13   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      6   \n",
       "14   https://www.semanticscholar.org/paper/DistilBE...   2     Title      0   \n",
       "15   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      0   \n",
       "16   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      1   \n",
       "17   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      2   \n",
       "18   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      3   \n",
       "19   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      4   \n",
       "20   https://www.semanticscholar.org/paper/BERT-Red...   3     Title      0   \n",
       "21   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      0   \n",
       "22   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      1   \n",
       "23   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      2   \n",
       "24   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      3   \n",
       "25   https://www.semanticscholar.org/paper/What-Doe...   4     Title      0   \n",
       "26   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      0   \n",
       "27   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      1   \n",
       "28   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      2   \n",
       "29   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      3   \n",
       "30   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      4   \n",
       "31   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      5   \n",
       "32   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      6   \n",
       "33   https://www.semanticscholar.org/paper/Passage-...   5     Title      0   \n",
       "34   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      0   \n",
       "35   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      1   \n",
       "36   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      2   \n",
       "37   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      3   \n",
       "38   https://www.semanticscholar.org/paper/Assessin...   6     Title      0   \n",
       "39   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      0   \n",
       "40   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      1   \n",
       "41   https://www.semanticscholar.org/paper/Beto%2C-...   7     Title      0   \n",
       "42   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      0   \n",
       "43   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      1   \n",
       "44   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      2   \n",
       "45   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      3   \n",
       "46   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      4   \n",
       "47   https://www.semanticscholar.org/paper/TinyBERT...   8     Title      0   \n",
       "48   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      0   \n",
       "49   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      1   \n",
       "50   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      2   \n",
       "51   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      3   \n",
       "52   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      4   \n",
       "53   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      5   \n",
       "54   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      6   \n",
       "55   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      7   \n",
       "56   https://www.semanticscholar.org/paper/Fine-tun...   9     Title      0   \n",
       "57   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      0   \n",
       "58   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      1   \n",
       "59   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      2   \n",
       "60   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      3   \n",
       "61   https://www.semanticscholar.org/paper/BERT-Pos...  10     Title      0   \n",
       "62   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      0   \n",
       "63   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      1   \n",
       "64   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      2   \n",
       "65   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      3   \n",
       "66   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      4   \n",
       "67   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      5   \n",
       "68   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      6   \n",
       "69   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      7   \n",
       "70   https://www.semanticscholar.org/paper/How-to-F...  11     Title      0   \n",
       "71   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      0   \n",
       "72   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      1   \n",
       "73   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      2   \n",
       "74   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      3   \n",
       "75   https://www.semanticscholar.org/paper/BERT-has...  12     Title      0   \n",
       "76   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      0   \n",
       "77   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      1   \n",
       "78   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      2   \n",
       "79   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      3   \n",
       "80   https://www.semanticscholar.org/paper/Utilizin...  13     Title      0   \n",
       "81   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      0   \n",
       "82   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      1   \n",
       "83   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      2   \n",
       "84   https://www.semanticscholar.org/paper/Simple-A...  14     Title      0   \n",
       "85   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      0   \n",
       "86   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      1   \n",
       "87   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      2   \n",
       "88   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      3   \n",
       "89   https://www.semanticscholar.org/paper/Revealin...  15     Title      0   \n",
       "90   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      0   \n",
       "91   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      1   \n",
       "92   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      2   \n",
       "93   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      3   \n",
       "94   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      4   \n",
       "95   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      5   \n",
       "96   https://www.semanticscholar.org/paper/Pre-Trai...  16     Title      0   \n",
       "97   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      0   \n",
       "98   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      1   \n",
       "99   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      2   \n",
       "100  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      3   \n",
       "101  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      4   \n",
       "102  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      5   \n",
       "103  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      6   \n",
       "104  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      7   \n",
       "105  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      8   \n",
       "106  https://www.semanticscholar.org/paper/Multi-pa...  17     Title      0   \n",
       "107  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      0   \n",
       "108  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      1   \n",
       "109  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      2   \n",
       "110  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      3   \n",
       "111  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      4   \n",
       "112  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      5   \n",
       "113  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      6   \n",
       "114  https://www.semanticscholar.org/paper/BERT-for...  18     Title      0   \n",
       "115  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      0   \n",
       "116  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      1   \n",
       "117  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      2   \n",
       "118  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      3   \n",
       "119  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      4   \n",
       "120  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      5   \n",
       "121  https://www.semanticscholar.org/paper/BERT-wit...  19     Title      0   \n",
       "122  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      0   \n",
       "123  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      1   \n",
       "124  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      2   \n",
       "125  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      3   \n",
       "126  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      4   \n",
       "127  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      5   \n",
       "128  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      6   \n",
       "129  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      7   \n",
       "130  https://www.semanticscholar.org/paper/Understa...  20     Title      0   \n",
       "131  https://www.semanticscholar.org/paper/Understa...  20  Abstract      0   \n",
       "132  https://www.semanticscholar.org/paper/Understa...  20  Abstract      1   \n",
       "133  https://www.semanticscholar.org/paper/Understa...  20  Abstract      2   \n",
       "134  https://www.semanticscholar.org/paper/Understa...  20  Abstract      3   \n",
       "135  https://www.semanticscholar.org/paper/Understa...  20  Abstract      4   \n",
       "136  https://www.semanticscholar.org/paper/Simple-B...  21     Title      0   \n",
       "137  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      0   \n",
       "138  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      1   \n",
       "139  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      2   \n",
       "140  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      3   \n",
       "141  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      4   \n",
       "142  https://www.semanticscholar.org/paper/BERT-and...  22     Title      0   \n",
       "143  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      0   \n",
       "144  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      1   \n",
       "145  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      2   \n",
       "146  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      3   \n",
       "147  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      4   \n",
       "148  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      5   \n",
       "149  https://www.semanticscholar.org/paper/What-BER...  23     Title      0   \n",
       "150  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      0   \n",
       "151  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      1   \n",
       "152  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      2   \n",
       "153  https://www.semanticscholar.org/paper/Visualiz...  24     Title      0   \n",
       "154  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      0   \n",
       "155  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      1   \n",
       "156  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      2   \n",
       "157  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      3   \n",
       "158  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      4   \n",
       "159  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      5   \n",
       "160  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      6   \n",
       "161  https://www.semanticscholar.org/paper/Conditio...  25     Title      0   \n",
       "162  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      0   \n",
       "163  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      1   \n",
       "164  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      2   \n",
       "165  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      3   \n",
       "166  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      4   \n",
       "167  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      5   \n",
       "168  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      6   \n",
       "169  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      7   \n",
       "170  https://www.semanticscholar.org/paper/Small-an...  26     Title      0   \n",
       "171  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      0   \n",
       "172  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      1   \n",
       "173  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      2   \n",
       "174  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      3   \n",
       "175  https://www.semanticscholar.org/paper/Data-Aug...  27     Title      0   \n",
       "176  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      0   \n",
       "177  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      1   \n",
       "178  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      2   \n",
       "179  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      3   \n",
       "180  https://www.semanticscholar.org/paper/BERT-is-...  28     Title      0   \n",
       "181  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      0   \n",
       "182  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      1   \n",
       "183  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      2   \n",
       "184  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      3   \n",
       "185  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      4   \n",
       "186  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      5   \n",
       "187  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      6   \n",
       "188  https://www.semanticscholar.org/paper/How-Cont...  29     Title      0   \n",
       "189  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      0   \n",
       "190  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      1   \n",
       "191  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      2   \n",
       "192  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      3   \n",
       "193  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      4   \n",
       "194  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      5   \n",
       "195  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      6   \n",
       "\n",
       "                                                  Text  \n",
       "0    BERT: Pre-training of Deep Bidirectional Trans...  \n",
       "1    We introduce a new language representation mod...  \n",
       "2    Unlike recent language representation models, ...  \n",
       "3    As a result, the pre-trained BERT model can be...  \n",
       "4    BERT is conceptually simple and empirically po...  \n",
       "5    It obtains new state-of-the-art results on ele...  \n",
       "6    RoBERTa: A Robustly Optimized BERT Pretraining...  \n",
       "7    Language model pretraining has led to signific...  \n",
       "8    Training is computationally expensive, often d...  \n",
       "9    We present a replication study of BERT pretrai...  \n",
       "10   We find that BERT was significantly undertrain...  \n",
       "11   Our best model achieves state-of-the-art resul...  \n",
       "12   These results highlight the importance of prev...  \n",
       "13                     We release our models and code.  \n",
       "14   DistilBERT, a distilled version of BERT: small...  \n",
       "15   As Transfer Learning from large-scale pre-trai...  \n",
       "16   In this work, we propose a method to pre-train...  \n",
       "17   While most prior work investigated the use of ...  \n",
       "18   To leverage the inductive biases learned by la...  \n",
       "19   Our smaller, faster and lighter model is cheap...  \n",
       "20         BERT Rediscovers the Classical NLP Pipeline  \n",
       "21   Pre-trained text encoders have rapidly advance...  \n",
       "22   We focus on one such model, BERT, and aim to q...  \n",
       "23   We find that the model represents the steps of...  \n",
       "24   Qualitative analysis reveals that the model ca...  \n",
       "25   What Does BERT Look At? An Analysis of BERT's ...  \n",
       "26   Large pre-trained neural networks such as BERT...  \n",
       "27   Most recent analysis has focused on model outp...  \n",
       "28   Complementary to these works, we propose metho...  \n",
       "29   BERT's attention heads exhibit patterns such a...  \n",
       "30   We further show that certain attention heads c...  \n",
       "31   For example, we find heads that attend to the ...  \n",
       "32   Lastly, we propose an attention-based probing ...  \n",
       "33                        Passage Re-ranking with BERT  \n",
       "34   Recently, neural models pretrained on a langua...  \n",
       "35   In this paper, we describe a simple re-impleme...  \n",
       "36   Our system is the state of the art on the TREC...  \n",
       "37   The code to reproduce our results is available...  \n",
       "38                Assessing BERT's Syntactic Abilities  \n",
       "39   I assess the extent to which the recently intr...  \n",
       "40   The BERT model performs remarkably well on all...  \n",
       "41   Beto, Bentz, Becas: The Surprising Cross-Lingu...  \n",
       "42   Pretrained contextual representation models (P...  \n",
       "43   A new release of BERT (Devlin, 2018) includes ...  \n",
       "44   This paper explores the broader cross-lingual ...  \n",
       "45   We compare mBERT with the best-published metho...  \n",
       "46   Additionally, we investigate the most effectiv...  \n",
       "47   TinyBERT: Distilling BERT for Natural Language...  \n",
       "48   Language model pre-training, such as BERT, has...  \n",
       "49   However, pre-trained language models are usual...  \n",
       "50   To accelerate inference and reduce model size ...  \n",
       "51   By leveraging this new KD method, the plenty o...  \n",
       "52   Moreover, we introduce a new two-stage learnin...  \n",
       "53   This framework ensures that TinyBERT can captu...  \n",
       "54   TinyBERT is empirically effective and achieves...  \n",
       "55   TinyBERT is also significantly better than sta...  \n",
       "56         Fine-tune BERT for Extractive Summarization  \n",
       "57   BERT, a pre-trained Transformer model, has ach...  \n",
       "58   In this paper, we describe BERTSUM, a simple v...  \n",
       "59   Our system is the state of the art on the CNN/...  \n",
       "60   The codes to reproduce our results are availab...  \n",
       "61   BERT Post-Training for Review Reading Comprehe...  \n",
       "62   Question-answering plays an important role in ...  \n",
       "63   Inspired by the recent success of machine read...  \n",
       "64   To the best of our knowledge, no existing work...  \n",
       "65   In this work, we first build an RRC dataset ca...  \n",
       "66   Since ReviewRC has limited training examples f...  \n",
       "67   To show the generality of the approach, the pr...  \n",
       "68   Experimental results demonstrate that the prop...  \n",
       "69   The datasets and code are available at this ht...  \n",
       "70      How to Fine-Tune BERT for Text Classification?  \n",
       "71   Language model pre-training has proven to be u...  \n",
       "72   As a state-of-the-art language model pre-train...  \n",
       "73   In this paper, we conduct exhaustive experimen...  \n",
       "74   Finally, the proposed solution obtains new sta...  \n",
       "75   BERT has a Mouth, and It Must Speak: BERT as a...  \n",
       "76   We show that BERT (Devlin et al., 2018) is a M...  \n",
       "77   This formulation gives way to a natural proced...  \n",
       "78   We generate from BERT and find that it can pro...  \n",
       "79   Compared to the generations of a traditional l...  \n",
       "80   Utilizing BERT for Aspect-Based Sentiment Anal...  \n",
       "81   Aspect-based sentiment analysis (ABSA), which ...  \n",
       "82   In this paper, we construct an auxiliary sente...  \n",
       "83   We fine-tune the pre-trained model from BERT a...  \n",
       "84   Simple Applications of BERT for Ad Hoc Documen...  \n",
       "85   Following recent successes in applying BERT to...  \n",
       "86   This required confronting the challenge posed ...  \n",
       "87   We address this issue by applying inference on...  \n",
       "88   Experiments on TREC microblog and newswire tes...  \n",
       "89                  Revealing the Dark Secrets of BERT  \n",
       "90   BERT-based architectures currently give state-...  \n",
       "91   In the current work, we focus on the interpret...  \n",
       "92   Using a subset of GLUE tasks and a set of hand...  \n",
       "93   Our findings suggest that there is a limited s...  \n",
       "94   While different heads consistently use the sam...  \n",
       "95   We show that manually disabling attention in c...  \n",
       "96   Pre-Training with Whole Word Masking for Chine...  \n",
       "97   Bidirectional Encoder Representations from Tra...  \n",
       "98   Recently, an upgraded version of BERT has been...  \n",
       "99   In this technical report, we adapt whole word ...  \n",
       "100  The model was trained on the latest Chinese Wi...  \n",
       "101  We aim to provide easy extensibility and bette...  \n",
       "102  The model is verified on various NLP tasks, ac...  \n",
       "103  Experimental results on these datasets show th...  \n",
       "104  Moreover, we also examine the effectiveness of...  \n",
       "105  We release the pre-trained model (both TensorF...  \n",
       "106  Multi-passage BERT: A Globally Normalized BERT...  \n",
       "107  BERT model has been successfully applied to op...  \n",
       "108  However, previous work trains BERT by viewing ...  \n",
       "109  To tackle this issue, we propose a multi-passa...  \n",
       "110  In addition, we find that splitting articles i...  \n",
       "111  By leveraging a passage ranker to select high-...  \n",
       "112  Experiments on four standard benchmarks showed...  \n",
       "113  In particular, on the OpenSQuAD dataset, our m...  \n",
       "114  BERT for Joint Intent Classification and Slot ...  \n",
       "115  Intent classification and slot filling are two...  \n",
       "116  They often suffer from small-scale human-label...  \n",
       "117  Recently a new language representation model, ...  \n",
       "118  However, there has not been much effort on exp...  \n",
       "119  In this work, we propose a joint intent classi...  \n",
       "120  Experimental results demonstrate that our prop...  \n",
       "121  BERT with History Answer Embedding for Convers...  \n",
       "122  Conversational search is an emerging topic in ...  \n",
       "123  One of the major challenges to multi-turn conv...  \n",
       "124  Existing methods either prepend history turns ...  \n",
       "125  We propose a conceptually simple yet highly ef...  \n",
       "126  It enables seamless integration of conversatio...  \n",
       "127  We first explain our view that ConvQA is a sim...  \n",
       "128  We further demonstrate the effectiveness of ou...  \n",
       "129  Finally, we analyze the impact of different nu...  \n",
       "130     Understanding the Behaviors of BERT in Ranking  \n",
       "131  This paper studies the performances and behavi...  \n",
       "132  We explore several different ways to leverage ...  \n",
       "133  Experimental results on MS MARCO demonstrate t...  \n",
       "134  Experimental results on TREC show the gaps bet...  \n",
       "135  Analyses illustrate how BERT allocates its att...  \n",
       "136  Simple BERT Models for Relation Extraction and...  \n",
       "137  We present simple BERT-based models for relati...  \n",
       "138  In recent years, state-of-the-art performance ...  \n",
       "139  In this paper, extensive experiments on datase...  \n",
       "140  To our knowledge, we are the first to successf...  \n",
       "141  Our models provide strong baselines for future...  \n",
       "142  BERT and PALs: Projected Attention Layers for ...  \n",
       "143  Multi-task learning allows the sharing of usef...  \n",
       "144  In natural language processing several recent ...  \n",
       "145  These results are based on fine-tuning on each...  \n",
       "146  We explore the multi-task learning setting for...  \n",
       "147  We introduce new adaptation modules, PALs or `...  \n",
       "148  By using PALs in parallel with BERT layers, we...  \n",
       "149  What BERT Is Not: Lessons from a New Suite of ...  \n",
       "150  Pre-training by language modeling has become a...  \n",
       "151  In this paper we introduce a suite of diagnost...  \n",
       "152  As a case study, we apply these diagnostics to...  \n",
       "153  Visualizing and Understanding the Effectivenes...  \n",
       "154  Language model pre-training, such as BERT, has...  \n",
       "155  However, it is unclear why the pre-training-th...  \n",
       "156  In this paper, we propose to visualize loss la...  \n",
       "157  First, we find that pre-training reaches a goo...  \n",
       "158  We also demonstrate that the fine-tuning proce...  \n",
       "159  Second, the visualization results indicate tha...  \n",
       "160  Third, the lower layers of BERT are more invar...  \n",
       "161           Conditional BERT Contextual Augmentation  \n",
       "162  Data augmentation methods are often applied to...  \n",
       "163  Recently proposed contextual augmentation augm...  \n",
       "164  Bidirectional Encoder Representations from Tra...  \n",
       "165  We propose a novel data augmentation method fo...  \n",
       "166  We retrofit BERT to conditional BERT by introd...  \n",
       "167  In our paper, conditional masked language mod...  \n",
       "168  The well trained conditional BERT can be appli...  \n",
       "169  Experiments on six various different text clas...  \n",
       "170  Small and Practical BERT Models for Sequence L...  \n",
       "171  We propose a practical scheme to train a singl...  \n",
       "172  Starting from a public multilingual BERT check...  \n",
       "173  We show that our model especially outperforms ...  \n",
       "174  We showcase the effectiveness of our method by...  \n",
       "175  Data Augmentation for BERT Fine-Tuning in Open...  \n",
       "176  Recently, a simple combination of passage retr...  \n",
       "177  In this paper, we present a data augmentation ...  \n",
       "178  We apply a stage-wise approach to fine tuning ...  \n",
       "179  Experimental results show large gains in effec...  \n",
       "180  BERT is Not a Knowledge Base (Yet): Factual Kn...  \n",
       "181  The BERT language model (LM) (Devlin et al., 2...  \n",
       "182  Petroni et al. (2019) take this as evidence th...  \n",
       "183  We take issue with this interpretation and arg...  \n",
       "184  More specifically, we show that BERT's precisi...  \n",
       "185  As a remedy, we propose E-BERT, an extension o...  \n",
       "186  E-BERT outperforms both BERT and ERNIE (Zhang ...  \n",
       "187  We take this as evidence that E-BERT is richer...  \n",
       "188  How Contextual are Contextualized Word Represe...  \n",
       "189  Replacing static word embeddings with contextu...  \n",
       "190  However, just how contextual are the contextua...  \n",
       "191  Are there infinitely many context-specific rep...  \n",
       "192  For one, we find that the contextualized repre...  \n",
       "193  While representations of the same word in diff...  \n",
       "194  This suggests that upper layers of contextuali...  \n",
       "195  In all layers of ELMo, BERT, and GPT-2, on ave...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "search_word = \"BERT\"\n",
    "df = pd.read_csv('data/nlp-align_BERT.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting functions\n",
    "\n",
    "Assume we have an input dataframe with some number of columns, at least one of which is titled `Text` and is the column containing each sentence of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_headers = ['text_split_0','text_split_1','text_split_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_split_0</th>\n",
       "      <th>text_split_1</th>\n",
       "      <th>text_split_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>: Pre-training of Deep Bidirectional Transform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, which stands for Bidirectional Encoder Repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike recent language representation models,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre-train deep bidirectional r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a result, the pre-trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine-tuned with just one additio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is conceptually simple and empirically powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ro</td>\n",
       "      <td>BERT</td>\n",
       "      <td>a: A Robustly Optimized BERT Pretraining Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining (Devlin et al., 2019) that carefu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained, and can match...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Distil</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, a distilled version of BERT: smaller, faster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, which can then be fine-tuned with good perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model by 40%, while retaining 97% of its lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>We focus on one such model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At? An Analysis of BERT's Attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Large pre-trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP, motivat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s attention heads exhibit patterns such as at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s attention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for query-based passage re-ranking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model captures English syntactic phenomena, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model performs remarkably well on all cases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneousl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(multilingual) as a zero shot language transf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>We compare m</td>\n",
       "      <td>BERT</td>\n",
       "      <td>with the best-published methods for zero-shot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner, determine to what extent mBER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>: Distilling BERT for Natural Language Underst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has significantly improved the performances ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be well transferred to a small student Ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>This framework ensures that Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can capture both the general-domain and task-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is empirically effective and achieves compara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is also significantly better than state-of-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>, a pre-trained Transformer model, has achieve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERT</td>\n",
       "      <td>SUM, a simple variant of BERT, for extractive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehensio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to enhance the performance of fine-tuning of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth, and It Must Speak: BERT as a Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018) is a Markov random fiel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high-quality, fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering, we explore simple appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based architectures currently give state-of-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s heads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>) has shown marvelous improvements across vari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Recently, an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking (WW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>without changing any neural architecture or e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE, BERT-wwm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>: A Globally Normalized BERT Model for Open-do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>However, previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models, and 5.8% EM and 6.5% $F_1$ over BERT-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question-answering focused passage ranking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query-docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>We present simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based models for relation extraction and sema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based model can achieve state-of-the-art perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tune...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model, finding that it can generally distingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over-parameterized for downstream t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the fla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Third, the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine-tuning, which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>) demonstrates that a deep bidirectional langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new cond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>In our paper, conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>language model (LM) (Devlin et al., 2019) is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Petroni et al. (2019) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre-traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about (the surface...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>More specifically, we show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s precision drops dramatically when we filter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>As a remedy, we propose E-</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, an extension of BERT that replaces entity me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>E-</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms both BERT and ERNIE (Zhang et al....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>We take this as evidence that E-</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is richer in factual knowledge, and we show t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ELMo, and GPT-2 Embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>In all layers of ELMo,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2, on average, less than 5% of the v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_split_0 text_split_1  \\\n",
       "0                                                              BERT   \n",
       "1    We introduce a new language representation mod...         BERT   \n",
       "2       Unlike recent language representation models,          BERT   \n",
       "3                        As a result, the pre-trained          BERT   \n",
       "4                                                              BERT   \n",
       "5    It obtains new state-of-the-art results on ele...                \n",
       "6                                                   Ro         BERT   \n",
       "7    Language model pretraining has led to signific...                \n",
       "8    Training is computationally expensive, often d...                \n",
       "9                   We present a replication study of          BERT   \n",
       "10                                       We find that          BERT   \n",
       "11   Our best model achieves state-of-the-art resul...                \n",
       "12   These results highlight the importance of prev...                \n",
       "13                     We release our models and code.                \n",
       "14                                              Distil         BERT   \n",
       "15   As Transfer Learning from large-scale pre-trai...                \n",
       "16   In this work, we propose a method to pre-train...         BERT   \n",
       "17   While most prior work investigated the use of ...         BERT   \n",
       "18   To leverage the inductive biases learned by la...                \n",
       "19   Our smaller, faster and lighter model is cheap...                \n",
       "20                                                             BERT   \n",
       "21   Pre-trained text encoders have rapidly advance...                \n",
       "22                        We focus on one such model,          BERT   \n",
       "23   We find that the model represents the steps of...                \n",
       "24   Qualitative analysis reveals that the model ca...                \n",
       "25                                          What Does          BERT   \n",
       "26          Large pre-trained neural networks such as          BERT   \n",
       "27   Most recent analysis has focused on model outp...                \n",
       "28   Complementary to these works, we propose metho...         BERT   \n",
       "29                                                             BERT   \n",
       "30   We further show that certain attention heads c...                \n",
       "31   For example, we find heads that attend to the ...                \n",
       "32   Lastly, we propose an attention-based probing ...         BERT   \n",
       "33                            Passage Re-ranking with          BERT   \n",
       "34   Recently, neural models pretrained on a langua...         BERT   \n",
       "35   In this paper, we describe a simple re-impleme...         BERT   \n",
       "36   Our system is the state of the art on the TREC...                \n",
       "37   The code to reproduce our results is available...                \n",
       "38                                          Assessing          BERT   \n",
       "39   I assess the extent to which the recently intr...         BERT   \n",
       "40                                                The          BERT   \n",
       "41   Beto, Bentz, Becas: The Surprising Cross-Lingu...         BERT   \n",
       "42   Pretrained contextual representation models (P...                \n",
       "43                                   A new release of          BERT   \n",
       "44   This paper explores the broader cross-lingual ...         BERT   \n",
       "45                                        We compare m         BERT   \n",
       "46   Additionally, we investigate the most effectiv...         BERT   \n",
       "47                                                Tiny         BERT   \n",
       "48               Language model pre-training, such as          BERT   \n",
       "49   However, pre-trained language models are usual...                \n",
       "50   To accelerate inference and reduce model size ...                \n",
       "51   By leveraging this new KD method, the plenty o...         BERT   \n",
       "52   Moreover, we introduce a new two-stage learnin...         BERT   \n",
       "53                    This framework ensures that Tiny         BERT   \n",
       "54                                                Tiny         BERT   \n",
       "55                                                Tiny         BERT   \n",
       "56                                          Fine-tune          BERT   \n",
       "57                                                             BERT   \n",
       "58                         In this paper, we describe          BERT   \n",
       "59   Our system is the state of the art on the CNN/...                \n",
       "60   The codes to reproduce our results are availab...                \n",
       "61                                                             BERT   \n",
       "62   Question-answering plays an important role in ...                \n",
       "63   Inspired by the recent success of machine read...                \n",
       "64   To the best of our knowledge, no existing work...                \n",
       "65   In this work, we first build an RRC dataset ca...                \n",
       "66   Since ReviewRC has limited training examples f...         BERT   \n",
       "67   To show the generality of the approach, the pr...                \n",
       "68   Experimental results demonstrate that the prop...                \n",
       "69   The datasets and code are available at this ht...                \n",
       "70                                   How to Fine-Tune          BERT   \n",
       "71   Language model pre-training has proven to be u...                \n",
       "72   As a state-of-the-art language model pre-train...         BERT   \n",
       "73   In this paper, we conduct exhaustive experimen...         BERT   \n",
       "74   Finally, the proposed solution obtains new sta...                \n",
       "75                                                             BERT   \n",
       "76                                       We show that          BERT   \n",
       "77   This formulation gives way to a natural proced...         BERT   \n",
       "78                                   We generate from          BERT   \n",
       "79   Compared to the generations of a traditional l...         BERT   \n",
       "80                                          Utilizing          BERT   \n",
       "81   Aspect-based sentiment analysis (ABSA), which ...                \n",
       "82   In this paper, we construct an auxiliary sente...                \n",
       "83            We fine-tune the pre-trained model from          BERT   \n",
       "84                             Simple Applications of          BERT   \n",
       "85             Following recent successes in applying          BERT   \n",
       "86   This required confronting the challenge posed ...         BERT   \n",
       "87   We address this issue by applying inference on...                \n",
       "88   Experiments on TREC microblog and newswire tes...                \n",
       "89                      Revealing the Dark Secrets of          BERT   \n",
       "90                                                             BERT   \n",
       "91   In the current work, we focus on the interpret...         BERT   \n",
       "92   Using a subset of GLUE tasks and a set of hand...         BERT   \n",
       "93   Our findings suggest that there is a limited s...                \n",
       "94   While different heads consistently use the sam...                \n",
       "95   We show that manually disabling attention in c...         BERT   \n",
       "96   Pre-Training with Whole Word Masking for Chinese          BERT   \n",
       "97   Bidirectional Encoder Representations from Tra...         BERT   \n",
       "98                   Recently, an upgraded version of          BERT   \n",
       "99   In this technical report, we adapt whole word ...                \n",
       "100  The model was trained on the latest Chinese Wi...                \n",
       "101  We aim to provide easy extensibility and bette...         BERT   \n",
       "102  The model is verified on various NLP tasks, ac...                \n",
       "103  Experimental results on these datasets show th...                \n",
       "104  Moreover, we also examine the effectiveness of...         BERT   \n",
       "105  We release the pre-trained model (both TensorF...                \n",
       "106                                     Multi-passage          BERT   \n",
       "107                                                            BERT   \n",
       "108                     However, previous work trains          BERT   \n",
       "109  To tackle this issue, we propose a multi-passage          BERT   \n",
       "110  In addition, we find that splitting articles i...                \n",
       "111  By leveraging a passage ranker to select high-...         BERT   \n",
       "112  Experiments on four standard benchmarks showed...         BERT   \n",
       "113  In particular, on the OpenSQuAD dataset, our m...         BERT   \n",
       "114                                                            BERT   \n",
       "115  Intent classification and slot filling are two...                \n",
       "116  They often suffer from small-scale human-label...                \n",
       "117     Recently a new language representation model,          BERT   \n",
       "118  However, there has not been much effort on exp...         BERT   \n",
       "119  In this work, we propose a joint intent classi...         BERT   \n",
       "120  Experimental results demonstrate that our prop...                \n",
       "121                                                            BERT   \n",
       "122  Conversational search is an emerging topic in ...                \n",
       "123  One of the major challenges to multi-turn conv...                \n",
       "124  Existing methods either prepend history turns ...                \n",
       "125  We propose a conceptually simple yet highly ef...                \n",
       "126  It enables seamless integration of conversatio...         BERT   \n",
       "127  We first explain our view that ConvQA is a sim...                \n",
       "128  We further demonstrate the effectiveness of ou...                \n",
       "129  Finally, we analyze the impact of different nu...                \n",
       "130                    Understanding the Behaviors of          BERT   \n",
       "131  This paper studies the performances and behavi...         BERT   \n",
       "132  We explore several different ways to leverage ...         BERT   \n",
       "133  Experimental results on MS MARCO demonstrate t...         BERT   \n",
       "134  Experimental results on TREC show the gaps bet...         BERT   \n",
       "135                           Analyses illustrate how          BERT   \n",
       "136                                            Simple          BERT   \n",
       "137                                 We present simple          BERT   \n",
       "138  In recent years, state-of-the-art performance ...                \n",
       "139  In this paper, extensive experiments on datase...         BERT   \n",
       "140  To our knowledge, we are the first to successf...         BERT   \n",
       "141  Our models provide strong baselines for future...                \n",
       "142                                                            BERT   \n",
       "143  Multi-task learning allows the sharing of usef...                \n",
       "144  In natural language processing several recent ...                \n",
       "145  These results are based on fine-tuning on each...                \n",
       "146  We explore the multi-task learning setting for...         BERT   \n",
       "147  We introduce new adaptation modules, PALs or `...                \n",
       "148                    By using PALs in parallel with          BERT   \n",
       "149                                              What          BERT   \n",
       "150  Pre-training by language modeling has become a...                \n",
       "151  In this paper we introduce a suite of diagnost...                \n",
       "152  As a case study, we apply these diagnostics to...         BERT   \n",
       "153  Visualizing and Understanding the Effectivenes...         BERT   \n",
       "154              Language model pre-training, such as          BERT   \n",
       "155  However, it is unclear why the pre-training-th...                \n",
       "156  In this paper, we propose to visualize loss la...         BERT   \n",
       "157  First, we find that pre-training reaches a goo...                \n",
       "158  We also demonstrate that the fine-tuning proce...         BERT   \n",
       "159  Second, the visualization results indicate tha...         BERT   \n",
       "160                        Third, the lower layers of          BERT   \n",
       "161                                       Conditional          BERT   \n",
       "162  Data augmentation methods are often applied to...                \n",
       "163  Recently proposed contextual augmentation augm...                \n",
       "164  Bidirectional Encoder Representations from Tra...         BERT   \n",
       "165  We propose a novel data augmentation method fo...         BERT   \n",
       "166                                       We retrofit          BERT   \n",
       "167  In our paper, conditional masked language mod...                \n",
       "168                      The well trained conditional          BERT   \n",
       "169  Experiments on six various different text clas...                \n",
       "170                               Small and Practical          BERT   \n",
       "171  We propose a practical scheme to train a singl...                \n",
       "172               Starting from a public multilingual          BERT   \n",
       "173  We show that our model especially outperforms ...                \n",
       "174  We showcase the effectiveness of our method by...                \n",
       "175                             Data Augmentation for          BERT   \n",
       "176  Recently, a simple combination of passage retr...         BERT   \n",
       "177  In this paper, we present a data augmentation ...                \n",
       "178     We apply a stage-wise approach to fine tuning          BERT   \n",
       "179  Experimental results show large gains in effec...                \n",
       "180                                                            BERT   \n",
       "181                                               The          BERT   \n",
       "182  Petroni et al. (2019) take this as evidence that          BERT   \n",
       "183  We take issue with this interpretation and arg...         BERT   \n",
       "184                   More specifically, we show that          BERT   \n",
       "185                         As a remedy, we propose E-         BERT   \n",
       "186                                                 E-         BERT   \n",
       "187                   We take this as evidence that E-         BERT   \n",
       "188  How Contextual are Contextualized Word Represe...         BERT   \n",
       "189  Replacing static word embeddings with contextu...                \n",
       "190  However, just how contextual are the contextua...         BERT   \n",
       "191  Are there infinitely many context-specific rep...                \n",
       "192  For one, we find that the contextualized repre...                \n",
       "193  While representations of the same word in diff...                \n",
       "194  This suggests that upper layers of contextuali...                \n",
       "195                            In all layers of ELMo,          BERT   \n",
       "\n",
       "                                          text_split_2  \n",
       "0    : Pre-training of Deep Bidirectional Transform...  \n",
       "1    , which stands for Bidirectional Encoder Repre...  \n",
       "2     is designed to pre-train deep bidirectional r...  \n",
       "3     model can be fine-tuned with just one additio...  \n",
       "4     is conceptually simple and empirically powerf...  \n",
       "5                                                       \n",
       "6    a: A Robustly Optimized BERT Pretraining Approach  \n",
       "7                                                       \n",
       "8                                                       \n",
       "9     pretraining (Devlin et al., 2019) that carefu...  \n",
       "10    was significantly undertrained, and can match...  \n",
       "11                                                      \n",
       "12                                                      \n",
       "13                                                      \n",
       "14   , a distilled version of BERT: smaller, faster...  \n",
       "15                                                      \n",
       "16   , which can then be fine-tuned with good perfo...  \n",
       "17    model by 40%, while retaining 97% of its lang...  \n",
       "18                                                      \n",
       "19                                                      \n",
       "20              Rediscovers the Classical NLP Pipeline  \n",
       "21                                                      \n",
       "22   , and aim to quantify where linguistic informa...  \n",
       "23                                                      \n",
       "24                                                      \n",
       "25            Look At? An Analysis of BERT's Attention  \n",
       "26    have had great recent success in NLP, motivat...  \n",
       "27                                                      \n",
       "28                                                  .   \n",
       "29   's attention heads exhibit patterns such as at...  \n",
       "30                                                      \n",
       "31                                                      \n",
       "32                                       's attention.  \n",
       "33                                                      \n",
       "34    (Devlin et al., 2018), have achieved impressi...  \n",
       "35                for query-based passage re-ranking.   \n",
       "36                                                      \n",
       "37                                                      \n",
       "38                              's Syntactic Abilities  \n",
       "39    model captures English syntactic phenomena, u...  \n",
       "40        model performs remarkably well on all cases.  \n",
       "41                                                      \n",
       "42                                                      \n",
       "43    (Devlin, 2018) includes a model simultaneousl...  \n",
       "44    (multilingual) as a zero shot language transf...  \n",
       "45    with the best-published methods for zero-shot...  \n",
       "46    in this manner, determine to what extent mBER...  \n",
       "47   : Distilling BERT for Natural Language Underst...  \n",
       "48   , has significantly improved the performances ...  \n",
       "49                                                      \n",
       "50                                                      \n",
       "51    can be well transferred to a small student Ti...  \n",
       "52   , which performs transformer distillation at b...  \n",
       "53    can capture both the general-domain and task-...  \n",
       "54    is empirically effective and achieves compara...  \n",
       "55    is also significantly better than state-of-th...  \n",
       "56                        for Extractive Summarization  \n",
       "57   , a pre-trained Transformer model, has achieve...  \n",
       "58   SUM, a simple variant of BERT, for extractive ...  \n",
       "59                                                      \n",
       "60                                                      \n",
       "61    Post-Training for Review Reading Comprehensio...  \n",
       "62                                                      \n",
       "63                                                      \n",
       "64                                                      \n",
       "65                                                      \n",
       "66    to enhance the performance of fine-tuning of ...  \n",
       "67                                                      \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                            for Text Classification?  \n",
       "71                                                      \n",
       "72    (Bidirectional Encoder Representations from T...  \n",
       "73    on text classification task and provide a gen...  \n",
       "74                                                      \n",
       "75    has a Mouth, and It Must Speak: BERT as a Mar...  \n",
       "76    (Devlin et al., 2018) is a Markov random fiel...  \n",
       "77                                                  .   \n",
       "78    and find that it can produce high-quality, fl...  \n",
       "79    generates sentences that are more diverse but...  \n",
       "80    for Aspect-Based Sentiment Analysis via Const...  \n",
       "81                                                      \n",
       "82                                                      \n",
       "83    and achieve new state-of-the-art results on S...  \n",
       "84                       for Ad Hoc Document Retrieval  \n",
       "85    to question answering, we explore simple appl...  \n",
       "86                            was designed to handle.   \n",
       "87                                                      \n",
       "88                                                      \n",
       "89                                                      \n",
       "90   -based architectures currently give state-of-t...  \n",
       "91                                                  .   \n",
       "92                                          's heads.   \n",
       "93                                                      \n",
       "94                                                      \n",
       "95                                              models  \n",
       "96                                                      \n",
       "97   ) has shown marvelous improvements across vari...  \n",
       "98    has been released with Whole Word Masking (WW...  \n",
       "99                                                      \n",
       "100                                                     \n",
       "101   without changing any neural architecture or e...  \n",
       "102                                                     \n",
       "103                                                     \n",
       "104                                , ERNIE, BERT-wwm.   \n",
       "105                                                     \n",
       "106  : A Globally Normalized BERT Model for Open-do...  \n",
       "107   model has been successfully applied to open-d...  \n",
       "108   by viewing passages corresponding to the same...  \n",
       "109   model to globally normalize answer scores acr...  \n",
       "110                                                     \n",
       "111                              gains additional 2%.   \n",
       "112   outperforms all state-of-the-art models on al...  \n",
       "113   models, and 5.8% EM and 6.5% $F_1$ over BERT-...  \n",
       "114   for Joint Intent Classification and Slot Filling  \n",
       "115                                                     \n",
       "116                                                     \n",
       "117   (Bidirectional Encoder Representations from T...  \n",
       "118               for natural language understanding.   \n",
       "119                                                 .   \n",
       "120                                                     \n",
       "121   with History Answer Embedding for Conversatio...  \n",
       "122                                                     \n",
       "123                                                     \n",
       "124                                                     \n",
       "125                                                     \n",
       "126   (Bidirectional Encoder Representations from T...  \n",
       "127                                                     \n",
       "128                                                     \n",
       "129                                                     \n",
       "130                                         in Ranking  \n",
       "131                                 in ranking tasks.   \n",
       "132   and fine-tune it on two ranking tasks: MS MAR...  \n",
       "133   in question-answering focused passage ranking...  \n",
       "134   pre-trained on surrounding contexts and the n...  \n",
       "135   allocates its attentions between query-docume...  \n",
       "136   Models for Relation Extraction and Semantic R...  \n",
       "137  -based models for relation extraction and sema...  \n",
       "138                                                     \n",
       "139  -based model can achieve state-of-the-art perf...  \n",
       "140                                   in this manner.   \n",
       "141                                                     \n",
       "142   and PALs: Projected Attention Layers for Effi...  \n",
       "143                                                     \n",
       "144                                                     \n",
       "145                                                     \n",
       "146   model on the GLUE benchmark, and how to best ...  \n",
       "147                                                     \n",
       "148   layers, we match the performance of fine-tune...  \n",
       "149   Is Not: Lessons from a New Suite of Psycholin...  \n",
       "150                                                     \n",
       "151                                                     \n",
       "152   model, finding that it can generally distingu...  \n",
       "153                                                     \n",
       "154  , has achieved remarkable results in many NLP ...  \n",
       "155                                                     \n",
       "156                             on specific datasets.   \n",
       "157                                                     \n",
       "158   is highly over-parameterized for downstream t...  \n",
       "159   tends to generalize better because of the fla...  \n",
       "160   are more invariant during fine-tuning, which ...  \n",
       "161                            Contextual Augmentation  \n",
       "162                                                     \n",
       "163                                                     \n",
       "164  ) demonstrates that a deep bidirectional langu...  \n",
       "165                          contextual augmentation.   \n",
       "166   to conditional BERT by introducing a new cond...  \n",
       "167                                                     \n",
       "168   can be applied to enhance contextual augmenta...  \n",
       "169                                                     \n",
       "170                       Models for Sequence Labeling  \n",
       "171                                                     \n",
       "172   checkpoint, our final model is 6x smaller and...  \n",
       "173                                                     \n",
       "174                                                     \n",
       "175      Fine-Tuning in Open-Domain Question Answering  \n",
       "176   reader was found to be very effective for que...  \n",
       "177                                                     \n",
       "178   on multiple datasets, starting with data that...  \n",
       "179                                                     \n",
       "180   is Not a Knowledge Base (Yet): Factual Knowle...  \n",
       "181   language model (LM) (Devlin et al., 2019) is ...  \n",
       "182   memorizes factual knowledge during pre-traini...  \n",
       "183   is partly due to reasoning about (the surface...  \n",
       "184  's precision drops dramatically when we filter...  \n",
       "185  , an extension of BERT that replaces entity me...  \n",
       "186   outperforms both BERT and ERNIE (Zhang et al....  \n",
       "187   is richer in factual knowledge, and we show t...  \n",
       "188                       , ELMo, and GPT-2 Embeddings  \n",
       "189                                                     \n",
       "190                                                 ?   \n",
       "191                                                     \n",
       "192                                                     \n",
       "193                                                     \n",
       "194                                                     \n",
       "195  , and GPT-2, on average, less than 5% of the v...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split on the search word, regardless of whitespace (if search word is A and we have word CAR, it slices it up)\n",
    "def split_search_term_literal(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    if search_word in row['Text']:\n",
    "        splitted = row['Text'].split(search_word, maxsplit=1)\n",
    "        splitted.insert(1, search_word)\n",
    "    else:\n",
    "        splitted = [row['Text'],'','']\n",
    "    return dict(zip(splitting_headers,splitted))\n",
    "\n",
    "# df = df.join()\n",
    "df.apply(lambda row: split_search_term_literal(row, search_word), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_split_0</th>\n",
       "      <th>text_split_1</th>\n",
       "      <th>text_split_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>which stands for Bidirectional Encoder Represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike recent language representation models,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre-train deep bidirectional re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a result, the pre-trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine-tuned with just one addition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is conceptually simple and empirically powerful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining (Devlin et al., 2019) that careful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained, and can match ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model by 40%, while retaining 97% of its langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>We focus on one such model,</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>and aim to quantify where linguistic informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At? An Analysis of BERT's Attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Large pre-trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP, motivati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>BERT's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>attention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for query-based passage re-ranking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model captures English syntactic phenomena, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model performs remarkably well on all cases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best-published methods for zero-shot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner, determine to what extent mBERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be well transferred to a small student Tin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>TinyBERT,</td>\n",
       "      <td>which performs transformer distillation at bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general-domain and task-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state-of-the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to enhance the performance of fine-tuning of B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth, and It Must Speak: BERT as a Mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018) is a Markov random field...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high-quality, flu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering, we explore simple appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Recently, an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking (WWM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ERNIE, BERT-wwm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open-do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>However, previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>non-BERT</td>\n",
       "      <td>models, and 5.8% EM and 6.5% $F_1$ over BERT-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question-answering focused passage ranking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query-documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>We present simple</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>models for relation extraction and semantic ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model, finding that it can generally distingui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has achieved remarkable results in many NLP ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over-parameterized for downstream ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Third, the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine-tuning, which s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>In our paper, conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>language model (LM) (Devlin et al., 2019) is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Petroni et al. (2019) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre-training.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about (the surface ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>More specifically, we show that</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>As a remedy, we propose</td>\n",
       "      <td>E-BERT,</td>\n",
       "      <td>an extension of BERT that replaces entity ment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td></td>\n",
       "      <td>E-BERT</td>\n",
       "      <td>outperforms both BERT and ERNIE (Zhang et al.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>We take this as evidence that</td>\n",
       "      <td>E-BERT</td>\n",
       "      <td>is richer in factual knowledge, and we show tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>BERT?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>In all layers of ELMo,</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>and GPT-2, on average, less than 5% of the var...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_split_0 text_split_1  \\\n",
       "0                                                             BERT:   \n",
       "1    We introduce a new language representation mod...        BERT,   \n",
       "2        Unlike recent language representation models,         BERT   \n",
       "3                         As a result, the pre-trained         BERT   \n",
       "4                                                              BERT   \n",
       "5    It obtains new state-of-the-art results on ele...                \n",
       "6                                                          RoBERTa:   \n",
       "7    Language model pretraining has led to signific...                \n",
       "8    Training is computationally expensive, often d...                \n",
       "9                    We present a replication study of         BERT   \n",
       "10                                        We find that         BERT   \n",
       "11   Our best model achieves state-of-the-art resul...                \n",
       "12   These results highlight the importance of prev...                \n",
       "13                     We release our models and code.                \n",
       "14                                                      DistilBERT,   \n",
       "15   As Transfer Learning from large-scale pre-trai...                \n",
       "16   In this work, we propose a method to pre-train...  DistilBERT,   \n",
       "17   While most prior work investigated the use of ...         BERT   \n",
       "18   To leverage the inductive biases learned by la...                \n",
       "19   Our smaller, faster and lighter model is cheap...                \n",
       "20                                                             BERT   \n",
       "21   Pre-trained text encoders have rapidly advance...                \n",
       "22                         We focus on one such model,        BERT,   \n",
       "23   We find that the model represents the steps of...                \n",
       "24   Qualitative analysis reveals that the model ca...                \n",
       "25                                           What Does         BERT   \n",
       "26           Large pre-trained neural networks such as         BERT   \n",
       "27   Most recent analysis has focused on model outp...                \n",
       "28   Complementary to these works, we propose metho...        BERT.   \n",
       "29                                                           BERT's   \n",
       "30   We further show that certain attention heads c...                \n",
       "31   For example, we find heads that attend to the ...                \n",
       "32   Lastly, we propose an attention-based probing ...       BERT's   \n",
       "33                             Passage Re-ranking with         BERT   \n",
       "34   Recently, neural models pretrained on a langua...         BERT   \n",
       "35   In this paper, we describe a simple re-impleme...         BERT   \n",
       "36   Our system is the state of the art on the TREC...                \n",
       "37   The code to reproduce our results is available...                \n",
       "38                                           Assessing       BERT's   \n",
       "39   I assess the extent to which the recently intr...         BERT   \n",
       "40                                                 The         BERT   \n",
       "41   Beto, Bentz, Becas: The Surprising Cross-Lingu...         BERT   \n",
       "42   Pretrained contextual representation models (P...                \n",
       "43                                    A new release of         BERT   \n",
       "44   This paper explores the broader cross-lingual ...        mBERT   \n",
       "45                                          We compare        mBERT   \n",
       "46   Additionally, we investigate the most effectiv...        mBERT   \n",
       "47                                                        TinyBERT:   \n",
       "48                Language model pre-training, such as        BERT,   \n",
       "49   However, pre-trained language models are usual...                \n",
       "50   To accelerate inference and reduce model size ...                \n",
       "51   By leveraging this new KD method, the plenty o...         BERT   \n",
       "52   Moreover, we introduce a new two-stage learnin...    TinyBERT,   \n",
       "53                         This framework ensures that     TinyBERT   \n",
       "54                                                         TinyBERT   \n",
       "55                                                         TinyBERT   \n",
       "56                                           Fine-tune         BERT   \n",
       "57                                                            BERT,   \n",
       "58                          In this paper, we describe     BERTSUM,   \n",
       "59   Our system is the state of the art on the CNN/...                \n",
       "60   The codes to reproduce our results are availab...                \n",
       "61                                                             BERT   \n",
       "62   Question-answering plays an important role in ...                \n",
       "63   Inspired by the recent success of machine read...                \n",
       "64   To the best of our knowledge, no existing work...                \n",
       "65   In this work, we first build an RRC dataset ca...                \n",
       "66   Since ReviewRC has limited training examples f...         BERT   \n",
       "67   To show the generality of the approach, the pr...                \n",
       "68   Experimental results demonstrate that the prop...                \n",
       "69   The datasets and code are available at this ht...                \n",
       "70                                    How to Fine-Tune         BERT   \n",
       "71   Language model pre-training has proven to be u...                \n",
       "72   As a state-of-the-art language model pre-train...         BERT   \n",
       "73   In this paper, we conduct exhaustive experimen...         BERT   \n",
       "74   Finally, the proposed solution obtains new sta...                \n",
       "75                                                             BERT   \n",
       "76                                        We show that         BERT   \n",
       "77   This formulation gives way to a natural proced...        BERT.   \n",
       "78                                    We generate from         BERT   \n",
       "79   Compared to the generations of a traditional l...         BERT   \n",
       "80                                           Utilizing         BERT   \n",
       "81   Aspect-based sentiment analysis (ABSA), which ...                \n",
       "82   In this paper, we construct an auxiliary sente...                \n",
       "83             We fine-tune the pre-trained model from         BERT   \n",
       "84                              Simple Applications of         BERT   \n",
       "85              Following recent successes in applying         BERT   \n",
       "86   This required confronting the challenge posed ...         BERT   \n",
       "87   We address this issue by applying inference on...                \n",
       "88   Experiments on TREC microblog and newswire tes...                \n",
       "89                       Revealing the Dark Secrets of         BERT   \n",
       "90                                                       BERT-based   \n",
       "91   In the current work, we focus on the interpret...        BERT.   \n",
       "92   Using a subset of GLUE tasks and a set of hand...       BERT's   \n",
       "93   Our findings suggest that there is a limited s...                \n",
       "94   While different heads consistently use the sam...                \n",
       "95   We show that manually disabling attention in c...         BERT   \n",
       "96    Pre-Training with Whole Word Masking for Chinese         BERT   \n",
       "97   Bidirectional Encoder Representations from Tra...       (BERT)   \n",
       "98                    Recently, an upgraded version of         BERT   \n",
       "99   In this technical report, we adapt whole word ...                \n",
       "100  The model was trained on the latest Chinese Wi...                \n",
       "101  We aim to provide easy extensibility and bette...         BERT   \n",
       "102  The model is verified on various NLP tasks, ac...                \n",
       "103  Experimental results on these datasets show th...                \n",
       "104  Moreover, we also examine the effectiveness of...        BERT,   \n",
       "105  We release the pre-trained model (both TensorF...                \n",
       "106                                      Multi-passage        BERT:   \n",
       "107                                                            BERT   \n",
       "108                      However, previous work trains         BERT   \n",
       "109   To tackle this issue, we propose a multi-passage         BERT   \n",
       "110  In addition, we find that splitting articles i...                \n",
       "111  By leveraging a passage ranker to select high-...         BERT   \n",
       "112  Experiments on four standard benchmarks showed...         BERT   \n",
       "113  In particular, on the OpenSQuAD dataset, our m...     non-BERT   \n",
       "114                                                            BERT   \n",
       "115  Intent classification and slot filling are two...                \n",
       "116  They often suffer from small-scale human-label...                \n",
       "117      Recently a new language representation model,         BERT   \n",
       "118  However, there has not been much effort on exp...         BERT   \n",
       "119  In this work, we propose a joint intent classi...        BERT.   \n",
       "120  Experimental results demonstrate that our prop...                \n",
       "121                                                            BERT   \n",
       "122  Conversational search is an emerging topic in ...                \n",
       "123  One of the major challenges to multi-turn conv...                \n",
       "124  Existing methods either prepend history turns ...                \n",
       "125  We propose a conceptually simple yet highly ef...                \n",
       "126  It enables seamless integration of conversatio...         BERT   \n",
       "127  We first explain our view that ConvQA is a sim...                \n",
       "128  We further demonstrate the effectiveness of ou...                \n",
       "129  Finally, we analyze the impact of different nu...                \n",
       "130                     Understanding the Behaviors of         BERT   \n",
       "131  This paper studies the performances and behavi...         BERT   \n",
       "132  We explore several different ways to leverage ...         BERT   \n",
       "133  Experimental results on MS MARCO demonstrate t...         BERT   \n",
       "134  Experimental results on TREC show the gaps bet...         BERT   \n",
       "135                            Analyses illustrate how         BERT   \n",
       "136                                             Simple         BERT   \n",
       "137                                  We present simple   BERT-based   \n",
       "138  In recent years, state-of-the-art performance ...                \n",
       "139  In this paper, extensive experiments on datase...   BERT-based   \n",
       "140  To our knowledge, we are the first to successf...         BERT   \n",
       "141  Our models provide strong baselines for future...                \n",
       "142                                                            BERT   \n",
       "143  Multi-task learning allows the sharing of usef...                \n",
       "144  In natural language processing several recent ...                \n",
       "145  These results are based on fine-tuning on each...                \n",
       "146  We explore the multi-task learning setting for...         BERT   \n",
       "147  We introduce new adaptation modules, PALs or `...                \n",
       "148                     By using PALs in parallel with         BERT   \n",
       "149                                               What         BERT   \n",
       "150  Pre-training by language modeling has become a...                \n",
       "151  In this paper we introduce a suite of diagnost...                \n",
       "152  As a case study, we apply these diagnostics to...         BERT   \n",
       "153  Visualizing and Understanding the Effectivenes...         BERT   \n",
       "154               Language model pre-training, such as        BERT,   \n",
       "155  However, it is unclear why the pre-training-th...                \n",
       "156  In this paper, we propose to visualize loss la...         BERT   \n",
       "157  First, we find that pre-training reaches a goo...                \n",
       "158  We also demonstrate that the fine-tuning proce...         BERT   \n",
       "159  Second, the visualization results indicate tha...         BERT   \n",
       "160                         Third, the lower layers of         BERT   \n",
       "161                                        Conditional         BERT   \n",
       "162  Data augmentation methods are often applied to...                \n",
       "163  Recently proposed contextual augmentation augm...                \n",
       "164  Bidirectional Encoder Representations from Tra...       (BERT)   \n",
       "165  We propose a novel data augmentation method fo...         BERT   \n",
       "166                                        We retrofit         BERT   \n",
       "167  In our paper, conditional masked language mod...                \n",
       "168                       The well trained conditional         BERT   \n",
       "169  Experiments on six various different text clas...                \n",
       "170                                Small and Practical         BERT   \n",
       "171  We propose a practical scheme to train a singl...                \n",
       "172                Starting from a public multilingual         BERT   \n",
       "173  We show that our model especially outperforms ...                \n",
       "174  We showcase the effectiveness of our method by...                \n",
       "175                              Data Augmentation for         BERT   \n",
       "176  Recently, a simple combination of passage retr...         BERT   \n",
       "177  In this paper, we present a data augmentation ...                \n",
       "178      We apply a stage-wise approach to fine tuning         BERT   \n",
       "179  Experimental results show large gains in effec...                \n",
       "180                                                            BERT   \n",
       "181                                                The         BERT   \n",
       "182   Petroni et al. (2019) take this as evidence that         BERT   \n",
       "183  We take issue with this interpretation and arg...         BERT   \n",
       "184                    More specifically, we show that       BERT's   \n",
       "185                            As a remedy, we propose      E-BERT,   \n",
       "186                                                          E-BERT   \n",
       "187                      We take this as evidence that       E-BERT   \n",
       "188  How Contextual are Contextualized Word Represe...        BERT,   \n",
       "189  Replacing static word embeddings with contextu...                \n",
       "190  However, just how contextual are the contextua...        BERT?   \n",
       "191  Are there infinitely many context-specific rep...                \n",
       "192  For one, we find that the contextualized repre...                \n",
       "193  While representations of the same word in diff...                \n",
       "194  This suggests that upper layers of contextuali...                \n",
       "195                             In all layers of ELMo,        BERT,   \n",
       "\n",
       "                                          text_split_2  \n",
       "0    Pre-training of Deep Bidirectional Transformer...  \n",
       "1    which stands for Bidirectional Encoder Represe...  \n",
       "2    is designed to pre-train deep bidirectional re...  \n",
       "3    model can be fine-tuned with just one addition...  \n",
       "4    is conceptually simple and empirically powerful.   \n",
       "5                                                       \n",
       "6       A Robustly Optimized BERT Pretraining Approach  \n",
       "7                                                       \n",
       "8                                                       \n",
       "9    pretraining (Devlin et al., 2019) that careful...  \n",
       "10   was significantly undertrained, and can match ...  \n",
       "11                                                      \n",
       "12                                                      \n",
       "13                                                      \n",
       "14   a distilled version of BERT: smaller, faster, ...  \n",
       "15                                                      \n",
       "16   which can then be fine-tuned with good perform...  \n",
       "17   model by 40%, while retaining 97% of its langu...  \n",
       "18                                                      \n",
       "19                                                      \n",
       "20              Rediscovers the Classical NLP Pipeline  \n",
       "21                                                      \n",
       "22   and aim to quantify where linguistic informati...  \n",
       "23                                                      \n",
       "24                                                      \n",
       "25            Look At? An Analysis of BERT's Attention  \n",
       "26   have had great recent success in NLP, motivati...  \n",
       "27                                                      \n",
       "28                                                      \n",
       "29   attention heads exhibit patterns such as atten...  \n",
       "30                                                      \n",
       "31                                                      \n",
       "32                                          attention.  \n",
       "33                                                      \n",
       "34   (Devlin et al., 2018), have achieved impressiv...  \n",
       "35                for query-based passage re-ranking.   \n",
       "36                                                      \n",
       "37                                                      \n",
       "38                                 Syntactic Abilities  \n",
       "39   model captures English syntactic phenomena, us...  \n",
       "40        model performs remarkably well on all cases.  \n",
       "41                                                      \n",
       "42                                                      \n",
       "43   (Devlin, 2018) includes a model simultaneously...  \n",
       "44   (multilingual) as a zero shot language transfe...  \n",
       "45   with the best-published methods for zero-shot ...  \n",
       "46   in this manner, determine to what extent mBERT...  \n",
       "47   Distilling BERT for Natural Language Understan...  \n",
       "48   has significantly improved the performances of...  \n",
       "49                                                      \n",
       "50                                                      \n",
       "51   can be well transferred to a small student Tin...  \n",
       "52   which performs transformer distillation at bot...  \n",
       "53   can capture both the general-domain and task-s...  \n",
       "54   is empirically effective and achieves comparab...  \n",
       "55   is also significantly better than state-of-the...  \n",
       "56                        for Extractive Summarization  \n",
       "57   a pre-trained Transformer model, has achieved ...  \n",
       "58   a simple variant of BERT, for extractive summa...  \n",
       "59                                                      \n",
       "60                                                      \n",
       "61   Post-Training for Review Reading Comprehension...  \n",
       "62                                                      \n",
       "63                                                      \n",
       "64                                                      \n",
       "65                                                      \n",
       "66   to enhance the performance of fine-tuning of B...  \n",
       "67                                                      \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                            for Text Classification?  \n",
       "71                                                      \n",
       "72   (Bidirectional Encoder Representations from Tr...  \n",
       "73   on text classification task and provide a gene...  \n",
       "74                                                      \n",
       "75   has a Mouth, and It Must Speak: BERT as a Mark...  \n",
       "76   (Devlin et al., 2018) is a Markov random field...  \n",
       "77                                                      \n",
       "78   and find that it can produce high-quality, flu...  \n",
       "79   generates sentences that are more diverse but ...  \n",
       "80   for Aspect-Based Sentiment Analysis via Constr...  \n",
       "81                                                      \n",
       "82                                                      \n",
       "83   and achieve new state-of-the-art results on Se...  \n",
       "84                       for Ad Hoc Document Retrieval  \n",
       "85   to question answering, we explore simple appli...  \n",
       "86                            was designed to handle.   \n",
       "87                                                      \n",
       "88                                                      \n",
       "89                                                      \n",
       "90   architectures currently give state-of-the-art ...  \n",
       "91                                                      \n",
       "92                                             heads.   \n",
       "93                                                      \n",
       "94                                                      \n",
       "95                                              models  \n",
       "96                                                      \n",
       "97   has shown marvelous improvements across variou...  \n",
       "98   has been released with Whole Word Masking (WWM...  \n",
       "99                                                      \n",
       "100                                                     \n",
       "101  without changing any neural architecture or ev...  \n",
       "102                                                     \n",
       "103                                                     \n",
       "104                                  ERNIE, BERT-wwm.   \n",
       "105                                                     \n",
       "106  A Globally Normalized BERT Model for Open-doma...  \n",
       "107  model has been successfully applied to open-do...  \n",
       "108  by viewing passages corresponding to the same ...  \n",
       "109  model to globally normalize answer scores acro...  \n",
       "110                                                     \n",
       "111                              gains additional 2%.   \n",
       "112  outperforms all state-of-the-art models on all...  \n",
       "113  models, and 5.8% EM and 6.5% $F_1$ over BERT-b...  \n",
       "114   for Joint Intent Classification and Slot Filling  \n",
       "115                                                     \n",
       "116                                                     \n",
       "117  (Bidirectional Encoder Representations from Tr...  \n",
       "118               for natural language understanding.   \n",
       "119                                                     \n",
       "120                                                     \n",
       "121  with History Answer Embedding for Conversation...  \n",
       "122                                                     \n",
       "123                                                     \n",
       "124                                                     \n",
       "125                                                     \n",
       "126  (Bidirectional Encoder Representations from Tr...  \n",
       "127                                                     \n",
       "128                                                     \n",
       "129                                                     \n",
       "130                                         in Ranking  \n",
       "131                                 in ranking tasks.   \n",
       "132  and fine-tune it on two ranking tasks: MS MARC...  \n",
       "133  in question-answering focused passage ranking ...  \n",
       "134  pre-trained on surrounding contexts and the ne...  \n",
       "135  allocates its attentions between query-documen...  \n",
       "136  Models for Relation Extraction and Semantic Ro...  \n",
       "137  models for relation extraction and semantic ro...  \n",
       "138                                                     \n",
       "139   model can achieve state-of-the-art performance.   \n",
       "140                                   in this manner.   \n",
       "141                                                     \n",
       "142  and PALs: Projected Attention Layers for Effic...  \n",
       "143                                                     \n",
       "144                                                     \n",
       "145                                                     \n",
       "146  model on the GLUE benchmark, and how to best a...  \n",
       "147                                                     \n",
       "148  layers, we match the performance of fine-tuned...  \n",
       "149  Is Not: Lessons from a New Suite of Psycholing...  \n",
       "150                                                     \n",
       "151                                                     \n",
       "152  model, finding that it can generally distingui...  \n",
       "153                                                     \n",
       "154  has achieved remarkable results in many NLP ta...  \n",
       "155                                                     \n",
       "156                             on specific datasets.   \n",
       "157                                                     \n",
       "158  is highly over-parameterized for downstream ta...  \n",
       "159  tends to generalize better because of the flat...  \n",
       "160  are more invariant during fine-tuning, which s...  \n",
       "161                            Contextual Augmentation  \n",
       "162                                                     \n",
       "163                                                     \n",
       "164  demonstrates that a deep bidirectional languag...  \n",
       "165                          contextual augmentation.   \n",
       "166  to conditional BERT by introducing a new condi...  \n",
       "167                                                     \n",
       "168  can be applied to enhance contextual augmentat...  \n",
       "169                                                     \n",
       "170                       Models for Sequence Labeling  \n",
       "171                                                     \n",
       "172  checkpoint, our final model is 6x smaller and ...  \n",
       "173                                                     \n",
       "174                                                     \n",
       "175      Fine-Tuning in Open-Domain Question Answering  \n",
       "176  reader was found to be very effective for ques...  \n",
       "177                                                     \n",
       "178  on multiple datasets, starting with data that ...  \n",
       "179                                                     \n",
       "180  is Not a Knowledge Base (Yet): Factual Knowled...  \n",
       "181  language model (LM) (Devlin et al., 2019) is s...  \n",
       "182  memorizes factual knowledge during pre-training.   \n",
       "183  is partly due to reasoning about (the surface ...  \n",
       "184  precision drops dramatically when we filter ce...  \n",
       "185  an extension of BERT that replaces entity ment...  \n",
       "186  outperforms both BERT and ERNIE (Zhang et al.,...  \n",
       "187  is richer in factual knowledge, and we show tw...  \n",
       "188                         ELMo, and GPT-2 Embeddings  \n",
       "189                                                     \n",
       "190                                                     \n",
       "191                                                     \n",
       "192                                                     \n",
       "193                                                     \n",
       "194                                                     \n",
       "195  and GPT-2, on average, less than 5% of the var...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split on the search word, taking care to only split on whitespace\n",
    "def split_search_term_whitespace(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    if search_word in row['Text']:\n",
    "        tokens = row['Text'].split(' ')\n",
    "        for i in range(len(tokens)):\n",
    "            if search_word in tokens[i]:\n",
    "                splitted=[' '.join(tokens[:i]), tokens[i], ' '.join(tokens[i+1:])]\n",
    "                break\n",
    "    else:\n",
    "        splitted = [row['Text'],'','']\n",
    "    return dict(zip(splitting_headers,splitted))\n",
    "\n",
    "df.apply(lambda row: split_search_term_whitespace(row, search_word), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "predictor.predict(\n",
    "  document=\"The woman reading a newspaper sat on the bench with her dog.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping functions\n",
    "\n",
    "Assume we have an input dataframe with column headers `['text_split_0','text_split_1','text_split_2']`\n",
    "\n",
    "`'text_split_1'` is the column that contains our search term / anchor point\n",
    "\n",
    "`'text_split_0'` and `'text_split_2'` are the columns that contain text before and after the search terms respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_split_0</th>\n",
       "      <th>text_split_1</th>\n",
       "      <th>text_split_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>which stands for Bidirectional Encoder Represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike recent language representation models,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre-train deep bidirectional re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a result, the pre-trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine-tuned with just one addition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is conceptually simple and empirically powerful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining (Devlin et al., 2019) that careful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained, and can match ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model by 40%, while retaining 97% of its langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>We focus on one such model,</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>and aim to quantify where linguistic informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At? An Analysis of BERT's Attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Large pre-trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP, motivati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>BERT's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>attention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for query-based passage re-ranking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model captures English syntactic phenomena, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model performs remarkably well on all cases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best-published methods for zero-shot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner, determine to what extent mBERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be well transferred to a small student Tin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>TinyBERT,</td>\n",
       "      <td>which performs transformer distillation at bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general-domain and task-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state-of-the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to enhance the performance of fine-tuning of B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth, and It Must Speak: BERT as a Mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018) is a Markov random field...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high-quality, flu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering, we explore simple appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Recently, an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking (WWM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ERNIE, BERT-wwm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open-do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>However, previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>non-BERT</td>\n",
       "      <td>models, and 5.8% EM and 6.5% $F_1$ over BERT-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question-answering focused passage ranking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query-documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>We present simple</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>models for relation extraction and semantic ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model, finding that it can generally distingui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has achieved remarkable results in many NLP ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over-parameterized for downstream ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Third, the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine-tuning, which s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>In our paper, conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>language model (LM) (Devlin et al., 2019) is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Petroni et al. (2019) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre-training.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about (the surface ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>More specifically, we show that</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>As a remedy, we propose</td>\n",
       "      <td>E-BERT,</td>\n",
       "      <td>an extension of BERT that replaces entity ment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td></td>\n",
       "      <td>E-BERT</td>\n",
       "      <td>outperforms both BERT and ERNIE (Zhang et al.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>We take this as evidence that</td>\n",
       "      <td>E-BERT</td>\n",
       "      <td>is richer in factual knowledge, and we show tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>BERT?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>In all layers of ELMo,</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>and GPT-2, on average, less than 5% of the var...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_split_0 text_split_1  \\\n",
       "0                                                             BERT:   \n",
       "1    We introduce a new language representation mod...        BERT,   \n",
       "2        Unlike recent language representation models,         BERT   \n",
       "3                         As a result, the pre-trained         BERT   \n",
       "4                                                              BERT   \n",
       "5    It obtains new state-of-the-art results on ele...                \n",
       "6                                                          RoBERTa:   \n",
       "7    Language model pretraining has led to signific...                \n",
       "8    Training is computationally expensive, often d...                \n",
       "9                    We present a replication study of         BERT   \n",
       "10                                        We find that         BERT   \n",
       "11   Our best model achieves state-of-the-art resul...                \n",
       "12   These results highlight the importance of prev...                \n",
       "13                     We release our models and code.                \n",
       "14                                                      DistilBERT,   \n",
       "15   As Transfer Learning from large-scale pre-trai...                \n",
       "16   In this work, we propose a method to pre-train...  DistilBERT,   \n",
       "17   While most prior work investigated the use of ...         BERT   \n",
       "18   To leverage the inductive biases learned by la...                \n",
       "19   Our smaller, faster and lighter model is cheap...                \n",
       "20                                                             BERT   \n",
       "21   Pre-trained text encoders have rapidly advance...                \n",
       "22                         We focus on one such model,        BERT,   \n",
       "23   We find that the model represents the steps of...                \n",
       "24   Qualitative analysis reveals that the model ca...                \n",
       "25                                           What Does         BERT   \n",
       "26           Large pre-trained neural networks such as         BERT   \n",
       "27   Most recent analysis has focused on model outp...                \n",
       "28   Complementary to these works, we propose metho...        BERT.   \n",
       "29                                                           BERT's   \n",
       "30   We further show that certain attention heads c...                \n",
       "31   For example, we find heads that attend to the ...                \n",
       "32   Lastly, we propose an attention-based probing ...       BERT's   \n",
       "33                             Passage Re-ranking with         BERT   \n",
       "34   Recently, neural models pretrained on a langua...         BERT   \n",
       "35   In this paper, we describe a simple re-impleme...         BERT   \n",
       "36   Our system is the state of the art on the TREC...                \n",
       "37   The code to reproduce our results is available...                \n",
       "38                                           Assessing       BERT's   \n",
       "39   I assess the extent to which the recently intr...         BERT   \n",
       "40                                                 The         BERT   \n",
       "41   Beto, Bentz, Becas: The Surprising Cross-Lingu...         BERT   \n",
       "42   Pretrained contextual representation models (P...                \n",
       "43                                    A new release of         BERT   \n",
       "44   This paper explores the broader cross-lingual ...        mBERT   \n",
       "45                                          We compare        mBERT   \n",
       "46   Additionally, we investigate the most effectiv...        mBERT   \n",
       "47                                                        TinyBERT:   \n",
       "48                Language model pre-training, such as        BERT,   \n",
       "49   However, pre-trained language models are usual...                \n",
       "50   To accelerate inference and reduce model size ...                \n",
       "51   By leveraging this new KD method, the plenty o...         BERT   \n",
       "52   Moreover, we introduce a new two-stage learnin...    TinyBERT,   \n",
       "53                         This framework ensures that     TinyBERT   \n",
       "54                                                         TinyBERT   \n",
       "55                                                         TinyBERT   \n",
       "56                                           Fine-tune         BERT   \n",
       "57                                                            BERT,   \n",
       "58                          In this paper, we describe     BERTSUM,   \n",
       "59   Our system is the state of the art on the CNN/...                \n",
       "60   The codes to reproduce our results are availab...                \n",
       "61                                                             BERT   \n",
       "62   Question-answering plays an important role in ...                \n",
       "63   Inspired by the recent success of machine read...                \n",
       "64   To the best of our knowledge, no existing work...                \n",
       "65   In this work, we first build an RRC dataset ca...                \n",
       "66   Since ReviewRC has limited training examples f...         BERT   \n",
       "67   To show the generality of the approach, the pr...                \n",
       "68   Experimental results demonstrate that the prop...                \n",
       "69   The datasets and code are available at this ht...                \n",
       "70                                    How to Fine-Tune         BERT   \n",
       "71   Language model pre-training has proven to be u...                \n",
       "72   As a state-of-the-art language model pre-train...         BERT   \n",
       "73   In this paper, we conduct exhaustive experimen...         BERT   \n",
       "74   Finally, the proposed solution obtains new sta...                \n",
       "75                                                             BERT   \n",
       "76                                        We show that         BERT   \n",
       "77   This formulation gives way to a natural proced...        BERT.   \n",
       "78                                    We generate from         BERT   \n",
       "79   Compared to the generations of a traditional l...         BERT   \n",
       "80                                           Utilizing         BERT   \n",
       "81   Aspect-based sentiment analysis (ABSA), which ...                \n",
       "82   In this paper, we construct an auxiliary sente...                \n",
       "83             We fine-tune the pre-trained model from         BERT   \n",
       "84                              Simple Applications of         BERT   \n",
       "85              Following recent successes in applying         BERT   \n",
       "86   This required confronting the challenge posed ...         BERT   \n",
       "87   We address this issue by applying inference on...                \n",
       "88   Experiments on TREC microblog and newswire tes...                \n",
       "89                       Revealing the Dark Secrets of         BERT   \n",
       "90                                                       BERT-based   \n",
       "91   In the current work, we focus on the interpret...        BERT.   \n",
       "92   Using a subset of GLUE tasks and a set of hand...       BERT's   \n",
       "93   Our findings suggest that there is a limited s...                \n",
       "94   While different heads consistently use the sam...                \n",
       "95   We show that manually disabling attention in c...         BERT   \n",
       "96    Pre-Training with Whole Word Masking for Chinese         BERT   \n",
       "97   Bidirectional Encoder Representations from Tra...       (BERT)   \n",
       "98                    Recently, an upgraded version of         BERT   \n",
       "99   In this technical report, we adapt whole word ...                \n",
       "100  The model was trained on the latest Chinese Wi...                \n",
       "101  We aim to provide easy extensibility and bette...         BERT   \n",
       "102  The model is verified on various NLP tasks, ac...                \n",
       "103  Experimental results on these datasets show th...                \n",
       "104  Moreover, we also examine the effectiveness of...        BERT,   \n",
       "105  We release the pre-trained model (both TensorF...                \n",
       "106                                      Multi-passage        BERT:   \n",
       "107                                                            BERT   \n",
       "108                      However, previous work trains         BERT   \n",
       "109   To tackle this issue, we propose a multi-passage         BERT   \n",
       "110  In addition, we find that splitting articles i...                \n",
       "111  By leveraging a passage ranker to select high-...         BERT   \n",
       "112  Experiments on four standard benchmarks showed...         BERT   \n",
       "113  In particular, on the OpenSQuAD dataset, our m...     non-BERT   \n",
       "114                                                            BERT   \n",
       "115  Intent classification and slot filling are two...                \n",
       "116  They often suffer from small-scale human-label...                \n",
       "117      Recently a new language representation model,         BERT   \n",
       "118  However, there has not been much effort on exp...         BERT   \n",
       "119  In this work, we propose a joint intent classi...        BERT.   \n",
       "120  Experimental results demonstrate that our prop...                \n",
       "121                                                            BERT   \n",
       "122  Conversational search is an emerging topic in ...                \n",
       "123  One of the major challenges to multi-turn conv...                \n",
       "124  Existing methods either prepend history turns ...                \n",
       "125  We propose a conceptually simple yet highly ef...                \n",
       "126  It enables seamless integration of conversatio...         BERT   \n",
       "127  We first explain our view that ConvQA is a sim...                \n",
       "128  We further demonstrate the effectiveness of ou...                \n",
       "129  Finally, we analyze the impact of different nu...                \n",
       "130                     Understanding the Behaviors of         BERT   \n",
       "131  This paper studies the performances and behavi...         BERT   \n",
       "132  We explore several different ways to leverage ...         BERT   \n",
       "133  Experimental results on MS MARCO demonstrate t...         BERT   \n",
       "134  Experimental results on TREC show the gaps bet...         BERT   \n",
       "135                            Analyses illustrate how         BERT   \n",
       "136                                             Simple         BERT   \n",
       "137                                  We present simple   BERT-based   \n",
       "138  In recent years, state-of-the-art performance ...                \n",
       "139  In this paper, extensive experiments on datase...   BERT-based   \n",
       "140  To our knowledge, we are the first to successf...         BERT   \n",
       "141  Our models provide strong baselines for future...                \n",
       "142                                                            BERT   \n",
       "143  Multi-task learning allows the sharing of usef...                \n",
       "144  In natural language processing several recent ...                \n",
       "145  These results are based on fine-tuning on each...                \n",
       "146  We explore the multi-task learning setting for...         BERT   \n",
       "147  We introduce new adaptation modules, PALs or `...                \n",
       "148                     By using PALs in parallel with         BERT   \n",
       "149                                               What         BERT   \n",
       "150  Pre-training by language modeling has become a...                \n",
       "151  In this paper we introduce a suite of diagnost...                \n",
       "152  As a case study, we apply these diagnostics to...         BERT   \n",
       "153  Visualizing and Understanding the Effectivenes...         BERT   \n",
       "154               Language model pre-training, such as        BERT,   \n",
       "155  However, it is unclear why the pre-training-th...                \n",
       "156  In this paper, we propose to visualize loss la...         BERT   \n",
       "157  First, we find that pre-training reaches a goo...                \n",
       "158  We also demonstrate that the fine-tuning proce...         BERT   \n",
       "159  Second, the visualization results indicate tha...         BERT   \n",
       "160                         Third, the lower layers of         BERT   \n",
       "161                                        Conditional         BERT   \n",
       "162  Data augmentation methods are often applied to...                \n",
       "163  Recently proposed contextual augmentation augm...                \n",
       "164  Bidirectional Encoder Representations from Tra...       (BERT)   \n",
       "165  We propose a novel data augmentation method fo...         BERT   \n",
       "166                                        We retrofit         BERT   \n",
       "167  In our paper, conditional masked language mod...                \n",
       "168                       The well trained conditional         BERT   \n",
       "169  Experiments on six various different text clas...                \n",
       "170                                Small and Practical         BERT   \n",
       "171  We propose a practical scheme to train a singl...                \n",
       "172                Starting from a public multilingual         BERT   \n",
       "173  We show that our model especially outperforms ...                \n",
       "174  We showcase the effectiveness of our method by...                \n",
       "175                              Data Augmentation for         BERT   \n",
       "176  Recently, a simple combination of passage retr...         BERT   \n",
       "177  In this paper, we present a data augmentation ...                \n",
       "178      We apply a stage-wise approach to fine tuning         BERT   \n",
       "179  Experimental results show large gains in effec...                \n",
       "180                                                            BERT   \n",
       "181                                                The         BERT   \n",
       "182   Petroni et al. (2019) take this as evidence that         BERT   \n",
       "183  We take issue with this interpretation and arg...         BERT   \n",
       "184                    More specifically, we show that       BERT's   \n",
       "185                            As a remedy, we propose      E-BERT,   \n",
       "186                                                          E-BERT   \n",
       "187                      We take this as evidence that       E-BERT   \n",
       "188  How Contextual are Contextualized Word Represe...        BERT,   \n",
       "189  Replacing static word embeddings with contextu...                \n",
       "190  However, just how contextual are the contextua...        BERT?   \n",
       "191  Are there infinitely many context-specific rep...                \n",
       "192  For one, we find that the contextualized repre...                \n",
       "193  While representations of the same word in diff...                \n",
       "194  This suggests that upper layers of contextuali...                \n",
       "195                             In all layers of ELMo,        BERT,   \n",
       "\n",
       "                                          text_split_2  \n",
       "0    Pre-training of Deep Bidirectional Transformer...  \n",
       "1    which stands for Bidirectional Encoder Represe...  \n",
       "2    is designed to pre-train deep bidirectional re...  \n",
       "3    model can be fine-tuned with just one addition...  \n",
       "4    is conceptually simple and empirically powerful.   \n",
       "5                                                       \n",
       "6       A Robustly Optimized BERT Pretraining Approach  \n",
       "7                                                       \n",
       "8                                                       \n",
       "9    pretraining (Devlin et al., 2019) that careful...  \n",
       "10   was significantly undertrained, and can match ...  \n",
       "11                                                      \n",
       "12                                                      \n",
       "13                                                      \n",
       "14   a distilled version of BERT: smaller, faster, ...  \n",
       "15                                                      \n",
       "16   which can then be fine-tuned with good perform...  \n",
       "17   model by 40%, while retaining 97% of its langu...  \n",
       "18                                                      \n",
       "19                                                      \n",
       "20              Rediscovers the Classical NLP Pipeline  \n",
       "21                                                      \n",
       "22   and aim to quantify where linguistic informati...  \n",
       "23                                                      \n",
       "24                                                      \n",
       "25            Look At? An Analysis of BERT's Attention  \n",
       "26   have had great recent success in NLP, motivati...  \n",
       "27                                                      \n",
       "28                                                      \n",
       "29   attention heads exhibit patterns such as atten...  \n",
       "30                                                      \n",
       "31                                                      \n",
       "32                                          attention.  \n",
       "33                                                      \n",
       "34   (Devlin et al., 2018), have achieved impressiv...  \n",
       "35                for query-based passage re-ranking.   \n",
       "36                                                      \n",
       "37                                                      \n",
       "38                                 Syntactic Abilities  \n",
       "39   model captures English syntactic phenomena, us...  \n",
       "40        model performs remarkably well on all cases.  \n",
       "41                                                      \n",
       "42                                                      \n",
       "43   (Devlin, 2018) includes a model simultaneously...  \n",
       "44   (multilingual) as a zero shot language transfe...  \n",
       "45   with the best-published methods for zero-shot ...  \n",
       "46   in this manner, determine to what extent mBERT...  \n",
       "47   Distilling BERT for Natural Language Understan...  \n",
       "48   has significantly improved the performances of...  \n",
       "49                                                      \n",
       "50                                                      \n",
       "51   can be well transferred to a small student Tin...  \n",
       "52   which performs transformer distillation at bot...  \n",
       "53   can capture both the general-domain and task-s...  \n",
       "54   is empirically effective and achieves comparab...  \n",
       "55   is also significantly better than state-of-the...  \n",
       "56                        for Extractive Summarization  \n",
       "57   a pre-trained Transformer model, has achieved ...  \n",
       "58   a simple variant of BERT, for extractive summa...  \n",
       "59                                                      \n",
       "60                                                      \n",
       "61   Post-Training for Review Reading Comprehension...  \n",
       "62                                                      \n",
       "63                                                      \n",
       "64                                                      \n",
       "65                                                      \n",
       "66   to enhance the performance of fine-tuning of B...  \n",
       "67                                                      \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                            for Text Classification?  \n",
       "71                                                      \n",
       "72   (Bidirectional Encoder Representations from Tr...  \n",
       "73   on text classification task and provide a gene...  \n",
       "74                                                      \n",
       "75   has a Mouth, and It Must Speak: BERT as a Mark...  \n",
       "76   (Devlin et al., 2018) is a Markov random field...  \n",
       "77                                                      \n",
       "78   and find that it can produce high-quality, flu...  \n",
       "79   generates sentences that are more diverse but ...  \n",
       "80   for Aspect-Based Sentiment Analysis via Constr...  \n",
       "81                                                      \n",
       "82                                                      \n",
       "83   and achieve new state-of-the-art results on Se...  \n",
       "84                       for Ad Hoc Document Retrieval  \n",
       "85   to question answering, we explore simple appli...  \n",
       "86                            was designed to handle.   \n",
       "87                                                      \n",
       "88                                                      \n",
       "89                                                      \n",
       "90   architectures currently give state-of-the-art ...  \n",
       "91                                                      \n",
       "92                                             heads.   \n",
       "93                                                      \n",
       "94                                                      \n",
       "95                                              models  \n",
       "96                                                      \n",
       "97   has shown marvelous improvements across variou...  \n",
       "98   has been released with Whole Word Masking (WWM...  \n",
       "99                                                      \n",
       "100                                                     \n",
       "101  without changing any neural architecture or ev...  \n",
       "102                                                     \n",
       "103                                                     \n",
       "104                                  ERNIE, BERT-wwm.   \n",
       "105                                                     \n",
       "106  A Globally Normalized BERT Model for Open-doma...  \n",
       "107  model has been successfully applied to open-do...  \n",
       "108  by viewing passages corresponding to the same ...  \n",
       "109  model to globally normalize answer scores acro...  \n",
       "110                                                     \n",
       "111                              gains additional 2%.   \n",
       "112  outperforms all state-of-the-art models on all...  \n",
       "113  models, and 5.8% EM and 6.5% $F_1$ over BERT-b...  \n",
       "114   for Joint Intent Classification and Slot Filling  \n",
       "115                                                     \n",
       "116                                                     \n",
       "117  (Bidirectional Encoder Representations from Tr...  \n",
       "118               for natural language understanding.   \n",
       "119                                                     \n",
       "120                                                     \n",
       "121  with History Answer Embedding for Conversation...  \n",
       "122                                                     \n",
       "123                                                     \n",
       "124                                                     \n",
       "125                                                     \n",
       "126  (Bidirectional Encoder Representations from Tr...  \n",
       "127                                                     \n",
       "128                                                     \n",
       "129                                                     \n",
       "130                                         in Ranking  \n",
       "131                                 in ranking tasks.   \n",
       "132  and fine-tune it on two ranking tasks: MS MARC...  \n",
       "133  in question-answering focused passage ranking ...  \n",
       "134  pre-trained on surrounding contexts and the ne...  \n",
       "135  allocates its attentions between query-documen...  \n",
       "136  Models for Relation Extraction and Semantic Ro...  \n",
       "137  models for relation extraction and semantic ro...  \n",
       "138                                                     \n",
       "139   model can achieve state-of-the-art performance.   \n",
       "140                                   in this manner.   \n",
       "141                                                     \n",
       "142  and PALs: Projected Attention Layers for Effic...  \n",
       "143                                                     \n",
       "144                                                     \n",
       "145                                                     \n",
       "146  model on the GLUE benchmark, and how to best a...  \n",
       "147                                                     \n",
       "148  layers, we match the performance of fine-tuned...  \n",
       "149  Is Not: Lessons from a New Suite of Psycholing...  \n",
       "150                                                     \n",
       "151                                                     \n",
       "152  model, finding that it can generally distingui...  \n",
       "153                                                     \n",
       "154  has achieved remarkable results in many NLP ta...  \n",
       "155                                                     \n",
       "156                             on specific datasets.   \n",
       "157                                                     \n",
       "158  is highly over-parameterized for downstream ta...  \n",
       "159  tends to generalize better because of the flat...  \n",
       "160  are more invariant during fine-tuning, which s...  \n",
       "161                            Contextual Augmentation  \n",
       "162                                                     \n",
       "163                                                     \n",
       "164  demonstrates that a deep bidirectional languag...  \n",
       "165                          contextual augmentation.   \n",
       "166  to conditional BERT by introducing a new condi...  \n",
       "167                                                     \n",
       "168  can be applied to enhance contextual augmentat...  \n",
       "169                                                     \n",
       "170                       Models for Sequence Labeling  \n",
       "171                                                     \n",
       "172  checkpoint, our final model is 6x smaller and ...  \n",
       "173                                                     \n",
       "174                                                     \n",
       "175      Fine-Tuning in Open-Domain Question Answering  \n",
       "176  reader was found to be very effective for ques...  \n",
       "177                                                     \n",
       "178  on multiple datasets, starting with data that ...  \n",
       "179                                                     \n",
       "180  is Not a Knowledge Base (Yet): Factual Knowled...  \n",
       "181  language model (LM) (Devlin et al., 2019) is s...  \n",
       "182  memorizes factual knowledge during pre-training.   \n",
       "183  is partly due to reasoning about (the surface ...  \n",
       "184  precision drops dramatically when we filter ce...  \n",
       "185  an extension of BERT that replaces entity ment...  \n",
       "186  outperforms both BERT and ERNIE (Zhang et al.,...  \n",
       "187  is richer in factual knowledge, and we show tw...  \n",
       "188                         ELMo, and GPT-2 Embeddings  \n",
       "189                                                     \n",
       "190                                                     \n",
       "191                                                     \n",
       "192                                                     \n",
       "193                                                     \n",
       "194                                                     \n",
       "195  and GPT-2, on average, less than 5% of the var...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input = df.apply(lambda row: split_search_term_whitespace(row, search_word), axis=1, result_type='expand')\n",
    "\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>text_split_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>(Bidirectional</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>(Bidirectional</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>(Bidirectional</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(Devlin</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>(Devlin</td>\n",
       "      <td>(Devlin et al., 2018) is a Markov random field...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(Devlin,</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(multilingual)</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>A</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Contextual</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Distilling</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>ELMo,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ERNIE,</td>\n",
       "      <td>ERNIE, BERT-wwm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Fine-Tuning</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Is</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Look</td>\n",
       "      <td>Look At? An Analysis of BERT's Attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Models</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Models</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Post-Training</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pre-training</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Rediscovers</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Syntactic</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>a</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>a</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>allocates</td>\n",
       "      <td>allocates its attentions between query-documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>an</td>\n",
       "      <td>an extension of BERT that replaces entity ment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>and</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>and</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>and</td>\n",
       "      <td>and GPT-2, on average, less than 5% of the var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>and</td>\n",
       "      <td>and find that it can produce high-quality, flu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>and</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>and</td>\n",
       "      <td>and aim to quantify where linguistic informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>architectures</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>are</td>\n",
       "      <td>are more invariant during fine-tuning, which s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>attention</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>attention.</td>\n",
       "      <td>attention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>by</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>can</td>\n",
       "      <td>can be well transferred to a small student Tin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>can</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>can</td>\n",
       "      <td>can capture both the general-domain and task-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>checkpoint,</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>contextual</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>demonstrates</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>for</td>\n",
       "      <td>for natural language understanding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>for</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>for</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>for</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>for</td>\n",
       "      <td>for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>for</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>for</td>\n",
       "      <td>for query-based passage re-ranking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>gains</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>generates</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>has</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>has</td>\n",
       "      <td>has a Mouth, and It Must Speak: BERT as a Mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>has</td>\n",
       "      <td>has achieved remarkable results in many NLP ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>has</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>has</td>\n",
       "      <td>has been released with Whole Word Masking (WWM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>have</td>\n",
       "      <td>have had great recent success in NLP, motivati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>heads.</td>\n",
       "      <td>heads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>in</td>\n",
       "      <td>in this manner, determine to what extent mBERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>in</td>\n",
       "      <td>in question-answering focused passage ranking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>in</td>\n",
       "      <td>in ranking tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>in</td>\n",
       "      <td>in this manner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>in</td>\n",
       "      <td>in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>is</td>\n",
       "      <td>is highly over-parameterized for downstream ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>is designed to pre-train deep bidirectional re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>is conceptually simple and empirically powerful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>is</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>is</td>\n",
       "      <td>is partly due to reasoning about (the surface ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>is</td>\n",
       "      <td>is also significantly better than state-of-the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>is</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>is</td>\n",
       "      <td>is richer in factual knowledge, and we show tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>language</td>\n",
       "      <td>language model (LM) (Devlin et al., 2019) is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>layers,</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>memorizes</td>\n",
       "      <td>memorizes factual knowledge during pre-training.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>model</td>\n",
       "      <td>model has been successfully applied to open-do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>model</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>model</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>model can be fine-tuned with just one addition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>model</td>\n",
       "      <td>model performs remarkably well on all cases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>model</td>\n",
       "      <td>model captures English syntactic phenomena, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>model</td>\n",
       "      <td>model by 40%, while retaining 97% of its langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>model</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>model,</td>\n",
       "      <td>model, finding that it can generally distingui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>models</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>models</td>\n",
       "      <td>models for relation extraction and semantic ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>models,</td>\n",
       "      <td>models, and 5.8% EM and 6.5% $F_1$ over BERT-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>on</td>\n",
       "      <td>on specific datasets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>on</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>on</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>outperforms</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>outperforms</td>\n",
       "      <td>outperforms both BERT and ERNIE (Zhang et al.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>pre-trained</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>precision</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pretraining</td>\n",
       "      <td>pretraining (Devlin et al., 2019) that careful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>reader</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>tends</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>to</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>to</td>\n",
       "      <td>to question answering, we explore simple appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>to</td>\n",
       "      <td>to enhance the performance of fine-tuning of B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>was</td>\n",
       "      <td>was designed to handle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>was</td>\n",
       "      <td>was significantly undertrained, and can match ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>which</td>\n",
       "      <td>which performs transformer distillation at bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>which</td>\n",
       "      <td>which stands for Bidirectional Encoder Represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>which</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>with</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>with</td>\n",
       "      <td>with the best-published methods for zero-shot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>without</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              group                                       text_split_2\n",
       "115                                                                   \n",
       "74                                                                    \n",
       "145                                                                   \n",
       "71                                                                    \n",
       "147                                                                   \n",
       "69                                                                    \n",
       "68                                                                    \n",
       "67                                                                    \n",
       "65                                                                    \n",
       "64                                                                    \n",
       "144                                                                   \n",
       "63                                                                    \n",
       "60                                                                    \n",
       "59                                                                    \n",
       "122                                                                   \n",
       "116                                                                   \n",
       "150                                                                   \n",
       "151                                                                   \n",
       "153                                                                   \n",
       "155                                                                   \n",
       "50                                                                    \n",
       "62                                                                    \n",
       "77                                                                    \n",
       "143                                                                   \n",
       "123                                                                   \n",
       "103                                                                   \n",
       "102                                                                   \n",
       "125                                                                   \n",
       "100                                                                   \n",
       "99                                                                    \n",
       "194                                                                   \n",
       "96                                                                    \n",
       "127                                                                   \n",
       "94                                                                    \n",
       "93                                                                    \n",
       "128                                                                   \n",
       "91                                                                    \n",
       "129                                                                   \n",
       "89                                                                    \n",
       "88                                                                    \n",
       "87                                                                    \n",
       "124                                                                   \n",
       "138                                                                   \n",
       "82                                                                    \n",
       "81                                                                    \n",
       "141                                                                   \n",
       "49                                                                    \n",
       "162                                                                   \n",
       "157                                                                   \n",
       "163                                                                   \n",
       "177                                                                   \n",
       "21                                                                    \n",
       "120                                                                   \n",
       "19                                                                    \n",
       "18                                                                    \n",
       "179                                                                   \n",
       "119                                                                   \n",
       "15                                                                    \n",
       "13                                                                    \n",
       "12                                                                    \n",
       "11                                                                    \n",
       "189                                                                   \n",
       "8                                                                     \n",
       "7                                                                     \n",
       "5                                                                     \n",
       "190                                                                   \n",
       "191                                                                   \n",
       "192                                                                   \n",
       "193                                                                   \n",
       "23                                                                    \n",
       "27                                                                    \n",
       "24                                                                    \n",
       "105                                                                   \n",
       "171                                                                   \n",
       "37                                                                    \n",
       "110                                                                   \n",
       "33                                                                    \n",
       "169                                                                   \n",
       "36                                                                    \n",
       "173                                                                   \n",
       "30                                                                    \n",
       "41                                                                    \n",
       "42                                                                    \n",
       "167                                                                   \n",
       "174                                                                   \n",
       "31                                                                    \n",
       "28                                                                    \n",
       "126  (Bidirectional  (Bidirectional Encoder Representations from Tr...\n",
       "117  (Bidirectional  (Bidirectional Encoder Representations from Tr...\n",
       "72   (Bidirectional  (Bidirectional Encoder Representations from Tr...\n",
       "34          (Devlin  (Devlin et al., 2018), have achieved impressiv...\n",
       "76          (Devlin  (Devlin et al., 2018) is a Markov random field...\n",
       "43         (Devlin,  (Devlin, 2018) includes a model simultaneously...\n",
       "44   (multilingual)  (multilingual) as a zero shot language transfe...\n",
       "6                 A     A Robustly Optimized BERT Pretraining Approach\n",
       "106               A  A Globally Normalized BERT Model for Open-doma...\n",
       "161      Contextual                            Contextual Augmentation\n",
       "47       Distilling  Distilling BERT for Natural Language Understan...\n",
       "188           ELMo,                         ELMo, and GPT-2 Embeddings\n",
       "104          ERNIE,                                  ERNIE, BERT-wwm. \n",
       "175     Fine-Tuning      Fine-Tuning in Open-Domain Question Answering\n",
       "149              Is  Is Not: Lessons from a New Suite of Psycholing...\n",
       "25             Look           Look At? An Analysis of BERT's Attention\n",
       "136          Models  Models for Relation Extraction and Semantic Ro...\n",
       "170          Models                       Models for Sequence Labeling\n",
       "61    Post-Training  Post-Training for Review Reading Comprehension...\n",
       "0      Pre-training  Pre-training of Deep Bidirectional Transformer...\n",
       "20      Rediscovers             Rediscovers the Classical NLP Pipeline\n",
       "38        Syntactic                                Syntactic Abilities\n",
       "58                a  a simple variant of BERT, for extractive summa...\n",
       "14                a  a distilled version of BERT: smaller, faster, ...\n",
       "57                a  a pre-trained Transformer model, has achieved ...\n",
       "135       allocates  allocates its attentions between query-documen...\n",
       "185              an  an extension of BERT that replaces entity ment...\n",
       "142             and  and PALs: Projected Attention Layers for Effic...\n",
       "132             and  and fine-tune it on two ranking tasks: MS MARC...\n",
       "195             and  and GPT-2, on average, less than 5% of the var...\n",
       "78              and  and find that it can produce high-quality, flu...\n",
       "83              and  and achieve new state-of-the-art results on Se...\n",
       "22              and  and aim to quantify where linguistic informati...\n",
       "90    architectures  architectures currently give state-of-the-art ...\n",
       "160             are  are more invariant during fine-tuning, which s...\n",
       "29        attention  attention heads exhibit patterns such as atten...\n",
       "32       attention.                                         attention.\n",
       "108              by  by viewing passages corresponding to the same ...\n",
       "51              can  can be well transferred to a small student Tin...\n",
       "168             can  can be applied to enhance contextual augmentat...\n",
       "53              can  can capture both the general-domain and task-s...\n",
       "172     checkpoint,  checkpoint, our final model is 6x smaller and ...\n",
       "165      contextual                          contextual augmentation. \n",
       "164    demonstrates  demonstrates that a deep bidirectional languag...\n",
       "118             for               for natural language understanding. \n",
       "80              for  for Aspect-Based Sentiment Analysis via Constr...\n",
       "114             for   for Joint Intent Classification and Slot Filling\n",
       "56              for                       for Extractive Summarization\n",
       "70              for                           for Text Classification?\n",
       "84              for                      for Ad Hoc Document Retrieval\n",
       "35              for               for query-based passage re-ranking. \n",
       "111           gains                              gains additional 2%. \n",
       "79        generates  generates sentences that are more diverse but ...\n",
       "48              has  has significantly improved the performances of...\n",
       "75              has  has a Mouth, and It Must Speak: BERT as a Mark...\n",
       "154             has  has achieved remarkable results in many NLP ta...\n",
       "97              has  has shown marvelous improvements across variou...\n",
       "98              has  has been released with Whole Word Masking (WWM...\n",
       "26             have  have had great recent success in NLP, motivati...\n",
       "92           heads.                                            heads. \n",
       "46               in  in this manner, determine to what extent mBERT...\n",
       "133              in  in question-answering focused passage ranking ...\n",
       "131              in                                 in ranking tasks. \n",
       "140              in                                   in this manner. \n",
       "130              in                                         in Ranking\n",
       "158              is  is highly over-parameterized for downstream ta...\n",
       "2                is  is designed to pre-train deep bidirectional re...\n",
       "4                is  is conceptually simple and empirically powerful. \n",
       "180              is  is Not a Knowledge Base (Yet): Factual Knowled...\n",
       "183              is  is partly due to reasoning about (the surface ...\n",
       "55               is  is also significantly better than state-of-the...\n",
       "54               is  is empirically effective and achieves comparab...\n",
       "187              is  is richer in factual knowledge, and we show tw...\n",
       "181        language  language model (LM) (Devlin et al., 2019) is s...\n",
       "148         layers,  layers, we match the performance of fine-tuned...\n",
       "182       memorizes  memorizes factual knowledge during pre-training. \n",
       "107           model  model has been successfully applied to open-do...\n",
       "146           model  model on the GLUE benchmark, and how to best a...\n",
       "109           model  model to globally normalize answer scores acro...\n",
       "3             model  model can be fine-tuned with just one addition...\n",
       "40            model       model performs remarkably well on all cases.\n",
       "39            model  model captures English syntactic phenomena, us...\n",
       "17            model  model by 40%, while retaining 97% of its langu...\n",
       "139           model   model can achieve state-of-the-art performance. \n",
       "152          model,  model, finding that it can generally distingui...\n",
       "95           models                                             models\n",
       "137          models  models for relation extraction and semantic ro...\n",
       "113         models,  models, and 5.8% EM and 6.5% $F_1$ over BERT-b...\n",
       "156              on                             on specific datasets. \n",
       "178              on  on multiple datasets, starting with data that ...\n",
       "73               on  on text classification task and provide a gene...\n",
       "112     outperforms  outperforms all state-of-the-art models on all...\n",
       "186     outperforms  outperforms both BERT and ERNIE (Zhang et al.,...\n",
       "134     pre-trained  pre-trained on surrounding contexts and the ne...\n",
       "184       precision  precision drops dramatically when we filter ce...\n",
       "9       pretraining  pretraining (Devlin et al., 2019) that careful...\n",
       "176          reader  reader was found to be very effective for ques...\n",
       "159           tends  tends to generalize better because of the flat...\n",
       "166              to  to conditional BERT by introducing a new condi...\n",
       "85               to  to question answering, we explore simple appli...\n",
       "66               to  to enhance the performance of fine-tuning of B...\n",
       "86              was                           was designed to handle. \n",
       "10              was  was significantly undertrained, and can match ...\n",
       "52            which  which performs transformer distillation at bot...\n",
       "1             which  which stands for Bidirectional Encoder Represe...\n",
       "16            which  which can then be fine-tuned with good perform...\n",
       "121            with  with History Answer Embedding for Conversation...\n",
       "45             with  with the best-published methods for zero-shot ...\n",
       "101         without  without changing any neural architecture or ev..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group on the first word (regardless of part of speech) that comes after the anchor point\n",
    "def group_first_word(row):\n",
    "    grouped = [row['text_split_2'].split(' ')[0], row['text_split_2']]\n",
    "    return dict(zip(['group', 'text_split_2'], grouped))\n",
    "\n",
    "sample_input.apply(lambda row: group_first_word(row), axis=1, result_type='expand').sort_values(by=['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cephcyn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>text_split_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td></td>\n",
       "      <td>heads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td></td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td></td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td></td>\n",
       "      <td>for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td></td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td></td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td></td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td></td>\n",
       "      <td>for natural language understanding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td></td>\n",
       "      <td>models, and 5.8% EM and 6.5% $F_1$ over BERT-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td></td>\n",
       "      <td>models for relation extraction and semantic ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td></td>\n",
       "      <td>gains additional 2%.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td></td>\n",
       "      <td>in this manner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td></td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td></td>\n",
       "      <td>ERNIE, BERT-wwm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td></td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td></td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>Look At? An Analysis of BERT's Attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td></td>\n",
       "      <td>outperforms both BERT and ERNIE (Zhang et al.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td></td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td></td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td></td>\n",
       "      <td>on specific datasets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td></td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td></td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td></td>\n",
       "      <td>contextual augmentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td>attention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td></td>\n",
       "      <td>model performs remarkably well on all cases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td></td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td></td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td>for query-based passage re-ranking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Is</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Labeling</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Projected</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Ranking</td>\n",
       "      <td>in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>achieve</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>achieve</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>allocates</td>\n",
       "      <td>allocates its attentions between query-documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>answering</td>\n",
       "      <td>to question answering, we explore simple appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>are</td>\n",
       "      <td>are more invariant during fine-tuning, which s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>be</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>be</td>\n",
       "      <td>and GPT-2, on average, less than 5% of the var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>be</td>\n",
       "      <td>model can be fine-tuned with just one addition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>be</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>be</td>\n",
       "      <td>can be well transferred to a small student Tin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>best</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>capture</td>\n",
       "      <td>can capture both the general-domain and task-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>changing</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>conditional</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>covering</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>demonstrates</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>enhance</td>\n",
       "      <td>to enhance the performance of fine-tuning of B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>exhibit</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>facilitates</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>filter</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>find</td>\n",
       "      <td>with the best-published methods for zero-shot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>find</td>\n",
       "      <td>and find that it can produce high-quality, flu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>finding</td>\n",
       "      <td>model, finding that it can generally distingui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>generalizes</td>\n",
       "      <td>in this manner, determine to what extent mBERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>generates</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>give</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>has</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>has</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>has</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>has</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>has</td>\n",
       "      <td>has achieved remarkable results in many NLP ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>has</td>\n",
       "      <td>model has been successfully applied to open-do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>has</td>\n",
       "      <td>has been released with Whole Word Masking (WWM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>has</td>\n",
       "      <td>has a Mouth, and It Must Speak: BERT as a Mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>have</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>have</td>\n",
       "      <td>have had great recent success in NLP, motivati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>includes</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>is</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>is</td>\n",
       "      <td>(Devlin et al., 2018) is a Markov random field...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>is designed to pre-train deep bidirectional re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>is conceptually simple and empirically powerful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>is</td>\n",
       "      <td>is highly over-parameterized for downstream ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>is</td>\n",
       "      <td>is richer in factual knowledge, and we show tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>is</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>is</td>\n",
       "      <td>language model (LM) (Devlin et al., 2019) is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>is</td>\n",
       "      <td>is also significantly better than state-of-the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>is</td>\n",
       "      <td>is partly due to reasoning about (the surface ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>is</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>match</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>measures</td>\n",
       "      <td>pretraining (Devlin et al., 2019) that careful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>memorizes</td>\n",
       "      <td>memorizes factual knowledge during pre-training.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>normalize</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>outperforms</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>performs</td>\n",
       "      <td>which performs transformer distillation at bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>provide</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>quantify</td>\n",
       "      <td>and aim to quantify where linguistic informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>ranking</td>\n",
       "      <td>in ranking tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>ranking</td>\n",
       "      <td>in question-answering focused passage ranking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ranking</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>replaces</td>\n",
       "      <td>an extension of BERT that replaces entity ment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>retaining</td>\n",
       "      <td>model by 40%, while retaining 97% of its langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stands</td>\n",
       "      <td>which stands for Bidirectional Encoder Represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>starting</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>surrounding</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>tends</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>using</td>\n",
       "      <td>model captures English syntactic phenomena, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>viewing</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>was</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>was</td>\n",
       "      <td>was significantly undertrained, and can match ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>was</td>\n",
       "      <td>was designed to handle.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            group                                       text_split_2\n",
       "0                  Pre-training of Deep Bidirectional Transformer...\n",
       "94                                                                  \n",
       "93                                                                  \n",
       "92                                                           heads. \n",
       "91                                                                  \n",
       "143                                                                 \n",
       "89                                                                  \n",
       "88                                                                  \n",
       "87                                                                  \n",
       "144                                                                 \n",
       "84                                     for Ad Hoc Document Retrieval\n",
       "82                                                                  \n",
       "81                                                                  \n",
       "80                 for Aspect-Based Sentiment Analysis via Constr...\n",
       "145                                                                 \n",
       "147                                                                 \n",
       "77                                                                  \n",
       "150                                                                 \n",
       "74                                                                  \n",
       "151                                                                 \n",
       "153                                                                 \n",
       "71                                                                  \n",
       "70                                          for Text Classification?\n",
       "69                                                                  \n",
       "68                                                                  \n",
       "67                                                                  \n",
       "155                                                                 \n",
       "64                                                                  \n",
       "95                                                            models\n",
       "96                                                                  \n",
       "194                                                                 \n",
       "99                                                                  \n",
       "129                                                                 \n",
       "128                                                                 \n",
       "127                                                                 \n",
       "126                (Bidirectional Encoder Representations from Tr...\n",
       "125                                                                 \n",
       "124                                                                 \n",
       "123                                                                 \n",
       "122                                                                 \n",
       "121                with History Answer Embedding for Conversation...\n",
       "120                                                                 \n",
       "119                                                                 \n",
       "118                             for natural language understanding. \n",
       "116                                                                 \n",
       "63                                                                  \n",
       "115                                                                 \n",
       "113                models, and 5.8% EM and 6.5% $F_1$ over BERT-b...\n",
       "137                models for relation extraction and semantic ro...\n",
       "111                                            gains additional 2%. \n",
       "110                                                                 \n",
       "138                                                                 \n",
       "140                                                 in this manner. \n",
       "106                A Globally Normalized BERT Model for Open-doma...\n",
       "105                                                                 \n",
       "104                                                ERNIE, BERT-wwm. \n",
       "103                                                                 \n",
       "102                                                                 \n",
       "141                                                                 \n",
       "100                                                                 \n",
       "114                 for Joint Intent Classification and Slot Filling\n",
       "62                                                                  \n",
       "65                                                                  \n",
       "60                                                                  \n",
       "174                                                                 \n",
       "28                                                                  \n",
       "27                                                                  \n",
       "175                    Fine-Tuning in Open-Domain Question Answering\n",
       "25                          Look At? An Analysis of BERT's Attention\n",
       "24                                                                  \n",
       "23                                                                  \n",
       "177                                                                 \n",
       "21                                                                  \n",
       "20                            Rediscovers the Classical NLP Pipeline\n",
       "19                                                                  \n",
       "18                                                                  \n",
       "179                                                                 \n",
       "186                outperforms both BERT and ERNIE (Zhang et al.,...\n",
       "15                                                                  \n",
       "14                 a distilled version of BERT: smaller, faster, ...\n",
       "13                                                                  \n",
       "12                                                                  \n",
       "11                                                                  \n",
       "188                                       ELMo, and GPT-2 Embeddings\n",
       "189                                                                 \n",
       "8                                                                   \n",
       "7                                                                   \n",
       "6                     A Robustly Optimized BERT Pretraining Approach\n",
       "5                                                                   \n",
       "190                                                                 \n",
       "191                                                                 \n",
       "192                                                                 \n",
       "193                                                                 \n",
       "61                 Post-Training for Review Reading Comprehension...\n",
       "31                                                                  \n",
       "30                                                                  \n",
       "33                                                                  \n",
       "59                                                                  \n",
       "58                 a simple variant of BERT, for extractive summa...\n",
       "156                                           on specific datasets. \n",
       "56                                      for Extractive Summarization\n",
       "157                                                                 \n",
       "161                                          Contextual Augmentation\n",
       "162                                                                 \n",
       "163                                                                 \n",
       "50                                                                  \n",
       "49                                                                  \n",
       "165                                        contextual augmentation. \n",
       "47                 Distilling BERT for Natural Language Understan...\n",
       "32                                                        attention.\n",
       "169                                                                 \n",
       "167                                                                 \n",
       "171                                                                 \n",
       "42                                                                  \n",
       "173                                                                 \n",
       "41                                                                  \n",
       "40                      model performs remarkably well on all cases.\n",
       "170                                     Models for Sequence Labeling\n",
       "38                                               Syntactic Abilities\n",
       "37                                                                  \n",
       "36                                                                  \n",
       "35                              for query-based passage re-ranking. \n",
       "149            Is  Is Not: Lessons from a New Suite of Psycholing...\n",
       "136      Labeling  Models for Relation Extraction and Semantic Ro...\n",
       "142     Projected  and PALs: Projected Attention Layers for Effic...\n",
       "130       Ranking                                         in Ranking\n",
       "83        achieve  and achieve new state-of-the-art results on Se...\n",
       "139       achieve   model can achieve state-of-the-art performance. \n",
       "135     allocates  allocates its attentions between query-documen...\n",
       "85      answering  to question answering, we explore simple appli...\n",
       "160           are  are more invariant during fine-tuning, which s...\n",
       "168            be  can be applied to enhance contextual augmentat...\n",
       "195            be  and GPT-2, on average, less than 5% of the var...\n",
       "3              be  model can be fine-tuned with just one addition...\n",
       "16             be  which can then be fine-tuned with good perform...\n",
       "51             be  can be well transferred to a small student Tin...\n",
       "146          best  model on the GLUE benchmark, and how to best a...\n",
       "53        capture  can capture both the general-domain and task-s...\n",
       "101      changing  without changing any neural architecture or ev...\n",
       "166   conditional  to conditional BERT by introducing a new condi...\n",
       "44       covering  (multilingual) as a zero shot language transfe...\n",
       "164  demonstrates  demonstrates that a deep bidirectional languag...\n",
       "66        enhance  to enhance the performance of fine-tuning of B...\n",
       "29        exhibit  attention heads exhibit patterns such as atten...\n",
       "117   facilitates  (Bidirectional Encoder Representations from Tr...\n",
       "184        filter  precision drops dramatically when we filter ce...\n",
       "45           find  with the best-published methods for zero-shot ...\n",
       "78           find  and find that it can produce high-quality, flu...\n",
       "152       finding  model, finding that it can generally distingui...\n",
       "46    generalizes  in this manner, determine to what extent mBERT...\n",
       "79      generates  generates sentences that are more diverse but ...\n",
       "90           give  architectures currently give state-of-the-art ...\n",
       "48            has  has significantly improved the performances of...\n",
       "57            has  a pre-trained Transformer model, has achieved ...\n",
       "97            has  has shown marvelous improvements across variou...\n",
       "72            has  (Bidirectional Encoder Representations from Tr...\n",
       "154           has  has achieved remarkable results in many NLP ta...\n",
       "107           has  model has been successfully applied to open-do...\n",
       "98            has  has been released with Whole Word Masking (WWM...\n",
       "75            has  has a Mouth, and It Must Speak: BERT as a Mark...\n",
       "34           have  (Devlin et al., 2018), have achieved impressiv...\n",
       "26           have  have had great recent success in NLP, motivati...\n",
       "43       includes  (Devlin, 2018) includes a model simultaneously...\n",
       "172            is  checkpoint, our final model is 6x smaller and ...\n",
       "76             is  (Devlin et al., 2018) is a Markov random field...\n",
       "2              is  is designed to pre-train deep bidirectional re...\n",
       "4              is  is conceptually simple and empirically powerful. \n",
       "158            is  is highly over-parameterized for downstream ta...\n",
       "187            is  is richer in factual knowledge, and we show tw...\n",
       "180            is  is Not a Knowledge Base (Yet): Factual Knowled...\n",
       "181            is  language model (LM) (Devlin et al., 2019) is s...\n",
       "55             is  is also significantly better than state-of-the...\n",
       "183            is  is partly due to reasoning about (the surface ...\n",
       "54             is  is empirically effective and achieves comparab...\n",
       "148         match  layers, we match the performance of fine-tuned...\n",
       "9        measures  pretraining (Devlin et al., 2019) that careful...\n",
       "182     memorizes  memorizes factual knowledge during pre-training. \n",
       "109     normalize  model to globally normalize answer scores acro...\n",
       "112   outperforms  outperforms all state-of-the-art models on all...\n",
       "52       performs  which performs transformer distillation at bot...\n",
       "73        provide  on text classification task and provide a gene...\n",
       "22       quantify  and aim to quantify where linguistic informati...\n",
       "131       ranking                                 in ranking tasks. \n",
       "133       ranking  in question-answering focused passage ranking ...\n",
       "132       ranking  and fine-tune it on two ranking tasks: MS MARC...\n",
       "185      replaces  an extension of BERT that replaces entity ment...\n",
       "17      retaining  model by 40%, while retaining 97% of its langu...\n",
       "1          stands  which stands for Bidirectional Encoder Represe...\n",
       "178      starting  on multiple datasets, starting with data that ...\n",
       "134   surrounding  pre-trained on surrounding contexts and the ne...\n",
       "159         tends  tends to generalize better because of the flat...\n",
       "39          using  model captures English syntactic phenomena, us...\n",
       "108       viewing  by viewing passages corresponding to the same ...\n",
       "176           was  reader was found to be very effective for ques...\n",
       "10            was  was significantly undertrained, and can match ...\n",
       "86            was                           was designed to handle. "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Group on the first verb that comes after the anchor point, using NLTK part-of-speech tagging\n",
    "def group_first_verb(row):\n",
    "    tokens = [nltk.word_tokenize(row['text_split_0']), \n",
    "              nltk.word_tokenize(row['text_split_1']),\n",
    "              nltk.word_tokenize(row['text_split_2'])]\n",
    "    tokens_pos = nltk.pos_tag([item for sublist in tokens for item in sublist])\n",
    "    verb = ''\n",
    "    for i in range(len(tokens[0])+len(tokens[1]), len(tokens_pos)):\n",
    "        if tokens_pos[i][1].startswith('V'):\n",
    "            verb = tokens_pos[i][0]\n",
    "            break\n",
    "    grouped = [verb, row['text_split_2']]\n",
    "    return dict(zip(['group', 'text_split_2'], grouped))\n",
    "\n",
    "sample_input.apply(lambda row: group_first_verb(row), axis=1, result_type='expand').sort_values(by=['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
