{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "search_word = \"BERT\"\n",
    "df = pd.read_csv('data/nlp-align_BERT.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataframe so we can run models without taking impractically long\n",
    "# temp_URL=['abc','abc','abc']\n",
    "# temp_ID=[0,0,0]\n",
    "# temp_Index=[0,1,2]\n",
    "# temp_Text=[\"The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts.\",\n",
    "#   \"Petroni et al. (2019) take this as evidence that it memorizes factual knowledge during pre-training.\",\n",
    "#   \"We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.\"]\n",
    "# temp_Type=['Abstract','Abstract','Abstract']\n",
    "\n",
    "# temp_df = pd.DataFrame.from_dict(\n",
    "#     {'URL': temp_URL, \n",
    "#      'ID': temp_ID, \n",
    "#      'Index': temp_Index,\n",
    "#      'Text': temp_Text,\n",
    "#      'Type': temp_Type\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting functions\n",
    "\n",
    "Assume we have an input dataframe with some number of columns, at least one of which is titled `Text` and is the column containing each sentence of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_headers = ['text_split_0','text_split_1','text_split_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, regardless of whitespace (if search word is A and we have word CAR, it slices it up)\n",
    "def split_search_term_literal(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        output = row['Text'].split(search_word, maxsplit=1)\n",
    "        output.insert(1, search_word)\n",
    "    else:\n",
    "        output = [row['Text'],'','']\n",
    "    return dict(zip(splitting_headers,output))\n",
    "\n",
    "# df = df.join()\n",
    "output = df.apply(lambda row: split_search_term_literal(row, search_word), axis=1, result_type='expand')\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, taking care to only split on whitespace\n",
    "def split_search_term_whitespace(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        tokens = row['Text'].split(' ')\n",
    "        for i in range(len(tokens)):\n",
    "            if search_word in tokens[i]:\n",
    "                output=[' '.join(tokens[:i]), tokens[i], ' '.join(tokens[i+1:])]\n",
    "                break\n",
    "    else:\n",
    "        output = [row['Text'],'','']\n",
    "    return dict(zip(splitting_headers,output))\n",
    "\n",
    "output = df.apply(lambda row: split_search_term_whitespace(row, search_word), axis=1, result_type='expand')\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "\n",
    "# Create dataframe with merged abstracts\n",
    "# we have to build the coref-resolved table first since we want to run coreference resolution over \n",
    "# the entire abstract, not individual sentences\n",
    "df_merged = df.sort_values(by=['Index']).groupby(['ID', 'Type'])['Text'].apply(' '.join).reset_index()\n",
    "output = df_merged.apply(lambda row: predictor.predict(row['Text']), axis=1, result_type='expand')\n",
    "df_merged = df_merged.join(output)\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# transform the output of coreference resolution into something that is more easily manipulated\n",
    "# split it across multiple sentences so each indiv sentence row can still work\n",
    "def reinterpret_coref_clusters(row, search_word, original):\n",
    "    # Create dicts to map full-document to indiv sentence data\n",
    "    src = original.loc[original['ID'] == row['ID']].loc[original['Type'] == row['Type']]['Text']\n",
    "    curr_sentence = 0\n",
    "    consumed = 0\n",
    "    sent_mapping = {}\n",
    "    sent_content = {}\n",
    "    last_sent_end = 0\n",
    "    doct_mapping = {}\n",
    "    doct_split = []\n",
    "    for i in range(len(row['document'])):\n",
    "        if row['document'][i].strip() != '':\n",
    "            if row['document'][i] not in src.iloc[curr_sentence][consumed:]:\n",
    "                doct_split.append(row['document'][last_sent_end:i])\n",
    "                last_sent_end = i\n",
    "                curr_sentence += 1\n",
    "                consumed = 0\n",
    "            offset = src.iloc[curr_sentence][consumed:].index(row['document'][i])\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            if curr_sentence not in sent_content:\n",
    "                sent_content[curr_sentence] = []\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "            consumed += offset + len(row['document'][i])\n",
    "        else:\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "        doct_split.append(row['document'][last_sent_end:])\n",
    "    # Select the cluster that best matches the search word\n",
    "    selcluster_idx = -1\n",
    "    selcluster_ct = 0\n",
    "    for i in range(len(row['clusters'])):\n",
    "        currcluster_ct = 0\n",
    "        for c in row['clusters'][i]:\n",
    "            currcluster_ct += len(re.findall(f'{search_word}', ' '.join(row['document'][c[0]:c[1]+1])))\n",
    "        if currcluster_ct > selcluster_ct:\n",
    "            selcluster_idx = i\n",
    "            selcluster_ct = currcluster_ct\n",
    "    # Build the output row\n",
    "    output = [sent_mapping, sent_content, doct_mapping, selcluster_idx]\n",
    "    return dict(zip(['sent_mapping', 'sent_content', 'doct_mapping', 'selcluster_idx'],output))\n",
    "\n",
    "output = df_merged.apply(\n",
    "    lambda row: reinterpret_coref_clusters(row, search_word, df), \n",
    "    axis=1, result_type='expand')\n",
    "# df_merged = df_merged.join(output)\n",
    "\n",
    "df_merged.join(output).to_csv(f'outputs/coreference-partial.csv')\n",
    "df_merged.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split based on co-references to any phrase containing BERT, using allennlp coreference resolution\n",
    "# This does NOT preserve the original sentence spacing\n",
    "def split_search_term_coreference(row, search_word, lookup, fallback):\n",
    "    # Splits on first coref instance ONLY\n",
    "    # there's probably a cleaner way to do this...\n",
    "    lookup_row = lookup.loc[lookup['ID']==row['ID']].loc[lookup['Type']==row['Type']].to_dict(orient='records')[0]\n",
    "    if lookup_row['selcluster_idx'] == -1:\n",
    "        # if we didn't identify any clusters that match the search term, use our fallback method\n",
    "        return fallback(row, search_word)\n",
    "    split_clusters = lookup_row['clusters'][lookup_row['selcluster_idx']]\n",
    "    output = []\n",
    "    for i in range(len(split_clusters)):\n",
    "        c = split_clusters[i]\n",
    "        if lookup_row['sent_mapping'][c[0]] == lookup_row['sent_mapping'][c[0]] == row['Index']:\n",
    "            sentence_start = lookup_row['sent_content'][row['Index']][0]\n",
    "            sentence_end = lookup_row['sent_content'][row['Index']][-1]\n",
    "            output=[' '.join(lookup_row['document'][sentence_start:c[0]]),\n",
    "                    ' '.join(lookup_row['document'][c[0]:c[1]+1]),\n",
    "                    ' '.join(lookup_row['document'][c[1]+1:sentence_end])]\n",
    "            break\n",
    "    if output == []:\n",
    "        # if there wasn't any reference in the sentence found, use our fallback method\n",
    "        return fallback(row, search_word)\n",
    "    return dict(zip(splitting_headers,output))\n",
    "\n",
    "metadata = output = df_merged.apply(\n",
    "    lambda row: reinterpret_coref_clusters(row, search_word, df), \n",
    "    axis=1, result_type='expand')\n",
    "output = df.apply(\n",
    "    lambda row: split_search_term_coreference(row, search_word, df_merged.join(metadata), split_search_term_whitespace), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping functions\n",
    "\n",
    "Assume we have an input dataframe with column headers `['text_split_0','text_split_1','text_split_2']`\n",
    "\n",
    "`'text_split_1'` is the column that contains our search term / anchor point\n",
    "\n",
    "`'text_split_0'` and `'text_split_2'` are the columns that contain text before and after the search terms respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_input = df.apply(lambda row: split_search_term_whitespace(row, search_word), axis=1, result_type='expand')\n",
    "\n",
    "sample_input = df.join(output)\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the first word (regardless of part of speech) that comes after the anchor point\n",
    "def group_first_word(row):\n",
    "    output = [row['text_split_2'].split(' ')[0]]\n",
    "    return dict(zip(['group'], output))\n",
    "\n",
    "output = sample_input.apply(lambda row: group_first_word(row), axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output.to_csv(f'outputs/coreference_firstword.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Group on the first verb that comes after the anchor point, using NLTK part-of-speech tagging\n",
    "def group_first_verb(row):\n",
    "    tokens = [nltk.word_tokenize(row['text_split_0']), \n",
    "              nltk.word_tokenize(row['text_split_1']),\n",
    "              nltk.word_tokenize(row['text_split_2'])]\n",
    "    tokens_pos = nltk.pos_tag([item for sublist in tokens for item in sublist])\n",
    "    verb = ''\n",
    "    for i in range(len(tokens[0])+len(tokens[1]), len(tokens_pos)):\n",
    "        if tokens_pos[i][1].startswith('V'):\n",
    "            verb = tokens_pos[i][0]\n",
    "            break\n",
    "    output = [verb]\n",
    "    return dict(zip(['group'], output))\n",
    "\n",
    "output = sample_input.apply(lambda row: group_first_verb(row), axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output.to_csv(f'outputs/coreference_firstverb.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\")\n",
    "\n",
    "# Group on the main verb in the sentence, using allennlp dependency parsing (based on demo code)\n",
    "def group_main_verb(row):\n",
    "    p = predictor.predict(\n",
    "        sentence=' '.join([row['text_split_0'], row['text_split_1'], row['text_split_2']]).strip()\n",
    "    )\n",
    "    output = [p['hierplane_tree']['root']['word']]\n",
    "    return dict(zip(['group'], output))\n",
    "\n",
    "output = sample_input.apply(lambda row: group_main_verb(row), axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output.to_csv(f'outputs/coreference_mainverb.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
