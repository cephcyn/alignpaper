{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "search_word = \"BERT\"\n",
    "df = pd.read_csv('data/nlp-align_BERT.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_split_0</th>\n",
       "      <th>text_split_1</th>\n",
       "      <th>text_split_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>: Pre-training of Deep Bidirectional Transform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, which stands for Bidirectional Encoder Repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike recent language representation models,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre-train deep bidirectional r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a result, the pre-trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine-tuned with just one additio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is conceptually simple and empirically powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ro</td>\n",
       "      <td>BERT</td>\n",
       "      <td>a: A Robustly Optimized BERT Pretraining Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining (Devlin et al., 2019) that carefu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained, and can match...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Distil</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, a distilled version of BERT: smaller, faster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, which can then be fine-tuned with good perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model by 40%, while retaining 97% of its lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>We focus on one such model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At? An Analysis of BERT's Attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Large pre-trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP, motivat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s attention heads exhibit patterns such as at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s attention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for query-based passage re-ranking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model captures English syntactic phenomena, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model performs remarkably well on all cases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneousl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(multilingual) as a zero shot language transf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>We compare m</td>\n",
       "      <td>BERT</td>\n",
       "      <td>with the best-published methods for zero-shot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner, determine to what extent mBER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>: Distilling BERT for Natural Language Underst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has significantly improved the performances ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be well transferred to a small student Ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>This framework ensures that Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can capture both the general-domain and task-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is empirically effective and achieves compara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is also significantly better than state-of-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>, a pre-trained Transformer model, has achieve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERT</td>\n",
       "      <td>SUM, a simple variant of BERT, for extractive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehensio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to enhance the performance of fine-tuning of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth, and It Must Speak: BERT as a Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018) is a Markov random fiel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high-quality, fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering, we explore simple appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based architectures currently give state-of-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s heads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>) has shown marvelous improvements across vari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Recently, an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking (WW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>without changing any neural architecture or e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE, BERT-wwm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>: A Globally Normalized BERT Model for Open-do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>However, previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models, and 5.8% EM and 6.5% $F_1$ over BERT-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question-answering focused passage ranking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query-docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>We present simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based models for relation extraction and sema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based model can achieve state-of-the-art perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tune...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model, finding that it can generally distingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over-parameterized for downstream t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the fla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Third, the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine-tuning, which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>) demonstrates that a deep bidirectional langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new cond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>In our paper, â€œconditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>language model (LM) (Devlin et al., 2019) is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Petroni et al. (2019) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre-traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about (the surface...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>More specifically, we show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s precision drops dramatically when we filter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>As a remedy, we propose E-</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, an extension of BERT that replaces entity me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>E-</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms both BERT and ERNIE (Zhang et al....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>We take this as evidence that E-</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is richer in factual knowledge, and we show t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ELMo, and GPT-2 Embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>In all layers of ELMo,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2, on average, less than 5% of the v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_split_0 text_split_1  \\\n",
       "0                                                              BERT   \n",
       "1    We introduce a new language representation mod...         BERT   \n",
       "2       Unlike recent language representation models,          BERT   \n",
       "3                        As a result, the pre-trained          BERT   \n",
       "4                                                              BERT   \n",
       "5    It obtains new state-of-the-art results on ele...                \n",
       "6                                                   Ro         BERT   \n",
       "7    Language model pretraining has led to signific...                \n",
       "8    Training is computationally expensive, often d...                \n",
       "9                   We present a replication study of          BERT   \n",
       "10                                       We find that          BERT   \n",
       "11   Our best model achieves state-of-the-art resul...                \n",
       "12   These results highlight the importance of prev...                \n",
       "13                     We release our models and code.                \n",
       "14                                              Distil         BERT   \n",
       "15   As Transfer Learning from large-scale pre-trai...                \n",
       "16   In this work, we propose a method to pre-train...         BERT   \n",
       "17   While most prior work investigated the use of ...         BERT   \n",
       "18   To leverage the inductive biases learned by la...                \n",
       "19   Our smaller, faster and lighter model is cheap...                \n",
       "20                                                             BERT   \n",
       "21   Pre-trained text encoders have rapidly advance...                \n",
       "22                        We focus on one such model,          BERT   \n",
       "23   We find that the model represents the steps of...                \n",
       "24   Qualitative analysis reveals that the model ca...                \n",
       "25                                          What Does          BERT   \n",
       "26          Large pre-trained neural networks such as          BERT   \n",
       "27   Most recent analysis has focused on model outp...                \n",
       "28   Complementary to these works, we propose metho...         BERT   \n",
       "29                                                             BERT   \n",
       "30   We further show that certain attention heads c...                \n",
       "31   For example, we find heads that attend to the ...                \n",
       "32   Lastly, we propose an attention-based probing ...         BERT   \n",
       "33                            Passage Re-ranking with          BERT   \n",
       "34   Recently, neural models pretrained on a langua...         BERT   \n",
       "35   In this paper, we describe a simple re-impleme...         BERT   \n",
       "36   Our system is the state of the art on the TREC...                \n",
       "37   The code to reproduce our results is available...                \n",
       "38                                          Assessing          BERT   \n",
       "39   I assess the extent to which the recently intr...         BERT   \n",
       "40                                                The          BERT   \n",
       "41   Beto, Bentz, Becas: The Surprising Cross-Lingu...         BERT   \n",
       "42   Pretrained contextual representation models (P...                \n",
       "43                                   A new release of          BERT   \n",
       "44   This paper explores the broader cross-lingual ...         BERT   \n",
       "45                                        We compare m         BERT   \n",
       "46   Additionally, we investigate the most effectiv...         BERT   \n",
       "47                                                Tiny         BERT   \n",
       "48               Language model pre-training, such as          BERT   \n",
       "49   However, pre-trained language models are usual...                \n",
       "50   To accelerate inference and reduce model size ...                \n",
       "51   By leveraging this new KD method, the plenty o...         BERT   \n",
       "52   Moreover, we introduce a new two-stage learnin...         BERT   \n",
       "53                    This framework ensures that Tiny         BERT   \n",
       "54                                                Tiny         BERT   \n",
       "55                                                Tiny         BERT   \n",
       "56                                          Fine-tune          BERT   \n",
       "57                                                             BERT   \n",
       "58                         In this paper, we describe          BERT   \n",
       "59   Our system is the state of the art on the CNN/...                \n",
       "60   The codes to reproduce our results are availab...                \n",
       "61                                                             BERT   \n",
       "62   Question-answering plays an important role in ...                \n",
       "63   Inspired by the recent success of machine read...                \n",
       "64   To the best of our knowledge, no existing work...                \n",
       "65   In this work, we first build an RRC dataset ca...                \n",
       "66   Since ReviewRC has limited training examples f...         BERT   \n",
       "67   To show the generality of the approach, the pr...                \n",
       "68   Experimental results demonstrate that the prop...                \n",
       "69   The datasets and code are available at this ht...                \n",
       "70                                   How to Fine-Tune          BERT   \n",
       "71   Language model pre-training has proven to be u...                \n",
       "72   As a state-of-the-art language model pre-train...         BERT   \n",
       "73   In this paper, we conduct exhaustive experimen...         BERT   \n",
       "74   Finally, the proposed solution obtains new sta...                \n",
       "75                                                             BERT   \n",
       "76                                       We show that          BERT   \n",
       "77   This formulation gives way to a natural proced...         BERT   \n",
       "78                                   We generate from          BERT   \n",
       "79   Compared to the generations of a traditional l...         BERT   \n",
       "80                                          Utilizing          BERT   \n",
       "81   Aspect-based sentiment analysis (ABSA), which ...                \n",
       "82   In this paper, we construct an auxiliary sente...                \n",
       "83            We fine-tune the pre-trained model from          BERT   \n",
       "84                             Simple Applications of          BERT   \n",
       "85             Following recent successes in applying          BERT   \n",
       "86   This required confronting the challenge posed ...         BERT   \n",
       "87   We address this issue by applying inference on...                \n",
       "88   Experiments on TREC microblog and newswire tes...                \n",
       "89                      Revealing the Dark Secrets of          BERT   \n",
       "90                                                             BERT   \n",
       "91   In the current work, we focus on the interpret...         BERT   \n",
       "92   Using a subset of GLUE tasks and a set of hand...         BERT   \n",
       "93   Our findings suggest that there is a limited s...                \n",
       "94   While different heads consistently use the sam...                \n",
       "95   We show that manually disabling attention in c...         BERT   \n",
       "96   Pre-Training with Whole Word Masking for Chinese          BERT   \n",
       "97   Bidirectional Encoder Representations from Tra...         BERT   \n",
       "98                   Recently, an upgraded version of          BERT   \n",
       "99   In this technical report, we adapt whole word ...                \n",
       "100  The model was trained on the latest Chinese Wi...                \n",
       "101  We aim to provide easy extensibility and bette...         BERT   \n",
       "102  The model is verified on various NLP tasks, ac...                \n",
       "103  Experimental results on these datasets show th...                \n",
       "104  Moreover, we also examine the effectiveness of...         BERT   \n",
       "105  We release the pre-trained model (both TensorF...                \n",
       "106                                     Multi-passage          BERT   \n",
       "107                                                            BERT   \n",
       "108                     However, previous work trains          BERT   \n",
       "109  To tackle this issue, we propose a multi-passage          BERT   \n",
       "110  In addition, we find that splitting articles i...                \n",
       "111  By leveraging a passage ranker to select high-...         BERT   \n",
       "112  Experiments on four standard benchmarks showed...         BERT   \n",
       "113  In particular, on the OpenSQuAD dataset, our m...         BERT   \n",
       "114                                                            BERT   \n",
       "115  Intent classification and slot filling are two...                \n",
       "116  They often suffer from small-scale human-label...                \n",
       "117     Recently a new language representation model,          BERT   \n",
       "118  However, there has not been much effort on exp...         BERT   \n",
       "119  In this work, we propose a joint intent classi...         BERT   \n",
       "120  Experimental results demonstrate that our prop...                \n",
       "121                                                            BERT   \n",
       "122  Conversational search is an emerging topic in ...                \n",
       "123  One of the major challenges to multi-turn conv...                \n",
       "124  Existing methods either prepend history turns ...                \n",
       "125  We propose a conceptually simple yet highly ef...                \n",
       "126  It enables seamless integration of conversatio...         BERT   \n",
       "127  We first explain our view that ConvQA is a sim...                \n",
       "128  We further demonstrate the effectiveness of ou...                \n",
       "129  Finally, we analyze the impact of different nu...                \n",
       "130                    Understanding the Behaviors of          BERT   \n",
       "131  This paper studies the performances and behavi...         BERT   \n",
       "132  We explore several different ways to leverage ...         BERT   \n",
       "133  Experimental results on MS MARCO demonstrate t...         BERT   \n",
       "134  Experimental results on TREC show the gaps bet...         BERT   \n",
       "135                           Analyses illustrate how          BERT   \n",
       "136                                            Simple          BERT   \n",
       "137                                 We present simple          BERT   \n",
       "138  In recent years, state-of-the-art performance ...                \n",
       "139  In this paper, extensive experiments on datase...         BERT   \n",
       "140  To our knowledge, we are the first to successf...         BERT   \n",
       "141  Our models provide strong baselines for future...                \n",
       "142                                                            BERT   \n",
       "143  Multi-task learning allows the sharing of usef...                \n",
       "144  In natural language processing several recent ...                \n",
       "145  These results are based on fine-tuning on each...                \n",
       "146  We explore the multi-task learning setting for...         BERT   \n",
       "147  We introduce new adaptation modules, PALs or `...                \n",
       "148                    By using PALs in parallel with          BERT   \n",
       "149                                              What          BERT   \n",
       "150  Pre-training by language modeling has become a...                \n",
       "151  In this paper we introduce a suite of diagnost...                \n",
       "152  As a case study, we apply these diagnostics to...         BERT   \n",
       "153  Visualizing and Understanding the Effectivenes...         BERT   \n",
       "154              Language model pre-training, such as          BERT   \n",
       "155  However, it is unclear why the pre-training-th...                \n",
       "156  In this paper, we propose to visualize loss la...         BERT   \n",
       "157  First, we find that pre-training reaches a goo...                \n",
       "158  We also demonstrate that the fine-tuning proce...         BERT   \n",
       "159  Second, the visualization results indicate tha...         BERT   \n",
       "160                        Third, the lower layers of          BERT   \n",
       "161                                       Conditional          BERT   \n",
       "162  Data augmentation methods are often applied to...                \n",
       "163  Recently proposed contextual augmentation augm...                \n",
       "164  Bidirectional Encoder Representations from Tra...         BERT   \n",
       "165  We propose a novel data augmentation method fo...         BERT   \n",
       "166                                       We retrofit          BERT   \n",
       "167  In our paper, â€œconditional masked language mod...                \n",
       "168                      The well trained conditional          BERT   \n",
       "169  Experiments on six various different text clas...                \n",
       "170                               Small and Practical          BERT   \n",
       "171  We propose a practical scheme to train a singl...                \n",
       "172               Starting from a public multilingual          BERT   \n",
       "173  We show that our model especially outperforms ...                \n",
       "174  We showcase the effectiveness of our method by...                \n",
       "175                             Data Augmentation for          BERT   \n",
       "176  Recently, a simple combination of passage retr...         BERT   \n",
       "177  In this paper, we present a data augmentation ...                \n",
       "178     We apply a stage-wise approach to fine tuning          BERT   \n",
       "179  Experimental results show large gains in effec...                \n",
       "180                                                            BERT   \n",
       "181                                               The          BERT   \n",
       "182  Petroni et al. (2019) take this as evidence that          BERT   \n",
       "183  We take issue with this interpretation and arg...         BERT   \n",
       "184                   More specifically, we show that          BERT   \n",
       "185                         As a remedy, we propose E-         BERT   \n",
       "186                                                 E-         BERT   \n",
       "187                   We take this as evidence that E-         BERT   \n",
       "188  How Contextual are Contextualized Word Represe...         BERT   \n",
       "189  Replacing static word embeddings with contextu...                \n",
       "190  However, just how contextual are the contextua...         BERT   \n",
       "191  Are there infinitely many context-specific rep...                \n",
       "192  For one, we find that the contextualized repre...                \n",
       "193  While representations of the same word in diff...                \n",
       "194  This suggests that upper layers of contextuali...                \n",
       "195                            In all layers of ELMo,          BERT   \n",
       "\n",
       "                                          text_split_2  \n",
       "0    : Pre-training of Deep Bidirectional Transform...  \n",
       "1    , which stands for Bidirectional Encoder Repre...  \n",
       "2     is designed to pre-train deep bidirectional r...  \n",
       "3     model can be fine-tuned with just one additio...  \n",
       "4     is conceptually simple and empirically powerf...  \n",
       "5                                                       \n",
       "6    a: A Robustly Optimized BERT Pretraining Approach  \n",
       "7                                                       \n",
       "8                                                       \n",
       "9     pretraining (Devlin et al., 2019) that carefu...  \n",
       "10    was significantly undertrained, and can match...  \n",
       "11                                                      \n",
       "12                                                      \n",
       "13                                                      \n",
       "14   , a distilled version of BERT: smaller, faster...  \n",
       "15                                                      \n",
       "16   , which can then be fine-tuned with good perfo...  \n",
       "17    model by 40%, while retaining 97% of its lang...  \n",
       "18                                                      \n",
       "19                                                      \n",
       "20              Rediscovers the Classical NLP Pipeline  \n",
       "21                                                      \n",
       "22   , and aim to quantify where linguistic informa...  \n",
       "23                                                      \n",
       "24                                                      \n",
       "25            Look At? An Analysis of BERT's Attention  \n",
       "26    have had great recent success in NLP, motivat...  \n",
       "27                                                      \n",
       "28                                                  .   \n",
       "29   's attention heads exhibit patterns such as at...  \n",
       "30                                                      \n",
       "31                                                      \n",
       "32                                       's attention.  \n",
       "33                                                      \n",
       "34    (Devlin et al., 2018), have achieved impressi...  \n",
       "35                for query-based passage re-ranking.   \n",
       "36                                                      \n",
       "37                                                      \n",
       "38                              's Syntactic Abilities  \n",
       "39    model captures English syntactic phenomena, u...  \n",
       "40        model performs remarkably well on all cases.  \n",
       "41                                                      \n",
       "42                                                      \n",
       "43    (Devlin, 2018) includes a model simultaneousl...  \n",
       "44    (multilingual) as a zero shot language transf...  \n",
       "45    with the best-published methods for zero-shot...  \n",
       "46    in this manner, determine to what extent mBER...  \n",
       "47   : Distilling BERT for Natural Language Underst...  \n",
       "48   , has significantly improved the performances ...  \n",
       "49                                                      \n",
       "50                                                      \n",
       "51    can be well transferred to a small student Ti...  \n",
       "52   , which performs transformer distillation at b...  \n",
       "53    can capture both the general-domain and task-...  \n",
       "54    is empirically effective and achieves compara...  \n",
       "55    is also significantly better than state-of-th...  \n",
       "56                        for Extractive Summarization  \n",
       "57   , a pre-trained Transformer model, has achieve...  \n",
       "58   SUM, a simple variant of BERT, for extractive ...  \n",
       "59                                                      \n",
       "60                                                      \n",
       "61    Post-Training for Review Reading Comprehensio...  \n",
       "62                                                      \n",
       "63                                                      \n",
       "64                                                      \n",
       "65                                                      \n",
       "66    to enhance the performance of fine-tuning of ...  \n",
       "67                                                      \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                            for Text Classification?  \n",
       "71                                                      \n",
       "72    (Bidirectional Encoder Representations from T...  \n",
       "73    on text classification task and provide a gen...  \n",
       "74                                                      \n",
       "75    has a Mouth, and It Must Speak: BERT as a Mar...  \n",
       "76    (Devlin et al., 2018) is a Markov random fiel...  \n",
       "77                                                  .   \n",
       "78    and find that it can produce high-quality, fl...  \n",
       "79    generates sentences that are more diverse but...  \n",
       "80    for Aspect-Based Sentiment Analysis via Const...  \n",
       "81                                                      \n",
       "82                                                      \n",
       "83    and achieve new state-of-the-art results on S...  \n",
       "84                       for Ad Hoc Document Retrieval  \n",
       "85    to question answering, we explore simple appl...  \n",
       "86                            was designed to handle.   \n",
       "87                                                      \n",
       "88                                                      \n",
       "89                                                      \n",
       "90   -based architectures currently give state-of-t...  \n",
       "91                                                  .   \n",
       "92                                          's heads.   \n",
       "93                                                      \n",
       "94                                                      \n",
       "95                                              models  \n",
       "96                                                      \n",
       "97   ) has shown marvelous improvements across vari...  \n",
       "98    has been released with Whole Word Masking (WW...  \n",
       "99                                                      \n",
       "100                                                     \n",
       "101   without changing any neural architecture or e...  \n",
       "102                                                     \n",
       "103                                                     \n",
       "104                                , ERNIE, BERT-wwm.   \n",
       "105                                                     \n",
       "106  : A Globally Normalized BERT Model for Open-do...  \n",
       "107   model has been successfully applied to open-d...  \n",
       "108   by viewing passages corresponding to the same...  \n",
       "109   model to globally normalize answer scores acr...  \n",
       "110                                                     \n",
       "111                              gains additional 2%.   \n",
       "112   outperforms all state-of-the-art models on al...  \n",
       "113   models, and 5.8% EM and 6.5% $F_1$ over BERT-...  \n",
       "114   for Joint Intent Classification and Slot Filling  \n",
       "115                                                     \n",
       "116                                                     \n",
       "117   (Bidirectional Encoder Representations from T...  \n",
       "118               for natural language understanding.   \n",
       "119                                                 .   \n",
       "120                                                     \n",
       "121   with History Answer Embedding for Conversatio...  \n",
       "122                                                     \n",
       "123                                                     \n",
       "124                                                     \n",
       "125                                                     \n",
       "126   (Bidirectional Encoder Representations from T...  \n",
       "127                                                     \n",
       "128                                                     \n",
       "129                                                     \n",
       "130                                         in Ranking  \n",
       "131                                 in ranking tasks.   \n",
       "132   and fine-tune it on two ranking tasks: MS MAR...  \n",
       "133   in question-answering focused passage ranking...  \n",
       "134   pre-trained on surrounding contexts and the n...  \n",
       "135   allocates its attentions between query-docume...  \n",
       "136   Models for Relation Extraction and Semantic R...  \n",
       "137  -based models for relation extraction and sema...  \n",
       "138                                                     \n",
       "139  -based model can achieve state-of-the-art perf...  \n",
       "140                                   in this manner.   \n",
       "141                                                     \n",
       "142   and PALs: Projected Attention Layers for Effi...  \n",
       "143                                                     \n",
       "144                                                     \n",
       "145                                                     \n",
       "146   model on the GLUE benchmark, and how to best ...  \n",
       "147                                                     \n",
       "148   layers, we match the performance of fine-tune...  \n",
       "149   Is Not: Lessons from a New Suite of Psycholin...  \n",
       "150                                                     \n",
       "151                                                     \n",
       "152   model, finding that it can generally distingu...  \n",
       "153                                                     \n",
       "154  , has achieved remarkable results in many NLP ...  \n",
       "155                                                     \n",
       "156                             on specific datasets.   \n",
       "157                                                     \n",
       "158   is highly over-parameterized for downstream t...  \n",
       "159   tends to generalize better because of the fla...  \n",
       "160   are more invariant during fine-tuning, which ...  \n",
       "161                            Contextual Augmentation  \n",
       "162                                                     \n",
       "163                                                     \n",
       "164  ) demonstrates that a deep bidirectional langu...  \n",
       "165                          contextual augmentation.   \n",
       "166   to conditional BERT by introducing a new cond...  \n",
       "167                                                     \n",
       "168   can be applied to enhance contextual augmenta...  \n",
       "169                                                     \n",
       "170                       Models for Sequence Labeling  \n",
       "171                                                     \n",
       "172   checkpoint, our final model is 6x smaller and...  \n",
       "173                                                     \n",
       "174                                                     \n",
       "175      Fine-Tuning in Open-Domain Question Answering  \n",
       "176   reader was found to be very effective for que...  \n",
       "177                                                     \n",
       "178   on multiple datasets, starting with data that...  \n",
       "179                                                     \n",
       "180   is Not a Knowledge Base (Yet): Factual Knowle...  \n",
       "181   language model (LM) (Devlin et al., 2019) is ...  \n",
       "182   memorizes factual knowledge during pre-traini...  \n",
       "183   is partly due to reasoning about (the surface...  \n",
       "184  's precision drops dramatically when we filter...  \n",
       "185  , an extension of BERT that replaces entity me...  \n",
       "186   outperforms both BERT and ERNIE (Zhang et al....  \n",
       "187   is richer in factual knowledge, and we show t...  \n",
       "188                       , ELMo, and GPT-2 Embeddings  \n",
       "189                                                     \n",
       "190                                                 ?   \n",
       "191                                                     \n",
       "192                                                     \n",
       "193                                                     \n",
       "194                                                     \n",
       "195  , and GPT-2, on average, less than 5% of the v...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_search_term_literal(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    if search_word in row['Text']:\n",
    "        splitted = row['Text'].split(search_word, maxsplit=1)\n",
    "        splitted.insert(1, search_word)\n",
    "    else:\n",
    "        splitted = [row['Text'],'','']\n",
    "    return dict(zip(['text_split_0','text_split_1','text_split_2'],splitted))\n",
    "\n",
    "# df = df.join()\n",
    "df.apply(lambda row: split_search_term_literal(row, search_word), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_split_0</th>\n",
       "      <th>text_split_1</th>\n",
       "      <th>text_split_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>which stands for Bidirectional Encoder Represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike recent language representation models,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre-train deep bidirectional re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a result, the pre-trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine-tuned with just one addition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is conceptually simple and empirically powerful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining (Devlin et al., 2019) that careful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained, and can match ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model by 40%, while retaining 97% of its langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>We focus on one such model,</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>and aim to quantify where linguistic informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At? An Analysis of BERT's Attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Large pre-trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP, motivati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>BERT's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>attention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for query-based passage re-ranking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model captures English syntactic phenomena, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model performs remarkably well on all cases.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best-published methods for zero-shot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner, determine to what extent mBERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be well transferred to a small student Tin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>TinyBERT,</td>\n",
       "      <td>which performs transformer distillation at bot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general-domain and task-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state-of-the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to enhance the performance of fine-tuning of B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth, and It Must Speak: BERT as a Mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018) is a Markov random field...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high-quality, flu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering, we explore simple appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Recently, an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking (WWM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ERNIE, BERT-wwm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open-do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>However, previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>non-BERT</td>\n",
       "      <td>models, and 5.8% EM and 6.5% $F_1$ over BERT-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question-answering focused passage ranking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query-documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>We present simple</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>models for relation extraction and semantic ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model, finding that it can generally distingui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has achieved remarkable results in many NLP ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over-parameterized for downstream ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Third, the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine-tuning, which s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>In our paper, â€œconditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>language model (LM) (Devlin et al., 2019) is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Petroni et al. (2019) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre-training.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about (the surface ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>More specifically, we show that</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>As a remedy, we propose</td>\n",
       "      <td>E-BERT,</td>\n",
       "      <td>an extension of BERT that replaces entity ment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td></td>\n",
       "      <td>E-BERT</td>\n",
       "      <td>outperforms both BERT and ERNIE (Zhang et al.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>We take this as evidence that</td>\n",
       "      <td>E-BERT</td>\n",
       "      <td>is richer in factual knowledge, and we show tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>BERT?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>In all layers of ELMo,</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>and GPT-2, on average, less than 5% of the var...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_split_0 text_split_1  \\\n",
       "0                                                             BERT:   \n",
       "1    We introduce a new language representation mod...        BERT,   \n",
       "2        Unlike recent language representation models,         BERT   \n",
       "3                         As a result, the pre-trained         BERT   \n",
       "4                                                              BERT   \n",
       "5    It obtains new state-of-the-art results on ele...                \n",
       "6                                                          RoBERTa:   \n",
       "7    Language model pretraining has led to signific...                \n",
       "8    Training is computationally expensive, often d...                \n",
       "9                    We present a replication study of         BERT   \n",
       "10                                        We find that         BERT   \n",
       "11   Our best model achieves state-of-the-art resul...                \n",
       "12   These results highlight the importance of prev...                \n",
       "13                     We release our models and code.                \n",
       "14                                                      DistilBERT,   \n",
       "15   As Transfer Learning from large-scale pre-trai...                \n",
       "16   In this work, we propose a method to pre-train...  DistilBERT,   \n",
       "17   While most prior work investigated the use of ...         BERT   \n",
       "18   To leverage the inductive biases learned by la...                \n",
       "19   Our smaller, faster and lighter model is cheap...                \n",
       "20                                                             BERT   \n",
       "21   Pre-trained text encoders have rapidly advance...                \n",
       "22                         We focus on one such model,        BERT,   \n",
       "23   We find that the model represents the steps of...                \n",
       "24   Qualitative analysis reveals that the model ca...                \n",
       "25                                           What Does         BERT   \n",
       "26           Large pre-trained neural networks such as         BERT   \n",
       "27   Most recent analysis has focused on model outp...                \n",
       "28   Complementary to these works, we propose metho...        BERT.   \n",
       "29                                                           BERT's   \n",
       "30   We further show that certain attention heads c...                \n",
       "31   For example, we find heads that attend to the ...                \n",
       "32   Lastly, we propose an attention-based probing ...       BERT's   \n",
       "33                             Passage Re-ranking with         BERT   \n",
       "34   Recently, neural models pretrained on a langua...         BERT   \n",
       "35   In this paper, we describe a simple re-impleme...         BERT   \n",
       "36   Our system is the state of the art on the TREC...                \n",
       "37   The code to reproduce our results is available...                \n",
       "38                                           Assessing       BERT's   \n",
       "39   I assess the extent to which the recently intr...         BERT   \n",
       "40                                                 The         BERT   \n",
       "41   Beto, Bentz, Becas: The Surprising Cross-Lingu...         BERT   \n",
       "42   Pretrained contextual representation models (P...                \n",
       "43                                    A new release of         BERT   \n",
       "44   This paper explores the broader cross-lingual ...        mBERT   \n",
       "45                                          We compare        mBERT   \n",
       "46   Additionally, we investigate the most effectiv...        mBERT   \n",
       "47                                                        TinyBERT:   \n",
       "48                Language model pre-training, such as        BERT,   \n",
       "49   However, pre-trained language models are usual...                \n",
       "50   To accelerate inference and reduce model size ...                \n",
       "51   By leveraging this new KD method, the plenty o...         BERT   \n",
       "52   Moreover, we introduce a new two-stage learnin...    TinyBERT,   \n",
       "53                         This framework ensures that     TinyBERT   \n",
       "54                                                         TinyBERT   \n",
       "55                                                         TinyBERT   \n",
       "56                                           Fine-tune         BERT   \n",
       "57                                                            BERT,   \n",
       "58                          In this paper, we describe     BERTSUM,   \n",
       "59   Our system is the state of the art on the CNN/...                \n",
       "60   The codes to reproduce our results are availab...                \n",
       "61                                                             BERT   \n",
       "62   Question-answering plays an important role in ...                \n",
       "63   Inspired by the recent success of machine read...                \n",
       "64   To the best of our knowledge, no existing work...                \n",
       "65   In this work, we first build an RRC dataset ca...                \n",
       "66   Since ReviewRC has limited training examples f...         BERT   \n",
       "67   To show the generality of the approach, the pr...                \n",
       "68   Experimental results demonstrate that the prop...                \n",
       "69   The datasets and code are available at this ht...                \n",
       "70                                    How to Fine-Tune         BERT   \n",
       "71   Language model pre-training has proven to be u...                \n",
       "72   As a state-of-the-art language model pre-train...         BERT   \n",
       "73   In this paper, we conduct exhaustive experimen...         BERT   \n",
       "74   Finally, the proposed solution obtains new sta...                \n",
       "75                                                             BERT   \n",
       "76                                        We show that         BERT   \n",
       "77   This formulation gives way to a natural proced...        BERT.   \n",
       "78                                    We generate from         BERT   \n",
       "79   Compared to the generations of a traditional l...         BERT   \n",
       "80                                           Utilizing         BERT   \n",
       "81   Aspect-based sentiment analysis (ABSA), which ...                \n",
       "82   In this paper, we construct an auxiliary sente...                \n",
       "83             We fine-tune the pre-trained model from         BERT   \n",
       "84                              Simple Applications of         BERT   \n",
       "85              Following recent successes in applying         BERT   \n",
       "86   This required confronting the challenge posed ...         BERT   \n",
       "87   We address this issue by applying inference on...                \n",
       "88   Experiments on TREC microblog and newswire tes...                \n",
       "89                       Revealing the Dark Secrets of         BERT   \n",
       "90                                                       BERT-based   \n",
       "91   In the current work, we focus on the interpret...        BERT.   \n",
       "92   Using a subset of GLUE tasks and a set of hand...       BERT's   \n",
       "93   Our findings suggest that there is a limited s...                \n",
       "94   While different heads consistently use the sam...                \n",
       "95   We show that manually disabling attention in c...         BERT   \n",
       "96    Pre-Training with Whole Word Masking for Chinese         BERT   \n",
       "97   Bidirectional Encoder Representations from Tra...       (BERT)   \n",
       "98                    Recently, an upgraded version of         BERT   \n",
       "99   In this technical report, we adapt whole word ...                \n",
       "100  The model was trained on the latest Chinese Wi...                \n",
       "101  We aim to provide easy extensibility and bette...         BERT   \n",
       "102  The model is verified on various NLP tasks, ac...                \n",
       "103  Experimental results on these datasets show th...                \n",
       "104  Moreover, we also examine the effectiveness of...        BERT,   \n",
       "105  We release the pre-trained model (both TensorF...                \n",
       "106                                      Multi-passage        BERT:   \n",
       "107                                                            BERT   \n",
       "108                      However, previous work trains         BERT   \n",
       "109   To tackle this issue, we propose a multi-passage         BERT   \n",
       "110  In addition, we find that splitting articles i...                \n",
       "111  By leveraging a passage ranker to select high-...         BERT   \n",
       "112  Experiments on four standard benchmarks showed...         BERT   \n",
       "113  In particular, on the OpenSQuAD dataset, our m...     non-BERT   \n",
       "114                                                            BERT   \n",
       "115  Intent classification and slot filling are two...                \n",
       "116  They often suffer from small-scale human-label...                \n",
       "117      Recently a new language representation model,         BERT   \n",
       "118  However, there has not been much effort on exp...         BERT   \n",
       "119  In this work, we propose a joint intent classi...        BERT.   \n",
       "120  Experimental results demonstrate that our prop...                \n",
       "121                                                            BERT   \n",
       "122  Conversational search is an emerging topic in ...                \n",
       "123  One of the major challenges to multi-turn conv...                \n",
       "124  Existing methods either prepend history turns ...                \n",
       "125  We propose a conceptually simple yet highly ef...                \n",
       "126  It enables seamless integration of conversatio...         BERT   \n",
       "127  We first explain our view that ConvQA is a sim...                \n",
       "128  We further demonstrate the effectiveness of ou...                \n",
       "129  Finally, we analyze the impact of different nu...                \n",
       "130                     Understanding the Behaviors of         BERT   \n",
       "131  This paper studies the performances and behavi...         BERT   \n",
       "132  We explore several different ways to leverage ...         BERT   \n",
       "133  Experimental results on MS MARCO demonstrate t...         BERT   \n",
       "134  Experimental results on TREC show the gaps bet...         BERT   \n",
       "135                            Analyses illustrate how         BERT   \n",
       "136                                             Simple         BERT   \n",
       "137                                  We present simple   BERT-based   \n",
       "138  In recent years, state-of-the-art performance ...                \n",
       "139  In this paper, extensive experiments on datase...   BERT-based   \n",
       "140  To our knowledge, we are the first to successf...         BERT   \n",
       "141  Our models provide strong baselines for future...                \n",
       "142                                                            BERT   \n",
       "143  Multi-task learning allows the sharing of usef...                \n",
       "144  In natural language processing several recent ...                \n",
       "145  These results are based on fine-tuning on each...                \n",
       "146  We explore the multi-task learning setting for...         BERT   \n",
       "147  We introduce new adaptation modules, PALs or `...                \n",
       "148                     By using PALs in parallel with         BERT   \n",
       "149                                               What         BERT   \n",
       "150  Pre-training by language modeling has become a...                \n",
       "151  In this paper we introduce a suite of diagnost...                \n",
       "152  As a case study, we apply these diagnostics to...         BERT   \n",
       "153  Visualizing and Understanding the Effectivenes...         BERT   \n",
       "154               Language model pre-training, such as        BERT,   \n",
       "155  However, it is unclear why the pre-training-th...                \n",
       "156  In this paper, we propose to visualize loss la...         BERT   \n",
       "157  First, we find that pre-training reaches a goo...                \n",
       "158  We also demonstrate that the fine-tuning proce...         BERT   \n",
       "159  Second, the visualization results indicate tha...         BERT   \n",
       "160                         Third, the lower layers of         BERT   \n",
       "161                                        Conditional         BERT   \n",
       "162  Data augmentation methods are often applied to...                \n",
       "163  Recently proposed contextual augmentation augm...                \n",
       "164  Bidirectional Encoder Representations from Tra...       (BERT)   \n",
       "165  We propose a novel data augmentation method fo...         BERT   \n",
       "166                                        We retrofit         BERT   \n",
       "167  In our paper, â€œconditional masked language mod...                \n",
       "168                       The well trained conditional         BERT   \n",
       "169  Experiments on six various different text clas...                \n",
       "170                                Small and Practical         BERT   \n",
       "171  We propose a practical scheme to train a singl...                \n",
       "172                Starting from a public multilingual         BERT   \n",
       "173  We show that our model especially outperforms ...                \n",
       "174  We showcase the effectiveness of our method by...                \n",
       "175                              Data Augmentation for         BERT   \n",
       "176  Recently, a simple combination of passage retr...         BERT   \n",
       "177  In this paper, we present a data augmentation ...                \n",
       "178      We apply a stage-wise approach to fine tuning         BERT   \n",
       "179  Experimental results show large gains in effec...                \n",
       "180                                                            BERT   \n",
       "181                                                The         BERT   \n",
       "182   Petroni et al. (2019) take this as evidence that         BERT   \n",
       "183  We take issue with this interpretation and arg...         BERT   \n",
       "184                    More specifically, we show that       BERT's   \n",
       "185                            As a remedy, we propose      E-BERT,   \n",
       "186                                                          E-BERT   \n",
       "187                      We take this as evidence that       E-BERT   \n",
       "188  How Contextual are Contextualized Word Represe...        BERT,   \n",
       "189  Replacing static word embeddings with contextu...                \n",
       "190  However, just how contextual are the contextua...        BERT?   \n",
       "191  Are there infinitely many context-specific rep...                \n",
       "192  For one, we find that the contextualized repre...                \n",
       "193  While representations of the same word in diff...                \n",
       "194  This suggests that upper layers of contextuali...                \n",
       "195                             In all layers of ELMo,        BERT,   \n",
       "\n",
       "                                          text_split_2  \n",
       "0    Pre-training of Deep Bidirectional Transformer...  \n",
       "1    which stands for Bidirectional Encoder Represe...  \n",
       "2    is designed to pre-train deep bidirectional re...  \n",
       "3    model can be fine-tuned with just one addition...  \n",
       "4    is conceptually simple and empirically powerful.   \n",
       "5                                                       \n",
       "6       A Robustly Optimized BERT Pretraining Approach  \n",
       "7                                                       \n",
       "8                                                       \n",
       "9    pretraining (Devlin et al., 2019) that careful...  \n",
       "10   was significantly undertrained, and can match ...  \n",
       "11                                                      \n",
       "12                                                      \n",
       "13                                                      \n",
       "14   a distilled version of BERT: smaller, faster, ...  \n",
       "15                                                      \n",
       "16   which can then be fine-tuned with good perform...  \n",
       "17   model by 40%, while retaining 97% of its langu...  \n",
       "18                                                      \n",
       "19                                                      \n",
       "20              Rediscovers the Classical NLP Pipeline  \n",
       "21                                                      \n",
       "22   and aim to quantify where linguistic informati...  \n",
       "23                                                      \n",
       "24                                                      \n",
       "25            Look At? An Analysis of BERT's Attention  \n",
       "26   have had great recent success in NLP, motivati...  \n",
       "27                                                      \n",
       "28                                                      \n",
       "29   attention heads exhibit patterns such as atten...  \n",
       "30                                                      \n",
       "31                                                      \n",
       "32                                          attention.  \n",
       "33                                                      \n",
       "34   (Devlin et al., 2018), have achieved impressiv...  \n",
       "35                for query-based passage re-ranking.   \n",
       "36                                                      \n",
       "37                                                      \n",
       "38                                 Syntactic Abilities  \n",
       "39   model captures English syntactic phenomena, us...  \n",
       "40        model performs remarkably well on all cases.  \n",
       "41                                                      \n",
       "42                                                      \n",
       "43   (Devlin, 2018) includes a model simultaneously...  \n",
       "44   (multilingual) as a zero shot language transfe...  \n",
       "45   with the best-published methods for zero-shot ...  \n",
       "46   in this manner, determine to what extent mBERT...  \n",
       "47   Distilling BERT for Natural Language Understan...  \n",
       "48   has significantly improved the performances of...  \n",
       "49                                                      \n",
       "50                                                      \n",
       "51   can be well transferred to a small student Tin...  \n",
       "52   which performs transformer distillation at bot...  \n",
       "53   can capture both the general-domain and task-s...  \n",
       "54   is empirically effective and achieves comparab...  \n",
       "55   is also significantly better than state-of-the...  \n",
       "56                        for Extractive Summarization  \n",
       "57   a pre-trained Transformer model, has achieved ...  \n",
       "58   a simple variant of BERT, for extractive summa...  \n",
       "59                                                      \n",
       "60                                                      \n",
       "61   Post-Training for Review Reading Comprehension...  \n",
       "62                                                      \n",
       "63                                                      \n",
       "64                                                      \n",
       "65                                                      \n",
       "66   to enhance the performance of fine-tuning of B...  \n",
       "67                                                      \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                            for Text Classification?  \n",
       "71                                                      \n",
       "72   (Bidirectional Encoder Representations from Tr...  \n",
       "73   on text classification task and provide a gene...  \n",
       "74                                                      \n",
       "75   has a Mouth, and It Must Speak: BERT as a Mark...  \n",
       "76   (Devlin et al., 2018) is a Markov random field...  \n",
       "77                                                      \n",
       "78   and find that it can produce high-quality, flu...  \n",
       "79   generates sentences that are more diverse but ...  \n",
       "80   for Aspect-Based Sentiment Analysis via Constr...  \n",
       "81                                                      \n",
       "82                                                      \n",
       "83   and achieve new state-of-the-art results on Se...  \n",
       "84                       for Ad Hoc Document Retrieval  \n",
       "85   to question answering, we explore simple appli...  \n",
       "86                            was designed to handle.   \n",
       "87                                                      \n",
       "88                                                      \n",
       "89                                                      \n",
       "90   architectures currently give state-of-the-art ...  \n",
       "91                                                      \n",
       "92                                             heads.   \n",
       "93                                                      \n",
       "94                                                      \n",
       "95                                              models  \n",
       "96                                                      \n",
       "97   has shown marvelous improvements across variou...  \n",
       "98   has been released with Whole Word Masking (WWM...  \n",
       "99                                                      \n",
       "100                                                     \n",
       "101  without changing any neural architecture or ev...  \n",
       "102                                                     \n",
       "103                                                     \n",
       "104                                  ERNIE, BERT-wwm.   \n",
       "105                                                     \n",
       "106  A Globally Normalized BERT Model for Open-doma...  \n",
       "107  model has been successfully applied to open-do...  \n",
       "108  by viewing passages corresponding to the same ...  \n",
       "109  model to globally normalize answer scores acro...  \n",
       "110                                                     \n",
       "111                              gains additional 2%.   \n",
       "112  outperforms all state-of-the-art models on all...  \n",
       "113  models, and 5.8% EM and 6.5% $F_1$ over BERT-b...  \n",
       "114   for Joint Intent Classification and Slot Filling  \n",
       "115                                                     \n",
       "116                                                     \n",
       "117  (Bidirectional Encoder Representations from Tr...  \n",
       "118               for natural language understanding.   \n",
       "119                                                     \n",
       "120                                                     \n",
       "121  with History Answer Embedding for Conversation...  \n",
       "122                                                     \n",
       "123                                                     \n",
       "124                                                     \n",
       "125                                                     \n",
       "126  (Bidirectional Encoder Representations from Tr...  \n",
       "127                                                     \n",
       "128                                                     \n",
       "129                                                     \n",
       "130                                         in Ranking  \n",
       "131                                 in ranking tasks.   \n",
       "132  and fine-tune it on two ranking tasks: MS MARC...  \n",
       "133  in question-answering focused passage ranking ...  \n",
       "134  pre-trained on surrounding contexts and the ne...  \n",
       "135  allocates its attentions between query-documen...  \n",
       "136  Models for Relation Extraction and Semantic Ro...  \n",
       "137  models for relation extraction and semantic ro...  \n",
       "138                                                     \n",
       "139   model can achieve state-of-the-art performance.   \n",
       "140                                   in this manner.   \n",
       "141                                                     \n",
       "142  and PALs: Projected Attention Layers for Effic...  \n",
       "143                                                     \n",
       "144                                                     \n",
       "145                                                     \n",
       "146  model on the GLUE benchmark, and how to best a...  \n",
       "147                                                     \n",
       "148  layers, we match the performance of fine-tuned...  \n",
       "149  Is Not: Lessons from a New Suite of Psycholing...  \n",
       "150                                                     \n",
       "151                                                     \n",
       "152  model, finding that it can generally distingui...  \n",
       "153                                                     \n",
       "154  has achieved remarkable results in many NLP ta...  \n",
       "155                                                     \n",
       "156                             on specific datasets.   \n",
       "157                                                     \n",
       "158  is highly over-parameterized for downstream ta...  \n",
       "159  tends to generalize better because of the flat...  \n",
       "160  are more invariant during fine-tuning, which s...  \n",
       "161                            Contextual Augmentation  \n",
       "162                                                     \n",
       "163                                                     \n",
       "164  demonstrates that a deep bidirectional languag...  \n",
       "165                          contextual augmentation.   \n",
       "166  to conditional BERT by introducing a new condi...  \n",
       "167                                                     \n",
       "168  can be applied to enhance contextual augmentat...  \n",
       "169                                                     \n",
       "170                       Models for Sequence Labeling  \n",
       "171                                                     \n",
       "172  checkpoint, our final model is 6x smaller and ...  \n",
       "173                                                     \n",
       "174                                                     \n",
       "175      Fine-Tuning in Open-Domain Question Answering  \n",
       "176  reader was found to be very effective for ques...  \n",
       "177                                                     \n",
       "178  on multiple datasets, starting with data that ...  \n",
       "179                                                     \n",
       "180  is Not a Knowledge Base (Yet): Factual Knowled...  \n",
       "181  language model (LM) (Devlin et al., 2019) is s...  \n",
       "182  memorizes factual knowledge during pre-training.   \n",
       "183  is partly due to reasoning about (the surface ...  \n",
       "184  precision drops dramatically when we filter ce...  \n",
       "185  an extension of BERT that replaces entity ment...  \n",
       "186  outperforms both BERT and ERNIE (Zhang et al.,...  \n",
       "187  is richer in factual knowledge, and we show tw...  \n",
       "188                         ELMo, and GPT-2 Embeddings  \n",
       "189                                                     \n",
       "190                                                     \n",
       "191                                                     \n",
       "192                                                     \n",
       "193                                                     \n",
       "194                                                     \n",
       "195  and GPT-2, on average, less than 5% of the var...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_search_term_whitespace(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    if search_word in row['Text']:\n",
    "        tokens = row['Text'].split(' ')\n",
    "        for i in range(len(tokens)):\n",
    "            if search_word in tokens[i]:\n",
    "                splitted=[' '.join(tokens[:i]), tokens[i], ' '.join(tokens[i+1:])]\n",
    "                break\n",
    "    else:\n",
    "        splitted = [row['Text'],'','']\n",
    "    return dict(zip(['text_split_0','text_split_1','text_split_2'],splitted))\n",
    "\n",
    "df.apply(lambda row: split_search_term_whitespace(row, search_word), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cephcyn/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/cephcyn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2043e629829f468e8708a3a1eb487d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1345947288.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa002cf6dbc43afabda570091223247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=414.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bd048787bc4f7fbade47c815ac1677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665132540.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "predictor.predict(\n",
    "  document=\"The woman reading a newspaper sat on the bench with her dog.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
