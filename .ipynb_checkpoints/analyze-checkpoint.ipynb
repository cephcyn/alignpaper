{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do one-time large imports\n",
    "\n",
    "# For sentence tokenization\n",
    "from nltk import tokenize\n",
    "\n",
    "# For coreference resolution\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "coref_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n",
    ")\n",
    "\n",
    "# For part-of-speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# For dependency parsing\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "dependency_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic input flags for the notebook / pipeline\n",
    "\n",
    "# Flag for whether to generate test outputs\n",
    "flag_debug = False\n",
    "\n",
    "# Identify the term we are splitting on\n",
    "search_word = \"DROP\"\n",
    "\n",
    "# TODO: what if there are other common names for this term that are unlikely to be coref-matched?\n",
    "# e.g. \"GPT-2\" vs \"GPT\" vs \"GPT-3\" etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Read in the dataframe containing entire paper abstracts (NOT pre-split into sentences)\n",
    "df = pd.read_csv(f'data/nlp-align_{search_word}.csv')\n",
    "\n",
    "# Create the outputs directory for this search word\n",
    "Path(f\"outputs/{search_word}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split apart the 'Title' and 'Abstract' columns\n",
    "def separate_title_abstract(group):\n",
    "    row = group.loc[0]\n",
    "    abs_text = tokenize.sent_tokenize(row['Abstract'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * 2,\n",
    "        'ID': [row['ID']] * 2,\n",
    "        'Type': ['Title', 'Abstract'],\n",
    "        'Text': [row['Title'], row['Abstract']]\n",
    "    })\n",
    "\n",
    "# Restructure the dataframe to be more usable...\n",
    "df = df.groupby('ID', group_keys=False).apply(\n",
    "    lambda row: separate_title_abstract(row)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "def sentence_tokenize(group):\n",
    "    row = group.reset_index(drop=True).loc[0]\n",
    "    sentences = tokenize.sent_tokenize(row['Text'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * (len(sentences)),\n",
    "        'ID': [row['ID']] * (len(sentences)),\n",
    "        'Type': [row['Type']] * (len(sentences)),\n",
    "        'Index': list(range(len(sentences))),\n",
    "        'Text': sentences\n",
    "    })\n",
    "\n",
    "df_sentences = df.groupby(['ID', 'Type'], group_keys=False).apply(\n",
    "    lambda row: sentence_tokenize(row)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a test dataframe so we can run models without taking impractically long\n",
    "# # TODO: this is causing some type inconsistencies, fix those?\n",
    "\n",
    "# temp_df = pd.DataFrame.from_dict(\n",
    "#     {'URL': 'abc', \n",
    "#      'ID': '0', \n",
    "#      'Title': 'Paper Title',\n",
    "#      'Abstract': 'The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. Petroni et al. (2019) take this as evidence that it memorizes factual knowledge during pre-training. We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.'\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting functions\n",
    "\n",
    "Assume we have an input dataframe with some number of columns, at least one of which is titled `Text` and is the column containing each sentence of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_0 is the text literally preceding the anchor\n",
    "# split_1 is the text that the anchor consists of\n",
    "# split_2 is the text literally following the anchor\n",
    "# split_tokens is the list of tokens that split identifies\n",
    "# split_anchor_span is a tuple (anchor_first_token_index, anchor_last_token_index) or null if there is no anchor\n",
    "splitting_headers = ['split_0','split_1','split_2', 'split_tokens', 'split_anchor_span']\n",
    "# Include ID, Type, Index in the split output to be able to join with df_sentences\n",
    "join_headers = ['ID', 'Type', 'Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, regardless of whitespace (if search word is A and we have word CAR, it slices it up)\n",
    "# Only splits on the first instance\n",
    "def split_term_literal_firstonly(row, search_word):\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        output = row['Text'].split(search_word, maxsplit=1)\n",
    "        output.insert(1, search_word.strip())\n",
    "        output = [i.strip() for i in output]\n",
    "        # if the beginning string is empty don't include it in the tokens list\n",
    "        pre_split = output[0].split(' ') if output[0] != '' else []\n",
    "        post_split = output[2].split(' ') if output[2] != '' else []\n",
    "        output.append(pre_split + [search_word.strip()] + post_split)\n",
    "        output.append((len(pre_split), len(pre_split)+1))\n",
    "    else:\n",
    "        output = [row['Text'].strip(),'','',row['Text'].strip().split(' '),None]\n",
    "    return dict(zip(splitting_headers+join_headers,output+list(row[join_headers])))\n",
    "\n",
    "if flag_debug:\n",
    "    literal_firstonly_output = df_sentences.apply(\n",
    "        lambda row: split_term_literal_firstonly(row, search_word), \n",
    "        axis=1, result_type='expand')\n",
    "\n",
    "    literal_firstonly_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, regardless of whitespace (if search word is A and we have word CAR, it slices it up)\n",
    "# Splits on ALL instances of the search word\n",
    "def split_term_literal(group, search_word):\n",
    "    row = group.iloc[0]\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        splits = row['Text'].split(search_word)\n",
    "        for i in range(len(splits) - 1):\n",
    "            output_i = [search_word.join(splits[:i+1]), search_word.strip(), search_word.join(splits[i+1:])]\n",
    "            output_i = [i.strip() for i in output_i]\n",
    "            # if the beginning string is empty don't include it in the tokens list\n",
    "            pre_split = output_i[0].split(' ') if output_i[0] != '' else []\n",
    "            post_split = output_i[2].split(' ') if output_i[2] != '' else []\n",
    "            output_i.append(pre_split + [search_word] + post_split)\n",
    "            output_i.append((len(pre_split), len(pre_split)+1))\n",
    "            output_i += list(row[join_headers])\n",
    "            output.append(output_i)\n",
    "    else:\n",
    "        output = [[row['Text'].strip(),'','',row['Text'].strip().split(' '),None]+list(row[join_headers])]\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    return pd.DataFrame(dict(zip(splitting_headers+join_headers,output_t)))\n",
    "\n",
    "if flag_debug:\n",
    "    literal_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "        lambda group: split_term_literal(group, search_word)).reset_index(drop=True)\n",
    "\n",
    "    literal_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, taking care to only split on whitespace\n",
    "# Only splits on the first instance\n",
    "def split_term_whitespace_firstonly(row, search_word):\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        tokens = row['Text'].split(' ')\n",
    "        for i in range(len(tokens)):\n",
    "            if search_word in tokens[i]:\n",
    "                output=[' '.join(tokens[:i]), tokens[i], ' '.join(tokens[i+1:])]\n",
    "                break\n",
    "        output = [i.strip() for i in output]\n",
    "        # if the beginning string is empty don't include it in the tokens list\n",
    "        pre_split = output[0].split(' ') if output[0] != '' else []\n",
    "        post_split = output[2].split(' ') if output[2] != '' else []\n",
    "        output.append(pre_split + [output[1]] + post_split)\n",
    "        output.append((len(pre_split), len(pre_split)+1))\n",
    "    else:\n",
    "        output = [row['Text'].strip(),'','',row['Text'].strip().split(' '),None]\n",
    "    return dict(zip(splitting_headers+join_headers,output+list(row[join_headers])))\n",
    "\n",
    "if flag_debug:\n",
    "    whitespace_firstonly_output = df_sentences.apply(\n",
    "        lambda row: split_term_whitespace_firstonly(row, search_word), \n",
    "        axis=1, result_type='expand')\n",
    "\n",
    "    whitespace_firstonly_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the search word, taking care to only split on whitespace\n",
    "# Splits on ALL instances of the search word\n",
    "def split_term_whitespace(group, search_word):\n",
    "    row = group.iloc[0]\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        tokens = row['Text'].split(' ')\n",
    "        for i in range(len(tokens)):\n",
    "            if search_word in tokens[i]:\n",
    "                output_i = [' '.join(tokens[:i]), tokens[i], ' '.join(tokens[i+1:])]\n",
    "                output_i = [i.strip() for i in output_i]\n",
    "                # if the beginning string is empty don't include it in the tokens list\n",
    "                pre_split = output_i[0].split(' ') if output_i[0] != '' else []\n",
    "                post_split = output_i[2].split(' ') if output_i[2] != '' else []\n",
    "                output_i.append(pre_split + [output_i[1]] + post_split)\n",
    "                output_i.append((len(pre_split), len(pre_split)+1))\n",
    "                output_i += list(row[join_headers])\n",
    "                output.append(output_i)\n",
    "    else:\n",
    "        output = [[row['Text'].strip(),'','',row['Text'].strip().split(' '),None]+list(row[join_headers])]\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    return pd.DataFrame(dict(zip(splitting_headers+join_headers,output_t)))\n",
    "\n",
    "if flag_debug:\n",
    "    whitespace_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "        lambda group: split_term_whitespace(group, search_word)).reset_index(drop=True)\n",
    "\n",
    "    whitespace_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run coreference resolution over the entire abstract, not individual sentences\n",
    "output = df.apply(\n",
    "    lambda row: coref_predictor.predict(row['Text']), axis=1, result_type='expand')\n",
    "df_merged = df.join(output)\n",
    "\n",
    "if flag_debug:\n",
    "    df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# transform the output of coreference resolution into something that is more easily manipulated\n",
    "# split it across multiple sentences so each indiv sentence row can still work\n",
    "def reinterpret_coref_clusters(row, search_word, sentences):\n",
    "    # Create dicts to map full-document to indiv sentence data\n",
    "    src = sentences.loc[sentences['ID'] == row['ID']].loc[sentences['Type'] == row['Type']]['Text']\n",
    "    curr_sentence = 0\n",
    "    consumed = 0\n",
    "    sent_mapping = {}\n",
    "    sent_content = {}\n",
    "    last_sent_end = 0\n",
    "    doct_mapping = {}\n",
    "    doct_split = []\n",
    "    for i in range(len(row['document'])):\n",
    "        if row['document'][i].strip() != '':\n",
    "            if row['document'][i] not in src.iloc[curr_sentence][consumed:]:\n",
    "                doct_split.append(row['document'][last_sent_end:i])\n",
    "                last_sent_end = i\n",
    "                curr_sentence += 1\n",
    "                consumed = 0\n",
    "            offset = src.iloc[curr_sentence][consumed:].index(row['document'][i])\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            if curr_sentence not in sent_content:\n",
    "                sent_content[curr_sentence] = []\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "            consumed += offset + len(row['document'][i])\n",
    "        else:\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "        doct_split.append(row['document'][last_sent_end:])\n",
    "    # Select the cluster that best matches the search word\n",
    "    selcluster_idx = -1\n",
    "    selcluster_ct = 0\n",
    "    for i in range(len(row['clusters'])):\n",
    "        currcluster_ct = 0\n",
    "        for c in row['clusters'][i]:\n",
    "            currcluster_ct += len(re.findall(f'{search_word}', ' '.join(row['document'][c[0]:c[1]+1])))\n",
    "        if currcluster_ct > selcluster_ct:\n",
    "            selcluster_idx = i\n",
    "            selcluster_ct = currcluster_ct\n",
    "    # Build the output row\n",
    "    output = [sent_mapping, sent_content, doct_mapping, selcluster_idx]\n",
    "    return dict(zip(['sent_mapping', 'sent_content', 'doct_mapping', 'selcluster_idx'],output))\n",
    "\n",
    "output = df_merged.apply(\n",
    "    lambda row: reinterpret_coref_clusters(row, search_word, df_sentences), \n",
    "    axis=1, result_type='expand')\n",
    "df_merged = df_merged.join(output)\n",
    "\n",
    "df_merged.to_csv(f'outputs/{search_word}/coreference-partial.csv')\n",
    "if flag_debug:\n",
    "    df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split based on co-references to any phrase containing search term, using allennlp coreference resolution\n",
    "# This does NOT preserve the original sentence spacing\n",
    "# REQUIRES THAT WE ALREADY RAN THE COREFERENCE PREDICTOR - this func does NOT do all of the work!\n",
    "# Only splits on the first instance\n",
    "def split_term_coreference_firstonly(row, search_word, lookup, fallback):\n",
    "    # there's probably a cleaner way to do this...\n",
    "    lookup_row = lookup.loc[lookup['ID']==row['ID']].loc[lookup['Type']==row['Type']].to_dict(orient='records')[0]\n",
    "    if lookup_row['selcluster_idx'] == -1:\n",
    "        # if we didn't identify any clusters that match the search term, use our fallback method\n",
    "        return fallback(row, search_word)\n",
    "    split_clusters = lookup_row['clusters'][lookup_row['selcluster_idx']]\n",
    "    output = []\n",
    "    for i in range(len(split_clusters)):\n",
    "        c = split_clusters[i]\n",
    "        if lookup_row['sent_mapping'][c[0]] == lookup_row['sent_mapping'][c[0]] == row['Index']:\n",
    "            sentence_start = lookup_row['sent_content'][row['Index']][0]\n",
    "            sentence_end = lookup_row['sent_content'][row['Index']][-1]\n",
    "            pre_split = lookup_row['document'][sentence_start:c[0]]\n",
    "            anchor = lookup_row['document'][c[0]:c[1]+1]\n",
    "            post_split = lookup_row['document'][c[1]+1:sentence_end+1]\n",
    "            output=[' '.join(pre_split),\n",
    "                    ' '.join(anchor),\n",
    "                    ' '.join(post_split)]\n",
    "            output.append(lookup_row['document'][sentence_start:sentence_end+1])\n",
    "            output.append((len(pre_split), len(pre_split)+len(anchor)))\n",
    "            break\n",
    "    if output == []:\n",
    "        # if there wasn't any reference in the sentence found, use our fallback method\n",
    "        return fallback(row, search_word)\n",
    "    return dict(zip(splitting_headers+join_headers,output+list(row[join_headers])))\n",
    "\n",
    "if flag_debug:\n",
    "    coreference_firstonly_output = df_sentences.apply(\n",
    "        lambda row: split_term_coreference_firstonly(row, search_word, df_merged, split_term_whitespace_firstonly), \n",
    "        axis=1, result_type='expand')\n",
    "\n",
    "    coreference_firstonly_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split based on co-references to any phrase containing search term, using allennlp coreference resolution\n",
    "# This does NOT preserve the original sentence spacing\n",
    "# REQUIRES THAT WE ALREADY RAN THE COREFERENCE PREDICTOR - this func does NOT do all of the work!\n",
    "# Splits on ALL instances of references to the search word\n",
    "def split_term_coreference(group, search_word, lookup, fallback):\n",
    "    row = group.iloc[0]\n",
    "    # there's probably a cleaner way to do this...\n",
    "    lookup_row = lookup.loc[lookup['ID']==row['ID']].loc[lookup['Type']==row['Type']].to_dict(orient='records')[0]\n",
    "    if lookup_row['selcluster_idx'] == -1:\n",
    "        # if we didn't identify any clusters that match the search term, use our fallback method\n",
    "        return fallback(group, search_word)\n",
    "    split_clusters = lookup_row['clusters'][lookup_row['selcluster_idx']]\n",
    "    output = []\n",
    "    for i in range(len(split_clusters)):\n",
    "        c = split_clusters[i]\n",
    "        if lookup_row['sent_mapping'][c[0]] == lookup_row['sent_mapping'][c[0]] == row['Index']:\n",
    "            sentence_start = lookup_row['sent_content'][row['Index']][0]\n",
    "            sentence_end = lookup_row['sent_content'][row['Index']][-1]\n",
    "            pre_split = lookup_row['document'][sentence_start:c[0]]\n",
    "            anchor = lookup_row['document'][c[0]:c[1]+1]\n",
    "            post_split = lookup_row['document'][c[1]+1:sentence_end+1]\n",
    "            output_i=[' '.join(pre_split),\n",
    "                    ' '.join(anchor),\n",
    "                    ' '.join(post_split)]\n",
    "            output_i.append(lookup_row['document'][sentence_start:sentence_end+1])\n",
    "            output_i.append((len(pre_split), len(pre_split)+len(anchor)))\n",
    "            output_i += list(row[join_headers])\n",
    "            output.append(output_i)\n",
    "    if output == []:\n",
    "        # if there wasn't any reference in the sentence found, use our fallback method\n",
    "        return fallback(group, search_word)\n",
    "    # Transpose the output format so we can use it in zip for dataframe generation\n",
    "    output_t = [list(t) for t in list(zip(*output))]\n",
    "    return pd.DataFrame(dict(zip(splitting_headers+join_headers,output_t)))\n",
    "\n",
    "if flag_debug:\n",
    "    coreference_output = df_sentences.groupby(df_sentences.index, group_keys=False).apply(\n",
    "        lambda group: split_term_coreference(group, search_word, df_merged, split_term_whitespace)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    coreference_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping functions\n",
    "\n",
    "Assume we have an input dataframe with column headers `['split_0','split_1','split_2', 'split_tokens', 'split_anchor_span']`\n",
    "\n",
    "`'split_1'` is the column that contains our search term / anchor point\n",
    "\n",
    "`'split_0'` and `'split_2'` are the columns that contain text before and after the search terms respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group is the text uniquely identifying a group\n",
    "grouping_headers = ['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the literal first word that comes after the anchor point\n",
    "def group_first_word(row):\n",
    "    if (row['split_anchor_span'] is not None) and \\\n",
    "            row['split_anchor_span'][1] < len(row['split_tokens']):\n",
    "        output = [row['split_tokens'][row['split_anchor_span'][1]]]\n",
    "    else:\n",
    "        output = ['']\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "if flag_debug:\n",
    "    output = sample_input.apply(\n",
    "        lambda row: group_first_word(row), \n",
    "        axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "    output = sample_input.join(output)\n",
    "    output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the first verb that comes after the anchor point, using NLTK part-of-speech tagging\n",
    "def group_first_verb(row):\n",
    "    tokens = [nltk.word_tokenize(row['split_0']), \n",
    "              nltk.word_tokenize(row['split_1']),\n",
    "              nltk.word_tokenize(row['split_2'])]\n",
    "    tokens_pos = nltk.pos_tag([item for sublist in tokens for item in sublist])\n",
    "    verb = ''\n",
    "    for i in range(len(tokens[0])+len(tokens[1]), len(tokens_pos)):\n",
    "        if tokens_pos[i][1].startswith('V'):\n",
    "            verb = tokens_pos[i][0]\n",
    "            break\n",
    "    output = [verb]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "if flag_debug:\n",
    "    output = sample_input.apply(\n",
    "        lambda row: group_first_verb(row), \n",
    "        axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "    output = sample_input.join(output)\n",
    "    output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do dependency parsing once for the entire sample_input to save processing time\n",
    "# for groupings that require dependency parsing later\n",
    "def parse_dependency(row):\n",
    "    p = dependency_predictor.predict(\n",
    "        sentence=' '.join(row['split_tokens']).strip()\n",
    "    )\n",
    "    return dict(zip(['dependency_parse'], [p]))\n",
    "\n",
    "if flag_debug:\n",
    "    sample_input_dep = sample_input.apply(\n",
    "        lambda row: parse_dependency(row), \n",
    "        axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group on the main verb in the sentence, using allennlp dependency parsing (based on demo code)\n",
    "def group_main_verb(row):\n",
    "    p = row['dependency_parse']\n",
    "    output = [p['hierplane_tree']['root']['word']]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "if flag_debug:\n",
    "    output = sample_input.join(sample_input_dep).apply(\n",
    "        lambda row: group_main_verb(row), \n",
    "        axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "    output = sample_input.join(output)\n",
    "    output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for phrase POS\n",
    "# Return the match bounds of the sequence of elements of given sizes starting at list1[i1] and list2[i2] \n",
    "# that match\n",
    "# If no given size is returned, returns max matching sequence length\n",
    "# (ratio of element matches must be 1:some or some:1 between l1 and l2)\n",
    "# Returns [(l1 bounds), (l2 bounds)] or None if they do not match\n",
    "def list_elements_match(list1, list2, i1, i2, size1=None, size2=None):\n",
    "    matchlen = 0\n",
    "    if size1 is not None and size2 is not None:\n",
    "        # check for exact text match\n",
    "        matchlen = len(''.join(list1[i1:i1+size1]))\n",
    "        if ''.join(list1[i1:i1+size1]) != ''.join(list2[i2:i2+size2]):\n",
    "            return None\n",
    "    elif size1 is not None:\n",
    "        # and size2 is none\n",
    "        matchlen = len(''.join(list1[i1:i1+size1]))\n",
    "        if ''.join(list1[i1:i1+size1]) != ''.join(list2[i2:])[:matchlen]:\n",
    "            return None\n",
    "    elif size2 is not None:\n",
    "        # and size1 is none\n",
    "        matchlen = len(''.join(list2[i2:i2+size2]))\n",
    "        if ''.join(list2[i2:i2+size2]) != ''.join(list1[i1:])[:matchlen]:\n",
    "            return None\n",
    "    else:\n",
    "        # both are none; just calculate the match length\n",
    "        matchlen = 0\n",
    "        while l1concat[matching] == l2concat[matching]:\n",
    "            matchlen += 1\n",
    "    matchphrase = ''.join(list1[i1:])[:matchlen]\n",
    "    # get the exact bounds for list1\n",
    "    bound1 = 1\n",
    "    for i in range(len(list1)-i1+1):\n",
    "        if ''.join(list1[i1:i1+i]) == matchphrase:\n",
    "            bound1 = i\n",
    "            break\n",
    "    # get the exact bounds for list2\n",
    "    bound2 = 1\n",
    "    for i in range(len(list2)-i2+1):\n",
    "        if ''.join(list2[i2:i2+i]) == matchphrase:\n",
    "            bound2 = i\n",
    "            break\n",
    "    return [(i1, i1+bound1), (i2, i2+bound2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group on the POS of the anchor point, using allennlp dependency parsing (based on demo code)\n",
    "# I'm defining the \"POS of a phrase\" as the POS of the lowest node that contains the entire phrase\n",
    "def group_pos_anchor(row, context=1):\n",
    "    if row['split_anchor_span'] is None:\n",
    "        return dict(zip(grouping_headers, ['']))\n",
    "    p = row['dependency_parse']\n",
    "    # build out a more usable version of the dependency tree with information about tree level!\n",
    "    # tree_array[n] = (parent, level) so the root node is (0, 0)\n",
    "    tree_array = [(h, -1) for h in p['predicted_heads']]\n",
    "    need_connection = [0]\n",
    "    level = 0\n",
    "    while len(need_connection) > 0:\n",
    "        need_connection_update = []\n",
    "        for i in range(len(tree_array)):\n",
    "            if tree_array[i][0] in need_connection:\n",
    "                tree_array[i] = (tree_array[i][0], level)\n",
    "                need_connection_update.append(i+1)\n",
    "        need_connection = need_connection_update\n",
    "        level += 1\n",
    "    # Figure out what indexes our anchor fits into\n",
    "    # Assume that the anchor is contiguous text\n",
    "    # TODO: is this always true?\n",
    "    for i in range(len(p['words'])):\n",
    "        match = list_elements_match(\n",
    "            p['words'], row['split_tokens'], i, row['split_anchor_span'][0], \n",
    "            size2=row['split_anchor_span'][1]-row['split_anchor_span'][0])\n",
    "        if match is not None:\n",
    "            break\n",
    "    # Find the smallest containing dependency node\n",
    "    matching_nodes = [(i, tree_array[i][0], tree_array[i][1]) for i in range(match[0][0], match[0][1])]\n",
    "    matching_nodes = list(set(matching_nodes))\n",
    "    while len(matching_nodes) > 1:\n",
    "        matching_nodes.sort(key=lambda x: x[2])\n",
    "        parent = matching_nodes.pop()[1]\n",
    "        matching_nodes.append((parent-1, tree_array[parent-1][0], tree_array[parent-1][1]))\n",
    "        matching_nodes = list(set(matching_nodes))\n",
    "    labeltiers = []\n",
    "    while len(labeltiers) < context:\n",
    "        # TODO include the actual word of the node too?\n",
    "        labeltiers.append(p['pos'][matching_nodes[0][0]])\n",
    "        parent = matching_nodes[0][1]\n",
    "        if parent == 0:\n",
    "            break\n",
    "        matching_nodes[0] = (parent-1, tree_array[parent-1][0], tree_array[parent-1][1])\n",
    "    return dict(zip(grouping_headers, [labeltiers]))\n",
    "\n",
    "if flag_debug:\n",
    "    output = sample_input.join(sample_input_dep).apply(\n",
    "        lambda row: group_pos_anchor(row, context=3), \n",
    "        axis=1, result_type='expand')\n",
    "\n",
    "    output = sample_input.join(output)\n",
    "    output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export whitespace-based split data\n",
    "splitted_sentences = df_sentences.merge(whitespace_output,\n",
    "                                  how='outer',\n",
    "                                  left_on=join_headers,\n",
    "                                  right_on=join_headers)\n",
    "\n",
    "splitted_sentences_dep = splitted_sentences.apply(\n",
    "    lambda row: parse_dependency(row), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output = splitted_sentences.apply(\n",
    "    lambda row: group_first_word(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "splitted_sentences.join(output).to_csv(f'outputs/{search_word}/whitespace_firstword.csv')\n",
    "\n",
    "output = splitted_sentences.apply(\n",
    "    lambda row: group_first_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "splitted_sentences.join(output).to_csv(f'outputs/{search_word}/whitespace_firstverb.csv')\n",
    "\n",
    "output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "    lambda row: group_main_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "splitted_sentences.join(output).to_csv(f'outputs/{search_word}/whitespace_mainverb.csv')\n",
    "\n",
    "output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "    lambda row: group_pos_anchor(row, context=3), \n",
    "    axis=1, result_type='expand')\n",
    "splitted_sentences.join(output).to_csv(f'outputs/{search_word}/whitespace_anchorpos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export coreference-based split data\n",
    "splitted_sentences = df_sentences.merge(coreference_output,\n",
    "                                  how='outer',\n",
    "                                  left_on=join_headers,\n",
    "                                  right_on=join_headers)\n",
    "\n",
    "splitted_sentences_dep = splitted_sentences.apply(\n",
    "    lambda row: parse_dependency(row), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output = splitted_sentences.apply(\n",
    "    lambda row: group_first_word(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "splitted_sentences.join(output).to_csv(f'outputs/{search_word}/coreference_firstword.csv')\n",
    "\n",
    "output = splitted_sentences.apply(\n",
    "    lambda row: group_first_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "splitted_sentences.join(output).to_csv(f'outputs/{search_word}/coreference_firstverb.csv')\n",
    "\n",
    "output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "    lambda row: group_main_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "splitted_sentences.join(output).to_csv(f'outputs/{search_word}/coreference_mainverb.csv')\n",
    "\n",
    "output = splitted_sentences.join(splitted_sentences_dep).apply(\n",
    "    lambda row: group_pos_anchor(row, context=3), \n",
    "    axis=1, result_type='expand')\n",
    "splitted_sentences.join(output).to_csv(f'outputs/{search_word}/coreference_anchorpos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "pd.read_csv(f'outputs/{search_word}/coreference_mainverb.csv').sort_values(by=['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
