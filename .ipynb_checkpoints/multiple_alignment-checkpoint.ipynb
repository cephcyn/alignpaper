{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model source: https://code.google.com/archive/p/word2vec/\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'model/GoogleNews-vectors-negative300.bin', \n",
    "    binary=True)\n",
    "cached_word2vec_phrases = {}\n",
    "\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word2vec embedding of a phrase\n",
    "def get_phrase_embed_word2vec(word2vec, phrase, remove_label=False):\n",
    "    try:\n",
    "        phraseS = phrase.split()\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    unknowns = []\n",
    "    emb = []\n",
    "    for w in phraseS:\n",
    "        try:\n",
    "            emb.append(word2vec[w])\n",
    "        except:\n",
    "            unknowns.append(w)\n",
    "    if len(emb) == 0:\n",
    "        return pd.DataFrame()\n",
    "    emb_sum = pd.DataFrame(emb).sum() / len(phraseS)\n",
    "    if not remove_label:\n",
    "        emb_sum['word'] = phrase\n",
    "    return pd.DataFrame([emb_sum])\n",
    "\n",
    "get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'test sentence')\n",
    "# get_phrase_embed_word2vec(\n",
    "#     word2vec, \n",
    "#     'This is a test sentence !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import sample dataset\n",
    "(The code to construct the file `temp/ebm-pio_consegments.hdf` is in analyze.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data we've already constructed out of constituency parse of specific phrases in specific sentences\n",
    "con_segments = pd.read_hdf(f'temp/ebm-pio_consegments.hdf','mydata')\n",
    "con_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform that data into the format that is more readable for alignment\n",
    "# (sorry, this is sort of an abuse of DataFrame datatypes)\n",
    "\n",
    "def transformTuples(row):\n",
    "    # turn each row into the segment tuples used for alignment\n",
    "    output = pd.DataFrame()\n",
    "    for i in range(len(row['alignsegments'])):\n",
    "        output[f'txt{i}'] = [(row['alignsegments'][i], row['aligntypes'][i], row['alignctypes'][i])]\n",
    "    return output.set_index(pd.Series([row.name]))\n",
    "\n",
    "transformTuples(con_segments.loc[7298])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_df = con_segments.groupby(con_segments.index, group_keys=False).apply(\n",
    "    lambda group: transformTuples(group.iloc[0]))\n",
    "alignment_df = alignment_df.applymap(lambda x: ('', '', []) if x is np.nan else x)\n",
    "alignment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment operations / transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeAdjacentNP(row):\n",
    "    # merge adjacent noun phrases\n",
    "    # TODO adapt this for alignment df format!!!\n",
    "    # TODO fix the deep copy bug\n",
    "    output = row.copy()\n",
    "    for i in reversed(range(len(row['alignsegments'])-1)):\n",
    "        if row['aligntypes'][i]=='NP' and row['aligntypes'][i+1]=='NP':\n",
    "            row['alignsegments'][i] += ' ' + row['alignsegments'][i+1]\n",
    "            row['alignsegments'][i+1] = []\n",
    "            row['aligntypes'][i+1] = []\n",
    "            row['alignctypes'][i] += row['alignctypes'][i+1]\n",
    "            row['alignctypes'][i+1] = []\n",
    "    row = row.drop('aligntup')\n",
    "    row['alignsegments'] = [e for e in row['alignsegments'] if e != []]\n",
    "    row['aligntypes'] = [e for e in row['aligntypes'] if e != []]\n",
    "    row['alignctypes'] = [e for e in row['alignctypes'] if e != []]\n",
    "    return output\n",
    "\n",
    "# temp_df = con_segments\n",
    "# temp_df = temp_df.apply(\n",
    "#     lambda row: mergeAdjacentNP(row), \n",
    "#     axis=1, result_type='expand')\n",
    "# temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_df.loc[[5130, 2552, 5126]]\n",
    "\n",
    "extractTup(alignment_df.loc[[5130]], tup_i='segment', is_frame=True)\n",
    "extractTup(alignment_df.loc[[5130]], tup_i='pos', is_frame=True)\n",
    "extractTup(alignment_df.loc[[5130]], tup_i='cpos', is_frame=True)\n",
    "\n",
    "alignment_df.loc[[5130]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this is still buggy, don't use it\n",
    "def mergeParentheses(row):\n",
    "    # merge parenthetical clauses\n",
    "    numOpenParens = 0\n",
    "    lastStart = -1\n",
    "    mergeSegments = []\n",
    "    for i in range(len(row['alignsegments'])):\n",
    "        for c in [c for c in row['alignsegments'][i] if c in ['(', ')']]:\n",
    "            if c == '(':\n",
    "                numOpenParens += 1\n",
    "                if numOpenParens == 1:\n",
    "                    lastStart = i\n",
    "            else:\n",
    "                numOpenParens -= 1\n",
    "                if numOpenParens == 0 and lastStart != i:\n",
    "                    # close the parentheses\n",
    "                    mergeSegments.append((lastStart, i))\n",
    "    if numOpenParens > 0:\n",
    "        mergeSegments.append((lastStart, len(row['alignsegments'])))\n",
    "    mergeSegments = list(set(mergeSegments))\n",
    "    if mergeSegments != []:\n",
    "        for t in reversed(mergeSegments):\n",
    "            print(row['aligntypes'][t[0]:t[1]+1])\n",
    "            print(row['alignsegments'][t[0]:t[1]+1])\n",
    "        print()\n",
    "    return row\n",
    "\n",
    "# temp_df = con_segments\n",
    "# temp_df.apply(\n",
    "#     lambda row: mergeParentheses(row), \n",
    "#     axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTup(data, tup_i=0, is_frame=True):\n",
    "    types = {\n",
    "        'segment': 0,\n",
    "        'pos': 1,\n",
    "        'cpos': 2\n",
    "    }\n",
    "    if tup_i in types:\n",
    "        tup_i = types[tup_i]\n",
    "    else:\n",
    "        raise ValueError(f'tup_i not in types: {types.keys()}')\n",
    "    if is_frame:\n",
    "        return data.applymap(lambda x: x[tup_i])\n",
    "    else:\n",
    "        return data.map(lambda x: x[tup_i])\n",
    "\n",
    "# extractTup(transformTuples(temp_df.loc[7298]), tup_i='segment', is_frame=True)\n",
    "extractTup(alignment_df.loc[[7298]], tup_i='segment', is_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEmptyColumns(align_df):\n",
    "    for c in align_df.columns:\n",
    "        align_df_c = extractTup(align_df.loc[:, c], tup_i='segment', is_frame=False)\n",
    "        if len([e for e in align_df_c if e != '']) == 0:\n",
    "            del align_df[c]\n",
    "    align_df.columns = [f'txt{i}' for i in range(len(align_df.columns))]\n",
    "    return align_df\n",
    "\n",
    "removeEmptyColumns(alignment_df.loc[[7298, 7321]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def alignRowMajorLocal(align_a, align_b, use_types=False, remove_empty_cols=False, debug_print=False):\n",
    "    # An implementation of Smith-Waterman alignment\n",
    "    # RETURNS: \n",
    "    #  1. The alignment DataFrame\n",
    "    #  2. The score associated with this alignment\n",
    "    def removeEmptyColumns(align_df):\n",
    "        output_columns = []\n",
    "        for c in align_df.columns:\n",
    "            align_df_c = extractTup(align_df.loc[:, c], tup_i='segment', is_frame=False)\n",
    "            if len([e for e in align_df_c if e.strip() != '']) != 0:\n",
    "                output_columns.append(c)\n",
    "        output_df = align_df[output_columns]\n",
    "        output_df.columns = [f'txt{i}' for i in range(len(output_df.columns))]\n",
    "        return output_df\n",
    "    if remove_empty_cols:\n",
    "        align_a = removeEmptyColumns(align_a)\n",
    "        align_b = removeEmptyColumns(align_b)\n",
    "    align_a_segment = extractTup(align_a, tup_i='segment')\n",
    "    align_b_segment = extractTup(align_b, tup_i='segment')\n",
    "    align_a_type = extractTup(align_a, tup_i='pos')\n",
    "    align_b_type = extractTup(align_b, tup_i='pos')\n",
    "    align_a_ctype = extractTup(align_a, tup_i='cpos')\n",
    "    align_b_ctype = extractTup(align_b, tup_i='cpos')\n",
    "    # If we are aligning purely on NP elements... not implemented currently.\n",
    "#     align_a_elems = [i for i in range(len(align_a.columns)) \n",
    "#                      if 'NP' in set(align_a_type[align_a.columns[i]])]\n",
    "#     align_b_elems = [i for i in range(len(align_b.columns)) \n",
    "#                      if 'NP' in set(align_b_type[align_a.columns[i]])]\n",
    "    # If we are doing a general alignment\n",
    "    align_a_elems = [i for i in range(len(align_a.columns))]\n",
    "    align_b_elems = [i for i in range(len(align_b.columns))]\n",
    "    if debug_print:\n",
    "        print(align_a_elems)\n",
    "        print(align_b_elems)\n",
    "        print()\n",
    "    def getScoreAligningIndices(index_a, index_b):\n",
    "        # A higher score is better / more match!\n",
    "        # make sure all the segment texts are precomputed lol\n",
    "        text_a = list(align_a_segment[align_a.columns[index_a]])\n",
    "        text_b = list(align_b_segment[align_b.columns[index_b]])\n",
    "        for text in text_a+text_b:\n",
    "            if text not in cached_word2vec_phrases:\n",
    "                try:\n",
    "                    cached_word2vec_phrases[text] = get_phrase_embed_word2vec(word2vec, text).drop('word', 1)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        # start off with phrase embedding distance (current max is 60 for perfect match)\n",
    "        # if we have embeds for any word in each set, ignore others and just use words we have embeds for\n",
    "        if any(s in cached_word2vec_phrases for s in text_a)\\\n",
    "                and any(s in cached_word2vec_phrases for s in text_b):\n",
    "            # calculate overall embeds\n",
    "            embed_a = pd.concat([cached_word2vec_phrases[text] for text \n",
    "                                 in text_a if text in cached_word2vec_phrases]).apply(lambda x: x.mean())\n",
    "            embed_b = pd.concat([cached_word2vec_phrases[text] for text \n",
    "                                 in text_b if text in cached_word2vec_phrases]).apply(lambda x: x.mean())\n",
    "            # TODO can tweak this scoring calculation a little for performance\n",
    "            score = 10 * (6 - np.linalg.norm(embed_a-embed_b))\n",
    "        else:\n",
    "            # use levenshtein dist as fallback... if either set has NO words with embeds available\n",
    "            scaled_edits_sum = 0\n",
    "            for phrase_a in [p for p in text_a if len(p) != 0]:\n",
    "                for phrase_b in [p for p in text_b if len(p) != 0]:\n",
    "                    scaled_edits_sum += edit_distance(phrase_a,phrase_b) / max(len(phrase_a), len(phrase_b))\n",
    "            score = 60 * (1 - (scaled_edits_sum / (len(text_a) * len(text_b))))\n",
    "        # add a component based on phrase type if flag is set to true (by default it is)\n",
    "        # TODO improve this?; this currently just returns -inf if mismatch of type sets\n",
    "        # Might want to add support for aligning different types of phrase together...\n",
    "        if use_types:\n",
    "            types_a = set([t for t in align_a_type[align_a.columns[index_a]] if t != ''])\n",
    "            types_b = set([t for t in align_b_type[align_b.columns[index_b]] if t != ''])\n",
    "            if len(types_a) != 0 and len(types_b) != 0 and types_a != types_b:\n",
    "                score = -1 * math.inf\n",
    "        # TODO: add a component based on phrase ctype (phrase POS breakdown) (?)\n",
    "        if debug_print:\n",
    "            print(f'scoring between '\n",
    "                  +f'\"{list(align_a_segment[align_a.columns[index_a]])}\" and '\n",
    "                  +f'\"{list(align_b_segment[align_b.columns[index_b]])}\": {score}')\n",
    "        return score\n",
    "    def getGapPenalty(length):\n",
    "        return -1 * (1 * min(length,1) + 0.5 * max(length-1,0))\n",
    "    # Build score matrix of size (a-alignables + 1)x(b-alignables + 1)\n",
    "    scores = np.zeros((len(align_a_elems)+1, len(align_b_elems)+1))\n",
    "    # Build traceback matrix\n",
    "    # traceback = 0 for end, 4 for W, 7 for NW, 9 for N (to calculate traceback, t%2 is N-ness, t%3 is W-ness)\n",
    "    traceback = np.zeros((len(align_a_elems)+1, len(align_b_elems)+1))\n",
    "    # Iterate through all of the cells to populate both the score and traceback matrices\n",
    "    for i in range(1, scores.shape[0]):\n",
    "        for j in range(1, scores.shape[1]):\n",
    "            score_map = {}\n",
    "            # calculate score for aligning nouns a[i] and b[j]\n",
    "            score_map[\n",
    "                scores[i-1,j-1] + getScoreAligningIndices(align_a_elems[i-1], align_b_elems[j-1])\n",
    "            ] = 7\n",
    "            # calculate score for gap in i\n",
    "            for i_gap in range(1, i):\n",
    "                igap_score = scores[i-i_gap,j] + getGapPenalty(i_gap)\n",
    "                score_map[igap_score] = 9\n",
    "            # calculate score for gap in j\n",
    "            for j_gap in range(1, j):\n",
    "                jgap_score = scores[i,j-j_gap] + getGapPenalty(j_gap)\n",
    "                score_map[jgap_score] = 4\n",
    "            # add the possibility for unrelatedness\n",
    "            score_map[0] = 0\n",
    "            if debug_print:\n",
    "                print(score_map)\n",
    "            scores[i,j] = max(score_map.keys())\n",
    "            traceback[i,j] = score_map[max(score_map.keys())]\n",
    "    if debug_print:\n",
    "        print()\n",
    "        print(scores)\n",
    "        print(traceback)\n",
    "        print()\n",
    "    # Do traceback to build our final alignment\n",
    "    tracepoint = np.unravel_index(np.argmax(scores, axis=None), scores.shape)\n",
    "    points_a = []\n",
    "    points_b = []\n",
    "    while traceback[tracepoint] != 0:\n",
    "        # contribute to the align information\n",
    "        if traceback[tracepoint] == 7:\n",
    "            # this is a point where two elements were aligned\n",
    "            points_a.append(align_a_elems[tracepoint[0]-1])\n",
    "            points_b.append(align_b_elems[tracepoint[1]-1])\n",
    "        elif traceback[tracepoint] == 4:\n",
    "            # this is a point where there was a gap inserted for row_a\n",
    "            points_a.append(-1)\n",
    "            points_b.append(align_b_elems[tracepoint[1]-1])\n",
    "        elif traceback[tracepoint] == 9:\n",
    "            # this is a point where there was a gap inserted for row_b\n",
    "            points_a.append(align_a_elems[tracepoint[0]-1])\n",
    "            points_b.append(-1)\n",
    "        # step backwards\n",
    "        tracepoint = (\n",
    "            tracepoint[0] - int(traceback[tracepoint] % 2),\n",
    "            tracepoint[1] - int(traceback[tracepoint] % 3))\n",
    "    points_a = list(reversed(points_a))\n",
    "    points_b = list(reversed(points_b))\n",
    "    if len(points_a) != len(points_b):\n",
    "        # enforce that align_a and align_b are the same length (they should be)\n",
    "        raise ValueError('should not occur; bug in S-W local alignment?')\n",
    "    if debug_print:\n",
    "        print(points_a)\n",
    "        print(points_b)\n",
    "        print()\n",
    "    # Create a nice neat form of this alignment\n",
    "    # TODO add support for NP-only alignment gaps?\n",
    "    range_a = [i for i in points_a if i >= 0]\n",
    "    range_b = [i for i in points_b if i >= 0]\n",
    "    range_a = (range_a[0], range_a[-1])\n",
    "    range_b = (range_b[0], range_b[-1])\n",
    "    output = pd.DataFrame(columns=[f'txt{i}' for i in range(\n",
    "        (range_a[0] + range_b[0]) + len(points_a)\n",
    "        + max(0, (len(align_a.columns) - range_a[1]) - 1) \n",
    "        + max(0, (len(align_b.columns) - range_b[1]) - 1)\n",
    "    )])\n",
    "    # build the segment from align_a\n",
    "    realign_a = align_a.loc[:, [f'txt{i}' for i in range(range_a[0])]]\n",
    "    for i in range(range_b[0]):\n",
    "        realign_a.insert(len(realign_a.columns), f'insx{i}', np.nan, True)\n",
    "    for i in points_a:\n",
    "        if i >= 0:\n",
    "            realign_a[align_a.columns[i]] = align_a.loc[:, align_a.columns[i]]\n",
    "        else:\n",
    "            realign_a.insert(len(realign_a.columns), f'ins{len(realign_a.columns)}', np.nan, True)\n",
    "    for i in range(range_a[1]+1, len(align_a.columns)):\n",
    "        realign_a[align_a.columns[i]] = align_a.loc[:, align_a.columns[i]]\n",
    "    for i in range(range_b[1]+1, len(align_b.columns)):\n",
    "        realign_a.insert(len(realign_a.columns), f'insx{i+range_b[0]}', np.nan, True)\n",
    "    # build the segment from align_b\n",
    "    realign_b = align_b.loc[:, [f'txt{i}' for i in range(range_b[0])]]\n",
    "    for i in range(range_a[0]):\n",
    "        realign_b.insert(0, f'insx{i}', np.nan, True)\n",
    "    for i in points_b:\n",
    "        if i >= 0:\n",
    "            realign_b[align_b.columns[i]] = align_b.loc[:, align_b.columns[i]]\n",
    "        else:\n",
    "            realign_b.insert(len(realign_b.columns), f'ins{len(realign_b.columns)}', np.nan, True)\n",
    "    for i in range(range_a[1]+1, len(align_a.columns)):\n",
    "        realign_b.insert(len(realign_b.columns), f'insx{i+range_a[0]}', np.nan, True)\n",
    "    for i in range(range_b[1]+1, len(align_b.columns)):\n",
    "        realign_b[align_b.columns[i]] = align_b.loc[:, align_b.columns[i]]\n",
    "    # build final output\n",
    "    realign_a.columns = output.columns\n",
    "    realign_b.columns = output.columns\n",
    "    output = output.append(realign_a)\n",
    "    output = output.append(realign_b)\n",
    "    return output.applymap(lambda x: ('', '', []) if x is np.nan else x), np.amax(scores, axis=None)\n",
    "\n",
    "toy_align, toy_align_score = alignRowMajorLocal(\n",
    "    alignment_df.loc[[7298]], \n",
    "    alignment_df.loc[[7321]], \n",
    "    remove_empty_cols=True)\n",
    "print(toy_align_score)\n",
    "toy_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the toy data\n",
    "toy_data = pd.DataFrame(\n",
    "    ['Asperger syndrome', \n",
    "     'high - functioning ASD', \n",
    "     'unrecognized and untreated anxiety', \n",
    "     'generalized anxiety disorders', \n",
    "     'anxiety', \n",
    "     'high - functioning autism spectrum disorders and anxiety', \n",
    "     'high - functioning ASD and anxiety', \n",
    "     'high - functioning ASD', \n",
    "     'high - functioning autism spectrum disorders', \n",
    "     'previously undetected anxiety', \n",
    "     'untreated anxiety']\n",
    ").rename({0: 'txt'}, axis=1)\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column split step 1: Build word tree with node = word units running right->left\n",
    "\n",
    "# add text to the given trienode\n",
    "def wordTreeHelper(tree_node, text, id_data=None, right_align=False):\n",
    "    text = text.strip()\n",
    "    # Check for base case\n",
    "    if text == '':\n",
    "        tree_node[id_data] = id_data\n",
    "        return tree_node\n",
    "    # Select the right key (for now, just pick the key based on right-to-left ordering)\n",
    "    key = ''\n",
    "    if right_align:\n",
    "        key = text.split(' ')[-1]\n",
    "        text = ' '.join(text.split(' ')[0:-1])\n",
    "    else:\n",
    "        key = text.split(' ')[0]\n",
    "        text = ' '.join(text.split(' ')[1:])\n",
    "    # Put the key and text into the trie\n",
    "    if key not in tree_node:\n",
    "        tree_node[key] = {}\n",
    "    tree_node[key] = wordTreeHelper(tree_node[key], text, id_data=id_data, right_align=right_align)\n",
    "    return tree_node\n",
    "\n",
    "def wordTree(df, colname, right_align=False):\n",
    "    tree = {}\n",
    "    for e_id in df.index:\n",
    "        tree = wordTreeHelper(tree, df.loc[e_id][colname], id_data=e_id, right_align=right_align)\n",
    "    return tree\n",
    "\n",
    "st = wordTree(toy_data, 'txt')\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column split step 2: Collapse the suffix trie (merge nodes with only one child)\n",
    "\n",
    "# edits the input trie\n",
    "def wordTreeCollapse(tree, right_align=False):\n",
    "    # Collapse children nodes first\n",
    "    added_keys = {}\n",
    "    removed_keys = []\n",
    "    for child in tree:\n",
    "        if type(child) is str:\n",
    "            tree[child] = wordTreeCollapse(tree[child], right_align=right_align)\n",
    "            # Check if the new child node is collapsible\n",
    "            if len(tree[child]) == 1 and type(list(tree[child])[0]) is str:\n",
    "                grandchild = list(tree[child])[0]\n",
    "                grandchild_tree = tree[child][grandchild]\n",
    "                # Perform the merge (put into edit queue)\n",
    "                removed_keys.append(child)\n",
    "                if right_align:\n",
    "                    added_keys[grandchild + ' ' + child] = grandchild_tree\n",
    "                else:\n",
    "                    added_keys[child + ' ' + grandchild] = grandchild_tree\n",
    "    # Perform removals\n",
    "    for key in removed_keys:\n",
    "        tree.pop(key)\n",
    "    # Perform additions\n",
    "    for key in added_keys:\n",
    "        tree[key] = added_keys[key]\n",
    "    return tree\n",
    "\n",
    "st = wordTreeCollapse(st)\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column split step 3: Output the suffix trie to multiple columns\n",
    "\n",
    "# Calculate how many output columns we'll need\n",
    "# Get the depth of the trie (a trie with one terminal node {0:0} has depth 0)\n",
    "def wordTreeDepth(tree):\n",
    "    max_depth = 0\n",
    "    for child in tree:\n",
    "        if type(child) is str:\n",
    "            max_depth = max(max_depth, 1 + wordTreeDepth(tree[child]))\n",
    "    return max_depth\n",
    "\n",
    "def wordTreeSplitHelper(tree, max_depth, output, so_far=[], right_align=False):\n",
    "    for child in tree:\n",
    "        if type(child) is not str:\n",
    "            # we have hit a base, put in an entry\n",
    "            if right_align:\n",
    "                output[child] = ['']*(max_depth - len(so_far)) + so_far\n",
    "            else:\n",
    "                output[child] = so_far + ['']*(max_depth - len(so_far))\n",
    "        else:\n",
    "            # this node has further children!\n",
    "            if right_align:\n",
    "                output = wordTreeSplitHelper(tree[child], \n",
    "                                             max_depth, \n",
    "                                             output, \n",
    "                                             [child] + so_far, \n",
    "                                             right_align=right_align)\n",
    "            else:\n",
    "                output = wordTreeSplitHelper(tree[child], \n",
    "                                             max_depth, \n",
    "                                             output, \n",
    "                                             so_far + [child], \n",
    "                                             right_align=right_align)\n",
    "    return output\n",
    "\n",
    "def wordTreeSplit(tree, colname, right_align=False):\n",
    "    tree_depth = wordTreeDepth(tree)\n",
    "    output = pd.DataFrame(columns=[f'{colname}{i}' for i in range(tree_depth)])\n",
    "    rearranged = wordTreeSplitHelper(tree, tree_depth, {}, right_align=right_align)\n",
    "    for id in rearranged:\n",
    "        output.loc[id] = rearranged[id]\n",
    "    return output\n",
    "    \n",
    "wordTreeSplit(st, 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitCol(src_alignment, split_col, right_align):\n",
    "    # TODO make splitCol preserve some POS information?\n",
    "    splitted = wordTreeSplit(\n",
    "        wordTreeCollapse(wordTree(\n",
    "            src_alignment,\n",
    "            split_col, \n",
    "            right_align=right_align), right_align=right_align),\n",
    "        f'{split_col}.', \n",
    "        right_align=right_align)\n",
    "    result = src_alignment.join(splitted)\n",
    "    result = result.drop(split_col, 1)\n",
    "    result = result.reindex(\n",
    "        columns=[x for _,x in sorted(zip(\n",
    "            [float(c[3:]) for c in result.columns],\n",
    "            result.columns))]\n",
    "    )\n",
    "    result.columns = [f'txt{i}' for i in range(len(result.columns))]\n",
    "    return result\n",
    "\n",
    "toy_align = splitCol(extractTup(toy_align, tup_i='segment'), 'txt0', right_align=True)\n",
    "toy_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO re-apply the old POS tags and info?\n",
    "\n",
    "def applyEmptyTup(row):\n",
    "    output = pd.DataFrame()\n",
    "    for i in row.index:\n",
    "        output[i] = [(row[i], '', [])]\n",
    "    return output.set_index(pd.Series([row.name]))\n",
    "\n",
    "# applyEmptyTup(toy_align.loc[7298])\n",
    "\n",
    "toy_align = toy_align.groupby(toy_align.index, group_keys=False).apply(\n",
    "    lambda group: applyEmptyTup(group.iloc[0])\n",
    ")\n",
    "toy_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeCol(src_alignment, merge_col):\n",
    "    # TODO make mergeCol preserve some POS information?\n",
    "    merge_col_next = src_alignment.columns[list(src_alignment.columns).index(merge_col)+1]\n",
    "    merged = src_alignment[merge_col] + ' ' + src_alignment[merge_col_next]\n",
    "    result = src_alignment.copy()\n",
    "    result[merge_col] = merged\n",
    "    del result[merge_col_next]\n",
    "    result.columns = [f'txt{i}' for i in range(len(result.columns))]\n",
    "    return result\n",
    "\n",
    "mergeCol(extractTup(toy_align, tup_i='segment'), 'txt0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [7298, 7321, 5126, 5134, 4594, 4618, 6507, 6474, 7308, 5130, 2552]:\n",
    "    print(i, temp_df[temp_df.index==i].iloc[0]['alignsegments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment in manually selected \"nice\" order with types enforced\n",
    "temp_align = []\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[5126]], alignment_df.loc[[5134]],\n",
    "                                    remove_empty_cols=True, use_types=True)[0])\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[7298]], alignment_df.loc[[7321]],\n",
    "                                    remove_empty_cols=True, use_types=True)[0])\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[4594]], alignment_df.loc[[4618]],\n",
    "                                    remove_empty_cols=True, use_types=True)[0])\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[5130]], alignment_df.loc[[2552]],\n",
    "                                    remove_empty_cols=True, use_types=True)[0])\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[6474]], alignment_df.loc[[7308]],\n",
    "                                    remove_empty_cols=True, use_types=True)[0])\n",
    "update_temp_align = []\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[2], temp_align[3], \n",
    "                                            remove_empty_cols=True, use_types=True)[0])\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[1], temp_align[4], \n",
    "                                            remove_empty_cols=True, use_types=True)[0])\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[0], alignment_df.loc[[6507]], \n",
    "                                            remove_empty_cols=True, use_types=True)[0])\n",
    "temp_align = update_temp_align\n",
    "update_temp_align = []\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[0], temp_align[2], \n",
    "                                            remove_empty_cols=True, use_types=True)[0])\n",
    "manually_aligned_group, manually_aligned_group_score = alignRowMajorLocal(\n",
    "    update_temp_align[0], temp_align[1], remove_empty_cols=True, use_types=True)\n",
    "print(manually_aligned_group_score)\n",
    "extractTup(manually_aligned_group, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment in manually selected \"nice\" order without types enforced\n",
    "temp_align = []\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[5126]], alignment_df.loc[[5134]], \n",
    "                                     remove_empty_cols=True)[0])\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[7298]], alignment_df.loc[[7321]], \n",
    "                                     remove_empty_cols=True)[0])\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[4594]], alignment_df.loc[[4618]], \n",
    "                                     remove_empty_cols=True)[0])\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[5130]], alignment_df.loc[[2552]], \n",
    "                                     remove_empty_cols=True)[0])\n",
    "temp_align.append(alignRowMajorLocal(alignment_df.loc[[6474]], alignment_df.loc[[7308]], \n",
    "                                     remove_empty_cols=True)[0])\n",
    "update_temp_align = []\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[2], temp_align[3])[0])\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[1], temp_align[4])[0])\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[0], alignment_df.loc[[6507]], \n",
    "                                            remove_empty_cols=True)[0])\n",
    "temp_align = update_temp_align\n",
    "update_temp_align = []\n",
    "update_temp_align.append(alignRowMajorLocal(temp_align[0], temp_align[2])[0])\n",
    "manually_aligned_group, manually_aligned_group_score = alignRowMajorLocal(update_temp_align[0], temp_align[1])\n",
    "print(manually_aligned_group_score)\n",
    "extractTup(manually_aligned_group, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment in random-ish order without types enforced\n",
    "temp_align = alignRowMajorLocal(alignment_df.loc[[7298]], alignment_df.loc[[7321]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[5126]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[5134]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[4594]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[4618]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[6507]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[6474]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[7308]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[5130]], remove_empty_cols=True)[0]\n",
    "manually_aligned_group2, temp_align_score = alignRowMajorLocal(\n",
    "    temp_align, alignment_df.loc[[2552]], remove_empty_cols=True)\n",
    "print(temp_align_score)\n",
    "extractTup(manually_aligned_group2, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment in random-ish order without types enforced (of a slightly different dataset!!!)\n",
    "temp_align = alignRowMajorLocal(alignment_df.loc[[7494]], alignment_df.loc[[7541]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[7549]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[7585]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[7594]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[416]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[423]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[443]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[447]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[409]], remove_empty_cols=True)[0]\n",
    "temp_align = alignRowMajorLocal(temp_align, alignment_df.loc[[1960]], remove_empty_cols=True)[0]\n",
    "manually_aligned_group2, temp_align_score = alignRowMajorLocal(\n",
    "    temp_align, alignment_df.loc[[1989]], remove_empty_cols=True)\n",
    "print(temp_align_score)\n",
    "extractTup(manually_aligned_group2, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate a merge operation\n",
    "manually_aligned_group_merge = mergeCol(extractTup(manually_aligned_group, tup_i='segment'), 'txt4')\n",
    "manually_aligned_group_merge = mergeCol(manually_aligned_group_merge, 'txt4')\n",
    "manually_aligned_group_merge = mergeCol(manually_aligned_group_merge, 'txt1')\n",
    "manually_aligned_group_merge = mergeCol(manually_aligned_group_merge, 'txt4')\n",
    "manually_aligned_group_merge.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate a split operation\n",
    "manually_aligned_group_split = splitCol(manually_aligned_group_merge, 'txt0', right_align=True)\n",
    "manually_aligned_group_split = splitCol(manually_aligned_group_split, 'txt4', right_align=False)\n",
    "manually_aligned_group_split = splitCol(manually_aligned_group_split, 'txt5', right_align=True)\n",
    "manually_aligned_group_split = splitCol(manually_aligned_group_split, 'txt7', right_align=False)\n",
    "manually_aligned_group_split.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the split operation output into something more standard alignment DF format\n",
    "manually_aligned_group_split = manually_aligned_group_split.groupby(\n",
    "    manually_aligned_group_split.index, group_keys=False).apply(\n",
    "    lambda group: applyEmptyTup(group.iloc[0])\n",
    ")\n",
    "manually_aligned_group_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-align the output of the split function using the same ordering as initial alignment\n",
    "manually_aligned_group_realign = []\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[5126]],\n",
    "                       manually_aligned_group_split.loc[[5134]],\n",
    "                       remove_empty_cols=True)[0])\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[7298]],\n",
    "                       manually_aligned_group_split.loc[[7321]],\n",
    "                       remove_empty_cols=True)[0])\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[4594]],\n",
    "                       manually_aligned_group_split.loc[[4618]],\n",
    "                       remove_empty_cols=True)[0])\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[5130]],\n",
    "                       manually_aligned_group_split.loc[[2552]],\n",
    "                       remove_empty_cols=True)[0])\n",
    "manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_split.loc[[6474]],\n",
    "                       manually_aligned_group_split.loc[[7308]],\n",
    "                       remove_empty_cols=True)[0])\n",
    "update_manually_aligned_group_realign = []\n",
    "update_manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_realign[2],\n",
    "                       manually_aligned_group_realign[3],\n",
    "                       remove_empty_cols=True)[0])\n",
    "update_manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_realign[1],\n",
    "                       manually_aligned_group_realign[4],\n",
    "                       remove_empty_cols=True)[0])\n",
    "update_manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_realign[0],\n",
    "                       manually_aligned_group_split.loc[[6507]],\n",
    "                       remove_empty_cols=True)[0])\n",
    "manually_aligned_group_realign = update_manually_aligned_group_realign\n",
    "update_manually_aligned_group_realign = []\n",
    "update_manually_aligned_group_realign.append(\n",
    "    alignRowMajorLocal(manually_aligned_group_realign[0],\n",
    "                       manually_aligned_group_realign[2],\n",
    "                       remove_empty_cols=True)[0])\n",
    "manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign[0], manually_aligned_group_realign[1], remove_empty_cols=True)[0]\n",
    "extractTup(manually_aligned_group_realign, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-align the output of the split function by a manual ordering\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    manually_aligned_group_split.loc[[7298]],\n",
    "    manually_aligned_group_split.loc[[7321]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5126]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[7308]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[6474]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[4594]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[4618]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5130]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5134]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[2552]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[6507]],\n",
    "    remove_empty_cols=True)[0]\n",
    "extractTup(update_manually_aligned_group_realign, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-align the output of the split function using a DIFFERENT manual ordering\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    manually_aligned_group_split.loc[[2552]],\n",
    "    manually_aligned_group_split.loc[[4618]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[4594]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5130]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[7298]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[7308]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[7321]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[6474]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5134]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[6507]],\n",
    "    remove_empty_cols=True)[0]\n",
    "update_manually_aligned_group_realign = alignRowMajorLocal(\n",
    "    update_manually_aligned_group_realign,\n",
    "    manually_aligned_group_split.loc[[5126]],\n",
    "    remove_empty_cols=True)[0]\n",
    "extractTup(update_manually_aligned_group_realign, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractTup(update_manually_aligned_group_realign, tup_i='segment').loc[\n",
    "#     [2552, 4594, 4618, 5126, 5130, 5134, 6474, 6507, 7298, 7308, 7321]\n",
    "# ]\n",
    "extractTup(update_manually_aligned_group_realign, tup_i='segment').loc[\n",
    "    [2552, 4594, 4618, 5130, 6474, 7298, 7308, 7321, 5126, 5134, 6507]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement a similarity score / phylo multiple sequence alignment ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment scoring / \"readability score\" functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reference_alignment = []\n",
    "reference_alignment_orderings = []\n",
    "reference_alignment_scores = []\n",
    "\n",
    "temp_ids_list = [\n",
    "    [5130, 5126, 6507, 6474, 7308, 5134, 2552, 4618, 7298, 4594, 7321], # decent\n",
    "    [7321, 5134, 4594, 6507, 2552, 5130, 7298, 7308, 4618, 6474, 5126], # decent\n",
    "    [5134, 7298, 4618, 6507, 7321, 5126, 6474, 5130, 4594, 2552, 7308], # bad\n",
    "    [5126, 5134, 4618, 7298, 6507, 5130, 6474, 4594, 7308, 2552, 7321], # quite bad (chaotic)\n",
    "    [5126, 7321, 7298, 5130, 6474, 4618, 4594, 7308, 2552, 6507, 5134]  # quite bad (sharply split)\n",
    "]\n",
    "\n",
    "for i in range(len(temp_ids_list)):\n",
    "    temp_ids = temp_ids_list[i]\n",
    "    alignment, alignment_score = alignRowMajorLocal(\n",
    "        alignment_df.loc[[temp_ids[0]]], \n",
    "        alignment_df.loc[[temp_ids[1]]], \n",
    "        remove_empty_cols=True\n",
    "    )\n",
    "    temp_scores = [0, alignment_score]\n",
    "    for j in range(2, len(temp_ids)):\n",
    "        alignment, alignment_score = alignRowMajorLocal(\n",
    "            alignment,\n",
    "            alignment_df.loc[[temp_ids[j]]], \n",
    "            remove_empty_cols=True\n",
    "        )\n",
    "        temp_scores.append(alignment_score)\n",
    "    reference_alignment.append(alignment)\n",
    "    reference_alignment_orderings.append(temp_ids)\n",
    "    reference_alignment_scores.append(temp_scores)\n",
    "#     print(temp_ids)\n",
    "#     extractTup(alignment, tup_i='segment').sort_index()\n",
    "#     print()\n",
    "\n",
    "# Add flat smushing alignment :)\n",
    "reference_alignment.append(\n",
    "    removeEmptyColumns(alignment_df.loc[\n",
    "        [2552, 4594, 4618, 5130, 6474, 7298, 7308, 7321, 5126, 5134, 6507]\n",
    "    ])\n",
    ")\n",
    "reference_alignment_orderings.append(['flat smush'])\n",
    "reference_alignment_scores.append(['no scores'])\n",
    "\n",
    "# Add manually tuned (?) alignment\n",
    "reference_alignment.append(manually_aligned_group)\n",
    "reference_alignment_orderings.append(['manually tuned'])\n",
    "reference_alignment_scores.append(['no scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here's an alignment that's really bad:\n",
    "toy_alignment_bad = reference_alignment[5]\n",
    "extractTup(toy_alignment_bad, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here's an alignment that's fairly bad:\n",
    "toy_alignment_poor = reference_alignment[4]\n",
    "extractTup(toy_alignment_poor, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here's an alignment that's a little better:\n",
    "toy_alignment_good = reference_alignment[6]\n",
    "extractTup(toy_alignment_good, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here's an alignment that's a lot better:\n",
    "toy_alignment_great = reference_alignment[1]\n",
    "extractTup(toy_alignment_great, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreNumColumns(align_df):\n",
    "    return len(align_df.columns)\n",
    "\n",
    "# Lower is better\n",
    "print('  bad', scoreNumColumns(toy_alignment_bad))\n",
    "print(' poor', scoreNumColumns(toy_alignment_poor))\n",
    "print(' good', scoreNumColumns(toy_alignment_good))\n",
    "print('great', scoreNumColumns(toy_alignment_great))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scoreColumnPhraseEmbedVariance(align_df, colname):\n",
    "    # Compute embeddings variance of all the phrases for a single column\n",
    "    # Reasoning for this operation (calculating variance as trace(covariance matrix) ...):\n",
    "    # https://stats.stackexchange.com/questions/225434/a-measure-of-variance-from-the-covariance-matrix\n",
    "    texts = [text for text in extractTup(align_df[colname], tup_i='segment', is_frame=False) \n",
    "             if text in cached_word2vec_phrases]\n",
    "    if len(texts) > 1:\n",
    "        output = pd.concat([cached_word2vec_phrases[text] \n",
    "                            for text in texts])\n",
    "        result = np.trace(output.cov())\n",
    "    else:\n",
    "        # one of two scenarios:\n",
    "        # 1. all of the contents of this column aren't considered words, so, pretend they're all the same\n",
    "        # 2. there is only one row in this column that contains text, so it has no variation\n",
    "        # TODO is there a theoretically better way to handle them?\n",
    "        result = 0\n",
    "    return result\n",
    "\n",
    "# Lower is better\n",
    "print('  bad', scoreColumnPhraseEmbedVariance(toy_alignment_bad, 'txt0'))\n",
    "print(' poor', scoreColumnPhraseEmbedVariance(toy_alignment_poor, 'txt0'))\n",
    "print(' good', scoreColumnPhraseEmbedVariance(toy_alignment_good, 'txt0'))\n",
    "print('great', scoreColumnPhraseEmbedVariance(toy_alignment_great, 'txt0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreColumnTokenCount(align_df, colname):\n",
    "    # Count the number of unique tokens in a single column\n",
    "    tokens = [text for text in extractTup(align_df[colname], tup_i='segment', is_frame=False)\n",
    "              if text.strip() != '']\n",
    "    return len(set(tokens))\n",
    "\n",
    "# Lower is better\n",
    "print('  bad', scoreColumnTokenCount(toy_alignment_bad, 'txt0'))\n",
    "print(' poor', scoreColumnTokenCount(toy_alignment_poor, 'txt0'))\n",
    "print(' good', scoreColumnTokenCount(toy_alignment_good, 'txt0'))\n",
    "print('great', scoreColumnTokenCount(toy_alignment_great, 'txt0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreColumnPhrasePOSCount(align_df, colname):\n",
    "    # Count the number of unique phrase parts-of-speech in a single column\n",
    "    tokens = [phrasepos for phrasepos in extractTup(align_df[colname], tup_i='pos', is_frame=False)\n",
    "              if phrasepos.strip() != '']\n",
    "    return len(set(tokens))\n",
    "\n",
    "# Lower is better\n",
    "print('  bad', scoreColumnPhrasePOSCount(toy_alignment_bad, 'txt0'))\n",
    "print(' poor', scoreColumnPhrasePOSCount(toy_alignment_poor, 'txt0'))\n",
    "print(' good', scoreColumnPhrasePOSCount(toy_alignment_good, 'txt0'))\n",
    "print('great', scoreColumnPhrasePOSCount(toy_alignment_great, 'txt0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreColumnPOSCount(align_df, colname):\n",
    "    # Count the number of unique token parts-of-speech in a single column\n",
    "    tokens = [pos_list for pos_list in extractTup(align_df[colname], tup_i='cpos', is_frame=False)]\n",
    "    tokens = [pos for pos_list in tokens for pos in pos_list\n",
    "              if pos.strip() != '']\n",
    "    return len(set(tokens))\n",
    "\n",
    "# Lower is better\n",
    "print('  bad', scoreColumnPOSCount(toy_alignment_bad, 'txt0'))\n",
    "print(' poor', scoreColumnPOSCount(toy_alignment_poor, 'txt0'))\n",
    "print(' good', scoreColumnPOSCount(toy_alignment_good, 'txt0'))\n",
    "print('great', scoreColumnPOSCount(toy_alignment_great, 'txt0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreColumnRepresentation(align_df, colname):\n",
    "    # Count the fraction of rows that are represented in the column (so penalizes gaps)\n",
    "    tokens = [text for text in extractTup(align_df[colname], tup_i='segment', is_frame=False)]\n",
    "    non_empty_count = len([text for text in tokens if text.strip() != ''])\n",
    "    return non_empty_count/len(tokens)\n",
    "\n",
    "# Higher is better\n",
    "print('  bad', scoreColumnRepresentation(toy_alignment_bad, 'txt0'))\n",
    "print(' poor', scoreColumnRepresentation(toy_alignment_poor, 'txt0'))\n",
    "print(' good', scoreColumnRepresentation(toy_alignment_good, 'txt0'))\n",
    "print('great', scoreColumnRepresentation(toy_alignment_great, 'txt0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreRowAlignment(align_df, focus_row):\n",
    "    # Calculate the alignment score that a specific row would get if aligned with the df\n",
    "    # Score is normalized by the number of operations that goes into calculating it\n",
    "    # (there is a score matrix that is len(mat_a)*len(mat_b) dimensions)\n",
    "    # TODO there should be a way to re-derive this based on the direct alignment\n",
    "    score = alignRowMajorLocal(align_df, focus_row, remove_empty_cols=True)[1]\n",
    "    return score / (len(align_df.columns) + len(focus_row.columns))\n",
    "\n",
    "# Higher is better\n",
    "print('  bad', scoreRowAlignment(toy_alignment_bad, toy_alignment_bad.loc[[5130]]))\n",
    "print(' poor', scoreRowAlignment(toy_alignment_poor, toy_alignment_poor.loc[[5130]]))\n",
    "print(' good', scoreRowAlignment(toy_alignment_good, toy_alignment_good.loc[[5130]]))\n",
    "print('great', scoreRowAlignment(toy_alignment_great, toy_alignment_great.loc[[5130]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreTermColumnCount(align_df, term):\n",
    "    # Count the number of columns that a certain phrase or term appears within\n",
    "    # TODO should this be a fraction instead? what would that imply?\n",
    "    # If it doesn't appear at all, returns 1 (TODO that might not be ideal?)\n",
    "    # TODO add support for regex patterns (eg numbers?)\n",
    "    tokens = [\n",
    "        [text for text in extractTup(align_df[colname], tup_i='segment', is_frame=False)]\n",
    "        for colname in align_df.columns\n",
    "    ]\n",
    "    tokens = [[e for e in col if (term.lower() in e.lower())] for col in tokens]\n",
    "    tokens = [col for col in tokens if len(col) != 0]\n",
    "    return max(1, len(tokens))\n",
    "\n",
    "def scoreTermListColumnCount(align_df, term_list, term_weights=None):\n",
    "    # if we don't have any terms to investigate, return 1 (default col count)\n",
    "    if len(term_list) == 0:\n",
    "        return 1\n",
    "    # by default, weight each term equally\n",
    "    if term_weights is None:\n",
    "        term_weights = [1]*len(term_list)\n",
    "    # And normalize the weights (assume that hasn't been done already)\n",
    "    tw_sum = sum(term_weights)\n",
    "    term_weights = [(tw/tw_sum) for tw in term_weights]\n",
    "    scores = [scoreTermColumnCount(align_df, term) for term in term_list]\n",
    "    return np.dot(scores, term_weights)\n",
    "\n",
    "# Lower is better\n",
    "print('  bad', scoreTermColumnCount(toy_alignment_bad, 'anxiety'))\n",
    "print(' poor', scoreTermColumnCount(toy_alignment_poor, 'anxiety'))\n",
    "print(' good', scoreTermColumnCount(toy_alignment_good, 'anxiety'))\n",
    "print('great', scoreTermColumnCount(toy_alignment_great, 'anxiety'))\n",
    "print()\n",
    "\n",
    "# Test of using scoreTermListColumnCount with multiple terms (weighted equally)\n",
    "# Lower is better\n",
    "temp_list = ['anxiety', 'patient', 'children', 'child']\n",
    "scores = scoreTermListColumnCount(toy_alignment_bad, temp_list)\n",
    "print('  bad', scores)\n",
    "scores = scoreTermListColumnCount(toy_alignment_poor, temp_list)\n",
    "print(' poor', scores)\n",
    "scores = scoreTermListColumnCount(toy_alignment_good, temp_list)\n",
    "print(' good', scores)\n",
    "scores = scoreTermListColumnCount(toy_alignment_great, temp_list)\n",
    "print('great', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreRowLayoutCount(align_df):\n",
    "    # Count the number of unique content-gap orderings that are present in the alignment\n",
    "    rows = [\n",
    "        list(extractTup(align_df.iloc[i], tup_i='segment', is_frame=False)) \n",
    "        for i in range(len(align_df))]\n",
    "    rows = [''.join([('.' if (e.strip() != '') else ' ') for e in r]) for r in rows]\n",
    "    return len(set(rows))\n",
    "\n",
    "# Lower is better\n",
    "print('  bad', scoreRowLayoutCount(toy_alignment_bad))\n",
    "print(' poor', scoreRowLayoutCount(toy_alignment_poor))\n",
    "print(' good', scoreRowLayoutCount(toy_alignment_good))\n",
    "print('great', scoreRowLayoutCount(toy_alignment_great))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempScoreVector(align_df, term_list=[], term_weights=None):\n",
    "    # TODO make this an actual nice function later\n",
    "    score_colptxtembed = [scoreColumnPhraseEmbedVariance(align_df, colname) for colname in align_df.columns]\n",
    "    score_coltokncount = [scoreColumnTokenCount(align_df, colname) for colname in align_df.columns]\n",
    "    score_colpposcount = [scoreColumnPhrasePOSCount(align_df, colname) for colname in align_df.columns]\n",
    "    score_coltposcount = [scoreColumnPOSCount(align_df, colname) for colname in align_df.columns]\n",
    "    score_colrepresent = [scoreColumnRepresentation(align_df, colname) for colname in align_df.columns]\n",
    "    score_termcolcount = scoreTermListColumnCount(align_df, term_list, term_weights)\n",
    "    scores = np.array([\n",
    "        scoreNumColumns(align_df), # lower is better\n",
    "        sum(score_colptxtembed)/len(score_colptxtembed), # lower is better\n",
    "        sum(score_coltokncount)/len(score_coltokncount), # lower is better\n",
    "        sum(score_colpposcount)/len(score_colpposcount), # lower is better\n",
    "        sum(score_coltposcount)/len(score_coltposcount), # lower is better\n",
    "        sum(score_colrepresent)/len(score_colrepresent), # higher is better\n",
    "        0,#scoreRowAlignment(align_df, align_df.loc[[5130]]), # higher is better\n",
    "        score_termcolcount, #  lower is better\n",
    "        scoreRowLayoutCount(align_df), # lower is better\n",
    "    ])\n",
    "    # weight and sum up the score (higher total score is better)\n",
    "    score_direction = np.array([-1, -1, -1, -1, -1,  1,  1, -1, -1])\n",
    "    score_weights   = np.array([ 0, 10,  0,  0,  0,  0, 0., 15,  0])\n",
    "    return np.dot(np.multiply(score_weights, score_direction), scores), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignmentTerms(align_df, all_stopwords=None, priority_pos=['NN', 'JJ', 'RB']):\n",
    "    if all_stopwords is None:\n",
    "        all_stopwords = sp.Defaults.stop_words\n",
    "    # collect list forms of words and cPOS\n",
    "    all_text = [\n",
    "        [text for text in extractTup(align_df.iloc[rownum], tup_i='segment', is_frame=False)]\n",
    "        for rownum in range(len(align_df))\n",
    "    ]\n",
    "    all_text = [' '.join(sublist).split() for sublist in all_text]\n",
    "    all_cpos = [\n",
    "        [text for sublist \n",
    "         in extractTup(align_df.iloc[rownum], tup_i='cpos', is_frame=False) \n",
    "         for text in sublist]\n",
    "        for rownum in range(len(align_df))\n",
    "    ]\n",
    "    # get count of how many rows each word is present in\n",
    "    tokens_df = dict([\n",
    "        (word, sum([(word in row) for row in tokens])) \n",
    "        for word \n",
    "        in set([item for sublist in tokens for item in sublist])\n",
    "        if word not in all_stopwords\n",
    "    ])\n",
    "    # remove the words that show up in less than one row\n",
    "    for word in [word for word in tokens_df if tokens_df[word] <= 1]:\n",
    "        discard = tokens_df.pop(word, None)\n",
    "    # flatten the word and cPOS lists\n",
    "    all_text = [e for sublist in all_text for e in sublist]\n",
    "    all_cpos = [e for sublist in all_cpos for e in sublist if e != '']\n",
    "    # count up how many POS is assigned to each word\n",
    "    pos_mapping = {}\n",
    "    for i in range(len(all_text)):\n",
    "        if all_text[i] not in pos_mapping:\n",
    "            pos_mapping[all_text[i]] = {}\n",
    "        if all_cpos[i] not in pos_mapping[all_text[i]]:\n",
    "            pos_mapping[all_text[i]][all_cpos[i]] = 0\n",
    "        pos_mapping[all_text[i]][all_cpos[i]] += 1\n",
    "    # pick the single POS that each word is tagged as most often\n",
    "    for word in pos_mapping:\n",
    "        max_pos = None\n",
    "        max_count = 0\n",
    "        for pos in pos_mapping[word]:\n",
    "            if pos_mapping[word][pos] > max_count:\n",
    "                max_pos = pos\n",
    "                max_count = pos_mapping[word][pos]\n",
    "        pos_mapping[word] = max_pos\n",
    "    # exponentiate the count of all of the words in the dict that are in POS classes we care about\n",
    "    for word in tokens_df:\n",
    "        if any([(pos in pos_mapping[word]) for pos in priority_pos]):\n",
    "            tokens_df[word] = pow(tokens_df[word], 2)\n",
    "    return tokens_df\n",
    "\n",
    "alignmentTerms(toy_alignment_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test how much sense this weighting works for scoreTermListColumnCount\n",
    "tokens_df = alignmentTerms(toy_alignment_bad)\n",
    "temp_list = list(tokens_df)\n",
    "temp_weights = list(tokens_df.values())\n",
    "scores = scoreTermListColumnCount(toy_alignment_bad, temp_list, term_weights=temp_weights)\n",
    "print('  bad', scores)\n",
    "scores = scoreTermListColumnCount(toy_alignment_poor, temp_list, term_weights=temp_weights)\n",
    "print(' poor', scores)\n",
    "scores = scoreTermListColumnCount(toy_alignment_good, temp_list, term_weights=temp_weights)\n",
    "print(' good', scores)\n",
    "scores = scoreTermListColumnCount(toy_alignment_great, temp_list, term_weights=temp_weights)\n",
    "print('great', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df = alignmentTerms(toy_alignment_bad)\n",
    "temp_list = list(tokens_df)\n",
    "temp_weights = list(tokens_df.values())\n",
    "score_vector_bad = tempScoreVector(\n",
    "    toy_alignment_bad, \n",
    "    term_list=temp_list,\n",
    "    term_weights=temp_weights)\n",
    "score_vector_poor = tempScoreVector(\n",
    "    toy_alignment_poor, \n",
    "    term_list=temp_list,\n",
    "    term_weights=temp_weights)\n",
    "score_vector_good = tempScoreVector(\n",
    "    toy_alignment_good, \n",
    "    term_list=temp_list,\n",
    "    term_weights=temp_weights)\n",
    "score_vector_great = tempScoreVector(\n",
    "    toy_alignment_great, \n",
    "    term_list=temp_list,\n",
    "    term_weights=temp_weights)\n",
    "\n",
    "score_vector_bad\n",
    "score_vector_poor\n",
    "score_vector_good\n",
    "score_vector_great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_alignment_scores = []\n",
    "for i in range(len(reference_alignment)):\n",
    "    ref_alignment_scores.append(\n",
    "        tempScoreVector(\n",
    "            reference_alignment[i], \n",
    "            term_list=temp_list,\n",
    "            term_weights=temp_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(reference_alignment)):\n",
    "    print('   ', ref_alignment_scores[i][0])\n",
    "    print(ref_alignment_scores[i][1])\n",
    "    print()\n",
    "#     ref_alignment_scores[i][0]\n",
    "#     extractTup(reference_alignment[i], tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO how do we design this score function that it may be comparable with other alignments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Experiment to see how much random ordering impacts alignment readability\n",
    "# temp_ids = [2552, 4594, 4618, 5130, 6474, 7298, 7308, 7321, 5126, 5134, 6507] # experiment 1\n",
    "# # temp_ids = [2030, 2078, 2380, 2437, 2711, 2849, 3194, 3285, 4887, 5437, 6915] # experiment 2\n",
    "# # temp_ids = [1248, 1275, 1381, 1387, 3871, 4039, 5202, 5204, 6563, 6569] # experiment 3\n",
    "# temp_alignment_orderings = []\n",
    "# temp_alignment_outputs = []\n",
    "# temp_alignment_score_progressions = []\n",
    "# for i in range(20):\n",
    "#     temp_ids = random.sample(temp_ids, len(temp_ids))\n",
    "#     alignment, alignment_score = alignRowMajorLocal(\n",
    "#         alignment_df.loc[[temp_ids[0]]], \n",
    "#         alignment_df.loc[[temp_ids[1]]], \n",
    "#         remove_empty_cols=True\n",
    "#     )\n",
    "#     temp_alignment_orderings.append(temp_ids)\n",
    "#     temp_alignment_outputs.append(alignment)\n",
    "#     temp_alignment_score_progressions.append([alignment_score])\n",
    "#     for j in range(2, len(temp_ids)):\n",
    "#         alignment, alignment_score = alignRowMajorLocal(\n",
    "#             temp_alignment_outputs[i],\n",
    "#             alignment_df.loc[[temp_ids[j]]], \n",
    "#             remove_empty_cols=True\n",
    "#         )\n",
    "#         temp_alignment_outputs[i] = alignment\n",
    "#         temp_alignment_score_progressions[i].append(alignment_score)\n",
    "# # extractTup(update_manually_aligned_group_realign, tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump((\n",
    "#     temp_alignment_orderings, temp_alignment_outputs, temp_alignment_score_progressions,\n",
    "#     temp_alignment2_orderings, temp_alignment2_outputs, temp_alignment2_score_progressions,\n",
    "#     temp_alignment3_orderings, temp_alignment3_outputs, temp_alignment3_score_progressions,\n",
    "# ), open('temp/ebm-alignmentswithscores.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "temp_alignment_orderings, temp_alignment_outputs, temp_alignment_score_progressions,\\\n",
    "temp_alignment2_orderings, temp_alignment2_outputs, temp_alignment2_score_progressions,\\\n",
    "temp_alignment3_orderings, temp_alignment3_outputs, temp_alignment3_score_progressions \\\n",
    "    = pickle.load(open('temp/ebm-alignmentswithscores.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_df = alignmentTerms(toy_alignment_bad)\n",
    "temp_list = list(tokens_df)\n",
    "temp_weights = list(tokens_df.values())\n",
    "\n",
    "temp_alignment_scores = []\n",
    "temp_alignment_scores_detail = []\n",
    "for i in range(len(temp_alignment_outputs)):\n",
    "    # score the alignments\n",
    "    alignment_score = tempScoreVector(\n",
    "        temp_alignment_outputs[i], \n",
    "        term_list=temp_list,\n",
    "        term_weights=temp_weights)\n",
    "    temp_alignment_scores.append(alignment_score[0])\n",
    "    temp_alignment_scores_detail.append(alignment_score[1])\n",
    "    \n",
    "# the \"best\" alignment\n",
    "index = temp_alignment_scores.index(max(temp_alignment_scores))\n",
    "temp_alignment_orderings[index]\n",
    "temp_alignment_scores[index]\n",
    "extractTup(temp_alignment_outputs[index], tup_i='segment').sort_index()\n",
    "\n",
    "# the \"worst\" alignment\n",
    "index = temp_alignment_scores.index(min(temp_alignment_scores))\n",
    "temp_alignment_orderings[index]\n",
    "temp_alignment_scores[index]\n",
    "extractTup(temp_alignment_outputs[index], tup_i='segment').sort_index()\n",
    "\n",
    "# some random alignment\n",
    "index = int(0.5*len(temp_alignment_scores))\n",
    "temp_alignment_orderings[index]\n",
    "temp_alignment_scores[index]\n",
    "extractTup(temp_alignment_outputs[index], tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(temp_alignment_scores)):\n",
    "    temp_alignment_orderings[index]\n",
    "    temp_alignment_scores[index]\n",
    "    extractTup(temp_alignment_outputs[index], tup_i='segment').sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment ordering space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignmentTerms(align_df, all_stopwords=None, priority_pos=['NN', 'JJ', 'RB']):\n",
    "    if all_stopwords is None:\n",
    "        all_stopwords = sp.Defaults.stop_words\n",
    "    # collect list forms of words and cPOS\n",
    "    all_text = [\n",
    "        [text for text in extractTup(align_df.iloc[rownum], tup_i='segment', is_frame=False)]\n",
    "        for rownum in range(len(align_df))\n",
    "    ]\n",
    "    all_text = [' '.join(sublist).split() for sublist in all_text]\n",
    "    all_cpos = [\n",
    "        [text for sublist \n",
    "         in extractTup(align_df.iloc[rownum], tup_i='cpos', is_frame=False) \n",
    "         for text in sublist]\n",
    "        for rownum in range(len(align_df))\n",
    "    ]\n",
    "    # flatten the word and cPOS lists\n",
    "    all_text = [e for sublist in all_text for e in sublist]\n",
    "    all_cpos = [e for sublist in all_cpos for e in sublist if e != '']\n",
    "    # count up how many POS is assigned to each word\n",
    "    pos_mapping = {}\n",
    "    for i in range(len(all_text)):\n",
    "        if all_text[i] not in pos_mapping:\n",
    "            pos_mapping[all_text[i]] = {}\n",
    "        if all_cpos[i] not in pos_mapping[all_text[i]]:\n",
    "            pos_mapping[all_text[i]][all_cpos[i]] = 0\n",
    "        pos_mapping[all_text[i]][all_cpos[i]] += 1\n",
    "    # pick the single POS that each word is tagged as most often\n",
    "    for word in pos_mapping:\n",
    "        max_pos = None\n",
    "        max_count = 0\n",
    "        for pos in pos_mapping[word]:\n",
    "            if pos_mapping[word][pos] > max_count:\n",
    "                max_pos = pos\n",
    "                max_count = pos_mapping[word][pos]\n",
    "        pos_mapping[word] = max_pos\n",
    "    # get count of how many rows each word is present in\n",
    "    # and remove stopwords at the same time\n",
    "    tokens_df = dict([\n",
    "        (word, sum([(word in row) for row in tokens])) \n",
    "        for word \n",
    "        in set(all_text)\n",
    "        if word not in all_stopwords\n",
    "    ])\n",
    "    # remove the words that show up in less than one row\n",
    "    for word in [word for word in tokens_df if tokens_df[word] <= 1]:\n",
    "        discard = tokens_df.pop(word, None)\n",
    "    # exponentiate the count of all of the words in the dict that are in POS classes we care about\n",
    "    for word in tokens_df:\n",
    "        if any([(pos in pos_mapping[word]) for pos in priority_pos]):\n",
    "            tokens_df[word] = pow(tokens_df[word], 2)\n",
    "    return tokens_df\n",
    "\n",
    "alignmentTerms(toy_alignment_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Do some variant of beam search to build up an alignment one at a time!!!\n",
    "# TODO implement a method to group multiple together at a time? / dynamic programming sort of approach?\n",
    "# TODO implement a faster method to do a sort of random walk\n",
    "def buildAlignmentBeamSearch(align_df_src, indices, size_seed=10, size_beam=10, size_filter=5, use_fullscore=True):\n",
    "    # Do a basic adjustment of size parameters if there is mismatch\n",
    "    size_seed = min(size_seed, len(indices))\n",
    "    size_beam = min(size_beam, len(indices))\n",
    "    # Set up our term scoring\n",
    "    alignment_terms = alignmentTerms(align_df_src.loc[indices])\n",
    "    # Seed the beam\n",
    "    beam = []\n",
    "    indices_sampling = random.sample(indices, size_seed)\n",
    "    for seed in indices_sampling:\n",
    "        # beam format: (alignment, alignment ordering, single penalty matrix score, single alignment score)\n",
    "        beam.append((align_df_src.loc[[seed]], [seed], 0, 0))\n",
    "    # Run the main body of beam search...\n",
    "    while len(beam[0][1]) < len(indices):\n",
    "        beam_update = []\n",
    "        for b in beam:\n",
    "            # sample a number of possible next indices\n",
    "            indices_sampling = [i for i in indices if (i not in b[1])]\n",
    "            indices_sampling = random.sample(indices_sampling, min(size_beam, len(indices_sampling)))\n",
    "            # special handling to account for first round: avoid AxB & BxA alignment duplication\n",
    "            if len(b[1]) == 1:\n",
    "                # get the list of all alignment orderings already tried and filter the samples out\n",
    "                attempted = [bu[1] for bu in beam_update]\n",
    "                indices_sampling = [i for i in indices_sampling if ([i]+b[1] not in attempted)]\n",
    "            # run through all of the indices sampled to search for next step\n",
    "            for next_i in indices_sampling:\n",
    "                alignment, alignment_score = alignRowMajorLocal(\n",
    "                    b[0],\n",
    "                    align_df_src.loc[[next_i]],\n",
    "                    remove_empty_cols=True\n",
    "                )\n",
    "                full_score = tempScoreVector(\n",
    "                    alignment, \n",
    "                    term_list=list(alignment_terms), \n",
    "                    term_weights=list(alignment_terms.values())\n",
    "                )[0]\n",
    "                beam_update.append((alignment, b[1]+[next_i], alignment_score, full_score))\n",
    "        # sort by descending score (since higher is better)\n",
    "        # TODO want to see if there's a huge difference between using full score vs alignment score\n",
    "        beam_update.sort(key=lambda e: e[3] if use_fullscore else e[2], reverse=True)\n",
    "#         for b in beam_update:\n",
    "#             print(f'BEAM {b[1]}    {b[2]}      {b[3]}')\n",
    "#             print(extractTup(b[0], tup_i='segment', is_frame=True))\n",
    "#             print('=====================')\n",
    "#         print('===================== END BEAM =====================')\n",
    "        beam = beam_update[:min(size_filter, len(beam_update))]\n",
    "    # the beam is sorted in order of descending quality already\n",
    "    return beam[0][0]\n",
    "\n",
    "# buildAlignmentBeamSearch(alignment_df, [2552, 4594, 4618, 5130, 6474, 7298, 7308, 7321, 5126, 5134, 6507])\n",
    "temp_alignment = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 5130], use_fullscore=False)\n",
    "temp_alignment\n",
    "alignment_terms = alignmentTerms(temp_alignment)\n",
    "tempScoreVector(\n",
    "    temp_alignment, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsp5 = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 5130, 6474, 7298], size_filter=5, use_fullscore=False)\n",
    "# bspx = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 5130, 6474, 7298], size_filter=10, use_fullscore=False)\n",
    "# bspl = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 5130, 6474, 7298], size_filter=100, use_fullscore=False)\n",
    "# bsf5 = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 5130, 6474, 7298], size_filter=5, use_fullscore=True)\n",
    "# bsfx = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 5130, 6474, 7298], size_filter=10, use_fullscore=True)\n",
    "# bsfl = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 5130, 6474, 7298], size_filter=100, use_fullscore=True)\n",
    "# blfx = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 4618, 5130, 6474, 7298, 7308, 7321, 5126, 5134, 6507], \n",
    "#                                 size_filter=10, use_fullscore=True)\n",
    "# blfl = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 4618, 5130, 6474, 7298, 7308, 7321, 5126, 5134, 6507], \n",
    "#                                 size_filter=100, use_fullscore=True)\n",
    "# blpx = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 4618, 5130, 6474, 7298, 7308, 7321, 5126, 5134, 6507], \n",
    "#                                 size_filter=10, use_fullscore=False)\n",
    "# blpl = buildAlignmentBeamSearch(alignment_df, [2552, 4594, 4618, 5130, 6474, 7298, 7308, 7321, 5126, 5134, 6507], \n",
    "#                                 size_filter=100, use_fullscore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump((\n",
    "#     bsp5, bspx, bspl,\n",
    "#     bsf5, bsfx, bsfl,\n",
    "#     blfx, blfl,\n",
    "#     blpx, blpl,\n",
    "# ), open('temp/ebm-beamsearchalignments.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "bsp5, bspx, bspl,\\\n",
    "bsf5, bsfx, bsfl,\\\n",
    "blfx, blfl,\\\n",
    "blpx, blpl \\\n",
    "    = pickle.load(open('temp/ebm-beamsearchalignments.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(bsp5)\n",
    "tempScoreVector(\n",
    "    bsp5, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(bsp5, tup_i='segment').sort_index()\n",
    "# bsp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(bspx)\n",
    "tempScoreVector(\n",
    "    bspx, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(bspx, tup_i='segment').sort_index()\n",
    "# bspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(bspl)\n",
    "tempScoreVector(\n",
    "    bspl, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(bspl, tup_i='segment').sort_index()\n",
    "# bspl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(bsf5)\n",
    "tempScoreVector(\n",
    "    bsf5, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(bsf5, tup_i='segment').sort_index()\n",
    "# bsf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(bsfx)\n",
    "tempScoreVector(\n",
    "    bsfx, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(bsfx, tup_i='segment').sort_index()\n",
    "# bsfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(bsfl)\n",
    "tempScoreVector(\n",
    "    bsfl, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(bsfl, tup_i='segment').sort_index()\n",
    "# bsfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(blfx)\n",
    "tempScoreVector(\n",
    "    blfx, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(blfx, tup_i='segment').sort_index()\n",
    "# blfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(blfl)\n",
    "tempScoreVector(\n",
    "    blfl, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(blfl, tup_i='segment').sort_index()\n",
    "# blfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(blpx)\n",
    "tempScoreVector(\n",
    "    blpx, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(blpx, tup_i='segment').sort_index()\n",
    "# blpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_terms = alignmentTerms(blpl)\n",
    "tempScoreVector(\n",
    "    blpl, \n",
    "    term_list=list(alignment_terms), \n",
    "    term_weights=list(alignment_terms.values())\n",
    ")\n",
    "extractTup(blpl, tup_i='segment').sort_index()\n",
    "# blpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At each step, either:\n",
    "# 1. Greedy step :3\n",
    "# 2. Random step\n",
    "# 3. Random restart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
