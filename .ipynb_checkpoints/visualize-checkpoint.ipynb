{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-da5405210a954d3eb7944edf960cc251\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-da5405210a954d3eb7944edf960cc251\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-da5405210a954d3eb7944edf960cc251\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-c2a3e89ba9d5d1687d5e8c28d630a033\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"nominal\", \"field\": \"a\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"b\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-c2a3e89ba9d5d1687d5e8c28d630a033\": [{\"a\": \"A\", \"b\": 28}, {\"a\": \"B\", \"b\": 55}, {\"a\": \"C\", \"b\": 43}, {\"a\": \"D\", \"b\": 91}, {\"a\": \"E\", \"b\": 81}, {\"a\": \"F\", \"b\": 53}, {\"a\": \"G\", \"b\": 19}, {\"a\": \"H\", \"b\": 87}, {\"a\": \"I\", \"b\": 52}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "# https://altair-viz.github.io/gallery/simple_bar_chart.html\n",
    "source = pd.DataFrame({\n",
    "    'a': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],\n",
    "    'b': [28, 55, 43, 91, 81, 53, 19, 87, 52]\n",
    "})\n",
    "\n",
    "alt.Chart(source).mark_bar().encode(\n",
    "    x='a',\n",
    "    y='b'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inheriting usage notes from my other code for now (not self-plagiarization)\n",
    "# https://github.com/cephcyn/cse517project/blob/master/embed_w2v.py\n",
    "# for word2vec code; see also word2vec documentation\n",
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "# model source: https://code.google.com/archive/p/word2vec/\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'model/GoogleNews-vectors-negative300.bin', \n",
    "    binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.024902</td>\n",
       "      <td>-0.27124</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>-0.215332</td>\n",
       "      <td>0.583496</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.197754</td>\n",
       "      <td>-0.028809</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.418945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06543</td>\n",
       "      <td>-0.22168</td>\n",
       "      <td>-0.335938</td>\n",
       "      <td>-0.096191</td>\n",
       "      <td>0.040985</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.259583</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>-0.182617</td>\n",
       "      <td>test sentence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1         2         3         4         5         6  \\\n",
       "0 -0.024902 -0.27124  0.612305 -0.215332  0.583496  0.126953  0.197754   \n",
       "\n",
       "          7         8         9  ...      291      292       293       294  \\\n",
       "0 -0.028809  0.001953  0.418945  ...  0.06543 -0.22168 -0.335938 -0.096191   \n",
       "\n",
       "        295       296       297       298       299           word  \n",
       "0  0.040985 -0.047363 -0.259583  0.021484 -0.182617  test sentence  \n",
       "\n",
       "[1 rows x 301 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the word2vec embedding of a phrase\n",
    "def get_phrase_embed_word2vec(word2vec, phrase):\n",
    "    try:\n",
    "        phraseS = phrase.split()\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    unknowns = []\n",
    "    emb = []\n",
    "    for w in phraseS:\n",
    "        try:\n",
    "            emb.append(word2vec[w])\n",
    "        except:\n",
    "            unknowns.append(w)\n",
    "    if len(emb) == 0:\n",
    "        return pd.DataFrame()\n",
    "    emb_sum = pd.DataFrame(emb).sum()\n",
    "    emb_sum['word'] = phrase\n",
    "    return pd.DataFrame([emb_sum])\n",
    "\n",
    "v = get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'test sentence')\n",
    "sent_v = get_phrase_embed_word2vec(\n",
    "    word2vec, \n",
    "    'This is a test sentence !')\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c2ad320748b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0membed_subtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'v' is not defined"
     ]
    }
   ],
   "source": [
    "def embed_subtract(v, sent_v, dim):\n",
    "    try:\n",
    "        inverse_v = sent_v.iloc[:, 0:dim].subtract(v.iloc[:, 0:dim])\n",
    "        inverse_v['word'] = v['word']\n",
    "        inverse_v['sentence'] = sent_v['word']\n",
    "        return inverse_v\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "embed_subtract(v, sent_v, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "\n",
    "#OPTIONAL - to disable outputs from Tensorflow\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "# This line only needs to be a one-time download\n",
    "# !python -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Load ELMo model\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "url = \"https://tfhub.dev/google/elmo/2\"\n",
    "elmo = hub.Module(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ELMo embedding of a phrase (with given span limits)\n",
    "def get_phrase_embed_elmo(elmo, sentence, span0, span1, phrase, pregenerated=None):\n",
    "    try:\n",
    "        # TODO make the NaN / none case check neater?\n",
    "        phraseS = phrase.split()\n",
    "        span0 = int(span0)\n",
    "        span1 = int(span1)\n",
    "    except:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    span0 = len(sentence[:span0].split(' ')) - 1\n",
    "    span1 = len(sentence[:span1].split(' ')) - 1\n",
    "    if pregenerated is None:\n",
    "        # If we don't have a handy pre-generated dict of sentence:vector already...\n",
    "        embeddings = elmo(\n",
    "            [sentence], \n",
    "            signature='default', \n",
    "            as_dict=True)['elmo']\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.tables_initializer())\n",
    "            x = sess.run(embeddings)\n",
    "            x = x[0]\n",
    "    else:\n",
    "        # otherwise just grab the vector\n",
    "        x = pregenerated[sentence]\n",
    "    emb_sum = pd.DataFrame(x[span0:span1]).sum()\n",
    "    emb_sum['word'] = phrase\n",
    "    sentence_sum = pd.DataFrame(x).sum()\n",
    "    sentence_sum['word'] = sentence\n",
    "    return pd.DataFrame([emb_sum]), pd.DataFrame([sentence_sum])\n",
    "\n",
    "v, sent_v = get_phrase_embed_elmo(\n",
    "    elmo, \n",
    "    'This is a test sentence !',\n",
    "    10, 24, 'test sentence')\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.879597</td>\n",
       "      <td>-0.9779</td>\n",
       "      <td>0.162178</td>\n",
       "      <td>0.792039</td>\n",
       "      <td>-1.1922</td>\n",
       "      <td>-0.107585</td>\n",
       "      <td>1.141867</td>\n",
       "      <td>1.254465</td>\n",
       "      <td>-2.401929</td>\n",
       "      <td>-0.671514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.087214</td>\n",
       "      <td>-0.485054</td>\n",
       "      <td>-0.576597</td>\n",
       "      <td>-0.435607</td>\n",
       "      <td>0.046713</td>\n",
       "      <td>-0.562051</td>\n",
       "      <td>1.049869</td>\n",
       "      <td>0.748659</td>\n",
       "      <td>test sentence</td>\n",
       "      <td>This is a test sentence !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1         2         3       4         5         6         7  \\\n",
       "0 -1.879597 -0.9779  0.162178  0.792039 -1.1922 -0.107585  1.141867  1.254465   \n",
       "\n",
       "          8         9  ...      1016      1017      1018      1019      1020  \\\n",
       "0 -2.401929 -0.671514  ... -1.087214 -0.485054 -0.576597 -0.435607  0.046713   \n",
       "\n",
       "       1021      1022      1023           word                   sentence  \n",
       "0 -0.562051  1.049869  0.748659  test sentence  This is a test sentence !  \n",
       "\n",
       "[1 rows x 1026 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_subtract(v, sent_v, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_0</th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>averb</th>\n",
       "      <th>averb_s</th>\n",
       "      <th>averb_o</th>\n",
       "      <th>averb_relation</th>\n",
       "      <th>averb_split</th>\n",
       "      <th>averb_span0</th>\n",
       "      <th>averb_span1</th>\n",
       "      <th>...</th>\n",
       "      <th>apos_w</th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>split_anchor_span</th>\n",
       "      <th>split_anchor_indices</th>\n",
       "      <th>within_anchor_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We introduce</td>\n",
       "      <td>a new language representation model called BER...</td>\n",
       "      <td>.</td>\n",
       "      <td>stands</td>\n",
       "      <td>['which', 'We introduce a new language represe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['stands']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>['We', 'introduce', 'a', 'new', 'language', 'r...</td>\n",
       "      <td>(2, 18)</td>\n",
       "      <td>(12, 134)</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unlike recent language representation models ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre - train deep bidirectional ...</td>\n",
       "      <td>is designed</td>\n",
       "      <td>['recent language representation models , BERT']</td>\n",
       "      <td>['pre - train deep bidirectional representatio...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', ',', 'models']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Unlike recent language representation models, ...</td>\n",
       "      <td>['Unlike', 'recent', 'language', 'representati...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(46, 50)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a result , the pre - trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine - tuned with just one additi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'model', 'fine']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a result, the pre-trained BERT model can be...</td>\n",
       "      <td>['As', 'a', 'result', ',', 'the', 'pre', '-', ...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>(31, 35)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is conceptually simple and empirically powerful .</td>\n",
       "      <td>is</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'is', 'conceptually']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT is conceptually simple and empirically po...</td>\n",
       "      <td>['BERT', 'is', 'conceptually', 'simple', 'and'...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>It</td>\n",
       "      <td>obtains new state - of - the - art results on ...</td>\n",
       "      <td>obtains</td>\n",
       "      <td>[]</td>\n",
       "      <td>['new state - - the -', 'of art', 'results on ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['It', 'obtains', 'question']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td>['It', 'obtains', 'new', 'state', '-', 'of', '...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>: Pre-training of Deep Bidirectional Transform...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td>['BERT', ':', 'Pre-training', 'of', 'Deep', 'B...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>['Language', 'model', 'pretraining', 'has', 'l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td>['Training', 'is', 'computationally', 'expensi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining ( Devlin et al . , 2019 ) that car...</td>\n",
       "      <td>present</td>\n",
       "      <td>['We']</td>\n",
       "      <td>['a replication study of BERT pretraining ( De...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'et', 'of']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We present a replication study of BERT pretrai...</td>\n",
       "      <td>['We', 'present', 'a', 'replication', 'study',...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(33, 37)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained , and can match...</td>\n",
       "      <td>find</td>\n",
       "      <td>[]</td>\n",
       "      <td>['that BERT was significantly undertrained , a...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'significantly', 'find']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "      <td>['We', 'find', 'that', 'BERT', 'was', 'signifi...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(12, 16)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>published</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['it', 'after', 'published']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "      <td>['We', 'find', 'that', 'BERT', 'was', 'signifi...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td>(121, 123)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td>['Our', 'best', 'model', 'achieves', 'state-of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td>['These', 'results', 'highlight', 'the', 'impo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td>['We', 'release', 'our', 'models', 'and', 'cod...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RoBERTa</td>\n",
       "      <td>: A Robustly Optimized BERT Pretraining Approa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['RoBERTa']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td>['RoBERTa', ':', 'A', 'Robustly', 'Optimized',...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 7)</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>['As', 'Transfer', 'Learning', 'from', 'large-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In this work , we propose a method to pre-trai...</td>\n",
       "      <td>DistilBERT</td>\n",
       "      <td>, which can then be fine-tuned with good perfo...</td>\n",
       "      <td>called</td>\n",
       "      <td>[]</td>\n",
       "      <td>['DistilBERT', ',']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['DistilBERT', 'called', 'model']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>['In', 'this', 'work', ',', 'we', 'propose', '...</td>\n",
       "      <td>(18, 19)</td>\n",
       "      <td>(112, 122)</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>a BERT model</td>\n",
       "      <td>by 40 % , while retaining 97 % of its language...</td>\n",
       "      <td>to reduce</td>\n",
       "      <td>[]</td>\n",
       "      <td>['we leverage knowledge distillation during ph...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'of', 'size']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>['While', 'most', 'prior', 'work', 'investigat...</td>\n",
       "      <td>(37, 40)</td>\n",
       "      <td>(214, 226)</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>its</td>\n",
       "      <td>language understanding capabilities and being ...</td>\n",
       "      <td>retaining</td>\n",
       "      <td>[]</td>\n",
       "      <td>['97 % of its language understanding capabilit...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['its', 'capabilities', 'of']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>['While', 'most', 'prior', 'work', 'investigat...</td>\n",
       "      <td>(49, 50)</td>\n",
       "      <td>(261, 264)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td>['To', 'leverage', 'the', 'inductive', 'biases...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td>['Our', 'smaller,', 'faster', 'and', 'lighter'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>DistilBERT</td>\n",
       "      <td>, a distilled version of BERT : smaller , fast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['DistilBERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>['DistilBERT', ',', 'a', 'distilled', 'version...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td>['Pre-trained', 'text', 'encoders', 'have', 'r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>We focus on</td>\n",
       "      <td>one such model , BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "      <td>focus</td>\n",
       "      <td>['We']</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'on', 'focus']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We focus on one such model, BERT, and aim to q...</td>\n",
       "      <td>['We', 'focus', 'on', 'one', 'such', 'model', ...</td>\n",
       "      <td>(3, 8)</td>\n",
       "      <td>(11, 32)</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>We find that</td>\n",
       "      <td>the model</td>\n",
       "      <td>represents the steps of the traditional NLP pi...</td>\n",
       "      <td>represents</td>\n",
       "      <td>['the model']</td>\n",
       "      <td>['the steps of the traditional NLP pipeline in...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'represents', 'find']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td>['We', 'find', 'that', 'the', 'model', 'repres...</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>(12, 21)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Qualitative analysis reveals that</td>\n",
       "      <td>the model</td>\n",
       "      <td>can and often does adjust this pipeline dynami...</td>\n",
       "      <td>does adjust</td>\n",
       "      <td>['the model can and often']</td>\n",
       "      <td>['this pipeline dynamically , revising lower -...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'adjust', 'reveals']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td>['Qualitative', 'analysis', 'reveals', 'that',...</td>\n",
       "      <td>(4, 6)</td>\n",
       "      <td>(33, 42)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'Rediscovers']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline.</td>\n",
       "      <td>['BERT', 'Rediscovers', 'the', 'Classical', 'N...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Large pre - trained neural networks such as BERT</td>\n",
       "      <td>have had great recent success in NLP , motivat...</td>\n",
       "      <td>have had</td>\n",
       "      <td>['Large pre - trained neural networks such as ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['Large', 'had', 'recent']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>['Large', 'pre', '-', 'trained', 'neural', 'ne...</td>\n",
       "      <td>(0, 9)</td>\n",
       "      <td>(0, 48)</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Large pre - trained neural networks such as BE...</td>\n",
       "      <td>they</td>\n",
       "      <td>are able to learn from unlabeled data .</td>\n",
       "      <td>motivating</td>\n",
       "      <td>[',']</td>\n",
       "      <td>['a growing body of research investigating what']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['they', 'able', 'motivating']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>['Large', 'pre', '-', 'trained', 'neural', 'ne...</td>\n",
       "      <td>(28, 29)</td>\n",
       "      <td>(164, 168)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Large pre - trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP , motivat...</td>\n",
       "      <td>have had</td>\n",
       "      <td>['Large pre - trained neural networks such as ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'as', 'networks']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>['Large', 'pre', '-', 'trained', 'neural', 'ne...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>(43, 47)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td>['Most', 'recent', 'analysis', 'has', 'focused...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Complementary to these works , we propose meth...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>apply</td>\n",
       "      <td>[]</td>\n",
       "      <td>['them']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'to', 'apply']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>['Complementary', 'to', 'these', 'works', ',',...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>(130, 134)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT's attention heads exhibit patterns such a...</td>\n",
       "      <td>['BERT', \"'s\", 'attention', 'heads', 'exhibit'...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(0, 7)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td>['We', 'further', 'show', 'that', 'certain', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td>['For', 'example,', 'we', 'find', 'heads', 'th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Lastly , we propose an attention - based probi...</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention .</td>\n",
       "      <td>is captured</td>\n",
       "      <td>['substantial syntactic information']</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'in', 'captured']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>['Lastly', ',', 'we', 'propose', 'an', 'attent...</td>\n",
       "      <td>(23, 25)</td>\n",
       "      <td>(147, 154)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At ?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What Does BERT Look At?</td>\n",
       "      <td>['What', 'Does', 'BERT', 'Look', 'At', '?']</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>(9, 13)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>An Analysis of</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>Attention .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'Analysis']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>An Analysis of BERT's Attention.</td>\n",
       "      <td>['An', 'Analysis', 'of', 'BERT', \"'s\", 'Attent...</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>(14, 21)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Recently , neural models pretrained on a langu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Devlin et al. , 2018 ) , have achieved impre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'et', 'al']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>['Recently', ',', 'neural', 'models', 'pretrai...</td>\n",
       "      <td>(33, 34)</td>\n",
       "      <td>(148, 152)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>In this paper , we describe</td>\n",
       "      <td>a simple re - implementation of BERT for query...</td>\n",
       "      <td>.</td>\n",
       "      <td>describe</td>\n",
       "      <td>[',', 'we']</td>\n",
       "      <td>['a simple re - implementation of BERT for que...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['implementation', 'describe']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>['In', 'this', 'paper', ',', 'we', 'describe',...</td>\n",
       "      <td>(6, 21)</td>\n",
       "      <td>(27, 102)</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Our system</td>\n",
       "      <td>is the state of the art on the TREC - CAR data...</td>\n",
       "      <td>is</td>\n",
       "      <td>['Our system']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['system', 'is', 'state']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td>['Our', 'system', 'is', 'the', 'state', 'of', ...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td>['The', 'code', 'to', 'reproduce', 'our', 'res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'with', 'ranking']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Passage Re-ranking with BERT.</td>\n",
       "      <td>['Passage', 'Re-ranking', 'with', 'BERT', '.']</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(23, 27)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I assess the extent to which</td>\n",
       "      <td>the recently introduced BERT model</td>\n",
       "      <td>captures English syntactic phenomena , using (...</td>\n",
       "      <td>assess</td>\n",
       "      <td>['I']</td>\n",
       "      <td>['the extent the recently introduced BERT mode...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'syntactic', 'extent']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>['I', 'assess', 'the', 'extent', 'to', 'which'...</td>\n",
       "      <td>(6, 11)</td>\n",
       "      <td>(28, 62)</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The BERT model</td>\n",
       "      <td>performs remarkably well on all cases .</td>\n",
       "      <td>performs</td>\n",
       "      <td>['The BERT model']</td>\n",
       "      <td>['well on all cases', '.']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'performs']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>The BERT model performs remarkably well on all...</td>\n",
       "      <td>['The', 'BERT', 'model', 'performs', 'remarkab...</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>(0, 14)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s Syntactic Abilities .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities.</td>\n",
       "      <td>['Assessing', 'BERT', \"'s\", 'Syntactic', 'Abil...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(9, 13)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td>['Pretrained', 'contextual', 'representation',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Devlin , 2018 ) includes a model simultaneou...</td>\n",
       "      <td>includes</td>\n",
       "      <td>['new release of BERT ( Devlin , 2018', ')']</td>\n",
       "      <td>['a model simultaneously pretrained on 104 lan...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'Devlin', 'of']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>A new release of BERT (Devlin, 2018) includes ...</td>\n",
       "      <td>['A', 'new', 'release', 'of', 'BERT', '(', 'De...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>(16, 20)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>( multilingual ) as a zero shot language trans...</td>\n",
       "      <td>explores</td>\n",
       "      <td>['This', 'paper']</td>\n",
       "      <td>['the broader cross - lingual potential of mBE...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['mBERT', 'multilingual', 'of']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>['This', 'paper', 'explores', 'the', 'broader'...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>(58, 63)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best - published methods for zero - s...</td>\n",
       "      <td>compare</td>\n",
       "      <td>['We']</td>\n",
       "      <td>['mBERT with the best - published methods for ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['mBERT', 'compare', '.']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We compare mBERT with the best-published metho...</td>\n",
       "      <td>['We', 'compare', 'mBERT', 'with', 'the', 'bes...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>(10, 15)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>We compare mBERT with the best - published met...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>competitive on each task .</td>\n",
       "      <td>find</td>\n",
       "      <td>['- lingual transfer']</td>\n",
       "      <td>['We compare mBERT with the best - published m...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['mBERT', 'competitive', 'find']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We compare mBERT with the best-published metho...</td>\n",
       "      <td>['We', 'compare', 'mBERT', 'with', 'the', 'bes...</td>\n",
       "      <td>(19, 20)</td>\n",
       "      <td>(100, 105)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner , determine to what extent mBER...</td>\n",
       "      <td>investigate</td>\n",
       "      <td>['Additionally']</td>\n",
       "      <td>['the most effective strategy for utilizing mB...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['mBERT', 'for', 'strategy']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>['Additionally', ',', 'we', 'investigate', 'th...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>(71, 76)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>generalizes away from language specific featur...</td>\n",
       "      <td>generalizes</td>\n",
       "      <td>['what extent mBERT']</td>\n",
       "      <td>['away from language specific features , and m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['mBERT', 'generalizes', 'determine']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>['Additionally', ',', 'we', 'investigate', 'th...</td>\n",
       "      <td>(19, 20)</td>\n",
       "      <td>(119, 124)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Beto , Bentz , Becas : The Surprising Cross-Li...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'Effectiveness']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>['Beto', ',', 'Bentz', ',', 'Becas', ':', 'The...</td>\n",
       "      <td>(11, 12)</td>\n",
       "      <td>(68, 72)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Language model pre - training , such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has significantly improved the performances ...</td>\n",
       "      <td>pre</td>\n",
       "      <td>['Language model']</td>\n",
       "      <td>['-', 'training , such as BERT ,']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'as', 'training']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>['Language', 'model', 'pre', '-', 'training', ...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>(39, 43)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td>['However,', 'pre-trained', 'language', 'model...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td>['To', 'accelerate', 'inference', 'and', 'redu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "      <td>a small student TinyBERT</td>\n",
       "      <td>.</td>\n",
       "      <td>can be transferred</td>\n",
       "      <td>['the plenty of knowledge encoded in a large t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['student', 'to', 'transferred']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>['By', 'leveraging', 'this', 'new', 'KD', 'met...</td>\n",
       "      <td>(22, 26)</td>\n",
       "      <td>(117, 141)</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "      <td>a large teacher BERT</td>\n",
       "      <td>can be well transferred to a small student Tin...</td>\n",
       "      <td>encoded</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'in', 'encoded']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>['By', 'leveraging', 'this', 'new', 'KD', 'met...</td>\n",
       "      <td>(13, 17)</td>\n",
       "      <td>(69, 89)</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Moreover , we introduce a new two - stage lear...</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "      <td>introduce</td>\n",
       "      <td>['we', ',']</td>\n",
       "      <td>['a new two - stage learning framework for Tin...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['TinyBERT', 'for', 'framework']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>['Moreover', ',', 'we', 'introduce', 'a', 'new...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>(64, 72)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Moreover , we introduce</td>\n",
       "      <td>a new two - stage learning framework for TinyB...</td>\n",
       "      <td>.</td>\n",
       "      <td>introduce</td>\n",
       "      <td>['we', ',']</td>\n",
       "      <td>['a new two - stage learning framework for Tin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['introduce']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>['Moreover', ',', 'we', 'introduce', 'a', 'new...</td>\n",
       "      <td>(4, 30)</td>\n",
       "      <td>(23, 177)</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general - domain and task...</td>\n",
       "      <td>can capture</td>\n",
       "      <td>['TinyBERT']</td>\n",
       "      <td>['both the general - domain and', 'task - spec...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['TinyBERT', 'capture', 'ensures']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "      <td>['This', 'framework', 'ensures', 'that', 'Tiny...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>(27, 35)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>NaN</td>\n",
       "      <td>This framework</td>\n",
       "      <td>ensures that TinyBERT can capture both the gen...</td>\n",
       "      <td>ensures</td>\n",
       "      <td>['This', 'framework']</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['ensures']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "      <td>['This', 'framework', 'ensures', 'that', 'Tiny...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(0, 14)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "      <td>the teacher BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>can capture</td>\n",
       "      <td>['TinyBERT']</td>\n",
       "      <td>['both the general - domain and', 'task - spec...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['teacher', 'of', 'knowledge']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "      <td>['This', 'framework', 'ensures', 'that', 'Tiny...</td>\n",
       "      <td>(18, 21)</td>\n",
       "      <td>(107, 123)</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "      <td>is</td>\n",
       "      <td>['TinyBERT']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['TinyBERT', 'is', 'empirically']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "      <td>['TinyBERT', 'is', 'empirically', 'effective',...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in GLUE datasets , while being 7.5x smaller an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'with', 'results']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "      <td>['TinyBERT', 'is', 'empirically', 'effective',...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>(70, 74)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state - of -...</td>\n",
       "      <td>is</td>\n",
       "      <td>['TinyBERT']</td>\n",
       "      <td>['also significantly better than state - of - ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['TinyBERT', 'is']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>TinyBERT is also significantly better than sta...</td>\n",
       "      <td>['TinyBERT', 'is', 'also', 'significantly', 'b...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>NaN</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>: Distilling BERT for Natural Language Underst...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['TinyBERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "      <td>['TinyBERT', ':', 'Distilling', 'BERT', 'for',...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, a pre-trained Transformer model , has achiev...</td>\n",
       "      <td>has achieved</td>\n",
       "      <td>['BERT , a pre - trained Transformer model ,']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'achieved', 'ground']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "      <td>['BERT', ',', 'a', 'pre-trained', 'Transformer...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>In this paper , we describe</td>\n",
       "      <td>BERTSUM</td>\n",
       "      <td>, a simple variant of BERT , for extractive su...</td>\n",
       "      <td>describe</td>\n",
       "      <td>['we']</td>\n",
       "      <td>['BERTSUM , a simple variant of BERT ,']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERTSUM', 'describe']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe BERTSUM, a simple v...</td>\n",
       "      <td>['In', 'this', 'paper', ',', 'we', 'describe',...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(27, 34)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td>['Our', 'system', 'is', 'the', 'state', 'of', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td>['The', 'codes', 'to', 'reproduce', 'our', 're...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization.</td>\n",
       "      <td>['Fine-tune', 'BERT', 'for', 'Extractive', 'Su...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(9, 13)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td>['Question-answering', 'plays', 'an', 'importa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td>['Inspired', 'by', 'the', 'recent', 'success',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td>['To', 'the', 'best', 'of', 'our', 'knowledge,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td>['In', 'this', 'work,', 'we', 'first', 'build'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>the popular language model BERT</td>\n",
       "      <td>to enhance the performance of fine - tuning of...</td>\n",
       "      <td>explore</td>\n",
       "      <td>[]</td>\n",
       "      <td>['a novel post - training approach on the popu...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'on', 'approach']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>['Since', 'ReviewRC', 'has', 'limited', 'train...</td>\n",
       "      <td>(29, 34)</td>\n",
       "      <td>(157, 188)</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for RRC .</td>\n",
       "      <td>enhance</td>\n",
       "      <td>['to']</td>\n",
       "      <td>['the performance of fine - tuning of BERT for...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'tuning']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>['Since', 'ReviewRC', 'has', 'limited', 'train...</td>\n",
       "      <td>(43, 44)</td>\n",
       "      <td>(236, 240)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>a novel post - training approach on the popula...</td>\n",
       "      <td>to enhance the performance of fine - tuning of...</td>\n",
       "      <td>explore</td>\n",
       "      <td>[]</td>\n",
       "      <td>['a novel post - training approach on the popu...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['approach', 'explore', 'then']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>['Since', 'ReviewRC', 'has', 'limited', 'train...</td>\n",
       "      <td>(22, 34)</td>\n",
       "      <td>(121, 188)</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>To show the generality of</td>\n",
       "      <td>the approach</td>\n",
       "      <td>, the proposed post - training is also applied...</td>\n",
       "      <td>To show</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the generality of the approach , the propose...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['approach', 'of', 'generality']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td>['To', 'show', 'the', 'generality', 'of', 'the...</td>\n",
       "      <td>(5, 7)</td>\n",
       "      <td>(25, 37)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>To show the generality of the approach ,</td>\n",
       "      <td>the proposed post - training</td>\n",
       "      <td>is also applied to some other review - based t...</td>\n",
       "      <td>To show</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the generality of the approach , the propose...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['training', 'of', 'generality']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td>['To', 'show', 'the', 'generality', 'of', 'the...</td>\n",
       "      <td>(8, 13)</td>\n",
       "      <td>(40, 68)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Experimental results demonstrate that</td>\n",
       "      <td>the proposed post - training</td>\n",
       "      <td>is highly effective .</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['training', 'highly', 'effective']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td>['Experimental', 'results', 'demonstrate', 'th...</td>\n",
       "      <td>(4, 9)</td>\n",
       "      <td>(37, 65)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td>['The', 'datasets', 'and', 'code', 'are', 'ava...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'Post']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "      <td>['BERT', 'Post-Training', 'for', 'Review', 'Re...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td>['Language', 'model', 'pre-training', 'has', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>As a state - of - the - art language model pre...</td>\n",
       "      <td>BERT ( Bidirectional Encoder Representations f...</td>\n",
       "      <td>has achieved amazing results in many language ...</td>\n",
       "      <td>achieved</td>\n",
       "      <td>['BERT ( Bidirectional Encoder Representations...</td>\n",
       "      <td>['amazing results in many language understandi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['Representations', 'achieved']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>['As', 'a', 'state', '-', 'of', '-', 'the', '-...</td>\n",
       "      <td>(16, 24)</td>\n",
       "      <td>(65, 129)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "      <td>to investigate</td>\n",
       "      <td>[]</td>\n",
       "      <td>['different fine - tuning methods of BERT on t...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'methods']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>['In', 'this', 'paper', ',', 'we', 'conduct', ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>(99, 103)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>fine - tuning .</td>\n",
       "      <td>provide</td>\n",
       "      <td>[]</td>\n",
       "      <td>['a general solution for BERT fine - tuning']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'tuning', 'for']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>['In', 'this', 'paper', ',', 'we', 'conduct', ...</td>\n",
       "      <td>(27, 28)</td>\n",
       "      <td>(167, 171)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "      <td>a general solution for BERT fine - tuning</td>\n",
       "      <td>.</td>\n",
       "      <td>provide</td>\n",
       "      <td>[]</td>\n",
       "      <td>['a general solution for BERT fine - tuning']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['solution', 'provide', 'task']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>['In', 'this', 'paper', ',', 'we', 'conduct', ...</td>\n",
       "      <td>(23, 31)</td>\n",
       "      <td>(144, 185)</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Finally ,</td>\n",
       "      <td>the proposed solution</td>\n",
       "      <td>obtains new state - of - the - art results on ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['solution', 'state']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td>['Finally', ',', 'the', 'proposed', 'solution'...</td>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>(9, 30)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification ? .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'Tune', 'Fine']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?.</td>\n",
       "      <td>['How', 'to', 'Fine-Tune', 'BERT', 'for', 'Tex...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(16, 20)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Devlin et al . , 2018 ) is a Markov random f...</td>\n",
       "      <td>show</td>\n",
       "      <td>['We']</td>\n",
       "      <td>['that BERT ( Devlin et al . , 2018 )', 'is a ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'et', 'that']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "      <td>['We', 'show', 'that', 'BERT', '(', 'Devlin', ...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(12, 16)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>gives</td>\n",
       "      <td>['This', 'formulation']</td>\n",
       "      <td>['way', 'to a natural procedure to sample sent...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'from', 'sentences']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>['This', 'formulation', 'gives', 'way', 'to', ...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>(74, 78)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high - quality , ...</td>\n",
       "      <td>generate</td>\n",
       "      <td>['We']</td>\n",
       "      <td>['find that it can produce high - quality , fl...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'from', 'generate']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "      <td>['We', 'generate', 'from', 'BERT', 'and', 'fin...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(16, 20)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>We generate from BERT and find that</td>\n",
       "      <td>it</td>\n",
       "      <td>can produce high - quality , fluent generations .</td>\n",
       "      <td>produce</td>\n",
       "      <td>['it', 'can']</td>\n",
       "      <td>['high - quality', ', fluent generations .']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['it', 'produce', 'find']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "      <td>['We', 'generate', 'from', 'BERT', 'and', 'fin...</td>\n",
       "      <td>(7, 8)</td>\n",
       "      <td>(35, 37)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "      <td>Compared</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'sentences', ',']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>['Compared', 'to', 'the', 'generations', 'of',...</td>\n",
       "      <td>(15, 16)</td>\n",
       "      <td>(79, 83)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth , and It Must Speak : BERT as a Ma...</td>\n",
       "      <td>has</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>['a Mouth , and It Must Speak : BERT as a Mark...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'has']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "      <td>['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It'...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>BERT has a Mouth , and</td>\n",
       "      <td>It</td>\n",
       "      <td>Must Speak : BERT as a Markov Random Field Lan...</td>\n",
       "      <td>Must</td>\n",
       "      <td>['It']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['It', 'Must', 'Speak']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "      <td>['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It'...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(22, 24)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td>['Aspect-based', 'sentiment', 'analysis', '(AB...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td>['In', 'this', 'paper,', 'we', 'construct', 'a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'from', 'model']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We fine-tune the pre-trained model from BERT a...</td>\n",
       "      <td>['We', 'fine-tune', 'the', 'pre-trained', 'mod...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(39, 43)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "      <td>['Utilizing', 'BERT', 'for', 'Aspect-Based', '...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(9, 13)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering , we explore simple appl...</td>\n",
       "      <td>applying</td>\n",
       "      <td>[]</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'applying', 'in']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "      <td>['Following', 'recent', 'successes', 'in', 'ap...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>(38, 42)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle .</td>\n",
       "      <td>posed</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'length']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>['This', 'required', 'confronting', 'the', 'ch...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>(109, 113)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>This required confronting</td>\n",
       "      <td>the challenge posed by documents that are typi...</td>\n",
       "      <td>.</td>\n",
       "      <td>confronting</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the challenge posed by documents that are ty...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['challenge', 'confronting', 'required']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>['This', 'required', 'confronting', 'the', 'ch...</td>\n",
       "      <td>(3, 22)</td>\n",
       "      <td>(25, 136)</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>We address</td>\n",
       "      <td>this issue</td>\n",
       "      <td>by applying inference on sentences individuall...</td>\n",
       "      <td>address</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['issue', 'address', 'then']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td>['We', 'address', 'this', 'issue', 'by', 'appl...</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>(10, 20)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td>['Experiments', 'on', 'TREC', 'microblog', 'an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'Applications']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "      <td>['Simple', 'Applications', 'of', 'BERT', 'for'...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(22, 26)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT - based architectures</td>\n",
       "      <td>currently give state - of - the - art performa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'currently']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "      <td>['BERT', '-', 'based', 'architectures', 'curre...</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>(0, 26)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>BERT - based architectures currently give stat...</td>\n",
       "      <td>its</td>\n",
       "      <td>success .</td>\n",
       "      <td>contribute</td>\n",
       "      <td>['that']</td>\n",
       "      <td>['its .']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['its', 'contribute', 'mechanisms']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "      <td>['BERT', '-', 'based', 'architectures', 'curre...</td>\n",
       "      <td>(30, 31)</td>\n",
       "      <td>(162, 165)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>In the current work , we focus on the interpre...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>focus</td>\n",
       "      <td>[',', 'we']</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'components']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>['In', 'the', 'current', 'work', ',', 'we', 'f...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td>(131, 135)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s heads .</td>\n",
       "      <td>encoded</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'individual', 'by']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>['Using', 'a', 'subset', 'of', 'GLUE', 'tasks'...</td>\n",
       "      <td>(32, 33)</td>\n",
       "      <td>(202, 206)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td>['Our', 'findings', 'suggest', 'that', 'there'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td>['While', 'different', 'heads', 'consistently'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models</td>\n",
       "      <td>leads</td>\n",
       "      <td>['manually disabling attention in certain heads']</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'models', 'over']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>['We', 'show', 'that', 'manually', 'disabling'...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td>(123, 127)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>Revealing</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the Dark Secrets of BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'Secrets']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Revealing the Dark Secrets of BERT.</td>\n",
       "      <td>['Revealing', 'the', 'Dark', 'Secrets', 'of', ...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>(29, 33)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>) has shown marvelous improvements across vari...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', ')', 'Transformers']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>['Bidirectional', 'Encoder', 'Representations'...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(57, 61)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Recently , an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking ( WW...</td>\n",
       "      <td>has been released</td>\n",
       "      <td>['an upgraded version of BERT', 'Recently ,']</td>\n",
       "      <td>['which mitigate the drawbacks of masking part...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'version']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, an upgraded version of BERT has been...</td>\n",
       "      <td>['Recently', ',', 'an', 'upgraded', 'version',...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(33, 37)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Recently , an upgraded version of BERT has bee...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>masking</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'training', 'pre']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, an upgraded version of BERT has been...</td>\n",
       "      <td>['Recently', ',', 'an', 'upgraded', 'version',...</td>\n",
       "      <td>(31, 32)</td>\n",
       "      <td>(173, 177)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td>['In', 'this', 'technical', 'report,', 'we', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The model</td>\n",
       "      <td>was trained on the latest Chinese Wikipedia du...</td>\n",
       "      <td>was trained</td>\n",
       "      <td>['The model']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'trained']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td>['The', 'model', 'was', 'trained', 'on', 'the'...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(0, 9)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>Chinese BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "      <td>to provide</td>\n",
       "      <td>[]</td>\n",
       "      <td>['easy', 'extensibility and', 'better performa...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'for', 'provide']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>['We', 'aim', 'to', 'provide', 'easy', 'extens...</td>\n",
       "      <td>(10, 12)</td>\n",
       "      <td>(63, 75)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The model</td>\n",
       "      <td>is verified on various NLP tasks , across sent...</td>\n",
       "      <td>is verified</td>\n",
       "      <td>['The model']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'verified', 'inference']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td>['The', 'model', 'is', 'verified', 'on', 'vari...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(0, 9)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td>['Experimental', 'results', 'on', 'these', 'da...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE , BERT - wwm .</td>\n",
       "      <td>examine</td>\n",
       "      <td>[',', 'we']</td>\n",
       "      <td>['the effectiveness of Chinese pre - trained m...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', ':', 'models']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>['Moreover', ',', 'we', 'also', 'examine', 'th...</td>\n",
       "      <td>(14, 15)</td>\n",
       "      <td>(78, 82)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "      <td>BERT - wwm</td>\n",
       "      <td>.</td>\n",
       "      <td>examine</td>\n",
       "      <td>[',', 'we']</td>\n",
       "      <td>['the effectiveness of Chinese pre - trained m...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['wwm', ',', 'ERNIE']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>['Moreover', ',', 'we', 'also', 'examine', 'th...</td>\n",
       "      <td>(18, 21)</td>\n",
       "      <td>(93, 103)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td>['We', 'release', 'the', 'pre-trained', 'model...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'for', 'Masking']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "      <td>['Pre-Training', 'with', 'Whole', 'Word', 'Mas...</td>\n",
       "      <td>(7, 8)</td>\n",
       "      <td>(48, 52)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open - ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'model', 'successfully']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "      <td>['BERT', 'model', 'has', 'been', 'successfully...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>However , previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>['previous work trains']</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'cause']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, previous work trains BERT by viewing ...</td>\n",
       "      <td>['However', ',', 'previous', 'work', 'trains',...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>(30, 34)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>To tackle this issue , we propose</td>\n",
       "      <td>a multi - passage BERT model to globally norma...</td>\n",
       "      <td>, and this change enables our QA model find be...</td>\n",
       "      <td>propose</td>\n",
       "      <td>['To', 'we', 'tackle this issue']</td>\n",
       "      <td>['a multi - passage BERT model to globally nor...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'propose']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To tackle this issue, we propose a multi-passa...</td>\n",
       "      <td>['To', 'tackle', 'this', 'issue', ',', 'we', '...</td>\n",
       "      <td>(7, 25)</td>\n",
       "      <td>(33, 138)</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td>['In', 'addition,', 'we', 'find', 'that', 'spl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2 % .</td>\n",
       "      <td>gains</td>\n",
       "      <td>['BERT .']</td>\n",
       "      <td>['additional 2 %']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'gains', 'passage']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>['By', 'leveraging', 'a', 'passage', 'ranker',...</td>\n",
       "      <td>(11, 12)</td>\n",
       "      <td>(78, 82)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>our multi - passage BERT</td>\n",
       "      <td>outperforms all state - of - the - art models ...</td>\n",
       "      <td>showed</td>\n",
       "      <td>['Experiments on four standard benchmarks']</td>\n",
       "      <td>['that our multi - passage BERT outperforms al...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'showed']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>['Experiments', 'on', 'four', 'standard', 'ben...</td>\n",
       "      <td>(7, 12)</td>\n",
       "      <td>(51, 75)</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>In particular , on the OpenSQuAD dataset , our...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models , and 5.8 % EM and 6.5 % $ F_1 $ over B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'models', 'non']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>['In', 'particular', ',', 'on', 'the', 'OpenSQ...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td>(102, 106)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>In particular , on the OpenSQuAD dataset ,</td>\n",
       "      <td>our model</td>\n",
       "      <td>gains 21.4 % EM and 21.5 % $ F_1 $ over all no...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'on', 'In']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>['In', 'particular', ',', 'on', 'the', 'OpenSQ...</td>\n",
       "      <td>(8, 10)</td>\n",
       "      <td>(42, 51)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>: A Globally Normalized BERT Model for Open-do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'passage']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "      <td>['Multi-passage', 'BERT', ':', 'A', 'Globally'...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(13, 17)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Multi-passage BERT : A Globally Normalized</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Model for Open-domain Question Answering .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'Normalized', 'Globally']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "      <td>['Multi-passage', 'BERT', ':', 'A', 'Globally'...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(42, 46)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td>['Intent', 'classification', 'and', 'slot', 'f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td>['They', 'often', 'suffer', 'from', 'small-sca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Recently a new language representation model ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Bidirectional Encoder Representations from T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', '(', 'model']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently a new language representation model, ...</td>\n",
       "      <td>['Recently', 'a', 'new', 'language', 'represen...</td>\n",
       "      <td>(7, 8)</td>\n",
       "      <td>(46, 50)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>However , there has not been much effort on ex...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding .</td>\n",
       "      <td>exploring</td>\n",
       "      <td>[]</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'exploring', 'on']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>['However', ',', 'there', 'has', 'not', 'been'...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>(53, 57)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>In this work , we propose a joint intent class...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>based</td>\n",
       "      <td>['a joint intent classification and slot filli...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'on', 'based']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>['In', 'this', 'work', ',', 'we', 'propose', '...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>(87, 91)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>In this work , we propose</td>\n",
       "      <td>a joint intent classification and slot filling...</td>\n",
       "      <td>.</td>\n",
       "      <td>based</td>\n",
       "      <td>['a joint intent classification and slot filli...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['based', 'propose']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>['In', 'this', 'work', ',', 'we', 'propose', '...</td>\n",
       "      <td>(6, 17)</td>\n",
       "      <td>(25, 91)</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Experimental results demonstrate that</td>\n",
       "      <td>our proposed model</td>\n",
       "      <td>achieves significant improvement on intent cla...</td>\n",
       "      <td>achieves</td>\n",
       "      <td>['our proposed model']</td>\n",
       "      <td>['significant improvement']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'achieves', 'demonstrate']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td>['Experimental', 'results', 'demonstrate', 'th...</td>\n",
       "      <td>(4, 7)</td>\n",
       "      <td>(37, 55)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "      <td>['BERT', 'for', 'Joint', 'Intent', 'Classifica...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td>['Conversational', 'search', 'is', 'an', 'emer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td>['One', 'of', 'the', 'major', 'challenges', 't...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td>['Existing', 'methods', 'either', 'prepend', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td>['We', 'propose', 'a', 'conceptually', 'simple...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Bidirectional Encoder Representations from T...</td>\n",
       "      <td>built</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'on', 'built']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>['It', 'enables', 'seamless', 'integration', '...</td>\n",
       "      <td>(18, 19)</td>\n",
       "      <td>(122, 126)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td>['We', 'first', 'explain', 'our', 'view', 'tha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td>['We', 'further', 'demonstrate', 'the', 'effec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td>['Finally,', 'we', 'analyze', 'the', 'impact',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "      <td>['BERT', 'with', 'History', 'Answer', 'Embeddi...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks .</td>\n",
       "      <td>studies</td>\n",
       "      <td>['This']</td>\n",
       "      <td>['the performances and behaviors of BERT in ra...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'behaviors']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>['This', 'paper', 'studies', 'the', 'performan...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>(52, 56)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>We explore several different ways to leverage</td>\n",
       "      <td>the pre - trained BERT</td>\n",
       "      <td>and fine - tune it on two ranking tasks : MS M...</td>\n",
       "      <td>to leverage</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the pre - trained BERT and', 'fine - tune it...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'leverage', 'ways']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>['We', 'explore', 'several', 'different', 'way...</td>\n",
       "      <td>(7, 12)</td>\n",
       "      <td>(45, 67)</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>it</td>\n",
       "      <td>on two ranking tasks : MS MARCO passage rerank...</td>\n",
       "      <td>to leverage</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the pre - trained BERT and', 'fine - tune it...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['it', 'tune', 'leverage']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>['We', 'explore', 'several', 'different', 'way...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>(84, 86)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question - answering focused passage rankin...</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>['Experimental results on MS MARCO', 'the stro...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'effectiveness']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>['Experimental', 'results', 'on', 'MS', 'MARCO...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>(72, 76)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is a strong interaction - based seq2seq matchi...</td>\n",
       "      <td>answering</td>\n",
       "      <td>[]</td>\n",
       "      <td>['focused passage ranking tasks']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'model', 'fact']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>['Experimental', 'results', 'on', 'MS', 'MARCO...</td>\n",
       "      <td>(26, 27)</td>\n",
       "      <td>(158, 162)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "      <td>pre</td>\n",
       "      <td>['the', 'BERT']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'pre', 'between']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>['Experimental', 'results', 'on', 'TREC', 'sho...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>(54, 58)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query - docum...</td>\n",
       "      <td>allocates</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>['its attentions between query - document toke...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'allocates', 'illustrate']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>['Analyses', 'illustrate', 'how', 'BERT', 'all...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(23, 27)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Analyses illustrate how BERT allocates</td>\n",
       "      <td>its</td>\n",
       "      <td>attentions between query - document tokens in ...</td>\n",
       "      <td>allocates</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>['its attentions between query - document toke...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['its', 'attentions', 'allocates']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>['Analyses', 'illustrate', 'how', 'BERT', 'all...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>(38, 41)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>its</td>\n",
       "      <td>Transformer layers , how it prefers semantic m...</td>\n",
       "      <td>allocates</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>['its attentions between query - document toke...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['its', 'layers', 'in']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>['Analyses', 'illustrate', 'how', 'BERT', 'all...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>(88, 91)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>it</td>\n",
       "      <td>prefers semantic matches between paraphrase to...</td>\n",
       "      <td>prefers</td>\n",
       "      <td>['it']</td>\n",
       "      <td>['semantic', 'how that differs with the soft m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['it', 'prefers', 'document']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>['Analyses', 'illustrate', 'how', 'BERT', 'all...</td>\n",
       "      <td>(18, 19)</td>\n",
       "      <td>(117, 119)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking .</td>\n",
       "      <td>Understanding</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the Behaviors of BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'Behaviors']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking.</td>\n",
       "      <td>['Understanding', 'the', 'Behaviors', 'of', 'B...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>(30, 34)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>We present</td>\n",
       "      <td>simple BERT - based models for relation extrac...</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['role']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "      <td>['We', 'present', 'simple', 'BERT', '-', 'base...</td>\n",
       "      <td>(2, 14)</td>\n",
       "      <td>(10, 87)</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td>['In', 'recent', 'years,', 'state-of-the-art',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>In this paper , extensive experiments on datas...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based model can achieve state-of-the-art perf...</td>\n",
       "      <td>achieve</td>\n",
       "      <td>['can']</td>\n",
       "      <td>['any external features , a simple BERT -based...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'model', 'of']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>['In', 'this', 'paper', ',', 'extensive', 'exp...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>(126, 130)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>To our knowledge , we are the first to success...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner .</td>\n",
       "      <td>to apply</td>\n",
       "      <td>[]</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'apply', 'first']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>['To', 'our', 'knowledge', ',', 'we', 'are', '...</td>\n",
       "      <td>(11, 12)</td>\n",
       "      <td>(57, 61)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Our models</td>\n",
       "      <td>provide strong baselines for future research</td>\n",
       "      <td>provide</td>\n",
       "      <td>['Our models']</td>\n",
       "      <td>['strong baselines']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['models', 'provide']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td>['Our', 'models', 'provide', 'strong', 'baseli...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'Models']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "      <td>['Simple', 'BERT', 'Models', 'for', 'Relation'...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(6, 10)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td>['Multi-task', 'learning', 'allows', 'the', 's...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td>['In', 'natural', 'language', 'processing', 's...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td>['These', 'results', 'are', 'based', 'on', 'fi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark , and how to best ...</td>\n",
       "      <td>explore</td>\n",
       "      <td>['We']</td>\n",
       "      <td>['the multi - task learning setting for the re...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'model', 'for']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>['We', 'explore', 'the', 'multi-task', 'learni...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>(57, 61)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>network , with a high degree of parameter shar...</td>\n",
       "      <td>add</td>\n",
       "      <td>['and how to best']</td>\n",
       "      <td>['We explore the multi - task learning setting...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'network', 'to']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>['We', 'explore', 'the', 'multi-task', 'learni...</td>\n",
       "      <td>(26, 27)</td>\n",
       "      <td>(154, 158)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td>['We', 'introduce', 'new', 'adaptation', 'modu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers , we match the performance of fine-tune...</td>\n",
       "      <td>using</td>\n",
       "      <td>[]</td>\n",
       "      <td>['PALs']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'layers', 'with']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>By using PALs in parallel with BERT layers, we...</td>\n",
       "      <td>['By', 'using', 'PALs', 'in', 'parallel', 'wit...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(30, 34)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>By using PALs in parallel with BERT layers , w...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on the GLUE benchmark with roughly 7 times few...</td>\n",
       "      <td>tuned</td>\n",
       "      <td>['-']</td>\n",
       "      <td>['BERT on the GLUE benchmark with roughly 7 ti...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'tuned', 'of']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>By using PALs in parallel with BERT layers, we...</td>\n",
       "      <td>['By', 'using', 'PALs', 'in', 'parallel', 'wit...</td>\n",
       "      <td>(15, 16)</td>\n",
       "      <td>(83, 87)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs : Projected Attention Layers for Effi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "      <td>['BERT', 'and', 'PALs', ':', 'Projected', 'Att...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td>['Pre-training', 'by', 'language', 'modeling',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td>['In', 'this', 'paper', 'we', 'introduce', 'a'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>As a case study , we apply these diagnostics to</td>\n",
       "      <td>the popular BERT model</td>\n",
       "      <td>, finding that it can generally distinguish go...</td>\n",
       "      <td>apply</td>\n",
       "      <td>['we', ',', 'finding that it can generally dis...</td>\n",
       "      <td>['these diagnostics']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'to', 'apply']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>['As', 'a', 'case', 'study', ',', 'we', 'apply...</td>\n",
       "      <td>(10, 14)</td>\n",
       "      <td>(47, 69)</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>As a case study , we apply these diagnostics t...</td>\n",
       "      <td>it</td>\n",
       "      <td>can generally distinguish good from bad comple...</td>\n",
       "      <td>can distinguish</td>\n",
       "      <td>['it']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['it', 'distinguish', 'finding']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>['As', 'a', 'case', 'study', ',', 'we', 'apply...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>(85, 87)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>As a case study , we apply these diagnostics t...</td>\n",
       "      <td>it</td>\n",
       "      <td>robustly retrieves noun hypernyms , but it str...</td>\n",
       "      <td>shows</td>\n",
       "      <td>['it', 'As a case study , we apply these diagn...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['it', 'shows', 'clear']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>['As', 'a', 'case', 'study', ',', 'we', 'apply...</td>\n",
       "      <td>(40, 41)</td>\n",
       "      <td>(232, 234)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>As a case study , we apply these diagnostics t...</td>\n",
       "      <td>it</td>\n",
       "      <td>struggles with challenging inference and role ...</td>\n",
       "      <td>struggles</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['it', 'struggles', 'shows']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>['As', 'a', 'case', 'study', ',', 'we', 'apply...</td>\n",
       "      <td>(47, 48)</td>\n",
       "      <td>(275, 277)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>As a case study , we apply these diagnostics t...</td>\n",
       "      <td>it</td>\n",
       "      <td>shows clear insensitivity to the contextual im...</td>\n",
       "      <td>shows</td>\n",
       "      <td>['it', 'As a case study , we apply these diagn...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['it', 'shows', 'clear']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>['As', 'a', 'case', 'study', ',', 'we', 'apply...</td>\n",
       "      <td>(64, 65)</td>\n",
       "      <td>(373, 375)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not : Lessons from a New Suite of Psycholin...</td>\n",
       "      <td>Is</td>\n",
       "      <td>['What', 'BERT']</td>\n",
       "      <td>['Not', ': Lessons from a New Suite of Psychol...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'Is']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "      <td>['What', 'BERT', 'Is', 'Not', ':', 'Lessons', ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(4, 8)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Language model pre - training , such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "      <td>pre</td>\n",
       "      <td>['model']</td>\n",
       "      <td>['-', 'training , such as BERT ,']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'as', 'training']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>['Language', 'model', 'pre', '-', 'training', ...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>(39, 43)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td>['However,', 'it', 'is', 'unclear', 'why', 'th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>In this paper , we propose to visualize loss l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets .</td>\n",
       "      <td>tuning</td>\n",
       "      <td>['-']</td>\n",
       "      <td>['BERT on specific datasets']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'tuning', 'of']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>['In', 'this', 'paper', ',', 'we', 'propose', ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>(102, 106)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td>['First,', 'we', 'find', 'that', 'pre-training...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>We also demonstrate that the fine - tuning pro...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over - parameterized for downstream ...</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>['We']</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'highly', 'robust']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>['We', 'also', 'demonstrate', 'that', 'the', '...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>(91, 95)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Second , the visualization results indicate th...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "      <td>tuning</td>\n",
       "      <td>['-']</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'tuning', 'tends']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>['Second', ',', 'the', 'visualization', 'resul...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>(62, 66)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Third , the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine - tuning , whic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'layers']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Third, the lower layers of BERT are more invar...</td>\n",
       "      <td>['Third', ',', 'the', 'lower', 'layers', 'of',...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(27, 31)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>Understanding</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the Effectiveness of BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'Effectiveness']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>['Visualizing', 'and', 'Understanding', 'the',...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(50, 54)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td>['Data', 'augmentation', 'methods', 'are', 'of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Recently proposed contextual augmentation</td>\n",
       "      <td>augments labeled sentences by randomly replaci...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['Recently']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td>['Recently', 'proposed', 'contextual', 'augmen...</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>(0, 41)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['Representations']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>['Bidirectional', 'Encoder', 'Representations'...</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>(0, 64)</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>We propose</td>\n",
       "      <td>a novel data augmentation method for labeled s...</td>\n",
       "      <td>.</td>\n",
       "      <td>propose</td>\n",
       "      <td>['We']</td>\n",
       "      <td>['a novel data augmentation method for labeled...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['method', 'propose']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>['We', 'propose', 'a', 'novel', 'data', 'augme...</td>\n",
       "      <td>(2, 15)</td>\n",
       "      <td>(10, 112)</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>We retrofit BERT to</td>\n",
       "      <td>conditional BERT</td>\n",
       "      <td>by introducing a new conditional masked langua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['to', 'BERT', '.']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "      <td>['We', 'retrofit', 'BERT', 'to', 'conditional'...</td>\n",
       "      <td>(4, 6)</td>\n",
       "      <td>(19, 35)</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', '.', '”']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "      <td>['We', 'retrofit', 'BERT', 'to', 'conditional'...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>(11, 15)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>paper , which indicates context - conditional ...</td>\n",
       "      <td>appeared</td>\n",
       "      <td>[]</td>\n",
       "      <td>['once']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'paper', 'in']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "      <td>['We', 'retrofit', 'BERT', 'to', 'conditional'...</td>\n",
       "      <td>(27, 28)</td>\n",
       "      <td>(166, 170)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td>['In', 'our', 'paper,', '“conditional', 'maske...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>task.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>task.</td>\n",
       "      <td>['task.']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "      <td>can</td>\n",
       "      <td>['The well trained conditional BERT']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'conditional', 'can']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The well trained conditional BERT can be appli...</td>\n",
       "      <td>['The', 'well', 'trained', 'conditional', 'BER...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>(28, 32)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td>our method</td>\n",
       "      <td>can be easily applied to both convolutional or...</td>\n",
       "      <td>show</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['method', 'easily', 'show']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td>['Experiments', 'on', 'six', 'various', 'diffe...</td>\n",
       "      <td>(10, 12)</td>\n",
       "      <td>(72, 82)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Conditional BERT Contextual Augmentation.</td>\n",
       "      <td>['Conditional', 'BERT', 'Contextual', 'Augment...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(11, 15)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td>['We', 'propose', 'a', 'practical', 'scheme', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint , our final model is 6x smaller and...</td>\n",
       "      <td>Starting</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'checkpoint', 'from']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Starting from a public multilingual BERT check...</td>\n",
       "      <td>['Starting', 'from', 'a', 'public', 'multiling...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>(35, 39)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td>['We', 'show', 'that', 'our', 'model', 'especi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td>['We', 'showcase', 'the', 'effectiveness', 'of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'Small']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "      <td>['Small', 'and', 'Practical', 'BERT', 'Models'...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(19, 23)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Recently , a simple combination of passage ret...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "      <td>using</td>\n",
       "      <td>[]</td>\n",
       "      <td>['off', '- the - shelf IR techniques and a BER...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'reader', 'and']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>['Recently', ',', 'a', 'simple', 'combination'...</td>\n",
       "      <td>(14, 15)</td>\n",
       "      <td>(92, 96)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td>['In', 'this', 'paper,', 'we', 'present', 'a',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets , starting with data that...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'tuning', 'to']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning ...</td>\n",
       "      <td>['We', 'apply', 'a', 'stage-wise', 'approach',...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>(45, 49)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td>['Experimental', 'results', 'show', 'large', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'for', 'Augmentation']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "      <td>['Data', 'Augmentation', 'for', 'BERT', 'Fine-...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(21, 25)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The BERT language model ( LM )</td>\n",
       "      <td>( Devlin et al . , 2019 ) is surprisingly good...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['model', 'good']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "      <td>['The', 'BERT', 'language', 'model', '(', 'LM'...</td>\n",
       "      <td>(0, 7)</td>\n",
       "      <td>(0, 30)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td>['Petroni', 'et', 'al.']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>( 2019 ) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre - train...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'evidence', 'as']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>(2019) take this as evidence that BERT memoriz...</td>\n",
       "      <td>['(', '2019', ')', 'take', 'this', 'as', 'evid...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>(35, 39)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>( 2019 ) take this as</td>\n",
       "      <td>evidence that BERT memorizes factual knowledge...</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['evidence', 'as', '(']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>(2019) take this as evidence that BERT memoriz...</td>\n",
       "      <td>['(', '2019', ')', 'take', 'this', 'as', 'evid...</td>\n",
       "      <td>(6, 16)</td>\n",
       "      <td>(21, 89)</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about ( the surface...</td>\n",
       "      <td>argue</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'performance']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>['We', 'take', 'issue', 'with', 'this', 'inter...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>(72, 76)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>We take issue with</td>\n",
       "      <td>this interpretation</td>\n",
       "      <td>and argue that the performance of BERT is part...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['interpretation', 'with', 'issue']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>['We', 'take', 'issue', 'with', 'this', 'inter...</td>\n",
       "      <td>(4, 6)</td>\n",
       "      <td>(18, 37)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>More specifically , we show that</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "      <td>drops</td>\n",
       "      <td>[\"BERT 's precision\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'precision', 'drops']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>More specifically, we show that BERT's precisi...</td>\n",
       "      <td>['More', 'specifically', ',', 'we', 'show', 't...</td>\n",
       "      <td>(6, 8)</td>\n",
       "      <td>(32, 39)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>As a remedy , we propose E - BERT , an extensi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>that replaces entity mentions with symbolic en...</td>\n",
       "      <td>propose</td>\n",
       "      <td>['we', 'As a remedy']</td>\n",
       "      <td>['E - BERT an extension of BERT that replaces ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'extension']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>As a remedy, we propose E-BERT, an extension o...</td>\n",
       "      <td>['As', 'a', 'remedy', ',', 'we', 'propose', 'E...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>(51, 55)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>As a remedy , we propose</td>\n",
       "      <td>E - BERT , an extension of BERT that replaces ...</td>\n",
       "      <td>.</td>\n",
       "      <td>propose</td>\n",
       "      <td>['we', 'As a remedy']</td>\n",
       "      <td>['E - BERT an extension of BERT that replaces ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['E', 'propose']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>As a remedy, we propose E-BERT, an extension o...</td>\n",
       "      <td>['As', 'a', 'remedy', ',', 'we', 'propose', 'E...</td>\n",
       "      <td>(6, 22)</td>\n",
       "      <td>(24, 117)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>E - BERT outperforms both</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and ERNIE ( Zhang et al . , 2019 ) on hard - t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>E-BERT outperforms both BERT and ERNIE (Zhang ...</td>\n",
       "      <td>['E', '-', 'BERT', 'outperforms', 'both', 'BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>(25, 29)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>NaN</td>\n",
       "      <td>E - BERT</td>\n",
       "      <td>outperforms both BERT and ERNIE ( Zhang et al ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'BERT']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>E-BERT outperforms both BERT and ERNIE (Zhang ...</td>\n",
       "      <td>['E', '-', 'BERT', 'outperforms', 'both', 'BER...</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and E - BERT</td>\n",
       "      <td>show</td>\n",
       "      <td>['we']</td>\n",
       "      <td>['two ways of ensembling BERT and E - BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'ways']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "      <td>['We', 'take', 'this', 'as', 'evidence', 'that...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>(106, 110)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>We take this as evidence that</td>\n",
       "      <td>E - BERT</td>\n",
       "      <td>is richer in factual knowledge , and we show t...</td>\n",
       "      <td>take</td>\n",
       "      <td>['We']</td>\n",
       "      <td>['this']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['richer', 'evidence', 'as']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "      <td>['We', 'take', 'this', 'as', 'evidence', 'that...</td>\n",
       "      <td>(6, 9)</td>\n",
       "      <td>(29, 37)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "      <td>E - BERT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>show</td>\n",
       "      <td>['we']</td>\n",
       "      <td>['two ways of ensembling BERT and E - BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'BERT', 'of']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "      <td>['We', 'take', 'this', 'as', 'evidence', 'that...</td>\n",
       "      <td>(24, 27)</td>\n",
       "      <td>(115, 123)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base ( Yet ) : Factual Know...</td>\n",
       "      <td>is</td>\n",
       "      <td>['BERT']</td>\n",
       "      <td>['Not a Knowledge Base ( Yet ) : Factual Knowl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'is']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "      <td>['BERT', 'is', 'Not', 'a', 'Knowledge', 'Base'...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td>['Replacing', 'static', 'word', 'embeddings', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>However , just how contextual are the contextu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>?</td>\n",
       "      <td>produced</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'ELMo', 'as']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>['However', ',', 'just', 'how', 'contextual', ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>(104, 108)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td>['Are', 'there', 'infinitely', 'many', 'contex...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td>['For', 'one,', 'we', 'find', 'that', 'the', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td>['While', 'representations', 'of', 'the', 'sam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td>['This', 'suggests', 'that', 'upper', 'layers'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>In all layers of ELMo ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2 , on average , less than 5 % of th...</td>\n",
       "      <td>be explained</td>\n",
       "      <td>['can']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'layers']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In all layers of ELMo, BERT, and GPT-2, on ave...</td>\n",
       "      <td>['In', 'all', 'layers', 'of', 'ELMo', ',', 'BE...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(23, 27)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>['How', 'Contextual', 'are', 'Contextualized',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Comparing the Geometry of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ELMo , and GPT-2 Embeddings .</td>\n",
       "      <td>Comparing</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the', 'Geometry of BERT']</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>['BERT', 'of', 'Geometry']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing the Geometry of BERT, ELMo, and GPT-...</td>\n",
       "      <td>['Comparing', 'the', 'Geometry', 'of', 'BERT',...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>(25, 29)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               split_0  \\\n",
       "0                                         We introduce   \n",
       "1       Unlike recent language representation models ,   \n",
       "2                      As a result , the pre - trained   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "5                                                  NaN   \n",
       "6    Language model pretraining has led to signific...   \n",
       "7    Training is computationally expensive, often d...   \n",
       "8                    We present a replication study of   \n",
       "9                                         We find that   \n",
       "10   We find that BERT was significantly undertrain...   \n",
       "11   Our best model achieves state-of-the-art resul...   \n",
       "12   These results highlight the importance of prev...   \n",
       "13                     We release our models and code.   \n",
       "14                                                 NaN   \n",
       "15   As Transfer Learning from large-scale pre-trai...   \n",
       "16   In this work , we propose a method to pre-trai...   \n",
       "17   While most prior work investigated the use of ...   \n",
       "18   While most prior work investigated the use of ...   \n",
       "19   To leverage the inductive biases learned by la...   \n",
       "20   Our smaller, faster and lighter model is cheap...   \n",
       "21                                                 NaN   \n",
       "22   Pre-trained text encoders have rapidly advance...   \n",
       "23                                         We focus on   \n",
       "24                                        We find that   \n",
       "25                   Qualitative analysis reveals that   \n",
       "26                                                 NaN   \n",
       "27                                                 NaN   \n",
       "28   Large pre - trained neural networks such as BE...   \n",
       "29         Large pre - trained neural networks such as   \n",
       "30   Most recent analysis has focused on model outp...   \n",
       "31   Complementary to these works , we propose meth...   \n",
       "32                                                 NaN   \n",
       "33   We further show that certain attention heads c...   \n",
       "34   For example, we find heads that attend to the ...   \n",
       "35   Lastly , we propose an attention - based probi...   \n",
       "36                                           What Does   \n",
       "37                                      An Analysis of   \n",
       "38   Recently , neural models pretrained on a langu...   \n",
       "39                         In this paper , we describe   \n",
       "40                                                 NaN   \n",
       "41   The code to reproduce our results is available...   \n",
       "42                             Passage Re-ranking with   \n",
       "43                        I assess the extent to which   \n",
       "44                                                 NaN   \n",
       "45                                           Assessing   \n",
       "46   Pretrained contextual representation models (P...   \n",
       "47                                    A new release of   \n",
       "48   This paper explores the broader cross-lingual ...   \n",
       "49                                          We compare   \n",
       "50   We compare mBERT with the best - published met...   \n",
       "51   Additionally , we investigate the most effecti...   \n",
       "52   Additionally , we investigate the most effecti...   \n",
       "53   Beto , Bentz , Becas : The Surprising Cross-Li...   \n",
       "54             Language model pre - training , such as   \n",
       "55   However, pre-trained language models are usual...   \n",
       "56   To accelerate inference and reduce model size ...   \n",
       "57   By leveraging this new KD method , the plenty ...   \n",
       "58   By leveraging this new KD method , the plenty ...   \n",
       "59   Moreover , we introduce a new two - stage lear...   \n",
       "60                             Moreover , we introduce   \n",
       "61                         This framework ensures that   \n",
       "62                                                 NaN   \n",
       "63   This framework ensures that TinyBERT can captu...   \n",
       "64                                                 NaN   \n",
       "65   TinyBERT is empirically effective and achieves...   \n",
       "66                                                 NaN   \n",
       "67                                                 NaN   \n",
       "68                                                 NaN   \n",
       "69                         In this paper , we describe   \n",
       "70   Our system is the state of the art on the CNN/...   \n",
       "71   The codes to reproduce our results are availab...   \n",
       "72                                           Fine-tune   \n",
       "73   Question-answering plays an important role in ...   \n",
       "74   Inspired by the recent success of machine read...   \n",
       "75   To the best of our knowledge, no existing work...   \n",
       "76   In this work, we first build an RRC dataset ca...   \n",
       "77   Since ReviewRC has limited training examples f...   \n",
       "78   Since ReviewRC has limited training examples f...   \n",
       "79   Since ReviewRC has limited training examples f...   \n",
       "80                           To show the generality of   \n",
       "81            To show the generality of the approach ,   \n",
       "82               Experimental results demonstrate that   \n",
       "83   The datasets and code are available at this ht...   \n",
       "84                                                 NaN   \n",
       "85   Language model pre-training has proven to be u...   \n",
       "86   As a state - of - the - art language model pre...   \n",
       "87   In this paper , we conduct exhaustive experime...   \n",
       "88   In this paper , we conduct exhaustive experime...   \n",
       "89   In this paper , we conduct exhaustive experime...   \n",
       "90                                           Finally ,   \n",
       "91                                    How to Fine-Tune   \n",
       "92                                        We show that   \n",
       "93   This formulation gives way to a natural proced...   \n",
       "94                                    We generate from   \n",
       "95                 We generate from BERT and find that   \n",
       "96   Compared to the generations of a traditional l...   \n",
       "97                                                 NaN   \n",
       "98                              BERT has a Mouth , and   \n",
       "99   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "100  In this paper, we construct an auxiliary sente...   \n",
       "101            We fine-tune the pre-trained model from   \n",
       "102                                          Utilizing   \n",
       "103             Following recent successes in applying   \n",
       "104  This required confronting the challenge posed ...   \n",
       "105                          This required confronting   \n",
       "106                                         We address   \n",
       "107  Experiments on TREC microblog and newswire tes...   \n",
       "108                             Simple Applications of   \n",
       "109                                                NaN   \n",
       "110  BERT - based architectures currently give stat...   \n",
       "111  In the current work , we focus on the interpre...   \n",
       "112  Using a subset of GLUE tasks and a set of hand...   \n",
       "113  Our findings suggest that there is a limited s...   \n",
       "114  While different heads consistently use the sam...   \n",
       "115  We show that manually disabling attention in c...   \n",
       "116                      Revealing the Dark Secrets of   \n",
       "117  Bidirectional Encoder Representations from Tra...   \n",
       "118                  Recently , an upgraded version of   \n",
       "119  Recently , an upgraded version of BERT has bee...   \n",
       "120  In this technical report, we adapt whole word ...   \n",
       "121                                                NaN   \n",
       "122  We aim to provide easy extensibility and bette...   \n",
       "123                                                NaN   \n",
       "124  Experimental results on these datasets show th...   \n",
       "125  Moreover , we also examine the effectiveness o...   \n",
       "126  Moreover , we also examine the effectiveness o...   \n",
       "127  We release the pre-trained model (both TensorF...   \n",
       "128   Pre-Training with Whole Word Masking for Chinese   \n",
       "129                                                NaN   \n",
       "130                     However , previous work trains   \n",
       "131                  To tackle this issue , we propose   \n",
       "132  In addition, we find that splitting articles i...   \n",
       "133  By leveraging a passage ranker to select high-...   \n",
       "134  Experiments on four standard benchmarks showed...   \n",
       "135  In particular , on the OpenSQuAD dataset , our...   \n",
       "136         In particular , on the OpenSQuAD dataset ,   \n",
       "137                                      Multi-passage   \n",
       "138         Multi-passage BERT : A Globally Normalized   \n",
       "139  Intent classification and slot filling are two...   \n",
       "140  They often suffer from small-scale human-label...   \n",
       "141     Recently a new language representation model ,   \n",
       "142  However , there has not been much effort on ex...   \n",
       "143  In this work , we propose a joint intent class...   \n",
       "144                          In this work , we propose   \n",
       "145              Experimental results demonstrate that   \n",
       "146                                                NaN   \n",
       "147  Conversational search is an emerging topic in ...   \n",
       "148  One of the major challenges to multi-turn conv...   \n",
       "149  Existing methods either prepend history turns ...   \n",
       "150  We propose a conceptually simple yet highly ef...   \n",
       "151  It enables seamless integration of conversatio...   \n",
       "152  We first explain our view that ConvQA is a sim...   \n",
       "153  We further demonstrate the effectiveness of ou...   \n",
       "154  Finally, we analyze the impact of different nu...   \n",
       "155                                                NaN   \n",
       "156  This paper studies the performances and behavi...   \n",
       "157      We explore several different ways to leverage   \n",
       "158  We explore several different ways to leverage ...   \n",
       "159  Experimental results on MS MARCO demonstrate t...   \n",
       "160  Experimental results on MS MARCO demonstrate t...   \n",
       "161  Experimental results on TREC show the gaps bet...   \n",
       "162                            Analyses illustrate how   \n",
       "163             Analyses illustrate how BERT allocates   \n",
       "164  Analyses illustrate how BERT allocates its att...   \n",
       "165  Analyses illustrate how BERT allocates its att...   \n",
       "166                     Understanding the Behaviors of   \n",
       "167                                         We present   \n",
       "168  In recent years, state-of-the-art performance ...   \n",
       "169  In this paper , extensive experiments on datas...   \n",
       "170  To our knowledge , we are the first to success...   \n",
       "171                                                NaN   \n",
       "172                                             Simple   \n",
       "173  Multi-task learning allows the sharing of usef...   \n",
       "174  In natural language processing several recent ...   \n",
       "175  These results are based on fine-tuning on each...   \n",
       "176  We explore the multi-task learning setting for...   \n",
       "177  We explore the multi-task learning setting for...   \n",
       "178  We introduce new adaptation modules, PALs or `...   \n",
       "179                     By using PALs in parallel with   \n",
       "180  By using PALs in parallel with BERT layers , w...   \n",
       "181                                                NaN   \n",
       "182  Pre-training by language modeling has become a...   \n",
       "183  In this paper we introduce a suite of diagnost...   \n",
       "184    As a case study , we apply these diagnostics to   \n",
       "185  As a case study , we apply these diagnostics t...   \n",
       "186  As a case study , we apply these diagnostics t...   \n",
       "187  As a case study , we apply these diagnostics t...   \n",
       "188  As a case study , we apply these diagnostics t...   \n",
       "189                                               What   \n",
       "190            Language model pre - training , such as   \n",
       "191  However, it is unclear why the pre-training-th...   \n",
       "192  In this paper , we propose to visualize loss l...   \n",
       "193  First, we find that pre-training reaches a goo...   \n",
       "194  We also demonstrate that the fine - tuning pro...   \n",
       "195  Second , the visualization results indicate th...   \n",
       "196                        Third , the lower layers of   \n",
       "197  Visualizing and Understanding the Effectivenes...   \n",
       "198  Data augmentation methods are often applied to...   \n",
       "199                                                NaN   \n",
       "200                                                NaN   \n",
       "201                                         We propose   \n",
       "202                                We retrofit BERT to   \n",
       "203                                        We retrofit   \n",
       "204  We retrofit BERT to conditional BERT by introd...   \n",
       "205  In our paper, “conditional masked language mod...   \n",
       "206                                              task.   \n",
       "207                       The well trained conditional   \n",
       "208  Experiments on six various different text clas...   \n",
       "209                                        Conditional   \n",
       "210  We propose a practical scheme to train a singl...   \n",
       "211                Starting from a public multilingual   \n",
       "212  We show that our model especially outperforms ...   \n",
       "213  We showcase the effectiveness of our method by...   \n",
       "214                                Small and Practical   \n",
       "215  Recently , a simple combination of passage ret...   \n",
       "216  In this paper, we present a data augmentation ...   \n",
       "217      We apply a stage-wise approach to fine tuning   \n",
       "218  Experimental results show large gains in effec...   \n",
       "219                              Data Augmentation for   \n",
       "220                                                NaN   \n",
       "221                                     Petroni et al.   \n",
       "222                ( 2019 ) take this as evidence that   \n",
       "223                              ( 2019 ) take this as   \n",
       "224  We take issue with this interpretation and arg...   \n",
       "225                                 We take issue with   \n",
       "226                   More specifically , we show that   \n",
       "227  As a remedy , we propose E - BERT , an extensi...   \n",
       "228                           As a remedy , we propose   \n",
       "229                          E - BERT outperforms both   \n",
       "230                                                NaN   \n",
       "231  We take this as evidence that E - BERT is rich...   \n",
       "232                      We take this as evidence that   \n",
       "233  We take this as evidence that E - BERT is rich...   \n",
       "234                                                NaN   \n",
       "235  Replacing static word embeddings with contextu...   \n",
       "236  However , just how contextual are the contextu...   \n",
       "237  Are there infinitely many context-specific rep...   \n",
       "238  For one, we find that the contextualized repre...   \n",
       "239  While representations of the same word in diff...   \n",
       "240  This suggests that upper layers of contextuali...   \n",
       "241                            In all layers of ELMo ,   \n",
       "242  How Contextual are Contextualized Word Represe...   \n",
       "243                          Comparing the Geometry of   \n",
       "\n",
       "                                               split_1  \\\n",
       "0    a new language representation model called BER...   \n",
       "1                                                 BERT   \n",
       "2                                                 BERT   \n",
       "3                                                 BERT   \n",
       "4                                                   It   \n",
       "5                                                 BERT   \n",
       "6                                                  NaN   \n",
       "7                                                  NaN   \n",
       "8                                                 BERT   \n",
       "9                                                 BERT   \n",
       "10                                                  it   \n",
       "11                                                 NaN   \n",
       "12                                                 NaN   \n",
       "13                                                 NaN   \n",
       "14                                             RoBERTa   \n",
       "15                                                 NaN   \n",
       "16                                          DistilBERT   \n",
       "17                                        a BERT model   \n",
       "18                                                 its   \n",
       "19                                                 NaN   \n",
       "20                                                 NaN   \n",
       "21                                          DistilBERT   \n",
       "22                                                 NaN   \n",
       "23                               one such model , BERT   \n",
       "24                                           the model   \n",
       "25                                           the model   \n",
       "26                                                BERT   \n",
       "27    Large pre - trained neural networks such as BERT   \n",
       "28                                                they   \n",
       "29                                                BERT   \n",
       "30                                                 NaN   \n",
       "31                                                BERT   \n",
       "32                                             BERT 's   \n",
       "33                                                 NaN   \n",
       "34                                                 NaN   \n",
       "35                                             BERT 's   \n",
       "36                                                BERT   \n",
       "37                                             BERT 's   \n",
       "38                                                BERT   \n",
       "39   a simple re - implementation of BERT for query...   \n",
       "40                                          Our system   \n",
       "41                                                 NaN   \n",
       "42                                                BERT   \n",
       "43                  the recently introduced BERT model   \n",
       "44                                      The BERT model   \n",
       "45                                                BERT   \n",
       "46                                                 NaN   \n",
       "47                                                BERT   \n",
       "48                                               mBERT   \n",
       "49                                               mBERT   \n",
       "50                                               mBERT   \n",
       "51                                               mBERT   \n",
       "52                                               mBERT   \n",
       "53                                                BERT   \n",
       "54                                                BERT   \n",
       "55                                                 NaN   \n",
       "56                                                 NaN   \n",
       "57                            a small student TinyBERT   \n",
       "58                                a large teacher BERT   \n",
       "59                                            TinyBERT   \n",
       "60   a new two - stage learning framework for TinyB...   \n",
       "61                                            TinyBERT   \n",
       "62                                      This framework   \n",
       "63                                    the teacher BERT   \n",
       "64                                            TinyBERT   \n",
       "65                                                BERT   \n",
       "66                                            TinyBERT   \n",
       "67                                            TinyBERT   \n",
       "68                                                BERT   \n",
       "69                                             BERTSUM   \n",
       "70                                                 NaN   \n",
       "71                                                 NaN   \n",
       "72                                                BERT   \n",
       "73                                                 NaN   \n",
       "74                                                 NaN   \n",
       "75                                                 NaN   \n",
       "76                                                 NaN   \n",
       "77                     the popular language model BERT   \n",
       "78                                                BERT   \n",
       "79   a novel post - training approach on the popula...   \n",
       "80                                        the approach   \n",
       "81                        the proposed post - training   \n",
       "82                        the proposed post - training   \n",
       "83                                                 NaN   \n",
       "84                                                BERT   \n",
       "85                                                 NaN   \n",
       "86   BERT ( Bidirectional Encoder Representations f...   \n",
       "87                                                BERT   \n",
       "88                                                BERT   \n",
       "89           a general solution for BERT fine - tuning   \n",
       "90                               the proposed solution   \n",
       "91                                                BERT   \n",
       "92                                                BERT   \n",
       "93                                                BERT   \n",
       "94                                                BERT   \n",
       "95                                                  it   \n",
       "96                                                BERT   \n",
       "97                                                BERT   \n",
       "98                                                  It   \n",
       "99                                                 NaN   \n",
       "100                                                NaN   \n",
       "101                                               BERT   \n",
       "102                                               BERT   \n",
       "103                                               BERT   \n",
       "104                                               BERT   \n",
       "105  the challenge posed by documents that are typi...   \n",
       "106                                         this issue   \n",
       "107                                                NaN   \n",
       "108                                               BERT   \n",
       "109                         BERT - based architectures   \n",
       "110                                                its   \n",
       "111                                               BERT   \n",
       "112                                               BERT   \n",
       "113                                                NaN   \n",
       "114                                                NaN   \n",
       "115                                               BERT   \n",
       "116                                               BERT   \n",
       "117                                               BERT   \n",
       "118                                               BERT   \n",
       "119                                               BERT   \n",
       "120                                                NaN   \n",
       "121                                          The model   \n",
       "122                                       Chinese BERT   \n",
       "123                                          The model   \n",
       "124                                                NaN   \n",
       "125                                               BERT   \n",
       "126                                         BERT - wwm   \n",
       "127                                                NaN   \n",
       "128                                               BERT   \n",
       "129                                               BERT   \n",
       "130                                               BERT   \n",
       "131  a multi - passage BERT model to globally norma...   \n",
       "132                                                NaN   \n",
       "133                                               BERT   \n",
       "134                           our multi - passage BERT   \n",
       "135                                               BERT   \n",
       "136                                          our model   \n",
       "137                                               BERT   \n",
       "138                                               BERT   \n",
       "139                                                NaN   \n",
       "140                                                NaN   \n",
       "141                                               BERT   \n",
       "142                                               BERT   \n",
       "143                                               BERT   \n",
       "144  a joint intent classification and slot filling...   \n",
       "145                                 our proposed model   \n",
       "146                                               BERT   \n",
       "147                                                NaN   \n",
       "148                                                NaN   \n",
       "149                                                NaN   \n",
       "150                                                NaN   \n",
       "151                                               BERT   \n",
       "152                                                NaN   \n",
       "153                                                NaN   \n",
       "154                                                NaN   \n",
       "155                                               BERT   \n",
       "156                                               BERT   \n",
       "157                             the pre - trained BERT   \n",
       "158                                                 it   \n",
       "159                                               BERT   \n",
       "160                                               BERT   \n",
       "161                                               BERT   \n",
       "162                                               BERT   \n",
       "163                                                its   \n",
       "164                                                its   \n",
       "165                                                 it   \n",
       "166                                               BERT   \n",
       "167  simple BERT - based models for relation extrac...   \n",
       "168                                                NaN   \n",
       "169                                               BERT   \n",
       "170                                               BERT   \n",
       "171                                         Our models   \n",
       "172                                               BERT   \n",
       "173                                                NaN   \n",
       "174                                                NaN   \n",
       "175                                                NaN   \n",
       "176                                               BERT   \n",
       "177                                               BERT   \n",
       "178                                                NaN   \n",
       "179                                               BERT   \n",
       "180                                               BERT   \n",
       "181                                               BERT   \n",
       "182                                                NaN   \n",
       "183                                                NaN   \n",
       "184                             the popular BERT model   \n",
       "185                                                 it   \n",
       "186                                                 it   \n",
       "187                                                 it   \n",
       "188                                                 it   \n",
       "189                                               BERT   \n",
       "190                                               BERT   \n",
       "191                                                NaN   \n",
       "192                                               BERT   \n",
       "193                                                NaN   \n",
       "194                                               BERT   \n",
       "195                                               BERT   \n",
       "196                                               BERT   \n",
       "197                                               BERT   \n",
       "198                                                NaN   \n",
       "199          Recently proposed contextual augmentation   \n",
       "200  Bidirectional Encoder Representations from Tra...   \n",
       "201  a novel data augmentation method for labeled s...   \n",
       "202                                   conditional BERT   \n",
       "203                                               BERT   \n",
       "204                                               BERT   \n",
       "205                                                NaN   \n",
       "206                                                NaN   \n",
       "207                                               BERT   \n",
       "208                                         our method   \n",
       "209                                               BERT   \n",
       "210                                                NaN   \n",
       "211                                               BERT   \n",
       "212                                                NaN   \n",
       "213                                                NaN   \n",
       "214                                               BERT   \n",
       "215                                               BERT   \n",
       "216                                                NaN   \n",
       "217                                               BERT   \n",
       "218                                                NaN   \n",
       "219                                               BERT   \n",
       "220                     The BERT language model ( LM )   \n",
       "221                                                NaN   \n",
       "222                                               BERT   \n",
       "223  evidence that BERT memorizes factual knowledge...   \n",
       "224                                               BERT   \n",
       "225                                this interpretation   \n",
       "226                                            BERT 's   \n",
       "227                                               BERT   \n",
       "228  E - BERT , an extension of BERT that replaces ...   \n",
       "229                                               BERT   \n",
       "230                                           E - BERT   \n",
       "231                                               BERT   \n",
       "232                                           E - BERT   \n",
       "233                                           E - BERT   \n",
       "234                                               BERT   \n",
       "235                                                NaN   \n",
       "236                                               BERT   \n",
       "237                                                NaN   \n",
       "238                                                NaN   \n",
       "239                                                NaN   \n",
       "240                                                NaN   \n",
       "241                                               BERT   \n",
       "242                                                NaN   \n",
       "243                                               BERT   \n",
       "\n",
       "                                               split_2               averb  \\\n",
       "0                                                    .              stands   \n",
       "1    is designed to pre - train deep bidirectional ...         is designed   \n",
       "2    model can be fine - tuned with just one additi...                 NaN   \n",
       "3    is conceptually simple and empirically powerful .                  is   \n",
       "4    obtains new state - of - the - art results on ...             obtains   \n",
       "5    : Pre-training of Deep Bidirectional Transform...                 NaN   \n",
       "6                                                  NaN                 NaN   \n",
       "7                                                  NaN                 NaN   \n",
       "8    pretraining ( Devlin et al . , 2019 ) that car...             present   \n",
       "9    was significantly undertrained , and can match...                find   \n",
       "10                                                   .           published   \n",
       "11                                                 NaN                 NaN   \n",
       "12                                                 NaN                 NaN   \n",
       "13                                                 NaN                 NaN   \n",
       "14   : A Robustly Optimized BERT Pretraining Approa...                 NaN   \n",
       "15                                                 NaN                 NaN   \n",
       "16   , which can then be fine-tuned with good perfo...              called   \n",
       "17   by 40 % , while retaining 97 % of its language...           to reduce   \n",
       "18   language understanding capabilities and being ...           retaining   \n",
       "19                                                 NaN                 NaN   \n",
       "20                                                 NaN                 NaN   \n",
       "21   , a distilled version of BERT : smaller , fast...                 NaN   \n",
       "22                                                 NaN                 NaN   \n",
       "23   , and aim to quantify where linguistic informa...               focus   \n",
       "24   represents the steps of the traditional NLP pi...          represents   \n",
       "25   can and often does adjust this pipeline dynami...         does adjust   \n",
       "26            Rediscovers the Classical NLP Pipeline .                 NaN   \n",
       "27   have had great recent success in NLP , motivat...            have had   \n",
       "28             are able to learn from unlabeled data .          motivating   \n",
       "29   have had great recent success in NLP , motivat...            have had   \n",
       "30                                                 NaN                 NaN   \n",
       "31                                                   .               apply   \n",
       "32   attention heads exhibit patterns such as atten...                 NaN   \n",
       "33                                                 NaN                 NaN   \n",
       "34                                                 NaN                 NaN   \n",
       "35                                         attention .         is captured   \n",
       "36                                           Look At ?                 NaN   \n",
       "37                                         Attention .                 NaN   \n",
       "38   ( Devlin et al. , 2018 ) , have achieved impre...                 NaN   \n",
       "39                                                   .            describe   \n",
       "40   is the state of the art on the TREC - CAR data...                  is   \n",
       "41                                                 NaN                 NaN   \n",
       "42                                                   .                 NaN   \n",
       "43   captures English syntactic phenomena , using (...              assess   \n",
       "44             performs remarkably well on all cases .            performs   \n",
       "45                            's Syntactic Abilities .                 NaN   \n",
       "46                                                 NaN                 NaN   \n",
       "47   ( Devlin , 2018 ) includes a model simultaneou...            includes   \n",
       "48   ( multilingual ) as a zero shot language trans...            explores   \n",
       "49   with the best - published methods for zero - s...             compare   \n",
       "50                          competitive on each task .                find   \n",
       "51   in this manner , determine to what extent mBER...         investigate   \n",
       "52   generalizes away from language specific featur...         generalizes   \n",
       "53                                                   .                 NaN   \n",
       "54   , has significantly improved the performances ...                 pre   \n",
       "55                                                 NaN                 NaN   \n",
       "56                                                 NaN                 NaN   \n",
       "57                                                   .  can be transferred   \n",
       "58   can be well transferred to a small student Tin...             encoded   \n",
       "59   , which performs transformer distillation at b...           introduce   \n",
       "60                                                   .           introduce   \n",
       "61   can capture both the general - domain and task...         can capture   \n",
       "62   ensures that TinyBERT can capture both the gen...             ensures   \n",
       "63                                                   .         can capture   \n",
       "64   is empirically effective and achieves comparab...                  is   \n",
       "65   in GLUE datasets , while being 7.5x smaller an...                 NaN   \n",
       "66   is also significantly better than state - of -...                  is   \n",
       "67   : Distilling BERT for Natural Language Underst...                 NaN   \n",
       "68   , a pre-trained Transformer model , has achiev...        has achieved   \n",
       "69   , a simple variant of BERT , for extractive su...            describe   \n",
       "70                                                 NaN                 NaN   \n",
       "71                                                 NaN                 NaN   \n",
       "72                      for Extractive Summarization .                 NaN   \n",
       "73                                                 NaN                 NaN   \n",
       "74                                                 NaN                 NaN   \n",
       "75                                                 NaN                 NaN   \n",
       "76                                                 NaN                 NaN   \n",
       "77   to enhance the performance of fine - tuning of...             explore   \n",
       "78                                           for RRC .             enhance   \n",
       "79   to enhance the performance of fine - tuning of...             explore   \n",
       "80   , the proposed post - training is also applied...             To show   \n",
       "81   is also applied to some other review - based t...             To show   \n",
       "82                               is highly effective .         demonstrate   \n",
       "83                                                 NaN                 NaN   \n",
       "84   Post-Training for Review Reading Comprehension...                 NaN   \n",
       "85                                                 NaN                 NaN   \n",
       "86   has achieved amazing results in many language ...            achieved   \n",
       "87   on text classification task and provide a gene...      to investigate   \n",
       "88                                     fine - tuning .             provide   \n",
       "89                                                   .             provide   \n",
       "90   obtains new state - of - the - art results on ...                 NaN   \n",
       "91                         for Text Classification ? .                 NaN   \n",
       "92   ( Devlin et al . , 2018 ) is a Markov random f...                show   \n",
       "93                                                   .               gives   \n",
       "94   and find that it can produce high - quality , ...            generate   \n",
       "95   can produce high - quality , fluent generations .             produce   \n",
       "96   generates sentences that are more diverse but ...            Compared   \n",
       "97   has a Mouth , and It Must Speak : BERT as a Ma...                 has   \n",
       "98   Must Speak : BERT as a Markov Random Field Lan...                Must   \n",
       "99                                                 NaN                 NaN   \n",
       "100                                                NaN                 NaN   \n",
       "101  and achieve new state-of-the-art results on Se...                 NaN   \n",
       "102  for Aspect-Based Sentiment Analysis via Constr...                 NaN   \n",
       "103  to question answering , we explore simple appl...            applying   \n",
       "104                           was designed to handle .               posed   \n",
       "105                                                  .         confronting   \n",
       "106  by applying inference on sentences individuall...             address   \n",
       "107                                                NaN                 NaN   \n",
       "108                    for Ad Hoc Document Retrieval .                 NaN   \n",
       "109  currently give state - of - the - art performa...                 NaN   \n",
       "110                                          success .          contribute   \n",
       "111                                                  .               focus   \n",
       "112                                         's heads .             encoded   \n",
       "113                                                NaN                 NaN   \n",
       "114                                                NaN                 NaN   \n",
       "115                                             models               leads   \n",
       "116                                                  .           Revealing   \n",
       "117  ) has shown marvelous improvements across vari...                 NaN   \n",
       "118  has been released with Whole Word Masking ( WW...   has been released   \n",
       "119                                                  .             masking   \n",
       "120                                                NaN                 NaN   \n",
       "121  was trained on the latest Chinese Wikipedia du...         was trained   \n",
       "122  without changing any neural architecture or ev...          to provide   \n",
       "123  is verified on various NLP tasks , across sent...         is verified   \n",
       "124                                                NaN                 NaN   \n",
       "125                             , ERNIE , BERT - wwm .             examine   \n",
       "126                                                  .             examine   \n",
       "127                                                NaN                 NaN   \n",
       "128                                                  .                 NaN   \n",
       "129  model has been successfully applied to open - ...                 NaN   \n",
       "130  by viewing passages corresponding to the same ...                BERT   \n",
       "131  , and this change enables our QA model find be...             propose   \n",
       "132                                                NaN                 NaN   \n",
       "133                             gains additional 2 % .               gains   \n",
       "134  outperforms all state - of - the - art models ...              showed   \n",
       "135  models , and 5.8 % EM and 6.5 % $ F_1 $ over B...                 NaN   \n",
       "136  gains 21.4 % EM and 21.5 % $ F_1 $ over all no...                 NaN   \n",
       "137  : A Globally Normalized BERT Model for Open-do...                 NaN   \n",
       "138         Model for Open-domain Question Answering .                 NaN   \n",
       "139                                                NaN                 NaN   \n",
       "140                                                NaN                 NaN   \n",
       "141  ( Bidirectional Encoder Representations from T...                 NaN   \n",
       "142               for natural language understanding .           exploring   \n",
       "143                                                  .               based   \n",
       "144                                                  .               based   \n",
       "145  achieves significant improvement on intent cla...            achieves   \n",
       "146  for Joint Intent Classification and Slot Filli...                 NaN   \n",
       "147                                                NaN                 NaN   \n",
       "148                                                NaN                 NaN   \n",
       "149                                                NaN                 NaN   \n",
       "150                                                NaN                 NaN   \n",
       "151  ( Bidirectional Encoder Representations from T...               built   \n",
       "152                                                NaN                 NaN   \n",
       "153                                                NaN                 NaN   \n",
       "154                                                NaN                 NaN   \n",
       "155  with History Answer Embedding for Conversation...                BERT   \n",
       "156                                 in ranking tasks .             studies   \n",
       "157  and fine - tune it on two ranking tasks : MS M...         to leverage   \n",
       "158  on two ranking tasks : MS MARCO passage rerank...         to leverage   \n",
       "159  in question - answering focused passage rankin...         demonstrate   \n",
       "160  is a strong interaction - based seq2seq matchi...           answering   \n",
       "161  pre-trained on surrounding contexts and the ne...                 pre   \n",
       "162  allocates its attentions between query - docum...           allocates   \n",
       "163  attentions between query - document tokens in ...           allocates   \n",
       "164  Transformer layers , how it prefers semantic m...           allocates   \n",
       "165  prefers semantic matches between paraphrase to...             prefers   \n",
       "166                                       in Ranking .       Understanding   \n",
       "167                                                  .                 NaN   \n",
       "168                                                NaN                 NaN   \n",
       "169  -based model can achieve state-of-the-art perf...             achieve   \n",
       "170                                   in this manner .            to apply   \n",
       "171       provide strong baselines for future research             provide   \n",
       "172  Models for Relation Extraction and Semantic Ro...                 NaN   \n",
       "173                                                NaN                 NaN   \n",
       "174                                                NaN                 NaN   \n",
       "175                                                NaN                 NaN   \n",
       "176  model on the GLUE benchmark , and how to best ...             explore   \n",
       "177  network , with a high degree of parameter shar...                 add   \n",
       "178                                                NaN                 NaN   \n",
       "179  layers , we match the performance of fine-tune...               using   \n",
       "180  on the GLUE benchmark with roughly 7 times few...               tuned   \n",
       "181  and PALs : Projected Attention Layers for Effi...                 NaN   \n",
       "182                                                NaN                 NaN   \n",
       "183                                                NaN                 NaN   \n",
       "184  , finding that it can generally distinguish go...               apply   \n",
       "185  can generally distinguish good from bad comple...     can distinguish   \n",
       "186  robustly retrieves noun hypernyms , but it str...               shows   \n",
       "187  struggles with challenging inference and role ...           struggles   \n",
       "188  shows clear insensitivity to the contextual im...               shows   \n",
       "189  Is Not : Lessons from a New Suite of Psycholin...                  Is   \n",
       "190  , has achieved remarkable results in many NLP ...                 pre   \n",
       "191                                                NaN                 NaN   \n",
       "192                             on specific datasets .              tuning   \n",
       "193                                                NaN                 NaN   \n",
       "194  is highly over - parameterized for downstream ...         demonstrate   \n",
       "195  tends to generalize better because of the flat...              tuning   \n",
       "196  are more invariant during fine - tuning , whic...                 NaN   \n",
       "197                                                  .       Understanding   \n",
       "198                                                NaN                 NaN   \n",
       "199  augments labeled sentences by randomly replaci...                 NaN   \n",
       "200  demonstrates that a deep bidirectional languag...                 NaN   \n",
       "201                                                  .             propose   \n",
       "202  by introducing a new conditional masked langua...                 NaN   \n",
       "203  to conditional BERT by introducing a new condi...                 NaN   \n",
       "204  paper , which indicates context - conditional ...            appeared   \n",
       "205                                                NaN                 NaN   \n",
       "206                                                NaN                 NaN   \n",
       "207  can be applied to enhance contextual augmentat...                 can   \n",
       "208  can be easily applied to both convolutional or...                show   \n",
       "209                          Contextual Augmentation .                 NaN   \n",
       "210                                                NaN                 NaN   \n",
       "211  checkpoint , our final model is 6x smaller and...            Starting   \n",
       "212                                                NaN                 NaN   \n",
       "213                                                NaN                 NaN   \n",
       "214                     Models for Sequence Labeling .                 NaN   \n",
       "215  reader was found to be very effective for ques...               using   \n",
       "216                                                NaN                 NaN   \n",
       "217  on multiple datasets , starting with data that...                 NaN   \n",
       "218                                                NaN                 NaN   \n",
       "219    Fine-Tuning in Open-Domain Question Answering .                 NaN   \n",
       "220  ( Devlin et al . , 2019 ) is surprisingly good...                 NaN   \n",
       "221                                                NaN                 NaN   \n",
       "222  memorizes factual knowledge during pre - train...                 NaN   \n",
       "223                                                  .                 NaN   \n",
       "224  is partly due to reasoning about ( the surface...               argue   \n",
       "225  and argue that the performance of BERT is part...                 NaN   \n",
       "226  precision drops dramatically when we filter ce...               drops   \n",
       "227  that replaces entity mentions with symbolic en...             propose   \n",
       "228                                                  .             propose   \n",
       "229  and ERNIE ( Zhang et al . , 2019 ) on hard - t...                 NaN   \n",
       "230  outperforms both BERT and ERNIE ( Zhang et al ...                 NaN   \n",
       "231                                       and E - BERT                show   \n",
       "232  is richer in factual knowledge , and we show t...                take   \n",
       "233                                                NaN                show   \n",
       "234  is Not a Knowledge Base ( Yet ) : Factual Know...                  is   \n",
       "235                                                NaN                 NaN   \n",
       "236                                                  ?            produced   \n",
       "237                                                NaN                 NaN   \n",
       "238                                                NaN                 NaN   \n",
       "239                                                NaN                 NaN   \n",
       "240                                                NaN                 NaN   \n",
       "241  , and GPT-2 , on average , less than 5 % of th...        be explained   \n",
       "242                                                NaN                 NaN   \n",
       "243                    , ELMo , and GPT-2 Embeddings .           Comparing   \n",
       "\n",
       "                                               averb_s  \\\n",
       "0    ['which', 'We introduce a new language represe...   \n",
       "1     ['recent language representation models , BERT']   \n",
       "2                                                  NaN   \n",
       "3                                             ['BERT']   \n",
       "4                                                   []   \n",
       "5                                                  NaN   \n",
       "6                                                  NaN   \n",
       "7                                                  NaN   \n",
       "8                                               ['We']   \n",
       "9                                                   []   \n",
       "10                                                  []   \n",
       "11                                                 NaN   \n",
       "12                                                 NaN   \n",
       "13                                                 NaN   \n",
       "14                                                 NaN   \n",
       "15                                                 NaN   \n",
       "16                                                  []   \n",
       "17                                                  []   \n",
       "18                                                  []   \n",
       "19                                                 NaN   \n",
       "20                                                 NaN   \n",
       "21                                                 NaN   \n",
       "22                                                 NaN   \n",
       "23                                              ['We']   \n",
       "24                                       ['the model']   \n",
       "25                         ['the model can and often']   \n",
       "26                                                 NaN   \n",
       "27   ['Large pre - trained neural networks such as ...   \n",
       "28                                               [',']   \n",
       "29   ['Large pre - trained neural networks such as ...   \n",
       "30                                                 NaN   \n",
       "31                                                  []   \n",
       "32                                                 NaN   \n",
       "33                                                 NaN   \n",
       "34                                                 NaN   \n",
       "35               ['substantial syntactic information']   \n",
       "36                                                 NaN   \n",
       "37                                                 NaN   \n",
       "38                                                 NaN   \n",
       "39                                         [',', 'we']   \n",
       "40                                      ['Our system']   \n",
       "41                                                 NaN   \n",
       "42                                                 NaN   \n",
       "43                                               ['I']   \n",
       "44                                  ['The BERT model']   \n",
       "45                                                 NaN   \n",
       "46                                                 NaN   \n",
       "47        ['new release of BERT ( Devlin , 2018', ')']   \n",
       "48                                   ['This', 'paper']   \n",
       "49                                              ['We']   \n",
       "50                              ['- lingual transfer']   \n",
       "51                                    ['Additionally']   \n",
       "52                               ['what extent mBERT']   \n",
       "53                                                 NaN   \n",
       "54                                  ['Language model']   \n",
       "55                                                 NaN   \n",
       "56                                                 NaN   \n",
       "57   ['the plenty of knowledge encoded in a large t...   \n",
       "58                                                  []   \n",
       "59                                         ['we', ',']   \n",
       "60                                         ['we', ',']   \n",
       "61                                        ['TinyBERT']   \n",
       "62                               ['This', 'framework']   \n",
       "63                                        ['TinyBERT']   \n",
       "64                                        ['TinyBERT']   \n",
       "65                                                 NaN   \n",
       "66                                        ['TinyBERT']   \n",
       "67                                                 NaN   \n",
       "68      ['BERT , a pre - trained Transformer model ,']   \n",
       "69                                              ['we']   \n",
       "70                                                 NaN   \n",
       "71                                                 NaN   \n",
       "72                                                 NaN   \n",
       "73                                                 NaN   \n",
       "74                                                 NaN   \n",
       "75                                                 NaN   \n",
       "76                                                 NaN   \n",
       "77                                                  []   \n",
       "78                                              ['to']   \n",
       "79                                                  []   \n",
       "80                                                  []   \n",
       "81                                                  []   \n",
       "82                                                  []   \n",
       "83                                                 NaN   \n",
       "84                                                 NaN   \n",
       "85                                                 NaN   \n",
       "86   ['BERT ( Bidirectional Encoder Representations...   \n",
       "87                                                  []   \n",
       "88                                                  []   \n",
       "89                                                  []   \n",
       "90                                                 NaN   \n",
       "91                                                 NaN   \n",
       "92                                              ['We']   \n",
       "93                             ['This', 'formulation']   \n",
       "94                                              ['We']   \n",
       "95                                       ['it', 'can']   \n",
       "96                                                  []   \n",
       "97                                            ['BERT']   \n",
       "98                                              ['It']   \n",
       "99                                                 NaN   \n",
       "100                                                NaN   \n",
       "101                                                NaN   \n",
       "102                                                NaN   \n",
       "103                                                 []   \n",
       "104                                                 []   \n",
       "105                                                 []   \n",
       "106                                                 []   \n",
       "107                                                NaN   \n",
       "108                                                NaN   \n",
       "109                                                NaN   \n",
       "110                                           ['that']   \n",
       "111                                        [',', 'we']   \n",
       "112                                                 []   \n",
       "113                                                NaN   \n",
       "114                                                NaN   \n",
       "115  ['manually disabling attention in certain heads']   \n",
       "116                                                 []   \n",
       "117                                                NaN   \n",
       "118      ['an upgraded version of BERT', 'Recently ,']   \n",
       "119                                                 []   \n",
       "120                                                NaN   \n",
       "121                                      ['The model']   \n",
       "122                                                 []   \n",
       "123                                      ['The model']   \n",
       "124                                                NaN   \n",
       "125                                        [',', 'we']   \n",
       "126                                        [',', 'we']   \n",
       "127                                                NaN   \n",
       "128                                                NaN   \n",
       "129                                                NaN   \n",
       "130                           ['previous work trains']   \n",
       "131                  ['To', 'we', 'tackle this issue']   \n",
       "132                                                NaN   \n",
       "133                                         ['BERT .']   \n",
       "134        ['Experiments on four standard benchmarks']   \n",
       "135                                                NaN   \n",
       "136                                                NaN   \n",
       "137                                                NaN   \n",
       "138                                                NaN   \n",
       "139                                                NaN   \n",
       "140                                                NaN   \n",
       "141                                                NaN   \n",
       "142                                                 []   \n",
       "143  ['a joint intent classification and slot filli...   \n",
       "144  ['a joint intent classification and slot filli...   \n",
       "145                             ['our proposed model']   \n",
       "146                                                NaN   \n",
       "147                                                NaN   \n",
       "148                                                NaN   \n",
       "149                                                NaN   \n",
       "150                                                NaN   \n",
       "151                                                 []   \n",
       "152                                                NaN   \n",
       "153                                                NaN   \n",
       "154                                                NaN   \n",
       "155                                                 []   \n",
       "156                                           ['This']   \n",
       "157                                                 []   \n",
       "158                                                 []   \n",
       "159  ['Experimental results on MS MARCO', 'the stro...   \n",
       "160                                                 []   \n",
       "161                                    ['the', 'BERT']   \n",
       "162                                           ['BERT']   \n",
       "163                                           ['BERT']   \n",
       "164                                           ['BERT']   \n",
       "165                                             ['it']   \n",
       "166                                                 []   \n",
       "167                                                NaN   \n",
       "168                                                NaN   \n",
       "169                                            ['can']   \n",
       "170                                                 []   \n",
       "171                                     ['Our models']   \n",
       "172                                                NaN   \n",
       "173                                                NaN   \n",
       "174                                                NaN   \n",
       "175                                                NaN   \n",
       "176                                             ['We']   \n",
       "177                                ['and how to best']   \n",
       "178                                                NaN   \n",
       "179                                                 []   \n",
       "180                                              ['-']   \n",
       "181                                                NaN   \n",
       "182                                                NaN   \n",
       "183                                                NaN   \n",
       "184  ['we', ',', 'finding that it can generally dis...   \n",
       "185                                             ['it']   \n",
       "186  ['it', 'As a case study , we apply these diagn...   \n",
       "187                                                 []   \n",
       "188  ['it', 'As a case study , we apply these diagn...   \n",
       "189                                   ['What', 'BERT']   \n",
       "190                                          ['model']   \n",
       "191                                                NaN   \n",
       "192                                              ['-']   \n",
       "193                                                NaN   \n",
       "194                                             ['We']   \n",
       "195                                              ['-']   \n",
       "196                                                NaN   \n",
       "197                                                 []   \n",
       "198                                                NaN   \n",
       "199                                                NaN   \n",
       "200                                                NaN   \n",
       "201                                             ['We']   \n",
       "202                                                NaN   \n",
       "203                                                NaN   \n",
       "204                                                 []   \n",
       "205                                                NaN   \n",
       "206                                                NaN   \n",
       "207              ['The well trained conditional BERT']   \n",
       "208                                                 []   \n",
       "209                                                NaN   \n",
       "210                                                NaN   \n",
       "211                                                 []   \n",
       "212                                                NaN   \n",
       "213                                                NaN   \n",
       "214                                                NaN   \n",
       "215                                                 []   \n",
       "216                                                NaN   \n",
       "217                                                NaN   \n",
       "218                                                NaN   \n",
       "219                                                NaN   \n",
       "220                                                NaN   \n",
       "221                                                NaN   \n",
       "222                                                NaN   \n",
       "223                                                NaN   \n",
       "224                                                 []   \n",
       "225                                                NaN   \n",
       "226                              [\"BERT 's precision\"]   \n",
       "227                              ['we', 'As a remedy']   \n",
       "228                              ['we', 'As a remedy']   \n",
       "229                                                NaN   \n",
       "230                                                NaN   \n",
       "231                                             ['we']   \n",
       "232                                             ['We']   \n",
       "233                                             ['we']   \n",
       "234                                           ['BERT']   \n",
       "235                                                NaN   \n",
       "236                                                 []   \n",
       "237                                                NaN   \n",
       "238                                                NaN   \n",
       "239                                                NaN   \n",
       "240                                                NaN   \n",
       "241                                            ['can']   \n",
       "242                                                NaN   \n",
       "243                                                 []   \n",
       "\n",
       "                                               averb_o  averb_relation  \\\n",
       "0                                                   []             0.0   \n",
       "1    ['pre - train deep bidirectional representatio...             1.0   \n",
       "2                                                  NaN             NaN   \n",
       "3                                                   []             1.0   \n",
       "4    ['new state - - the -', 'of art', 'results on ...             1.0   \n",
       "5                                                  NaN             NaN   \n",
       "6                                                  NaN             NaN   \n",
       "7                                                  NaN             NaN   \n",
       "8    ['a replication study of BERT pretraining ( De...            -1.0   \n",
       "9    ['that BERT was significantly undertrained , a...            -1.0   \n",
       "10                                                  []            -1.0   \n",
       "11                                                 NaN             NaN   \n",
       "12                                                 NaN             NaN   \n",
       "13                                                 NaN             NaN   \n",
       "14                                                 NaN             NaN   \n",
       "15                                                 NaN             NaN   \n",
       "16                                 ['DistilBERT', ',']            -1.0   \n",
       "17   ['we leverage knowledge distillation during ph...            -1.0   \n",
       "18   ['97 % of its language understanding capabilit...            -1.0   \n",
       "19                                                 NaN             NaN   \n",
       "20                                                 NaN             NaN   \n",
       "21                                                 NaN             NaN   \n",
       "22                                                 NaN             NaN   \n",
       "23                                                  []            -1.0   \n",
       "24   ['the steps of the traditional NLP pipeline in...             1.0   \n",
       "25   ['this pipeline dynamically , revising lower -...             1.0   \n",
       "26                                                 NaN             NaN   \n",
       "27                                                  []             1.0   \n",
       "28   ['a growing body of research investigating what']            -1.0   \n",
       "29                                                  []             1.0   \n",
       "30                                                 NaN             NaN   \n",
       "31                                            ['them']            -1.0   \n",
       "32                                                 NaN             NaN   \n",
       "33                                                 NaN             NaN   \n",
       "34                                                 NaN             NaN   \n",
       "35                                                  []            -1.0   \n",
       "36                                                 NaN             NaN   \n",
       "37                                                 NaN             NaN   \n",
       "38                                                 NaN             NaN   \n",
       "39   ['a simple re - implementation of BERT for que...            -1.0   \n",
       "40                                                  []             1.0   \n",
       "41                                                 NaN             NaN   \n",
       "42                                                 NaN             NaN   \n",
       "43   ['the extent the recently introduced BERT mode...            -1.0   \n",
       "44                          ['well on all cases', '.']             1.0   \n",
       "45                                                 NaN             NaN   \n",
       "46                                                 NaN             NaN   \n",
       "47   ['a model simultaneously pretrained on 104 lan...             1.0   \n",
       "48   ['the broader cross - lingual potential of mBE...            -1.0   \n",
       "49   ['mBERT with the best - published methods for ...            -1.0   \n",
       "50   ['We compare mBERT with the best - published m...            -1.0   \n",
       "51   ['the most effective strategy for utilizing mB...            -1.0   \n",
       "52   ['away from language specific features , and m...             1.0   \n",
       "53                                                 NaN             NaN   \n",
       "54                  ['-', 'training , such as BERT ,']            -1.0   \n",
       "55                                                 NaN             NaN   \n",
       "56                                                 NaN             NaN   \n",
       "57                                                  []            -1.0   \n",
       "58                                                  []            -1.0   \n",
       "59   ['a new two - stage learning framework for Tin...            -1.0   \n",
       "60   ['a new two - stage learning framework for Tin...             0.0   \n",
       "61   ['both the general - domain and', 'task - spec...             1.0   \n",
       "62                                                  []             0.0   \n",
       "63   ['both the general - domain and', 'task - spec...            -1.0   \n",
       "64                                                  []             1.0   \n",
       "65                                                 NaN             NaN   \n",
       "66   ['also significantly better than state - of - ...             1.0   \n",
       "67                                                 NaN             NaN   \n",
       "68                                                  []             1.0   \n",
       "69            ['BERTSUM , a simple variant of BERT ,']            -1.0   \n",
       "70                                                 NaN             NaN   \n",
       "71                                                 NaN             NaN   \n",
       "72                                                 NaN             NaN   \n",
       "73                                                 NaN             NaN   \n",
       "74                                                 NaN             NaN   \n",
       "75                                                 NaN             NaN   \n",
       "76                                                 NaN             NaN   \n",
       "77   ['a novel post - training approach on the popu...            -1.0   \n",
       "78   ['the performance of fine - tuning of BERT for...            -1.0   \n",
       "79   ['a novel post - training approach on the popu...            -1.0   \n",
       "80   ['the generality of the approach , the propose...            -1.0   \n",
       "81   ['the generality of the approach , the propose...            -1.0   \n",
       "82                                                  []            -1.0   \n",
       "83                                                 NaN             NaN   \n",
       "84                                                 NaN             NaN   \n",
       "85                                                 NaN             NaN   \n",
       "86   ['amazing results in many language understandi...             1.0   \n",
       "87   ['different fine - tuning methods of BERT on t...            -1.0   \n",
       "88       ['a general solution for BERT fine - tuning']            -1.0   \n",
       "89       ['a general solution for BERT fine - tuning']            -1.0   \n",
       "90                                                 NaN             NaN   \n",
       "91                                                 NaN             NaN   \n",
       "92   ['that BERT ( Devlin et al . , 2018 )', 'is a ...            -1.0   \n",
       "93   ['way', 'to a natural procedure to sample sent...            -1.0   \n",
       "94   ['find that it can produce high - quality , fl...            -1.0   \n",
       "95        ['high - quality', ', fluent generations .']             1.0   \n",
       "96                                                  []            -1.0   \n",
       "97   ['a Mouth , and It Must Speak : BERT as a Mark...             1.0   \n",
       "98                                                  []             1.0   \n",
       "99                                                 NaN             NaN   \n",
       "100                                                NaN             NaN   \n",
       "101                                                NaN             NaN   \n",
       "102                                                NaN             NaN   \n",
       "103                                           ['BERT']            -1.0   \n",
       "104                                                 []            -1.0   \n",
       "105  ['the challenge posed by documents that are ty...            -1.0   \n",
       "106                                                 []            -1.0   \n",
       "107                                                NaN             NaN   \n",
       "108                                                NaN             NaN   \n",
       "109                                                NaN             NaN   \n",
       "110                                          ['its .']            -1.0   \n",
       "111                                                 []            -1.0   \n",
       "112                                                 []            -1.0   \n",
       "113                                                NaN             NaN   \n",
       "114                                                NaN             NaN   \n",
       "115                                                 []            -1.0   \n",
       "116                       ['the Dark Secrets of BERT']            -1.0   \n",
       "117                                                NaN             NaN   \n",
       "118  ['which mitigate the drawbacks of masking part...             1.0   \n",
       "119                                                 []            -1.0   \n",
       "120                                                NaN             NaN   \n",
       "121                                                 []             1.0   \n",
       "122  ['easy', 'extensibility and', 'better performa...            -1.0   \n",
       "123                                                 []             1.0   \n",
       "124                                                NaN             NaN   \n",
       "125  ['the effectiveness of Chinese pre - trained m...            -1.0   \n",
       "126  ['the effectiveness of Chinese pre - trained m...            -1.0   \n",
       "127                                                NaN             NaN   \n",
       "128                                                NaN             NaN   \n",
       "129                                                NaN             NaN   \n",
       "130                                                 []             0.0   \n",
       "131  ['a multi - passage BERT model to globally nor...            -1.0   \n",
       "132                                                NaN             NaN   \n",
       "133                                 ['additional 2 %']             1.0   \n",
       "134  ['that our multi - passage BERT outperforms al...            -1.0   \n",
       "135                                                NaN             NaN   \n",
       "136                                                NaN             NaN   \n",
       "137                                                NaN             NaN   \n",
       "138                                                NaN             NaN   \n",
       "139                                                NaN             NaN   \n",
       "140                                                NaN             NaN   \n",
       "141                                                NaN             NaN   \n",
       "142                                           ['BERT']            -1.0   \n",
       "143                                                 []            -1.0   \n",
       "144                                                 []             0.0   \n",
       "145                        ['significant improvement']             1.0   \n",
       "146                                                NaN             NaN   \n",
       "147                                                NaN             NaN   \n",
       "148                                                NaN             NaN   \n",
       "149                                                NaN             NaN   \n",
       "150                                                NaN             NaN   \n",
       "151                                                 []            -1.0   \n",
       "152                                                NaN             NaN   \n",
       "153                                                NaN             NaN   \n",
       "154                                                NaN             NaN   \n",
       "155                                                 []             0.0   \n",
       "156  ['the performances and behaviors of BERT in ra...            -1.0   \n",
       "157  ['the pre - trained BERT and', 'fine - tune it...            -1.0   \n",
       "158  ['the pre - trained BERT and', 'fine - tune it...            -1.0   \n",
       "159                                                 []            -1.0   \n",
       "160                  ['focused passage ranking tasks']            -1.0   \n",
       "161                                                 []             1.0   \n",
       "162  ['its attentions between query - document toke...             1.0   \n",
       "163  ['its attentions between query - document toke...            -1.0   \n",
       "164  ['its attentions between query - document toke...            -1.0   \n",
       "165  ['semantic', 'how that differs with the soft m...             1.0   \n",
       "166                          ['the Behaviors of BERT']            -1.0   \n",
       "167                                                NaN             NaN   \n",
       "168                                                NaN             NaN   \n",
       "169  ['any external features , a simple BERT -based...             1.0   \n",
       "170                                           ['BERT']            -1.0   \n",
       "171                               ['strong baselines']             1.0   \n",
       "172                                                NaN             NaN   \n",
       "173                                                NaN             NaN   \n",
       "174                                                NaN             NaN   \n",
       "175                                                NaN             NaN   \n",
       "176  ['the multi - task learning setting for the re...            -1.0   \n",
       "177  ['We explore the multi - task learning setting...            -1.0   \n",
       "178                                                NaN             NaN   \n",
       "179                                           ['PALs']            -1.0   \n",
       "180  ['BERT on the GLUE benchmark with roughly 7 ti...            -1.0   \n",
       "181                                                NaN             NaN   \n",
       "182                                                NaN             NaN   \n",
       "183                                                NaN             NaN   \n",
       "184                              ['these diagnostics']            -1.0   \n",
       "185                                                 []             1.0   \n",
       "186                                                 []             1.0   \n",
       "187                                                 []             1.0   \n",
       "188                                                 []             1.0   \n",
       "189  ['Not', ': Lessons from a New Suite of Psychol...             1.0   \n",
       "190                 ['-', 'training , such as BERT ,']            -1.0   \n",
       "191                                                NaN             NaN   \n",
       "192                      ['BERT on specific datasets']            -1.0   \n",
       "193                                                NaN             NaN   \n",
       "194                                                 []            -1.0   \n",
       "195                                           ['BERT']            -1.0   \n",
       "196                                                NaN             NaN   \n",
       "197                      ['the Effectiveness of BERT']            -1.0   \n",
       "198                                                NaN             NaN   \n",
       "199                                                NaN             NaN   \n",
       "200                                                NaN             NaN   \n",
       "201  ['a novel data augmentation method for labeled...            -1.0   \n",
       "202                                                NaN             NaN   \n",
       "203                                                NaN             NaN   \n",
       "204                                           ['once']            -1.0   \n",
       "205                                                NaN             NaN   \n",
       "206                                                NaN             NaN   \n",
       "207                                                 []             1.0   \n",
       "208                                                 []            -1.0   \n",
       "209                                                NaN             NaN   \n",
       "210                                                NaN             NaN   \n",
       "211                                                 []            -1.0   \n",
       "212                                                NaN             NaN   \n",
       "213                                                NaN             NaN   \n",
       "214                                                NaN             NaN   \n",
       "215  ['off', '- the - shelf IR techniques and a BER...            -1.0   \n",
       "216                                                NaN             NaN   \n",
       "217                                                NaN             NaN   \n",
       "218                                                NaN             NaN   \n",
       "219                                                NaN             NaN   \n",
       "220                                                NaN             NaN   \n",
       "221                                                NaN             NaN   \n",
       "222                                                NaN             NaN   \n",
       "223                                                NaN             NaN   \n",
       "224                                                 []            -1.0   \n",
       "225                                                NaN             NaN   \n",
       "226                                                 []             1.0   \n",
       "227  ['E - BERT an extension of BERT that replaces ...            -1.0   \n",
       "228  ['E - BERT an extension of BERT that replaces ...            -1.0   \n",
       "229                                                NaN             NaN   \n",
       "230                                                NaN             NaN   \n",
       "231       ['two ways of ensembling BERT and E - BERT']            -1.0   \n",
       "232                                           ['this']            -1.0   \n",
       "233       ['two ways of ensembling BERT and E - BERT']            -1.0   \n",
       "234  ['Not a Knowledge Base ( Yet ) : Factual Knowl...             1.0   \n",
       "235                                                NaN             NaN   \n",
       "236                                                 []            -1.0   \n",
       "237                                                NaN             NaN   \n",
       "238                                                NaN             NaN   \n",
       "239                                                NaN             NaN   \n",
       "240                                                NaN             NaN   \n",
       "241                                                 []             1.0   \n",
       "242                                                NaN             NaN   \n",
       "243                        ['the', 'Geometry of BERT']            -1.0   \n",
       "\n",
       "     averb_split  averb_span0  averb_span1  ...  \\\n",
       "0            1.0         69.0         76.0  ...   \n",
       "1            2.0         52.0         64.0  ...   \n",
       "2            NaN          NaN          NaN  ...   \n",
       "3            2.0          5.0          8.0  ...   \n",
       "4            2.0          3.0         11.0  ...   \n",
       "5            NaN          NaN          NaN  ...   \n",
       "6            NaN          NaN          NaN  ...   \n",
       "7            NaN          NaN          NaN  ...   \n",
       "8            0.0          3.0         11.0  ...   \n",
       "9            0.0          3.0          8.0  ...   \n",
       "10           0.0        106.0        116.0  ...   \n",
       "11           NaN          NaN          NaN  ...   \n",
       "12           NaN          NaN          NaN  ...   \n",
       "13           NaN          NaN          NaN  ...   \n",
       "14           NaN          NaN          NaN  ...   \n",
       "15           NaN          NaN          NaN  ...   \n",
       "16           0.0        110.0        117.0  ...   \n",
       "17           0.0        193.0        203.0  ...   \n",
       "18           0.0        244.0        254.0  ...   \n",
       "19           NaN          NaN          NaN  ...   \n",
       "20           NaN          NaN          NaN  ...   \n",
       "21           NaN          NaN          NaN  ...   \n",
       "22           NaN          NaN          NaN  ...   \n",
       "23           0.0          3.0          9.0  ...   \n",
       "24           2.0         23.0         34.0  ...   \n",
       "25           2.0         58.0         70.0  ...   \n",
       "26           NaN          NaN          NaN  ...   \n",
       "27           2.0         49.0         58.0  ...   \n",
       "28           0.0         88.0         99.0  ...   \n",
       "29           2.0         49.0         58.0  ...   \n",
       "30           NaN          NaN          NaN  ...   \n",
       "31           0.0        117.0        123.0  ...   \n",
       "32           NaN          NaN          NaN  ...   \n",
       "33           NaN          NaN          NaN  ...   \n",
       "34           NaN          NaN          NaN  ...   \n",
       "35           0.0        133.0        145.0  ...   \n",
       "36           NaN          NaN          NaN  ...   \n",
       "37           NaN          NaN          NaN  ...   \n",
       "38           NaN          NaN          NaN  ...   \n",
       "39           0.0         19.0         28.0  ...   \n",
       "40           2.0         11.0         14.0  ...   \n",
       "41           NaN          NaN          NaN  ...   \n",
       "42           NaN          NaN          NaN  ...   \n",
       "43           0.0          2.0          9.0  ...   \n",
       "44           2.0         15.0         24.0  ...   \n",
       "45           NaN          NaN          NaN  ...   \n",
       "46           NaN          NaN          NaN  ...   \n",
       "47           2.0         40.0         49.0  ...   \n",
       "48           0.0         11.0         20.0  ...   \n",
       "49           0.0          3.0         11.0  ...   \n",
       "50           0.0         96.0        101.0  ...   \n",
       "51           0.0         18.0         30.0  ...   \n",
       "52           2.0        126.0        138.0  ...   \n",
       "53           NaN          NaN          NaN  ...   \n",
       "54           0.0         15.0         19.0  ...   \n",
       "55           NaN          NaN          NaN  ...   \n",
       "56           NaN          NaN          NaN  ...   \n",
       "57           0.0         91.0        115.0  ...   \n",
       "58           0.0         59.0         67.0  ...   \n",
       "59           0.0         14.0         24.0  ...   \n",
       "60           0.0         14.0         24.0  ...   \n",
       "61           2.0         37.0         49.0  ...   \n",
       "62           2.0         15.0         23.0  ...   \n",
       "63           0.0         37.0         49.0  ...   \n",
       "64           2.0          9.0         12.0  ...   \n",
       "65           NaN          NaN          NaN  ...   \n",
       "66           2.0          9.0         12.0  ...   \n",
       "67           NaN          NaN          NaN  ...   \n",
       "68           2.0         43.0         56.0  ...   \n",
       "69           0.0         19.0         28.0  ...   \n",
       "70           NaN          NaN          NaN  ...   \n",
       "71           NaN          NaN          NaN  ...   \n",
       "72           NaN          NaN          NaN  ...   \n",
       "73           NaN          NaN          NaN  ...   \n",
       "74           NaN          NaN          NaN  ...   \n",
       "75           NaN          NaN          NaN  ...   \n",
       "76           NaN          NaN          NaN  ...   \n",
       "77           0.0        114.0        122.0  ...   \n",
       "78           0.0        193.0        201.0  ...   \n",
       "79           0.0        114.0        122.0  ...   \n",
       "80           0.0          0.0          8.0  ...   \n",
       "81           0.0          0.0          8.0  ...   \n",
       "82           0.0         21.0         33.0  ...   \n",
       "83           NaN          NaN          NaN  ...   \n",
       "84           NaN          NaN          NaN  ...   \n",
       "85           NaN          NaN          NaN  ...   \n",
       "86           2.0        135.0        144.0  ...   \n",
       "87           0.0         50.0         65.0  ...   \n",
       "88           0.0        137.0        145.0  ...   \n",
       "89           0.0        137.0        145.0  ...   \n",
       "90           NaN          NaN          NaN  ...   \n",
       "91           NaN          NaN          NaN  ...   \n",
       "92           0.0          3.0          8.0  ...   \n",
       "93           0.0         17.0         23.0  ...   \n",
       "94           0.0          3.0         12.0  ...   \n",
       "95           2.0         43.0         51.0  ...   \n",
       "96           0.0          0.0          9.0  ...   \n",
       "97           2.0          5.0          9.0  ...   \n",
       "98           2.0         26.0         31.0  ...   \n",
       "99           NaN          NaN          NaN  ...   \n",
       "100          NaN          NaN          NaN  ...   \n",
       "101          NaN          NaN          NaN  ...   \n",
       "102          NaN          NaN          NaN  ...   \n",
       "103          0.0         30.0         39.0  ...   \n",
       "104          0.0         40.0         46.0  ...   \n",
       "105          0.0         14.0         26.0  ...   \n",
       "106          0.0          3.0         11.0  ...   \n",
       "107          NaN          NaN          NaN  ...   \n",
       "108          NaN          NaN          NaN  ...   \n",
       "109          NaN          NaN          NaN  ...   \n",
       "110          0.0        149.0        160.0  ...   \n",
       "111          0.0         25.0         31.0  ...   \n",
       "112          0.0        181.0        189.0  ...   \n",
       "113          NaN          NaN          NaN  ...   \n",
       "114          NaN          NaN          NaN  ...   \n",
       "115          0.0         59.0         65.0  ...   \n",
       "116          0.0          0.0         10.0  ...   \n",
       "117          NaN          NaN          NaN  ...   \n",
       "118          2.0         39.0         57.0  ...   \n",
       "119          0.0        123.0        131.0  ...   \n",
       "120          NaN          NaN          NaN  ...   \n",
       "121          2.0         10.0         22.0  ...   \n",
       "122          0.0          7.0         18.0  ...   \n",
       "123          2.0         10.0         22.0  ...   \n",
       "124          NaN          NaN          NaN  ...   \n",
       "125          0.0         19.0         27.0  ...   \n",
       "126          0.0         19.0         27.0  ...   \n",
       "127          NaN          NaN          NaN  ...   \n",
       "128          NaN          NaN          NaN  ...   \n",
       "129          NaN          NaN          NaN  ...   \n",
       "130          1.0         31.0         36.0  ...   \n",
       "131          0.0         26.0         34.0  ...   \n",
       "132          NaN          NaN          NaN  ...   \n",
       "133          2.0         88.0         94.0  ...   \n",
       "134          0.0         40.0         47.0  ...   \n",
       "135          NaN          NaN          NaN  ...   \n",
       "136          NaN          NaN          NaN  ...   \n",
       "137          NaN          NaN          NaN  ...   \n",
       "138          NaN          NaN          NaN  ...   \n",
       "139          NaN          NaN          NaN  ...   \n",
       "140          NaN          NaN          NaN  ...   \n",
       "141          NaN          NaN          NaN  ...   \n",
       "142          0.0         44.0         54.0  ...   \n",
       "143          0.0         79.0         85.0  ...   \n",
       "144          1.0         79.0         85.0  ...   \n",
       "145          2.0         57.0         66.0  ...   \n",
       "146          NaN          NaN          NaN  ...   \n",
       "147          NaN          NaN          NaN  ...   \n",
       "148          NaN          NaN          NaN  ...   \n",
       "149          NaN          NaN          NaN  ...   \n",
       "150          NaN          NaN          NaN  ...   \n",
       "151          0.0        114.0        120.0  ...   \n",
       "152          NaN          NaN          NaN  ...   \n",
       "153          NaN          NaN          NaN  ...   \n",
       "154          NaN          NaN          NaN  ...   \n",
       "155          1.0          0.0          5.0  ...   \n",
       "156          0.0         11.0         19.0  ...   \n",
       "157          0.0         34.0         46.0  ...   \n",
       "158          0.0         34.0         46.0  ...   \n",
       "159          0.0         33.0         45.0  ...   \n",
       "160          0.0         92.0        102.0  ...   \n",
       "161          2.0         60.0         64.0  ...   \n",
       "162          2.0         29.0         39.0  ...   \n",
       "163          0.0         29.0         39.0  ...   \n",
       "164          0.0         29.0         39.0  ...   \n",
       "165          2.0        121.0        129.0  ...   \n",
       "166          0.0          0.0         14.0  ...   \n",
       "167          NaN          NaN          NaN  ...   \n",
       "168          NaN          NaN          NaN  ...   \n",
       "169          2.0        149.0        157.0  ...   \n",
       "170          0.0         36.0         58.0  ...   \n",
       "171          2.0         11.0         19.0  ...   \n",
       "172          NaN          NaN          NaN  ...   \n",
       "173          NaN          NaN          NaN  ...   \n",
       "174          NaN          NaN          NaN  ...   \n",
       "175          NaN          NaN          NaN  ...   \n",
       "176          0.0          3.0         11.0  ...   \n",
       "177          0.0        111.0        115.0  ...   \n",
       "178          NaN          NaN          NaN  ...   \n",
       "179          0.0          3.0          9.0  ...   \n",
       "180          0.0         80.0         86.0  ...   \n",
       "181          NaN          NaN          NaN  ...   \n",
       "182          NaN          NaN          NaN  ...   \n",
       "183          NaN          NaN          NaN  ...   \n",
       "184          0.0         21.0         27.0  ...   \n",
       "185          2.0         89.0        115.0  ...   \n",
       "186          2.0        377.0        383.0  ...   \n",
       "187          2.0        279.0        289.0  ...   \n",
       "188          2.0        377.0        383.0  ...   \n",
       "189          2.0         10.0         13.0  ...   \n",
       "190          0.0         15.0         19.0  ...   \n",
       "191          NaN          NaN          NaN  ...   \n",
       "192          0.0         96.0        103.0  ...   \n",
       "193          NaN          NaN          NaN  ...   \n",
       "194          0.0          8.0         20.0  ...   \n",
       "195          0.0         56.0         63.0  ...   \n",
       "196          NaN          NaN          NaN  ...   \n",
       "197          0.0         16.0         30.0  ...   \n",
       "198          NaN          NaN          NaN  ...   \n",
       "199          NaN          NaN          NaN  ...   \n",
       "200          NaN          NaN          NaN  ...   \n",
       "201          0.0          3.0         11.0  ...   \n",
       "202          NaN          NaN          NaN  ...   \n",
       "203          NaN          NaN          NaN  ...   \n",
       "204          0.0        141.0        150.0  ...   \n",
       "205          NaN          NaN          NaN  ...   \n",
       "206          NaN          NaN          NaN  ...   \n",
       "207          2.0         34.0         38.0  ...   \n",
       "208          0.0         63.0         68.0  ...   \n",
       "209          NaN          NaN          NaN  ...   \n",
       "210          NaN          NaN          NaN  ...   \n",
       "211          0.0          0.0          9.0  ...   \n",
       "212          NaN          NaN          NaN  ...   \n",
       "213          NaN          NaN          NaN  ...   \n",
       "214          NaN          NaN          NaN  ...   \n",
       "215          0.0         53.0         59.0  ...   \n",
       "216          NaN          NaN          NaN  ...   \n",
       "217          NaN          NaN          NaN  ...   \n",
       "218          NaN          NaN          NaN  ...   \n",
       "219          NaN          NaN          NaN  ...   \n",
       "220          NaN          NaN          NaN  ...   \n",
       "221          NaN          NaN          NaN  ...   \n",
       "222          NaN          NaN          NaN  ...   \n",
       "223          NaN          NaN          NaN  ...   \n",
       "224          0.0         43.0         49.0  ...   \n",
       "225          NaN          NaN          NaN  ...   \n",
       "226          2.0         51.0         57.0  ...   \n",
       "227          0.0         17.0         25.0  ...   \n",
       "228          0.0         17.0         25.0  ...   \n",
       "229          NaN          NaN          NaN  ...   \n",
       "230          NaN          NaN          NaN  ...   \n",
       "231          0.0         79.0         84.0  ...   \n",
       "232          0.0          3.0          8.0  ...   \n",
       "233          0.0         79.0         84.0  ...   \n",
       "234          2.0          5.0          8.0  ...   \n",
       "235          NaN          NaN          NaN  ...   \n",
       "236          0.0         69.0         78.0  ...   \n",
       "237          NaN          NaN          NaN  ...   \n",
       "238          NaN          NaN          NaN  ...   \n",
       "239          NaN          NaN          NaN  ...   \n",
       "240          NaN          NaN          NaN  ...   \n",
       "241          2.0        134.0        147.0  ...   \n",
       "242          NaN          NaN          NaN  ...   \n",
       "243          0.0          0.0         10.0  ...   \n",
       "\n",
       "                                       apos_w  \\\n",
       "0                                  ['stands']   \n",
       "1                     ['BERT', ',', 'models']   \n",
       "2                   ['BERT', 'model', 'fine']   \n",
       "3              ['BERT', 'is', 'conceptually']   \n",
       "4               ['It', 'obtains', 'question']   \n",
       "5                                    ['BERT']   \n",
       "6                                         NaN   \n",
       "7                                         NaN   \n",
       "8                        ['BERT', 'et', 'of']   \n",
       "9           ['BERT', 'significantly', 'find']   \n",
       "10               ['it', 'after', 'published']   \n",
       "11                                        NaN   \n",
       "12                                        NaN   \n",
       "13                                        NaN   \n",
       "14                                ['RoBERTa']   \n",
       "15                                        NaN   \n",
       "16          ['DistilBERT', 'called', 'model']   \n",
       "17                    ['model', 'of', 'size']   \n",
       "18              ['its', 'capabilities', 'of']   \n",
       "19                                        NaN   \n",
       "20                                        NaN   \n",
       "21                             ['DistilBERT']   \n",
       "22                                        NaN   \n",
       "23                   ['model', 'on', 'focus']   \n",
       "24            ['model', 'represents', 'find']   \n",
       "25             ['model', 'adjust', 'reveals']   \n",
       "26                    ['BERT', 'Rediscovers']   \n",
       "27                 ['Large', 'had', 'recent']   \n",
       "28             ['they', 'able', 'motivating']   \n",
       "29                 ['BERT', 'as', 'networks']   \n",
       "30                                        NaN   \n",
       "31                    ['BERT', 'to', 'apply']   \n",
       "32                                   ['BERT']   \n",
       "33                                        NaN   \n",
       "34                                        NaN   \n",
       "35                 ['BERT', 'in', 'captured']   \n",
       "36                                   ['BERT']   \n",
       "37                 ['BERT', 'of', 'Analysis']   \n",
       "38                       ['BERT', 'et', 'al']   \n",
       "39             ['implementation', 'describe']   \n",
       "40                  ['system', 'is', 'state']   \n",
       "41                                        NaN   \n",
       "42                ['BERT', 'with', 'ranking']   \n",
       "43           ['model', 'syntactic', 'extent']   \n",
       "44                      ['model', 'performs']   \n",
       "45                                   ['BERT']   \n",
       "46                                        NaN   \n",
       "47                   ['BERT', 'Devlin', 'of']   \n",
       "48            ['mBERT', 'multilingual', 'of']   \n",
       "49                  ['mBERT', 'compare', '.']   \n",
       "50           ['mBERT', 'competitive', 'find']   \n",
       "51               ['mBERT', 'for', 'strategy']   \n",
       "52      ['mBERT', 'generalizes', 'determine']   \n",
       "53            ['BERT', 'of', 'Effectiveness']   \n",
       "54                 ['BERT', 'as', 'training']   \n",
       "55                                        NaN   \n",
       "56                                        NaN   \n",
       "57           ['student', 'to', 'transferred']   \n",
       "58                  ['BERT', 'in', 'encoded']   \n",
       "59           ['TinyBERT', 'for', 'framework']   \n",
       "60                              ['introduce']   \n",
       "61         ['TinyBERT', 'capture', 'ensures']   \n",
       "62                                ['ensures']   \n",
       "63             ['teacher', 'of', 'knowledge']   \n",
       "64          ['TinyBERT', 'is', 'empirically']   \n",
       "65                ['BERT', 'with', 'results']   \n",
       "66                         ['TinyBERT', 'is']   \n",
       "67                               ['TinyBERT']   \n",
       "68             ['BERT', 'achieved', 'ground']   \n",
       "69                    ['BERTSUM', 'describe']   \n",
       "70                                        NaN   \n",
       "71                                        NaN   \n",
       "72                                   ['BERT']   \n",
       "73                                        NaN   \n",
       "74                                        NaN   \n",
       "75                                        NaN   \n",
       "76                                        NaN   \n",
       "77                ['model', 'on', 'approach']   \n",
       "78                   ['BERT', 'of', 'tuning']   \n",
       "79            ['approach', 'explore', 'then']   \n",
       "80           ['approach', 'of', 'generality']   \n",
       "81           ['training', 'of', 'generality']   \n",
       "82        ['training', 'highly', 'effective']   \n",
       "83                                        NaN   \n",
       "84                           ['BERT', 'Post']   \n",
       "85                                        NaN   \n",
       "86            ['Representations', 'achieved']   \n",
       "87                  ['BERT', 'of', 'methods']   \n",
       "88                  ['BERT', 'tuning', 'for']   \n",
       "89            ['solution', 'provide', 'task']   \n",
       "90                      ['solution', 'state']   \n",
       "91                   ['BERT', 'Tune', 'Fine']   \n",
       "92                     ['BERT', 'et', 'that']   \n",
       "93              ['BERT', 'from', 'sentences']   \n",
       "94               ['BERT', 'from', 'generate']   \n",
       "95                  ['it', 'produce', 'find']   \n",
       "96                 ['BERT', 'sentences', ',']   \n",
       "97                            ['BERT', 'has']   \n",
       "98                    ['It', 'Must', 'Speak']   \n",
       "99                                        NaN   \n",
       "100                                       NaN   \n",
       "101                 ['BERT', 'from', 'model']   \n",
       "102                                  ['BERT']   \n",
       "103                ['BERT', 'applying', 'in']   \n",
       "104                  ['BERT', 'of', 'length']   \n",
       "105  ['challenge', 'confronting', 'required']   \n",
       "106              ['issue', 'address', 'then']   \n",
       "107                                       NaN   \n",
       "108            ['BERT', 'of', 'Applications']   \n",
       "109                     ['BERT', 'currently']   \n",
       "110       ['its', 'contribute', 'mechanisms']   \n",
       "111              ['BERT', 'of', 'components']   \n",
       "112              ['BERT', 'individual', 'by']   \n",
       "113                                       NaN   \n",
       "114                                       NaN   \n",
       "115                ['BERT', 'models', 'over']   \n",
       "116                 ['BERT', 'of', 'Secrets']   \n",
       "117             ['BERT', ')', 'Transformers']   \n",
       "118                 ['BERT', 'of', 'version']   \n",
       "119               ['BERT', 'training', 'pre']   \n",
       "120                                       NaN   \n",
       "121                      ['model', 'trained']   \n",
       "122                ['BERT', 'for', 'provide']   \n",
       "123        ['model', 'verified', 'inference']   \n",
       "124                                       NaN   \n",
       "125                   ['BERT', ':', 'models']   \n",
       "126                     ['wwm', ',', 'ERNIE']   \n",
       "127                                       NaN   \n",
       "128                ['BERT', 'for', 'Masking']   \n",
       "129         ['BERT', 'model', 'successfully']   \n",
       "130                         ['BERT', 'cause']   \n",
       "131                      ['model', 'propose']   \n",
       "132                                       NaN   \n",
       "133              ['BERT', 'gains', 'passage']   \n",
       "134                        ['BERT', 'showed']   \n",
       "135                 ['BERT', 'models', 'non']   \n",
       "136                     ['model', 'on', 'In']   \n",
       "137                       ['BERT', 'passage']   \n",
       "138        ['BERT', 'Normalized', 'Globally']   \n",
       "139                                       NaN   \n",
       "140                                       NaN   \n",
       "141                    ['BERT', '(', 'model']   \n",
       "142               ['BERT', 'exploring', 'on']   \n",
       "143                   ['BERT', 'on', 'based']   \n",
       "144                      ['based', 'propose']   \n",
       "145      ['model', 'achieves', 'demonstrate']   \n",
       "146                                  ['BERT']   \n",
       "147                                       NaN   \n",
       "148                                       NaN   \n",
       "149                                       NaN   \n",
       "150                                       NaN   \n",
       "151                   ['BERT', 'on', 'built']   \n",
       "152                                       NaN   \n",
       "153                                       NaN   \n",
       "154                                       NaN   \n",
       "155                                  ['BERT']   \n",
       "156               ['BERT', 'of', 'behaviors']   \n",
       "157              ['BERT', 'leverage', 'ways']   \n",
       "158                ['it', 'tune', 'leverage']   \n",
       "159           ['BERT', 'of', 'effectiveness']   \n",
       "160                 ['BERT', 'model', 'fact']   \n",
       "161                ['BERT', 'pre', 'between']   \n",
       "162       ['BERT', 'allocates', 'illustrate']   \n",
       "163        ['its', 'attentions', 'allocates']   \n",
       "164                   ['its', 'layers', 'in']   \n",
       "165             ['it', 'prefers', 'document']   \n",
       "166               ['BERT', 'of', 'Behaviors']   \n",
       "167                                  ['role']   \n",
       "168                                       NaN   \n",
       "169                   ['BERT', 'model', 'of']   \n",
       "170                ['BERT', 'apply', 'first']   \n",
       "171                     ['models', 'provide']   \n",
       "172                        ['BERT', 'Models']   \n",
       "173                                       NaN   \n",
       "174                                       NaN   \n",
       "175                                       NaN   \n",
       "176                  ['BERT', 'model', 'for']   \n",
       "177                 ['BERT', 'network', 'to']   \n",
       "178                                       NaN   \n",
       "179                ['BERT', 'layers', 'with']   \n",
       "180                   ['BERT', 'tuned', 'of']   \n",
       "181                                  ['BERT']   \n",
       "182                                       NaN   \n",
       "183                                       NaN   \n",
       "184                  ['model', 'to', 'apply']   \n",
       "185          ['it', 'distinguish', 'finding']   \n",
       "186                  ['it', 'shows', 'clear']   \n",
       "187              ['it', 'struggles', 'shows']   \n",
       "188                  ['it', 'shows', 'clear']   \n",
       "189                            ['BERT', 'Is']   \n",
       "190                ['BERT', 'as', 'training']   \n",
       "191                                       NaN   \n",
       "192                  ['BERT', 'tuning', 'of']   \n",
       "193                                       NaN   \n",
       "194              ['BERT', 'highly', 'robust']   \n",
       "195               ['BERT', 'tuning', 'tends']   \n",
       "196                  ['BERT', 'of', 'layers']   \n",
       "197           ['BERT', 'of', 'Effectiveness']   \n",
       "198                                       NaN   \n",
       "199                              ['Recently']   \n",
       "200                       ['Representations']   \n",
       "201                     ['method', 'propose']   \n",
       "202                       ['to', 'BERT', '.']   \n",
       "203                        ['BERT', '.', '”']   \n",
       "204                   ['BERT', 'paper', 'in']   \n",
       "205                                       NaN   \n",
       "206                                       NaN   \n",
       "207            ['BERT', 'conditional', 'can']   \n",
       "208              ['method', 'easily', 'show']   \n",
       "209                                  ['BERT']   \n",
       "210                                       NaN   \n",
       "211            ['BERT', 'checkpoint', 'from']   \n",
       "212                                       NaN   \n",
       "213                                       NaN   \n",
       "214                         ['BERT', 'Small']   \n",
       "215                 ['BERT', 'reader', 'and']   \n",
       "216                                       NaN   \n",
       "217                  ['BERT', 'tuning', 'to']   \n",
       "218                                       NaN   \n",
       "219           ['BERT', 'for', 'Augmentation']   \n",
       "220                         ['model', 'good']   \n",
       "221                                       NaN   \n",
       "222                ['BERT', 'evidence', 'as']   \n",
       "223                   ['evidence', 'as', '(']   \n",
       "224             ['BERT', 'of', 'performance']   \n",
       "225       ['interpretation', 'with', 'issue']   \n",
       "226            ['BERT', 'precision', 'drops']   \n",
       "227               ['BERT', 'of', 'extension']   \n",
       "228                          ['E', 'propose']   \n",
       "229                                  ['BERT']   \n",
       "230                          ['BERT', 'BERT']   \n",
       "231                    ['BERT', 'of', 'ways']   \n",
       "232              ['richer', 'evidence', 'as']   \n",
       "233                    ['BERT', 'BERT', 'of']   \n",
       "234                            ['BERT', 'is']   \n",
       "235                                       NaN   \n",
       "236                    ['BERT', 'ELMo', 'as']   \n",
       "237                                       NaN   \n",
       "238                                       NaN   \n",
       "239                                       NaN   \n",
       "240                                       NaN   \n",
       "241                  ['BERT', 'of', 'layers']   \n",
       "242                                       NaN   \n",
       "243                ['BERT', 'of', 'Geometry']   \n",
       "\n",
       "                                                   URL  ID      Type Index  \\\n",
       "0    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract     0   \n",
       "1    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract     1   \n",
       "2    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract     2   \n",
       "3    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract     3   \n",
       "4    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract     4   \n",
       "5    https://www.semanticscholar.org/paper/BERT%3A-...   0     Title     0   \n",
       "6    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract     0   \n",
       "7    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract     1   \n",
       "8    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract     2   \n",
       "9    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract     3   \n",
       "10   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract     3   \n",
       "11   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract     4   \n",
       "12   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract     5   \n",
       "13   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract     6   \n",
       "14   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title     0   \n",
       "15   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract     0   \n",
       "16   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract     1   \n",
       "17   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract     2   \n",
       "18   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract     2   \n",
       "19   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract     3   \n",
       "20   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract     4   \n",
       "21   https://www.semanticscholar.org/paper/DistilBE...   2     Title     0   \n",
       "22   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract     0   \n",
       "23   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract     1   \n",
       "24   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract     2   \n",
       "25   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract     3   \n",
       "26   https://www.semanticscholar.org/paper/BERT-Red...   3     Title     0   \n",
       "27   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract     0   \n",
       "28   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract     0   \n",
       "29   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract     0   \n",
       "30   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract     1   \n",
       "31   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract     2   \n",
       "32   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract     3   \n",
       "33   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract     4   \n",
       "34   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract     5   \n",
       "35   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract     6   \n",
       "36   https://www.semanticscholar.org/paper/What-Doe...   4     Title     0   \n",
       "37   https://www.semanticscholar.org/paper/What-Doe...   4     Title     1   \n",
       "38   https://www.semanticscholar.org/paper/Passage-...   5  Abstract     0   \n",
       "39   https://www.semanticscholar.org/paper/Passage-...   5  Abstract     1   \n",
       "40   https://www.semanticscholar.org/paper/Passage-...   5  Abstract     2   \n",
       "41   https://www.semanticscholar.org/paper/Passage-...   5  Abstract     3   \n",
       "42   https://www.semanticscholar.org/paper/Passage-...   5     Title     0   \n",
       "43   https://www.semanticscholar.org/paper/Assessin...   6  Abstract     0   \n",
       "44   https://www.semanticscholar.org/paper/Assessin...   6  Abstract     1   \n",
       "45   https://www.semanticscholar.org/paper/Assessin...   6     Title     0   \n",
       "46   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract     0   \n",
       "47   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract     1   \n",
       "48   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract     2   \n",
       "49   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract     3   \n",
       "50   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract     3   \n",
       "51   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract     4   \n",
       "52   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract     4   \n",
       "53   https://www.semanticscholar.org/paper/Beto%2C-...   7     Title     0   \n",
       "54   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     0   \n",
       "55   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     1   \n",
       "56   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     2   \n",
       "57   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     3   \n",
       "58   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     3   \n",
       "59   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     4   \n",
       "60   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     4   \n",
       "61   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     5   \n",
       "62   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     5   \n",
       "63   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     5   \n",
       "64   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     6   \n",
       "65   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     6   \n",
       "66   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract     7   \n",
       "67   https://www.semanticscholar.org/paper/TinyBERT...   8     Title     0   \n",
       "68   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract     0   \n",
       "69   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract     1   \n",
       "70   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract     2   \n",
       "71   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract     3   \n",
       "72   https://www.semanticscholar.org/paper/Fine-tun...   9     Title     0   \n",
       "73   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     0   \n",
       "74   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     1   \n",
       "75   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     2   \n",
       "76   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     3   \n",
       "77   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     4   \n",
       "78   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     4   \n",
       "79   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     4   \n",
       "80   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     5   \n",
       "81   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     5   \n",
       "82   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     6   \n",
       "83   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract     7   \n",
       "84   https://www.semanticscholar.org/paper/BERT-Pos...  10     Title     0   \n",
       "85   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract     0   \n",
       "86   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract     1   \n",
       "87   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract     2   \n",
       "88   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract     2   \n",
       "89   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract     2   \n",
       "90   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract     3   \n",
       "91   https://www.semanticscholar.org/paper/How-to-F...  11     Title     0   \n",
       "92   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract     0   \n",
       "93   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract     1   \n",
       "94   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract     2   \n",
       "95   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract     2   \n",
       "96   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract     3   \n",
       "97   https://www.semanticscholar.org/paper/BERT-has...  12     Title     0   \n",
       "98   https://www.semanticscholar.org/paper/BERT-has...  12     Title     0   \n",
       "99   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract     0   \n",
       "100  https://www.semanticscholar.org/paper/Utilizin...  13  Abstract     1   \n",
       "101  https://www.semanticscholar.org/paper/Utilizin...  13  Abstract     2   \n",
       "102  https://www.semanticscholar.org/paper/Utilizin...  13     Title     0   \n",
       "103  https://www.semanticscholar.org/paper/Simple-A...  14  Abstract     0   \n",
       "104  https://www.semanticscholar.org/paper/Simple-A...  14  Abstract     1   \n",
       "105  https://www.semanticscholar.org/paper/Simple-A...  14  Abstract     1   \n",
       "106  https://www.semanticscholar.org/paper/Simple-A...  14  Abstract     2   \n",
       "107  https://www.semanticscholar.org/paper/Simple-A...  14  Abstract     3   \n",
       "108  https://www.semanticscholar.org/paper/Simple-A...  14     Title     0   \n",
       "109  https://www.semanticscholar.org/paper/Revealin...  15  Abstract     0   \n",
       "110  https://www.semanticscholar.org/paper/Revealin...  15  Abstract     0   \n",
       "111  https://www.semanticscholar.org/paper/Revealin...  15  Abstract     1   \n",
       "112  https://www.semanticscholar.org/paper/Revealin...  15  Abstract     2   \n",
       "113  https://www.semanticscholar.org/paper/Revealin...  15  Abstract     3   \n",
       "114  https://www.semanticscholar.org/paper/Revealin...  15  Abstract     4   \n",
       "115  https://www.semanticscholar.org/paper/Revealin...  15  Abstract     5   \n",
       "116  https://www.semanticscholar.org/paper/Revealin...  15     Title     0   \n",
       "117  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     0   \n",
       "118  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     1   \n",
       "119  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     1   \n",
       "120  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     2   \n",
       "121  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     3   \n",
       "122  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     4   \n",
       "123  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     5   \n",
       "124  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     6   \n",
       "125  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     7   \n",
       "126  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     7   \n",
       "127  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract     8   \n",
       "128  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title     0   \n",
       "129  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract     0   \n",
       "130  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract     1   \n",
       "131  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract     2   \n",
       "132  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract     3   \n",
       "133  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract     4   \n",
       "134  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract     5   \n",
       "135  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract     6   \n",
       "136  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract     6   \n",
       "137  https://www.semanticscholar.org/paper/Multi-pa...  17     Title     0   \n",
       "138  https://www.semanticscholar.org/paper/Multi-pa...  17     Title     0   \n",
       "139  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract     0   \n",
       "140  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract     1   \n",
       "141  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract     2   \n",
       "142  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract     3   \n",
       "143  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract     4   \n",
       "144  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract     4   \n",
       "145  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract     5   \n",
       "146  https://www.semanticscholar.org/paper/BERT-for...  18     Title     0   \n",
       "147  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract     0   \n",
       "148  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract     1   \n",
       "149  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract     2   \n",
       "150  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract     3   \n",
       "151  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract     4   \n",
       "152  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract     5   \n",
       "153  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract     6   \n",
       "154  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract     7   \n",
       "155  https://www.semanticscholar.org/paper/BERT-wit...  19     Title     0   \n",
       "156  https://www.semanticscholar.org/paper/Understa...  20  Abstract     0   \n",
       "157  https://www.semanticscholar.org/paper/Understa...  20  Abstract     1   \n",
       "158  https://www.semanticscholar.org/paper/Understa...  20  Abstract     1   \n",
       "159  https://www.semanticscholar.org/paper/Understa...  20  Abstract     2   \n",
       "160  https://www.semanticscholar.org/paper/Understa...  20  Abstract     2   \n",
       "161  https://www.semanticscholar.org/paper/Understa...  20  Abstract     3   \n",
       "162  https://www.semanticscholar.org/paper/Understa...  20  Abstract     4   \n",
       "163  https://www.semanticscholar.org/paper/Understa...  20  Abstract     4   \n",
       "164  https://www.semanticscholar.org/paper/Understa...  20  Abstract     4   \n",
       "165  https://www.semanticscholar.org/paper/Understa...  20  Abstract     4   \n",
       "166  https://www.semanticscholar.org/paper/Understa...  20     Title     0   \n",
       "167  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract     0   \n",
       "168  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract     1   \n",
       "169  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract     2   \n",
       "170  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract     3   \n",
       "171  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract     4   \n",
       "172  https://www.semanticscholar.org/paper/Simple-B...  21     Title     0   \n",
       "173  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract     0   \n",
       "174  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract     1   \n",
       "175  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract     2   \n",
       "176  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract     3   \n",
       "177  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract     3   \n",
       "178  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract     4   \n",
       "179  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract     5   \n",
       "180  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract     5   \n",
       "181  https://www.semanticscholar.org/paper/BERT-and...  22     Title     0   \n",
       "182  https://www.semanticscholar.org/paper/What-BER...  23  Abstract     0   \n",
       "183  https://www.semanticscholar.org/paper/What-BER...  23  Abstract     1   \n",
       "184  https://www.semanticscholar.org/paper/What-BER...  23  Abstract     2   \n",
       "185  https://www.semanticscholar.org/paper/What-BER...  23  Abstract     2   \n",
       "186  https://www.semanticscholar.org/paper/What-BER...  23  Abstract     2   \n",
       "187  https://www.semanticscholar.org/paper/What-BER...  23  Abstract     2   \n",
       "188  https://www.semanticscholar.org/paper/What-BER...  23  Abstract     2   \n",
       "189  https://www.semanticscholar.org/paper/What-BER...  23     Title     0   \n",
       "190  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract     0   \n",
       "191  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract     1   \n",
       "192  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract     2   \n",
       "193  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract     3   \n",
       "194  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract     4   \n",
       "195  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract     5   \n",
       "196  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract     6   \n",
       "197  https://www.semanticscholar.org/paper/Visualiz...  24     Title     0   \n",
       "198  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     0   \n",
       "199  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     1   \n",
       "200  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     2   \n",
       "201  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     3   \n",
       "202  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     4   \n",
       "203  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     4   \n",
       "204  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     4   \n",
       "205  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     5   \n",
       "206  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     6   \n",
       "207  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     7   \n",
       "208  https://www.semanticscholar.org/paper/Conditio...  25  Abstract     8   \n",
       "209  https://www.semanticscholar.org/paper/Conditio...  25     Title     0   \n",
       "210  https://www.semanticscholar.org/paper/Small-an...  26  Abstract     0   \n",
       "211  https://www.semanticscholar.org/paper/Small-an...  26  Abstract     1   \n",
       "212  https://www.semanticscholar.org/paper/Small-an...  26  Abstract     2   \n",
       "213  https://www.semanticscholar.org/paper/Small-an...  26  Abstract     3   \n",
       "214  https://www.semanticscholar.org/paper/Small-an...  26     Title     0   \n",
       "215  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract     0   \n",
       "216  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract     1   \n",
       "217  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract     2   \n",
       "218  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract     3   \n",
       "219  https://www.semanticscholar.org/paper/Data-Aug...  27     Title     0   \n",
       "220  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     0   \n",
       "221  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     1   \n",
       "222  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     2   \n",
       "223  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     2   \n",
       "224  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     3   \n",
       "225  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     3   \n",
       "226  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     4   \n",
       "227  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     5   \n",
       "228  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     5   \n",
       "229  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     6   \n",
       "230  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     6   \n",
       "231  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     7   \n",
       "232  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     7   \n",
       "233  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract     7   \n",
       "234  https://www.semanticscholar.org/paper/BERT-is-...  28     Title     0   \n",
       "235  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract     0   \n",
       "236  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract     1   \n",
       "237  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract     2   \n",
       "238  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract     3   \n",
       "239  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract     4   \n",
       "240  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract     5   \n",
       "241  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract     6   \n",
       "242  https://www.semanticscholar.org/paper/How-Cont...  29     Title     0   \n",
       "243  https://www.semanticscholar.org/paper/How-Cont...  29     Title     1   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    We introduce a new language representation mod...   \n",
       "1    Unlike recent language representation models, ...   \n",
       "2    As a result, the pre-trained BERT model can be...   \n",
       "3    BERT is conceptually simple and empirically po...   \n",
       "4    It obtains new state-of-the-art results on ele...   \n",
       "5    BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "6    Language model pretraining has led to signific...   \n",
       "7    Training is computationally expensive, often d...   \n",
       "8    We present a replication study of BERT pretrai...   \n",
       "9    We find that BERT was significantly undertrain...   \n",
       "10   We find that BERT was significantly undertrain...   \n",
       "11   Our best model achieves state-of-the-art resul...   \n",
       "12   These results highlight the importance of prev...   \n",
       "13                     We release our models and code.   \n",
       "14   RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "15   As Transfer Learning from large-scale pre-trai...   \n",
       "16   In this work, we propose a method to pre-train...   \n",
       "17   While most prior work investigated the use of ...   \n",
       "18   While most prior work investigated the use of ...   \n",
       "19   To leverage the inductive biases learned by la...   \n",
       "20   Our smaller, faster and lighter model is cheap...   \n",
       "21   DistilBERT, a distilled version of BERT: small...   \n",
       "22   Pre-trained text encoders have rapidly advance...   \n",
       "23   We focus on one such model, BERT, and aim to q...   \n",
       "24   We find that the model represents the steps of...   \n",
       "25   Qualitative analysis reveals that the model ca...   \n",
       "26        BERT Rediscovers the Classical NLP Pipeline.   \n",
       "27   Large pre-trained neural networks such as BERT...   \n",
       "28   Large pre-trained neural networks such as BERT...   \n",
       "29   Large pre-trained neural networks such as BERT...   \n",
       "30   Most recent analysis has focused on model outp...   \n",
       "31   Complementary to these works, we propose metho...   \n",
       "32   BERT's attention heads exhibit patterns such a...   \n",
       "33   We further show that certain attention heads c...   \n",
       "34   For example, we find heads that attend to the ...   \n",
       "35   Lastly, we propose an attention-based probing ...   \n",
       "36                             What Does BERT Look At?   \n",
       "37                    An Analysis of BERT's Attention.   \n",
       "38   Recently, neural models pretrained on a langua...   \n",
       "39   In this paper, we describe a simple re-impleme...   \n",
       "40   Our system is the state of the art on the TREC...   \n",
       "41   The code to reproduce our results is available...   \n",
       "42                       Passage Re-ranking with BERT.   \n",
       "43   I assess the extent to which the recently intr...   \n",
       "44   The BERT model performs remarkably well on all...   \n",
       "45               Assessing BERT's Syntactic Abilities.   \n",
       "46   Pretrained contextual representation models (P...   \n",
       "47   A new release of BERT (Devlin, 2018) includes ...   \n",
       "48   This paper explores the broader cross-lingual ...   \n",
       "49   We compare mBERT with the best-published metho...   \n",
       "50   We compare mBERT with the best-published metho...   \n",
       "51   Additionally, we investigate the most effectiv...   \n",
       "52   Additionally, we investigate the most effectiv...   \n",
       "53   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "54   Language model pre-training, such as BERT, has...   \n",
       "55   However, pre-trained language models are usual...   \n",
       "56   To accelerate inference and reduce model size ...   \n",
       "57   By leveraging this new KD method, the plenty o...   \n",
       "58   By leveraging this new KD method, the plenty o...   \n",
       "59   Moreover, we introduce a new two-stage learnin...   \n",
       "60   Moreover, we introduce a new two-stage learnin...   \n",
       "61   This framework ensures that TinyBERT can captu...   \n",
       "62   This framework ensures that TinyBERT can captu...   \n",
       "63   This framework ensures that TinyBERT can captu...   \n",
       "64   TinyBERT is empirically effective and achieves...   \n",
       "65   TinyBERT is empirically effective and achieves...   \n",
       "66   TinyBERT is also significantly better than sta...   \n",
       "67   TinyBERT: Distilling BERT for Natural Language...   \n",
       "68   BERT, a pre-trained Transformer model, has ach...   \n",
       "69   In this paper, we describe BERTSUM, a simple v...   \n",
       "70   Our system is the state of the art on the CNN/...   \n",
       "71   The codes to reproduce our results are availab...   \n",
       "72        Fine-tune BERT for Extractive Summarization.   \n",
       "73   Question-answering plays an important role in ...   \n",
       "74   Inspired by the recent success of machine read...   \n",
       "75   To the best of our knowledge, no existing work...   \n",
       "76   In this work, we first build an RRC dataset ca...   \n",
       "77   Since ReviewRC has limited training examples f...   \n",
       "78   Since ReviewRC has limited training examples f...   \n",
       "79   Since ReviewRC has limited training examples f...   \n",
       "80   To show the generality of the approach, the pr...   \n",
       "81   To show the generality of the approach, the pr...   \n",
       "82   Experimental results demonstrate that the prop...   \n",
       "83   The datasets and code are available at this ht...   \n",
       "84   BERT Post-Training for Review Reading Comprehe...   \n",
       "85   Language model pre-training has proven to be u...   \n",
       "86   As a state-of-the-art language model pre-train...   \n",
       "87   In this paper, we conduct exhaustive experimen...   \n",
       "88   In this paper, we conduct exhaustive experimen...   \n",
       "89   In this paper, we conduct exhaustive experimen...   \n",
       "90   Finally, the proposed solution obtains new sta...   \n",
       "91     How to Fine-Tune BERT for Text Classification?.   \n",
       "92   We show that BERT (Devlin et al., 2018) is a M...   \n",
       "93   This formulation gives way to a natural proced...   \n",
       "94   We generate from BERT and find that it can pro...   \n",
       "95   We generate from BERT and find that it can pro...   \n",
       "96   Compared to the generations of a traditional l...   \n",
       "97   BERT has a Mouth, and It Must Speak: BERT as a...   \n",
       "98   BERT has a Mouth, and It Must Speak: BERT as a...   \n",
       "99   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "100  In this paper, we construct an auxiliary sente...   \n",
       "101  We fine-tune the pre-trained model from BERT a...   \n",
       "102  Utilizing BERT for Aspect-Based Sentiment Anal...   \n",
       "103  Following recent successes in applying BERT to...   \n",
       "104  This required confronting the challenge posed ...   \n",
       "105  This required confronting the challenge posed ...   \n",
       "106  We address this issue by applying inference on...   \n",
       "107  Experiments on TREC microblog and newswire tes...   \n",
       "108  Simple Applications of BERT for Ad Hoc Documen...   \n",
       "109  BERT-based architectures currently give state-...   \n",
       "110  BERT-based architectures currently give state-...   \n",
       "111  In the current work, we focus on the interpret...   \n",
       "112  Using a subset of GLUE tasks and a set of hand...   \n",
       "113  Our findings suggest that there is a limited s...   \n",
       "114  While different heads consistently use the sam...   \n",
       "115  We show that manually disabling attention in c...   \n",
       "116                Revealing the Dark Secrets of BERT.   \n",
       "117  Bidirectional Encoder Representations from Tra...   \n",
       "118  Recently, an upgraded version of BERT has been...   \n",
       "119  Recently, an upgraded version of BERT has been...   \n",
       "120  In this technical report, we adapt whole word ...   \n",
       "121  The model was trained on the latest Chinese Wi...   \n",
       "122  We aim to provide easy extensibility and bette...   \n",
       "123  The model is verified on various NLP tasks, ac...   \n",
       "124  Experimental results on these datasets show th...   \n",
       "125  Moreover, we also examine the effectiveness of...   \n",
       "126  Moreover, we also examine the effectiveness of...   \n",
       "127  We release the pre-trained model (both TensorF...   \n",
       "128  Pre-Training with Whole Word Masking for Chine...   \n",
       "129  BERT model has been successfully applied to op...   \n",
       "130  However, previous work trains BERT by viewing ...   \n",
       "131  To tackle this issue, we propose a multi-passa...   \n",
       "132  In addition, we find that splitting articles i...   \n",
       "133  By leveraging a passage ranker to select high-...   \n",
       "134  Experiments on four standard benchmarks showed...   \n",
       "135  In particular, on the OpenSQuAD dataset, our m...   \n",
       "136  In particular, on the OpenSQuAD dataset, our m...   \n",
       "137  Multi-passage BERT: A Globally Normalized BERT...   \n",
       "138  Multi-passage BERT: A Globally Normalized BERT...   \n",
       "139  Intent classification and slot filling are two...   \n",
       "140  They often suffer from small-scale human-label...   \n",
       "141  Recently a new language representation model, ...   \n",
       "142  However, there has not been much effort on exp...   \n",
       "143  In this work, we propose a joint intent classi...   \n",
       "144  In this work, we propose a joint intent classi...   \n",
       "145  Experimental results demonstrate that our prop...   \n",
       "146  BERT for Joint Intent Classification and Slot ...   \n",
       "147  Conversational search is an emerging topic in ...   \n",
       "148  One of the major challenges to multi-turn conv...   \n",
       "149  Existing methods either prepend history turns ...   \n",
       "150  We propose a conceptually simple yet highly ef...   \n",
       "151  It enables seamless integration of conversatio...   \n",
       "152  We first explain our view that ConvQA is a sim...   \n",
       "153  We further demonstrate the effectiveness of ou...   \n",
       "154  Finally, we analyze the impact of different nu...   \n",
       "155  BERT with History Answer Embedding for Convers...   \n",
       "156  This paper studies the performances and behavi...   \n",
       "157  We explore several different ways to leverage ...   \n",
       "158  We explore several different ways to leverage ...   \n",
       "159  Experimental results on MS MARCO demonstrate t...   \n",
       "160  Experimental results on MS MARCO demonstrate t...   \n",
       "161  Experimental results on TREC show the gaps bet...   \n",
       "162  Analyses illustrate how BERT allocates its att...   \n",
       "163  Analyses illustrate how BERT allocates its att...   \n",
       "164  Analyses illustrate how BERT allocates its att...   \n",
       "165  Analyses illustrate how BERT allocates its att...   \n",
       "166    Understanding the Behaviors of BERT in Ranking.   \n",
       "167  We present simple BERT-based models for relati...   \n",
       "168  In recent years, state-of-the-art performance ...   \n",
       "169  In this paper, extensive experiments on datase...   \n",
       "170  To our knowledge, we are the first to successf...   \n",
       "171  Our models provide strong baselines for future...   \n",
       "172  Simple BERT Models for Relation Extraction and...   \n",
       "173  Multi-task learning allows the sharing of usef...   \n",
       "174  In natural language processing several recent ...   \n",
       "175  These results are based on fine-tuning on each...   \n",
       "176  We explore the multi-task learning setting for...   \n",
       "177  We explore the multi-task learning setting for...   \n",
       "178  We introduce new adaptation modules, PALs or `...   \n",
       "179  By using PALs in parallel with BERT layers, we...   \n",
       "180  By using PALs in parallel with BERT layers, we...   \n",
       "181  BERT and PALs: Projected Attention Layers for ...   \n",
       "182  Pre-training by language modeling has become a...   \n",
       "183  In this paper we introduce a suite of diagnost...   \n",
       "184  As a case study, we apply these diagnostics to...   \n",
       "185  As a case study, we apply these diagnostics to...   \n",
       "186  As a case study, we apply these diagnostics to...   \n",
       "187  As a case study, we apply these diagnostics to...   \n",
       "188  As a case study, we apply these diagnostics to...   \n",
       "189  What BERT Is Not: Lessons from a New Suite of ...   \n",
       "190  Language model pre-training, such as BERT, has...   \n",
       "191  However, it is unclear why the pre-training-th...   \n",
       "192  In this paper, we propose to visualize loss la...   \n",
       "193  First, we find that pre-training reaches a goo...   \n",
       "194  We also demonstrate that the fine-tuning proce...   \n",
       "195  Second, the visualization results indicate tha...   \n",
       "196  Third, the lower layers of BERT are more invar...   \n",
       "197  Visualizing and Understanding the Effectivenes...   \n",
       "198  Data augmentation methods are often applied to...   \n",
       "199  Recently proposed contextual augmentation augm...   \n",
       "200  Bidirectional Encoder Representations from Tra...   \n",
       "201  We propose a novel data augmentation method fo...   \n",
       "202  We retrofit BERT to conditional BERT by introd...   \n",
       "203  We retrofit BERT to conditional BERT by introd...   \n",
       "204  We retrofit BERT to conditional BERT by introd...   \n",
       "205  In our paper, “conditional masked language mod...   \n",
       "206                                              task.   \n",
       "207  The well trained conditional BERT can be appli...   \n",
       "208  Experiments on six various different text clas...   \n",
       "209          Conditional BERT Contextual Augmentation.   \n",
       "210  We propose a practical scheme to train a singl...   \n",
       "211  Starting from a public multilingual BERT check...   \n",
       "212  We show that our model especially outperforms ...   \n",
       "213  We showcase the effectiveness of our method by...   \n",
       "214  Small and Practical BERT Models for Sequence L...   \n",
       "215  Recently, a simple combination of passage retr...   \n",
       "216  In this paper, we present a data augmentation ...   \n",
       "217  We apply a stage-wise approach to fine tuning ...   \n",
       "218  Experimental results show large gains in effec...   \n",
       "219  Data Augmentation for BERT Fine-Tuning in Open...   \n",
       "220  The BERT language model (LM) (Devlin et al., 2...   \n",
       "221                                     Petroni et al.   \n",
       "222  (2019) take this as evidence that BERT memoriz...   \n",
       "223  (2019) take this as evidence that BERT memoriz...   \n",
       "224  We take issue with this interpretation and arg...   \n",
       "225  We take issue with this interpretation and arg...   \n",
       "226  More specifically, we show that BERT's precisi...   \n",
       "227  As a remedy, we propose E-BERT, an extension o...   \n",
       "228  As a remedy, we propose E-BERT, an extension o...   \n",
       "229  E-BERT outperforms both BERT and ERNIE (Zhang ...   \n",
       "230  E-BERT outperforms both BERT and ERNIE (Zhang ...   \n",
       "231  We take this as evidence that E-BERT is richer...   \n",
       "232  We take this as evidence that E-BERT is richer...   \n",
       "233  We take this as evidence that E-BERT is richer...   \n",
       "234  BERT is Not a Knowledge Base (Yet): Factual Kn...   \n",
       "235  Replacing static word embeddings with contextu...   \n",
       "236  However, just how contextual are the contextua...   \n",
       "237  Are there infinitely many context-specific rep...   \n",
       "238  For one, we find that the contextualized repre...   \n",
       "239  While representations of the same word in diff...   \n",
       "240  This suggests that upper layers of contextuali...   \n",
       "241  In all layers of ELMo, BERT, and GPT-2, on ave...   \n",
       "242  How Contextual are Contextualized Word Represe...   \n",
       "243  Comparing the Geometry of BERT, ELMo, and GPT-...   \n",
       "\n",
       "                                          split_tokens  split_anchor_span  \\\n",
       "0    ['We', 'introduce', 'a', 'new', 'language', 'r...            (2, 18)   \n",
       "1    ['Unlike', 'recent', 'language', 'representati...             (6, 7)   \n",
       "2    ['As', 'a', 'result', ',', 'the', 'pre', '-', ...             (8, 9)   \n",
       "3    ['BERT', 'is', 'conceptually', 'simple', 'and'...             (0, 1)   \n",
       "4    ['It', 'obtains', 'new', 'state', '-', 'of', '...             (0, 1)   \n",
       "5    ['BERT', ':', 'Pre-training', 'of', 'Deep', 'B...             (0, 1)   \n",
       "6    ['Language', 'model', 'pretraining', 'has', 'l...                NaN   \n",
       "7    ['Training', 'is', 'computationally', 'expensi...                NaN   \n",
       "8    ['We', 'present', 'a', 'replication', 'study',...             (6, 7)   \n",
       "9    ['We', 'find', 'that', 'BERT', 'was', 'signifi...             (3, 4)   \n",
       "10   ['We', 'find', 'that', 'BERT', 'was', 'signifi...           (20, 21)   \n",
       "11   ['Our', 'best', 'model', 'achieves', 'state-of...                NaN   \n",
       "12   ['These', 'results', 'highlight', 'the', 'impo...                NaN   \n",
       "13   ['We', 'release', 'our', 'models', 'and', 'cod...                NaN   \n",
       "14   ['RoBERTa', ':', 'A', 'Robustly', 'Optimized',...             (0, 1)   \n",
       "15   ['As', 'Transfer', 'Learning', 'from', 'large-...                NaN   \n",
       "16   ['In', 'this', 'work', ',', 'we', 'propose', '...           (18, 19)   \n",
       "17   ['While', 'most', 'prior', 'work', 'investigat...           (37, 40)   \n",
       "18   ['While', 'most', 'prior', 'work', 'investigat...           (49, 50)   \n",
       "19   ['To', 'leverage', 'the', 'inductive', 'biases...                NaN   \n",
       "20   ['Our', 'smaller,', 'faster', 'and', 'lighter'...                NaN   \n",
       "21   ['DistilBERT', ',', 'a', 'distilled', 'version...             (0, 1)   \n",
       "22   ['Pre-trained', 'text', 'encoders', 'have', 'r...                NaN   \n",
       "23   ['We', 'focus', 'on', 'one', 'such', 'model', ...             (3, 8)   \n",
       "24   ['We', 'find', 'that', 'the', 'model', 'repres...             (3, 5)   \n",
       "25   ['Qualitative', 'analysis', 'reveals', 'that',...             (4, 6)   \n",
       "26   ['BERT', 'Rediscovers', 'the', 'Classical', 'N...             (0, 1)   \n",
       "27   ['Large', 'pre', '-', 'trained', 'neural', 'ne...             (0, 9)   \n",
       "28   ['Large', 'pre', '-', 'trained', 'neural', 'ne...           (28, 29)   \n",
       "29   ['Large', 'pre', '-', 'trained', 'neural', 'ne...             (8, 9)   \n",
       "30   ['Most', 'recent', 'analysis', 'has', 'focused...                NaN   \n",
       "31   ['Complementary', 'to', 'these', 'works', ',',...           (22, 23)   \n",
       "32   ['BERT', \"'s\", 'attention', 'heads', 'exhibit'...             (0, 2)   \n",
       "33   ['We', 'further', 'show', 'that', 'certain', '...                NaN   \n",
       "34   ['For', 'example,', 'we', 'find', 'heads', 'th...                NaN   \n",
       "35   ['Lastly', ',', 'we', 'propose', 'an', 'attent...           (23, 25)   \n",
       "36         ['What', 'Does', 'BERT', 'Look', 'At', '?']             (2, 3)   \n",
       "37   ['An', 'Analysis', 'of', 'BERT', \"'s\", 'Attent...             (3, 5)   \n",
       "38   ['Recently', ',', 'neural', 'models', 'pretrai...           (33, 34)   \n",
       "39   ['In', 'this', 'paper', ',', 'we', 'describe',...            (6, 21)   \n",
       "40   ['Our', 'system', 'is', 'the', 'state', 'of', ...             (0, 2)   \n",
       "41   ['The', 'code', 'to', 'reproduce', 'our', 'res...                NaN   \n",
       "42      ['Passage', 'Re-ranking', 'with', 'BERT', '.']             (3, 4)   \n",
       "43   ['I', 'assess', 'the', 'extent', 'to', 'which'...            (6, 11)   \n",
       "44   ['The', 'BERT', 'model', 'performs', 'remarkab...             (0, 3)   \n",
       "45   ['Assessing', 'BERT', \"'s\", 'Syntactic', 'Abil...             (1, 2)   \n",
       "46   ['Pretrained', 'contextual', 'representation',...                NaN   \n",
       "47   ['A', 'new', 'release', 'of', 'BERT', '(', 'De...             (4, 5)   \n",
       "48   ['This', 'paper', 'explores', 'the', 'broader'...             (8, 9)   \n",
       "49   ['We', 'compare', 'mBERT', 'with', 'the', 'bes...             (2, 3)   \n",
       "50   ['We', 'compare', 'mBERT', 'with', 'the', 'bes...           (19, 20)   \n",
       "51   ['Additionally', ',', 'we', 'investigate', 'th...           (10, 11)   \n",
       "52   ['Additionally', ',', 'we', 'investigate', 'th...           (19, 20)   \n",
       "53   ['Beto', ',', 'Bentz', ',', 'Becas', ':', 'The...           (11, 12)   \n",
       "54   ['Language', 'model', 'pre', '-', 'training', ...             (8, 9)   \n",
       "55   ['However,', 'pre-trained', 'language', 'model...                NaN   \n",
       "56   ['To', 'accelerate', 'inference', 'and', 'redu...                NaN   \n",
       "57   ['By', 'leveraging', 'this', 'new', 'KD', 'met...           (22, 26)   \n",
       "58   ['By', 'leveraging', 'this', 'new', 'KD', 'met...           (13, 17)   \n",
       "59   ['Moreover', ',', 'we', 'introduce', 'a', 'new...           (12, 13)   \n",
       "60   ['Moreover', ',', 'we', 'introduce', 'a', 'new...            (4, 30)   \n",
       "61   ['This', 'framework', 'ensures', 'that', 'Tiny...             (4, 5)   \n",
       "62   ['This', 'framework', 'ensures', 'that', 'Tiny...             (0, 2)   \n",
       "63   ['This', 'framework', 'ensures', 'that', 'Tiny...           (18, 21)   \n",
       "64   ['TinyBERT', 'is', 'empirically', 'effective',...             (0, 1)   \n",
       "65   ['TinyBERT', 'is', 'empirically', 'effective',...            (9, 10)   \n",
       "66   ['TinyBERT', 'is', 'also', 'significantly', 'b...             (0, 1)   \n",
       "67   ['TinyBERT', ':', 'Distilling', 'BERT', 'for',...             (0, 1)   \n",
       "68   ['BERT', ',', 'a', 'pre-trained', 'Transformer...             (0, 1)   \n",
       "69   ['In', 'this', 'paper', ',', 'we', 'describe',...             (6, 7)   \n",
       "70   ['Our', 'system', 'is', 'the', 'state', 'of', ...                NaN   \n",
       "71   ['The', 'codes', 'to', 'reproduce', 'our', 're...                NaN   \n",
       "72   ['Fine-tune', 'BERT', 'for', 'Extractive', 'Su...             (1, 2)   \n",
       "73   ['Question-answering', 'plays', 'an', 'importa...                NaN   \n",
       "74   ['Inspired', 'by', 'the', 'recent', 'success',...                NaN   \n",
       "75   ['To', 'the', 'best', 'of', 'our', 'knowledge,...                NaN   \n",
       "76   ['In', 'this', 'work,', 'we', 'first', 'build'...                NaN   \n",
       "77   ['Since', 'ReviewRC', 'has', 'limited', 'train...           (29, 34)   \n",
       "78   ['Since', 'ReviewRC', 'has', 'limited', 'train...           (43, 44)   \n",
       "79   ['Since', 'ReviewRC', 'has', 'limited', 'train...           (22, 34)   \n",
       "80   ['To', 'show', 'the', 'generality', 'of', 'the...             (5, 7)   \n",
       "81   ['To', 'show', 'the', 'generality', 'of', 'the...            (8, 13)   \n",
       "82   ['Experimental', 'results', 'demonstrate', 'th...             (4, 9)   \n",
       "83   ['The', 'datasets', 'and', 'code', 'are', 'ava...                NaN   \n",
       "84   ['BERT', 'Post-Training', 'for', 'Review', 'Re...             (0, 1)   \n",
       "85   ['Language', 'model', 'pre-training', 'has', '...                NaN   \n",
       "86   ['As', 'a', 'state', '-', 'of', '-', 'the', '-...           (16, 24)   \n",
       "87   ['In', 'this', 'paper', ',', 'we', 'conduct', ...           (16, 17)   \n",
       "88   ['In', 'this', 'paper', ',', 'we', 'conduct', ...           (27, 28)   \n",
       "89   ['In', 'this', 'paper', ',', 'we', 'conduct', ...           (23, 31)   \n",
       "90   ['Finally', ',', 'the', 'proposed', 'solution'...             (2, 5)   \n",
       "91   ['How', 'to', 'Fine-Tune', 'BERT', 'for', 'Tex...             (3, 4)   \n",
       "92   ['We', 'show', 'that', 'BERT', '(', 'Devlin', ...             (3, 4)   \n",
       "93   ['This', 'formulation', 'gives', 'way', 'to', ...           (12, 13)   \n",
       "94   ['We', 'generate', 'from', 'BERT', 'and', 'fin...             (3, 4)   \n",
       "95   ['We', 'generate', 'from', 'BERT', 'and', 'fin...             (7, 8)   \n",
       "96   ['Compared', 'to', 'the', 'generations', 'of',...           (15, 16)   \n",
       "97   ['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It'...             (0, 1)   \n",
       "98   ['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It'...             (6, 7)   \n",
       "99   ['Aspect-based', 'sentiment', 'analysis', '(AB...                NaN   \n",
       "100  ['In', 'this', 'paper,', 'we', 'construct', 'a...                NaN   \n",
       "101  ['We', 'fine-tune', 'the', 'pre-trained', 'mod...             (6, 7)   \n",
       "102  ['Utilizing', 'BERT', 'for', 'Aspect-Based', '...             (1, 2)   \n",
       "103  ['Following', 'recent', 'successes', 'in', 'ap...             (5, 6)   \n",
       "104  ['This', 'required', 'confronting', 'the', 'ch...           (17, 18)   \n",
       "105  ['This', 'required', 'confronting', 'the', 'ch...            (3, 22)   \n",
       "106  ['We', 'address', 'this', 'issue', 'by', 'appl...             (2, 4)   \n",
       "107  ['Experiments', 'on', 'TREC', 'microblog', 'an...                NaN   \n",
       "108  ['Simple', 'Applications', 'of', 'BERT', 'for'...             (3, 4)   \n",
       "109  ['BERT', '-', 'based', 'architectures', 'curre...             (0, 4)   \n",
       "110  ['BERT', '-', 'based', 'architectures', 'curre...           (30, 31)   \n",
       "111  ['In', 'the', 'current', 'work', ',', 'we', 'f...           (24, 25)   \n",
       "112  ['Using', 'a', 'subset', 'of', 'GLUE', 'tasks'...           (32, 33)   \n",
       "113  ['Our', 'findings', 'suggest', 'that', 'there'...                NaN   \n",
       "114  ['While', 'different', 'heads', 'consistently'...                NaN   \n",
       "115  ['We', 'show', 'that', 'manually', 'disabling'...           (20, 21)   \n",
       "116  ['Revealing', 'the', 'Dark', 'Secrets', 'of', ...             (5, 6)   \n",
       "117  ['Bidirectional', 'Encoder', 'Representations'...             (6, 7)   \n",
       "118  ['Recently', ',', 'an', 'upgraded', 'version',...             (6, 7)   \n",
       "119  ['Recently', ',', 'an', 'upgraded', 'version',...           (31, 32)   \n",
       "120  ['In', 'this', 'technical', 'report,', 'we', '...                NaN   \n",
       "121  ['The', 'model', 'was', 'trained', 'on', 'the'...             (0, 2)   \n",
       "122  ['We', 'aim', 'to', 'provide', 'easy', 'extens...           (10, 12)   \n",
       "123  ['The', 'model', 'is', 'verified', 'on', 'vari...             (0, 2)   \n",
       "124  ['Experimental', 'results', 'on', 'these', 'da...                NaN   \n",
       "125  ['Moreover', ',', 'we', 'also', 'examine', 'th...           (14, 15)   \n",
       "126  ['Moreover', ',', 'we', 'also', 'examine', 'th...           (18, 21)   \n",
       "127  ['We', 'release', 'the', 'pre-trained', 'model...                NaN   \n",
       "128  ['Pre-Training', 'with', 'Whole', 'Word', 'Mas...             (7, 8)   \n",
       "129  ['BERT', 'model', 'has', 'been', 'successfully...             (0, 1)   \n",
       "130  ['However', ',', 'previous', 'work', 'trains',...             (5, 6)   \n",
       "131  ['To', 'tackle', 'this', 'issue', ',', 'we', '...            (7, 25)   \n",
       "132  ['In', 'addition,', 'we', 'find', 'that', 'spl...                NaN   \n",
       "133  ['By', 'leveraging', 'a', 'passage', 'ranker',...           (11, 12)   \n",
       "134  ['Experiments', 'on', 'four', 'standard', 'ben...            (7, 12)   \n",
       "135  ['In', 'particular', ',', 'on', 'the', 'OpenSQ...           (24, 25)   \n",
       "136  ['In', 'particular', ',', 'on', 'the', 'OpenSQ...            (8, 10)   \n",
       "137  ['Multi-passage', 'BERT', ':', 'A', 'Globally'...             (1, 2)   \n",
       "138  ['Multi-passage', 'BERT', ':', 'A', 'Globally'...             (6, 7)   \n",
       "139  ['Intent', 'classification', 'and', 'slot', 'f...                NaN   \n",
       "140  ['They', 'often', 'suffer', 'from', 'small-sca...                NaN   \n",
       "141  ['Recently', 'a', 'new', 'language', 'represen...             (7, 8)   \n",
       "142  ['However', ',', 'there', 'has', 'not', 'been'...           (10, 11)   \n",
       "143  ['In', 'this', 'work', ',', 'we', 'propose', '...           (16, 17)   \n",
       "144  ['In', 'this', 'work', ',', 'we', 'propose', '...            (6, 17)   \n",
       "145  ['Experimental', 'results', 'demonstrate', 'th...             (4, 7)   \n",
       "146  ['BERT', 'for', 'Joint', 'Intent', 'Classifica...             (0, 1)   \n",
       "147  ['Conversational', 'search', 'is', 'an', 'emer...                NaN   \n",
       "148  ['One', 'of', 'the', 'major', 'challenges', 't...                NaN   \n",
       "149  ['Existing', 'methods', 'either', 'prepend', '...                NaN   \n",
       "150  ['We', 'propose', 'a', 'conceptually', 'simple...                NaN   \n",
       "151  ['It', 'enables', 'seamless', 'integration', '...           (18, 19)   \n",
       "152  ['We', 'first', 'explain', 'our', 'view', 'tha...                NaN   \n",
       "153  ['We', 'further', 'demonstrate', 'the', 'effec...                NaN   \n",
       "154  ['Finally,', 'we', 'analyze', 'the', 'impact',...                NaN   \n",
       "155  ['BERT', 'with', 'History', 'Answer', 'Embeddi...             (0, 1)   \n",
       "156  ['This', 'paper', 'studies', 'the', 'performan...             (8, 9)   \n",
       "157  ['We', 'explore', 'several', 'different', 'way...            (7, 12)   \n",
       "158  ['We', 'explore', 'several', 'different', 'way...           (16, 17)   \n",
       "159  ['Experimental', 'results', 'on', 'MS', 'MARCO...           (10, 11)   \n",
       "160  ['Experimental', 'results', 'on', 'MS', 'MARCO...           (26, 27)   \n",
       "161  ['Experimental', 'results', 'on', 'TREC', 'sho...            (9, 10)   \n",
       "162  ['Analyses', 'illustrate', 'how', 'BERT', 'all...             (3, 4)   \n",
       "163  ['Analyses', 'illustrate', 'how', 'BERT', 'all...             (5, 6)   \n",
       "164  ['Analyses', 'illustrate', 'how', 'BERT', 'all...           (13, 14)   \n",
       "165  ['Analyses', 'illustrate', 'how', 'BERT', 'all...           (18, 19)   \n",
       "166  ['Understanding', 'the', 'Behaviors', 'of', 'B...             (4, 5)   \n",
       "167  ['We', 'present', 'simple', 'BERT', '-', 'base...            (2, 14)   \n",
       "168  ['In', 'recent', 'years,', 'state-of-the-art',...                NaN   \n",
       "169  ['In', 'this', 'paper', ',', 'extensive', 'exp...           (22, 23)   \n",
       "170  ['To', 'our', 'knowledge', ',', 'we', 'are', '...           (11, 12)   \n",
       "171  ['Our', 'models', 'provide', 'strong', 'baseli...             (0, 2)   \n",
       "172  ['Simple', 'BERT', 'Models', 'for', 'Relation'...             (1, 2)   \n",
       "173  ['Multi-task', 'learning', 'allows', 'the', 's...                NaN   \n",
       "174  ['In', 'natural', 'language', 'processing', 's...                NaN   \n",
       "175  ['These', 'results', 'are', 'based', 'on', 'fi...                NaN   \n",
       "176  ['We', 'explore', 'the', 'multi-task', 'learni...            (9, 10)   \n",
       "177  ['We', 'explore', 'the', 'multi-task', 'learni...           (26, 27)   \n",
       "178  ['We', 'introduce', 'new', 'adaptation', 'modu...                NaN   \n",
       "179  ['By', 'using', 'PALs', 'in', 'parallel', 'wit...             (6, 7)   \n",
       "180  ['By', 'using', 'PALs', 'in', 'parallel', 'wit...           (15, 16)   \n",
       "181  ['BERT', 'and', 'PALs', ':', 'Projected', 'Att...             (0, 1)   \n",
       "182  ['Pre-training', 'by', 'language', 'modeling',...                NaN   \n",
       "183  ['In', 'this', 'paper', 'we', 'introduce', 'a'...                NaN   \n",
       "184  ['As', 'a', 'case', 'study', ',', 'we', 'apply...           (10, 14)   \n",
       "185  ['As', 'a', 'case', 'study', ',', 'we', 'apply...           (17, 18)   \n",
       "186  ['As', 'a', 'case', 'study', ',', 'we', 'apply...           (40, 41)   \n",
       "187  ['As', 'a', 'case', 'study', ',', 'we', 'apply...           (47, 48)   \n",
       "188  ['As', 'a', 'case', 'study', ',', 'we', 'apply...           (64, 65)   \n",
       "189  ['What', 'BERT', 'Is', 'Not', ':', 'Lessons', ...             (1, 2)   \n",
       "190  ['Language', 'model', 'pre', '-', 'training', ...             (8, 9)   \n",
       "191  ['However,', 'it', 'is', 'unclear', 'why', 'th...                NaN   \n",
       "192  ['In', 'this', 'paper', ',', 'we', 'propose', ...           (17, 18)   \n",
       "193  ['First,', 'we', 'find', 'that', 'pre-training...                NaN   \n",
       "194  ['We', 'also', 'demonstrate', 'that', 'the', '...           (16, 17)   \n",
       "195  ['Second', ',', 'the', 'visualization', 'resul...           (10, 11)   \n",
       "196  ['Third', ',', 'the', 'lower', 'layers', 'of',...             (6, 7)   \n",
       "197  ['Visualizing', 'and', 'Understanding', 'the',...             (6, 7)   \n",
       "198  ['Data', 'augmentation', 'methods', 'are', 'of...                NaN   \n",
       "199  ['Recently', 'proposed', 'contextual', 'augmen...             (0, 4)   \n",
       "200  ['Bidirectional', 'Encoder', 'Representations'...             (0, 8)   \n",
       "201  ['We', 'propose', 'a', 'novel', 'data', 'augme...            (2, 15)   \n",
       "202  ['We', 'retrofit', 'BERT', 'to', 'conditional'...             (4, 6)   \n",
       "203  ['We', 'retrofit', 'BERT', 'to', 'conditional'...             (2, 3)   \n",
       "204  ['We', 'retrofit', 'BERT', 'to', 'conditional'...           (27, 28)   \n",
       "205  ['In', 'our', 'paper,', '“conditional', 'maske...                NaN   \n",
       "206                                          ['task.']                NaN   \n",
       "207  ['The', 'well', 'trained', 'conditional', 'BER...             (4, 5)   \n",
       "208  ['Experiments', 'on', 'six', 'various', 'diffe...           (10, 12)   \n",
       "209  ['Conditional', 'BERT', 'Contextual', 'Augment...             (1, 2)   \n",
       "210  ['We', 'propose', 'a', 'practical', 'scheme', ...                NaN   \n",
       "211  ['Starting', 'from', 'a', 'public', 'multiling...             (5, 6)   \n",
       "212  ['We', 'show', 'that', 'our', 'model', 'especi...                NaN   \n",
       "213  ['We', 'showcase', 'the', 'effectiveness', 'of...                NaN   \n",
       "214  ['Small', 'and', 'Practical', 'BERT', 'Models'...             (3, 4)   \n",
       "215  ['Recently', ',', 'a', 'simple', 'combination'...           (14, 15)   \n",
       "216  ['In', 'this', 'paper,', 'we', 'present', 'a',...                NaN   \n",
       "217  ['We', 'apply', 'a', 'stage-wise', 'approach',...             (8, 9)   \n",
       "218  ['Experimental', 'results', 'show', 'large', '...                NaN   \n",
       "219  ['Data', 'Augmentation', 'for', 'BERT', 'Fine-...             (3, 4)   \n",
       "220  ['The', 'BERT', 'language', 'model', '(', 'LM'...             (0, 7)   \n",
       "221                           ['Petroni', 'et', 'al.']                NaN   \n",
       "222  ['(', '2019', ')', 'take', 'this', 'as', 'evid...             (8, 9)   \n",
       "223  ['(', '2019', ')', 'take', 'this', 'as', 'evid...            (6, 16)   \n",
       "224  ['We', 'take', 'issue', 'with', 'this', 'inter...           (12, 13)   \n",
       "225  ['We', 'take', 'issue', 'with', 'this', 'inter...             (4, 6)   \n",
       "226  ['More', 'specifically', ',', 'we', 'show', 't...             (6, 8)   \n",
       "227  ['As', 'a', 'remedy', ',', 'we', 'propose', 'E...           (13, 14)   \n",
       "228  ['As', 'a', 'remedy', ',', 'we', 'propose', 'E...            (6, 22)   \n",
       "229  ['E', '-', 'BERT', 'outperforms', 'both', 'BER...             (5, 6)   \n",
       "230  ['E', '-', 'BERT', 'outperforms', 'both', 'BER...             (0, 3)   \n",
       "231  ['We', 'take', 'this', 'as', 'evidence', 'that...           (22, 23)   \n",
       "232  ['We', 'take', 'this', 'as', 'evidence', 'that...             (6, 9)   \n",
       "233  ['We', 'take', 'this', 'as', 'evidence', 'that...           (24, 27)   \n",
       "234  ['BERT', 'is', 'Not', 'a', 'Knowledge', 'Base'...             (0, 1)   \n",
       "235  ['Replacing', 'static', 'word', 'embeddings', ...                NaN   \n",
       "236  ['However', ',', 'just', 'how', 'contextual', ...           (16, 17)   \n",
       "237  ['Are', 'there', 'infinitely', 'many', 'contex...                NaN   \n",
       "238  ['For', 'one,', 'we', 'find', 'that', 'the', '...                NaN   \n",
       "239  ['While', 'representations', 'of', 'the', 'sam...                NaN   \n",
       "240  ['This', 'suggests', 'that', 'upper', 'layers'...                NaN   \n",
       "241  ['In', 'all', 'layers', 'of', 'ELMo', ',', 'BE...             (6, 7)   \n",
       "242  ['How', 'Contextual', 'are', 'Contextualized',...                NaN   \n",
       "243  ['Comparing', 'the', 'Geometry', 'of', 'BERT',...             (4, 5)   \n",
       "\n",
       "     split_anchor_indices  within_anchor_index  \n",
       "0               (12, 134)                 43.0  \n",
       "1                (46, 50)                  0.0  \n",
       "2                (31, 35)                  0.0  \n",
       "3                  (0, 4)                  0.0  \n",
       "4                  (0, 2)                 -1.0  \n",
       "5                  (0, 4)                  0.0  \n",
       "6                     NaN                  NaN  \n",
       "7                     NaN                  NaN  \n",
       "8                (33, 37)                  0.0  \n",
       "9                (12, 16)                  0.0  \n",
       "10             (121, 123)                 -1.0  \n",
       "11                    NaN                  NaN  \n",
       "12                    NaN                  NaN  \n",
       "13                    NaN                  NaN  \n",
       "14                 (0, 7)                  2.0  \n",
       "15                    NaN                  NaN  \n",
       "16             (112, 122)                  6.0  \n",
       "17             (214, 226)                  2.0  \n",
       "18             (261, 264)                 -1.0  \n",
       "19                    NaN                  NaN  \n",
       "20                    NaN                  NaN  \n",
       "21                (0, 10)                  6.0  \n",
       "22                    NaN                  NaN  \n",
       "23               (11, 32)                 17.0  \n",
       "24               (12, 21)                 -1.0  \n",
       "25               (33, 42)                 -1.0  \n",
       "26                 (0, 4)                  0.0  \n",
       "27                (0, 48)                 44.0  \n",
       "28             (164, 168)                 -1.0  \n",
       "29               (43, 47)                  0.0  \n",
       "30                    NaN                  NaN  \n",
       "31             (130, 134)                  0.0  \n",
       "32                 (0, 7)                  0.0  \n",
       "33                    NaN                  NaN  \n",
       "34                    NaN                  NaN  \n",
       "35             (147, 154)                  0.0  \n",
       "36                (9, 13)                  0.0  \n",
       "37               (14, 21)                  0.0  \n",
       "38             (148, 152)                  0.0  \n",
       "39              (27, 102)                 32.0  \n",
       "40                (0, 10)                 -1.0  \n",
       "41                    NaN                  NaN  \n",
       "42               (23, 27)                  0.0  \n",
       "43               (28, 62)                 24.0  \n",
       "44                (0, 14)                  4.0  \n",
       "45                (9, 13)                  0.0  \n",
       "46                    NaN                  NaN  \n",
       "47               (16, 20)                  0.0  \n",
       "48               (58, 63)                  1.0  \n",
       "49               (10, 15)                  1.0  \n",
       "50             (100, 105)                  1.0  \n",
       "51               (71, 76)                  1.0  \n",
       "52             (119, 124)                  1.0  \n",
       "53               (68, 72)                  0.0  \n",
       "54               (39, 43)                  0.0  \n",
       "55                    NaN                  NaN  \n",
       "56                    NaN                  NaN  \n",
       "57             (117, 141)                 20.0  \n",
       "58               (69, 89)                 16.0  \n",
       "59               (64, 72)                  4.0  \n",
       "60              (23, 177)                 45.0  \n",
       "61               (27, 35)                  4.0  \n",
       "62                (0, 14)                 -1.0  \n",
       "63             (107, 123)                 12.0  \n",
       "64                 (0, 8)                  4.0  \n",
       "65               (70, 74)                  0.0  \n",
       "66                 (0, 8)                  4.0  \n",
       "67                 (0, 8)                  4.0  \n",
       "68                 (0, 4)                  0.0  \n",
       "69               (27, 34)                  0.0  \n",
       "70                    NaN                  NaN  \n",
       "71                    NaN                  NaN  \n",
       "72                (9, 13)                  0.0  \n",
       "73                    NaN                  NaN  \n",
       "74                    NaN                  NaN  \n",
       "75                    NaN                  NaN  \n",
       "76                    NaN                  NaN  \n",
       "77             (157, 188)                 27.0  \n",
       "78             (236, 240)                  0.0  \n",
       "79             (121, 188)                 63.0  \n",
       "80               (25, 37)                 -1.0  \n",
       "81               (40, 68)                 -1.0  \n",
       "82               (37, 65)                 -1.0  \n",
       "83                    NaN                  NaN  \n",
       "84                 (0, 4)                  0.0  \n",
       "85                    NaN                  NaN  \n",
       "86              (65, 129)                  0.0  \n",
       "87              (99, 103)                  0.0  \n",
       "88             (167, 171)                  0.0  \n",
       "89             (144, 185)                 23.0  \n",
       "90                (9, 30)                 -1.0  \n",
       "91               (16, 20)                  0.0  \n",
       "92               (12, 16)                  0.0  \n",
       "93               (74, 78)                  0.0  \n",
       "94               (16, 20)                  0.0  \n",
       "95               (35, 37)                 -1.0  \n",
       "96               (79, 83)                  0.0  \n",
       "97                 (0, 4)                  0.0  \n",
       "98               (22, 24)                 -1.0  \n",
       "99                    NaN                  NaN  \n",
       "100                   NaN                  NaN  \n",
       "101              (39, 43)                  0.0  \n",
       "102               (9, 13)                  0.0  \n",
       "103              (38, 42)                  0.0  \n",
       "104            (109, 113)                  0.0  \n",
       "105             (25, 136)                 84.0  \n",
       "106              (10, 20)                 -1.0  \n",
       "107                   NaN                  NaN  \n",
       "108              (22, 26)                  0.0  \n",
       "109               (0, 26)                  0.0  \n",
       "110            (162, 165)                 -1.0  \n",
       "111            (131, 135)                  0.0  \n",
       "112            (202, 206)                  0.0  \n",
       "113                   NaN                  NaN  \n",
       "114                   NaN                  NaN  \n",
       "115            (123, 127)                  0.0  \n",
       "116              (29, 33)                  0.0  \n",
       "117              (57, 61)                  0.0  \n",
       "118              (33, 37)                  0.0  \n",
       "119            (173, 177)                  0.0  \n",
       "120                   NaN                  NaN  \n",
       "121                (0, 9)                 -1.0  \n",
       "122              (63, 75)                  8.0  \n",
       "123                (0, 9)                 -1.0  \n",
       "124                   NaN                  NaN  \n",
       "125              (78, 82)                  0.0  \n",
       "126             (93, 103)                  0.0  \n",
       "127                   NaN                  NaN  \n",
       "128              (48, 52)                  0.0  \n",
       "129                (0, 4)                  0.0  \n",
       "130              (30, 34)                  0.0  \n",
       "131             (33, 138)                 18.0  \n",
       "132                   NaN                  NaN  \n",
       "133              (78, 82)                  0.0  \n",
       "134              (51, 75)                 20.0  \n",
       "135            (102, 106)                  0.0  \n",
       "136              (42, 51)                 -1.0  \n",
       "137              (13, 17)                  0.0  \n",
       "138              (42, 46)                  0.0  \n",
       "139                   NaN                  NaN  \n",
       "140                   NaN                  NaN  \n",
       "141              (46, 50)                  0.0  \n",
       "142              (53, 57)                  0.0  \n",
       "143              (87, 91)                  0.0  \n",
       "144              (25, 91)                 62.0  \n",
       "145              (37, 55)                 -1.0  \n",
       "146                (0, 4)                  0.0  \n",
       "147                   NaN                  NaN  \n",
       "148                   NaN                  NaN  \n",
       "149                   NaN                  NaN  \n",
       "150                   NaN                  NaN  \n",
       "151            (122, 126)                  0.0  \n",
       "152                   NaN                  NaN  \n",
       "153                   NaN                  NaN  \n",
       "154                   NaN                  NaN  \n",
       "155                (0, 4)                  0.0  \n",
       "156              (52, 56)                  0.0  \n",
       "157              (45, 67)                 18.0  \n",
       "158              (84, 86)                 -1.0  \n",
       "159              (72, 76)                  0.0  \n",
       "160            (158, 162)                  0.0  \n",
       "161              (54, 58)                  0.0  \n",
       "162              (23, 27)                  0.0  \n",
       "163              (38, 41)                 -1.0  \n",
       "164              (88, 91)                 -1.0  \n",
       "165            (117, 119)                 -1.0  \n",
       "166              (30, 34)                  0.0  \n",
       "167              (10, 87)                  7.0  \n",
       "168                   NaN                  NaN  \n",
       "169            (126, 130)                  0.0  \n",
       "170              (57, 61)                  0.0  \n",
       "171               (0, 10)                 -1.0  \n",
       "172               (6, 10)                  0.0  \n",
       "173                   NaN                  NaN  \n",
       "174                   NaN                  NaN  \n",
       "175                   NaN                  NaN  \n",
       "176              (57, 61)                  0.0  \n",
       "177            (154, 158)                  0.0  \n",
       "178                   NaN                  NaN  \n",
       "179              (30, 34)                  0.0  \n",
       "180              (83, 87)                  0.0  \n",
       "181                (0, 4)                  0.0  \n",
       "182                   NaN                  NaN  \n",
       "183                   NaN                  NaN  \n",
       "184              (47, 69)                 12.0  \n",
       "185              (85, 87)                 -1.0  \n",
       "186            (232, 234)                 -1.0  \n",
       "187            (275, 277)                 -1.0  \n",
       "188            (373, 375)                 -1.0  \n",
       "189                (4, 8)                  0.0  \n",
       "190              (39, 43)                  0.0  \n",
       "191                   NaN                  NaN  \n",
       "192            (102, 106)                  0.0  \n",
       "193                   NaN                  NaN  \n",
       "194              (91, 95)                  0.0  \n",
       "195              (62, 66)                  0.0  \n",
       "196              (27, 31)                  0.0  \n",
       "197              (50, 54)                  0.0  \n",
       "198                   NaN                  NaN  \n",
       "199               (0, 41)                 -1.0  \n",
       "200               (0, 64)                 58.0  \n",
       "201             (10, 112)                 74.0  \n",
       "202              (19, 35)                 12.0  \n",
       "203              (11, 15)                  0.0  \n",
       "204            (166, 170)                  0.0  \n",
       "205                   NaN                  NaN  \n",
       "206                   NaN                  NaN  \n",
       "207              (28, 32)                  0.0  \n",
       "208              (72, 82)                 -1.0  \n",
       "209              (11, 15)                  0.0  \n",
       "210                   NaN                  NaN  \n",
       "211              (35, 39)                  0.0  \n",
       "212                   NaN                  NaN  \n",
       "213                   NaN                  NaN  \n",
       "214              (19, 23)                  0.0  \n",
       "215              (92, 96)                  0.0  \n",
       "216                   NaN                  NaN  \n",
       "217              (45, 49)                  0.0  \n",
       "218                   NaN                  NaN  \n",
       "219              (21, 25)                  0.0  \n",
       "220               (0, 30)                  4.0  \n",
       "221                   NaN                  NaN  \n",
       "222              (35, 39)                  0.0  \n",
       "223              (21, 89)                 14.0  \n",
       "224              (72, 76)                  0.0  \n",
       "225              (18, 37)                 -1.0  \n",
       "226              (32, 39)                  0.0  \n",
       "227              (51, 55)                  0.0  \n",
       "228             (24, 117)                  4.0  \n",
       "229              (25, 29)                  0.0  \n",
       "230                (0, 8)                  4.0  \n",
       "231            (106, 110)                  0.0  \n",
       "232              (29, 37)                  4.0  \n",
       "233            (115, 123)                  4.0  \n",
       "234                (0, 4)                  0.0  \n",
       "235                   NaN                  NaN  \n",
       "236            (104, 108)                  0.0  \n",
       "237                   NaN                  NaN  \n",
       "238                   NaN                  NaN  \n",
       "239                   NaN                  NaN  \n",
       "240                   NaN                  NaN  \n",
       "241              (23, 27)                  0.0  \n",
       "242                   NaN                  NaN  \n",
       "243              (25, 29)                  0.0  \n",
       "\n",
       "[244 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_word = 'BERT'\n",
    "anchor_type = 'coreference'\n",
    "\n",
    "csv = pd.read_csv(\n",
    "    f'outputs/{search_word}/{anchor_type}.csv', \n",
    "    index_col='Unnamed: 0')\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.147461</td>\n",
       "      <td>-0.022949</td>\n",
       "      <td>0.169922</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>-0.121094</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>-0.109375</td>\n",
       "      <td>0.075684</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>-0.023560</td>\n",
       "      <td>0.115723</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>0.162109</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.023071</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>stands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.074493</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.302734</td>\n",
       "      <td>-0.024536</td>\n",
       "      <td>-0.278320</td>\n",
       "      <td>0.155518</td>\n",
       "      <td>0.194824</td>\n",
       "      <td>-0.141113</td>\n",
       "      <td>-0.006836</td>\n",
       "      <td>-0.109985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025208</td>\n",
       "      <td>-0.250977</td>\n",
       "      <td>0.134521</td>\n",
       "      <td>-0.034668</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.081177</td>\n",
       "      <td>0.179199</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0.062256</td>\n",
       "      <td>is designed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.061768</td>\n",
       "      <td>0.017456</td>\n",
       "      <td>-0.184570</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>-0.151367</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>0.207031</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.169922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>-0.283203</td>\n",
       "      <td>0.414062</td>\n",
       "      <td>-0.160156</td>\n",
       "      <td>0.408203</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>-0.240234</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>obtains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.040039</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>-0.183594</td>\n",
       "      <td>-0.047119</td>\n",
       "      <td>0.294922</td>\n",
       "      <td>-0.032471</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044434</td>\n",
       "      <td>-0.032715</td>\n",
       "      <td>0.122559</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>-0.069336</td>\n",
       "      <td>-0.150391</td>\n",
       "      <td>-0.000477</td>\n",
       "      <td>0.120605</td>\n",
       "      <td>-0.163086</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.006958</td>\n",
       "      <td>-0.043701</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>0.176758</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>-0.030518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>-0.148438</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.098145</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.102051</td>\n",
       "      <td>-0.138672</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>0.449219</td>\n",
       "      <td>0.053467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333984</td>\n",
       "      <td>-0.198242</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>-0.006927</td>\n",
       "      <td>-0.011414</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>-0.027710</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>published</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.304688</td>\n",
       "      <td>-0.092285</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>-0.023804</td>\n",
       "      <td>0.025879</td>\n",
       "      <td>-0.122070</td>\n",
       "      <td>-0.051514</td>\n",
       "      <td>0.040771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>-0.124023</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>-0.028564</td>\n",
       "      <td>-0.191406</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>called</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.308594</td>\n",
       "      <td>0.056152</td>\n",
       "      <td>-0.054443</td>\n",
       "      <td>-0.029541</td>\n",
       "      <td>-0.069336</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>0.021973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243164</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>-0.388672</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>-0.131836</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>-0.120117</td>\n",
       "      <td>0.029419</td>\n",
       "      <td>to reduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.123535</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>-0.095215</td>\n",
       "      <td>-0.242188</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>-0.169922</td>\n",
       "      <td>0.032471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363281</td>\n",
       "      <td>-0.095703</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>-0.188477</td>\n",
       "      <td>-0.032959</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>-0.253906</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.099121</td>\n",
       "      <td>retaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.160156</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>0.010986</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.248047</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.017944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>0.211914</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>focus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.079102</td>\n",
       "      <td>-0.141602</td>\n",
       "      <td>0.306641</td>\n",
       "      <td>-0.102539</td>\n",
       "      <td>-0.092285</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.172852</td>\n",
       "      <td>-0.273438</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172852</td>\n",
       "      <td>-0.046387</td>\n",
       "      <td>0.267578</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>-0.028320</td>\n",
       "      <td>0.277344</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>represents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.015625</td>\n",
       "      <td>0.204346</td>\n",
       "      <td>-0.158691</td>\n",
       "      <td>0.264648</td>\n",
       "      <td>-0.403320</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>0.108643</td>\n",
       "      <td>-0.110840</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>-0.386719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346191</td>\n",
       "      <td>-0.107178</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>-0.266724</td>\n",
       "      <td>0.324951</td>\n",
       "      <td>0.382324</td>\n",
       "      <td>0.126465</td>\n",
       "      <td>-0.080582</td>\n",
       "      <td>-0.017822</td>\n",
       "      <td>does adjust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.197754</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>-0.040405</td>\n",
       "      <td>0.179365</td>\n",
       "      <td>0.068115</td>\n",
       "      <td>-0.149780</td>\n",
       "      <td>-0.036163</td>\n",
       "      <td>-0.199707</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>-0.132324</td>\n",
       "      <td>-0.292969</td>\n",
       "      <td>-0.025528</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>have had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.146484</td>\n",
       "      <td>0.154297</td>\n",
       "      <td>0.221680</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>-0.110840</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.044189</td>\n",
       "      <td>-0.050781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>-0.021484</td>\n",
       "      <td>-0.104492</td>\n",
       "      <td>-0.302734</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>motivating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.197754</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>-0.040405</td>\n",
       "      <td>0.179365</td>\n",
       "      <td>0.068115</td>\n",
       "      <td>-0.149780</td>\n",
       "      <td>-0.036163</td>\n",
       "      <td>-0.199707</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>-0.132324</td>\n",
       "      <td>-0.292969</td>\n",
       "      <td>-0.025528</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>have had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.144531</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.030518</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.095215</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.084961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>-0.047852</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>-0.019287</td>\n",
       "      <td>apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.027802</td>\n",
       "      <td>0.323242</td>\n",
       "      <td>0.207764</td>\n",
       "      <td>-0.164917</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>-0.077148</td>\n",
       "      <td>-0.016113</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.162109</td>\n",
       "      <td>0.235718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141357</td>\n",
       "      <td>-0.039795</td>\n",
       "      <td>0.141113</td>\n",
       "      <td>-0.134033</td>\n",
       "      <td>-0.108398</td>\n",
       "      <td>0.116821</td>\n",
       "      <td>-0.381348</td>\n",
       "      <td>0.367188</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>is captured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.109375</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>-0.077637</td>\n",
       "      <td>0.277344</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052734</td>\n",
       "      <td>-0.121582</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.417969</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.099121</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.205078</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>-0.084961</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>-0.232422</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>-0.490234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294922</td>\n",
       "      <td>0.023560</td>\n",
       "      <td>0.092285</td>\n",
       "      <td>-0.211914</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>-0.097168</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>-0.062012</td>\n",
       "      <td>-0.188477</td>\n",
       "      <td>assess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>-0.148438</td>\n",
       "      <td>-0.219727</td>\n",
       "      <td>-0.198242</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>0.231445</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>-0.345703</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>-0.263672</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.345703</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.176758</td>\n",
       "      <td>0.196289</td>\n",
       "      <td>performs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.083008</td>\n",
       "      <td>-0.089844</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>-0.076660</td>\n",
       "      <td>-0.047607</td>\n",
       "      <td>-0.069336</td>\n",
       "      <td>-0.022339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>-0.004486</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>includes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.050293</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.094238</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>-0.128906</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>-0.271484</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296875</td>\n",
       "      <td>-0.337891</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>-0.382812</td>\n",
       "      <td>-0.209961</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>0.155273</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>explores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.066895</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>-0.031006</td>\n",
       "      <td>0.373047</td>\n",
       "      <td>-0.228516</td>\n",
       "      <td>-0.214844</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.192383</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>compare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.006958</td>\n",
       "      <td>-0.043701</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>0.176758</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>-0.030518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>-0.148438</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.382812</td>\n",
       "      <td>-0.156250</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.018311</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>-0.182617</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>-0.150391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090332</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>-0.253906</td>\n",
       "      <td>-0.190430</td>\n",
       "      <td>-0.012390</td>\n",
       "      <td>-0.190430</td>\n",
       "      <td>0.015991</td>\n",
       "      <td>-0.026855</td>\n",
       "      <td>investigate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.195312</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>0.034912</td>\n",
       "      <td>-0.176758</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.024048</td>\n",
       "      <td>0.043213</td>\n",
       "      <td>0.107910</td>\n",
       "      <td>-0.275391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029541</td>\n",
       "      <td>-0.092285</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>-0.184570</td>\n",
       "      <td>-0.021973</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>-0.169922</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>-0.074219</td>\n",
       "      <td>generalizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.043457</td>\n",
       "      <td>-0.058105</td>\n",
       "      <td>-0.059814</td>\n",
       "      <td>0.062988</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.054932</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>0.115723</td>\n",
       "      <td>-0.123047</td>\n",
       "      <td>-0.095215</td>\n",
       "      <td>0.028564</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.094482</td>\n",
       "      <td>-0.045410</td>\n",
       "      <td>0.308105</td>\n",
       "      <td>0.336670</td>\n",
       "      <td>-0.514648</td>\n",
       "      <td>-0.099854</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>-0.057861</td>\n",
       "      <td>0.228210</td>\n",
       "      <td>0.196533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323975</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.170654</td>\n",
       "      <td>0.409180</td>\n",
       "      <td>0.455078</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>-0.660156</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>-0.098160</td>\n",
       "      <td>can be transferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.096191</td>\n",
       "      <td>-0.059570</td>\n",
       "      <td>0.137695</td>\n",
       "      <td>0.162109</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>-0.069824</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277344</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>0.042725</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>-0.001770</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>-0.410156</td>\n",
       "      <td>-0.042480</td>\n",
       "      <td>0.292969</td>\n",
       "      <td>encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.120605</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>-0.069824</td>\n",
       "      <td>0.016113</td>\n",
       "      <td>-0.230469</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>-0.094727</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>-0.106445</td>\n",
       "      <td>-0.369141</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.120605</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>-0.069824</td>\n",
       "      <td>0.016113</td>\n",
       "      <td>-0.230469</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>-0.094727</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>-0.106445</td>\n",
       "      <td>-0.369141</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.131348</td>\n",
       "      <td>0.243652</td>\n",
       "      <td>-0.042969</td>\n",
       "      <td>-0.094727</td>\n",
       "      <td>-0.321289</td>\n",
       "      <td>-0.008057</td>\n",
       "      <td>0.126465</td>\n",
       "      <td>-0.038330</td>\n",
       "      <td>0.120056</td>\n",
       "      <td>0.143799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193604</td>\n",
       "      <td>-0.023193</td>\n",
       "      <td>0.077026</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>-0.478516</td>\n",
       "      <td>0.049316</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>can capture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.363281</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>-0.225586</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.051514</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>-0.062256</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>-0.023071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>-0.015991</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>0.065918</td>\n",
       "      <td>0.176758</td>\n",
       "      <td>ensures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.131348</td>\n",
       "      <td>0.243652</td>\n",
       "      <td>-0.042969</td>\n",
       "      <td>-0.094727</td>\n",
       "      <td>-0.321289</td>\n",
       "      <td>-0.008057</td>\n",
       "      <td>0.126465</td>\n",
       "      <td>-0.038330</td>\n",
       "      <td>0.120056</td>\n",
       "      <td>0.143799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193604</td>\n",
       "      <td>-0.023193</td>\n",
       "      <td>0.077026</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>-0.478516</td>\n",
       "      <td>0.049316</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>can capture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-0.186523</td>\n",
       "      <td>0.163940</td>\n",
       "      <td>-0.051117</td>\n",
       "      <td>0.162354</td>\n",
       "      <td>0.127686</td>\n",
       "      <td>-0.186768</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>-0.348633</td>\n",
       "      <td>0.315674</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>-0.184082</td>\n",
       "      <td>0.204285</td>\n",
       "      <td>-0.359863</td>\n",
       "      <td>-0.129150</td>\n",
       "      <td>0.124268</td>\n",
       "      <td>0.043762</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>has achieved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.109375</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>-0.077637</td>\n",
       "      <td>0.277344</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052734</td>\n",
       "      <td>-0.121582</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.417969</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.099121</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.208984</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>0.122559</td>\n",
       "      <td>-0.151367</td>\n",
       "      <td>-0.002731</td>\n",
       "      <td>-0.182617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003967</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>0.025024</td>\n",
       "      <td>0.061768</td>\n",
       "      <td>-0.094727</td>\n",
       "      <td>explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.310547</td>\n",
       "      <td>0.243164</td>\n",
       "      <td>-0.004700</td>\n",
       "      <td>-0.183594</td>\n",
       "      <td>-0.022705</td>\n",
       "      <td>0.236328</td>\n",
       "      <td>-0.001732</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>-0.079590</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324219</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>-0.059082</td>\n",
       "      <td>-0.106934</td>\n",
       "      <td>-0.033691</td>\n",
       "      <td>-0.074219</td>\n",
       "      <td>-0.180664</td>\n",
       "      <td>-0.098633</td>\n",
       "      <td>-0.014893</td>\n",
       "      <td>enhance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.208984</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>0.122559</td>\n",
       "      <td>-0.151367</td>\n",
       "      <td>-0.002731</td>\n",
       "      <td>-0.182617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003967</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>0.025024</td>\n",
       "      <td>0.061768</td>\n",
       "      <td>-0.094727</td>\n",
       "      <td>explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.027954</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>0.069092</td>\n",
       "      <td>0.253235</td>\n",
       "      <td>0.426758</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>-0.157898</td>\n",
       "      <td>0.233948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>-0.431641</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>-0.069946</td>\n",
       "      <td>-0.167236</td>\n",
       "      <td>-0.022842</td>\n",
       "      <td>0.060669</td>\n",
       "      <td>0.030029</td>\n",
       "      <td>-0.553711</td>\n",
       "      <td>To show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.027954</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>0.069092</td>\n",
       "      <td>0.253235</td>\n",
       "      <td>0.426758</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>-0.157898</td>\n",
       "      <td>0.233948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>-0.431641</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>-0.069946</td>\n",
       "      <td>-0.167236</td>\n",
       "      <td>-0.022842</td>\n",
       "      <td>0.060669</td>\n",
       "      <td>0.030029</td>\n",
       "      <td>-0.553711</td>\n",
       "      <td>To show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-0.285156</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>-0.226562</td>\n",
       "      <td>-0.020386</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>-0.021729</td>\n",
       "      <td>0.223633</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>-0.139648</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>-0.087402</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>-0.095703</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.141602</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>-0.219727</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>-0.107422</td>\n",
       "      <td>0.373047</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243164</td>\n",
       "      <td>-0.074219</td>\n",
       "      <td>0.013855</td>\n",
       "      <td>-0.429688</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>0.105957</td>\n",
       "      <td>-0.273438</td>\n",
       "      <td>achieved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.382812</td>\n",
       "      <td>-0.156250</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.018311</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>-0.182617</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>-0.150391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090332</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>-0.253906</td>\n",
       "      <td>-0.190430</td>\n",
       "      <td>-0.012390</td>\n",
       "      <td>-0.190430</td>\n",
       "      <td>0.015991</td>\n",
       "      <td>-0.026855</td>\n",
       "      <td>to investigate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-0.198242</td>\n",
       "      <td>-0.185547</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>-0.026123</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>-0.049561</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>-0.044922</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.198242</td>\n",
       "      <td>-0.185547</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>-0.026123</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>-0.049561</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>-0.044922</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-0.018677</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>-0.004578</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>0.042480</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>-0.037354</td>\n",
       "      <td>-0.022217</td>\n",
       "      <td>-0.018921</td>\n",
       "      <td>0.072754</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.138672</td>\n",
       "      <td>-0.066406</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>-0.030884</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-0.009766</td>\n",
       "      <td>0.123535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>-0.067383</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.294922</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>0.060303</td>\n",
       "      <td>gives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.146484</td>\n",
       "      <td>0.019287</td>\n",
       "      <td>0.030640</td>\n",
       "      <td>-0.122070</td>\n",
       "      <td>-0.316406</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.022949</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>-0.074707</td>\n",
       "      <td>0.021973</td>\n",
       "      <td>-0.216797</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>-0.294922</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.322266</td>\n",
       "      <td>generate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.125977</td>\n",
       "      <td>-0.038574</td>\n",
       "      <td>-0.045166</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.091309</td>\n",
       "      <td>-0.049561</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>-0.118164</td>\n",
       "      <td>-0.006683</td>\n",
       "      <td>-0.014893</td>\n",
       "      <td>-0.220703</td>\n",
       "      <td>0.213867</td>\n",
       "      <td>-0.098145</td>\n",
       "      <td>produce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.007477</td>\n",
       "      <td>0.423828</td>\n",
       "      <td>-0.021484</td>\n",
       "      <td>-0.039062</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>-0.203125</td>\n",
       "      <td>0.310547</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119141</td>\n",
       "      <td>-0.015564</td>\n",
       "      <td>0.376953</td>\n",
       "      <td>-0.085938</td>\n",
       "      <td>-0.222656</td>\n",
       "      <td>-0.066406</td>\n",
       "      <td>0.010559</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>Compared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.044922</td>\n",
       "      <td>-0.030396</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>-0.006592</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.032959</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>-0.241211</td>\n",
       "      <td>-0.057373</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>-0.109863</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>0.018311</td>\n",
       "      <td>0.027588</td>\n",
       "      <td>0.012268</td>\n",
       "      <td>-0.088379</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.378906</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>-0.221680</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>0.363281</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>-0.048828</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>-0.018555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.353516</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>-0.304688</td>\n",
       "      <td>-0.110840</td>\n",
       "      <td>-0.494141</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>0.092773</td>\n",
       "      <td>Must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-0.014099</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.035400</td>\n",
       "      <td>-0.008728</td>\n",
       "      <td>0.016968</td>\n",
       "      <td>0.060059</td>\n",
       "      <td>-0.069824</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.302734</td>\n",
       "      <td>0.025879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.092285</td>\n",
       "      <td>0.304688</td>\n",
       "      <td>0.349609</td>\n",
       "      <td>0.061768</td>\n",
       "      <td>0.025146</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>applying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-0.109863</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>-0.025146</td>\n",
       "      <td>-0.195312</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.222656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112305</td>\n",
       "      <td>-0.047119</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.090332</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>-0.292969</td>\n",
       "      <td>0.048584</td>\n",
       "      <td>-0.102539</td>\n",
       "      <td>posed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.322266</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>0.016235</td>\n",
       "      <td>0.123047</td>\n",
       "      <td>0.055908</td>\n",
       "      <td>-0.042480</td>\n",
       "      <td>0.212891</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134766</td>\n",
       "      <td>0.042725</td>\n",
       "      <td>0.141602</td>\n",
       "      <td>-0.188477</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>-0.029541</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>0.243164</td>\n",
       "      <td>confronting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.109375</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>-0.120605</td>\n",
       "      <td>-0.144531</td>\n",
       "      <td>-0.101074</td>\n",
       "      <td>0.062988</td>\n",
       "      <td>-0.076660</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>-0.048828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102051</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>0.098145</td>\n",
       "      <td>0.159180</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>-0.181641</td>\n",
       "      <td>-0.185547</td>\n",
       "      <td>-0.214844</td>\n",
       "      <td>address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.210938</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.102539</td>\n",
       "      <td>-0.119141</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>0.247070</td>\n",
       "      <td>-0.052490</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.176758</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>-0.177734</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>-0.113770</td>\n",
       "      <td>contribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.160156</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>0.010986</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.248047</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.017944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>0.211914</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>focus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.096191</td>\n",
       "      <td>-0.059570</td>\n",
       "      <td>0.137695</td>\n",
       "      <td>0.162109</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>-0.069824</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277344</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>0.042725</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>-0.001770</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>-0.410156</td>\n",
       "      <td>-0.042480</td>\n",
       "      <td>0.292969</td>\n",
       "      <td>encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.123535</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.154297</td>\n",
       "      <td>-0.224609</td>\n",
       "      <td>-0.032715</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>-0.098633</td>\n",
       "      <td>-0.197266</td>\n",
       "      <td>-0.012085</td>\n",
       "      <td>0.104004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099121</td>\n",
       "      <td>-0.005035</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>-0.229492</td>\n",
       "      <td>-0.009644</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>-0.006500</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>leads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>-0.193359</td>\n",
       "      <td>-0.006470</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.376953</td>\n",
       "      <td>0.059326</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.165039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010376</td>\n",
       "      <td>-0.193359</td>\n",
       "      <td>0.051514</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>Revealing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-0.000488</td>\n",
       "      <td>-0.103516</td>\n",
       "      <td>-0.138214</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.263428</td>\n",
       "      <td>-0.425293</td>\n",
       "      <td>-0.293945</td>\n",
       "      <td>0.262878</td>\n",
       "      <td>0.474609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170654</td>\n",
       "      <td>-0.167053</td>\n",
       "      <td>0.448730</td>\n",
       "      <td>0.141479</td>\n",
       "      <td>-0.033691</td>\n",
       "      <td>0.150635</td>\n",
       "      <td>-0.033142</td>\n",
       "      <td>-0.098785</td>\n",
       "      <td>-0.283447</td>\n",
       "      <td>has been released</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.162109</td>\n",
       "      <td>-0.102539</td>\n",
       "      <td>-0.004761</td>\n",
       "      <td>-0.014343</td>\n",
       "      <td>0.056152</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.240234</td>\n",
       "      <td>0.267578</td>\n",
       "      <td>0.363281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.044678</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>-0.275391</td>\n",
       "      <td>0.123535</td>\n",
       "      <td>-0.032471</td>\n",
       "      <td>masking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-0.253296</td>\n",
       "      <td>0.225647</td>\n",
       "      <td>0.299805</td>\n",
       "      <td>-0.088867</td>\n",
       "      <td>-0.039795</td>\n",
       "      <td>0.128418</td>\n",
       "      <td>-0.127808</td>\n",
       "      <td>-0.310059</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>-0.227051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097656</td>\n",
       "      <td>-0.452148</td>\n",
       "      <td>0.418945</td>\n",
       "      <td>-0.203979</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>-0.238281</td>\n",
       "      <td>-0.272949</td>\n",
       "      <td>0.289551</td>\n",
       "      <td>-0.136230</td>\n",
       "      <td>was trained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-0.198242</td>\n",
       "      <td>-0.185547</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>-0.026123</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>-0.049561</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>-0.044922</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>to provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.465607</td>\n",
       "      <td>-0.584961</td>\n",
       "      <td>0.127441</td>\n",
       "      <td>0.095825</td>\n",
       "      <td>-0.254883</td>\n",
       "      <td>0.253418</td>\n",
       "      <td>0.114624</td>\n",
       "      <td>-0.009277</td>\n",
       "      <td>0.452637</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027283</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.491211</td>\n",
       "      <td>0.011597</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>-0.079224</td>\n",
       "      <td>-0.371582</td>\n",
       "      <td>-0.064453</td>\n",
       "      <td>-0.089355</td>\n",
       "      <td>is verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.412109</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>-0.028809</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>-0.030640</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>-0.104492</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>-0.233398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044189</td>\n",
       "      <td>0.060791</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>-0.175781</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>-0.097656</td>\n",
       "      <td>examine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.412109</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>-0.028809</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>-0.030640</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>-0.104492</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>-0.233398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044189</td>\n",
       "      <td>0.060791</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>-0.175781</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>-0.097656</td>\n",
       "      <td>examine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-0.188477</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>-0.169922</td>\n",
       "      <td>-0.183594</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162109</td>\n",
       "      <td>0.196289</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.550781</td>\n",
       "      <td>-0.178711</td>\n",
       "      <td>-0.141602</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>-0.106445</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.209961</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.001236</td>\n",
       "      <td>-0.044434</td>\n",
       "      <td>-0.010315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>-0.279297</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.057861</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.023071</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-0.013916</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>-0.032959</td>\n",
       "      <td>0.097168</td>\n",
       "      <td>-0.006531</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>-0.443359</td>\n",
       "      <td>-0.163086</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>-0.174805</td>\n",
       "      <td>-0.180664</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>0.019653</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>-0.084473</td>\n",
       "      <td>-0.044189</td>\n",
       "      <td>gains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-0.088867</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>-0.074219</td>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.009399</td>\n",
       "      <td>-0.201172</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>-0.195312</td>\n",
       "      <td>0.065918</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198242</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.210938</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>showed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.031006</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.191406</td>\n",
       "      <td>-0.009399</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170898</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>-0.040771</td>\n",
       "      <td>-0.170898</td>\n",
       "      <td>-0.064453</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>0.093262</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>-0.002548</td>\n",
       "      <td>exploring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.103027</td>\n",
       "      <td>-0.008728</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>-0.077148</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.085938</td>\n",
       "      <td>-0.029663</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.292969</td>\n",
       "      <td>-0.204102</td>\n",
       "      <td>0.169922</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.103027</td>\n",
       "      <td>-0.008728</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>-0.077148</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.085938</td>\n",
       "      <td>-0.029663</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.292969</td>\n",
       "      <td>-0.204102</td>\n",
       "      <td>0.169922</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-0.110352</td>\n",
       "      <td>0.053223</td>\n",
       "      <td>0.005798</td>\n",
       "      <td>-0.042236</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>-0.050293</td>\n",
       "      <td>0.373047</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.384766</td>\n",
       "      <td>0.094238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>-0.263672</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>-0.605469</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>achieves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-0.047607</td>\n",
       "      <td>0.316406</td>\n",
       "      <td>0.125977</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>-0.067871</td>\n",
       "      <td>-0.186523</td>\n",
       "      <td>0.026489</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447266</td>\n",
       "      <td>-0.292969</td>\n",
       "      <td>-0.055176</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>-0.109375</td>\n",
       "      <td>-0.083496</td>\n",
       "      <td>-0.157227</td>\n",
       "      <td>-0.030151</td>\n",
       "      <td>-0.235352</td>\n",
       "      <td>built</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-0.188477</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>-0.169922</td>\n",
       "      <td>-0.183594</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162109</td>\n",
       "      <td>0.196289</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.550781</td>\n",
       "      <td>-0.178711</td>\n",
       "      <td>-0.141602</td>\n",
       "      <td>0.126953</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>-0.122559</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.369141</td>\n",
       "      <td>-0.316406</td>\n",
       "      <td>-0.015747</td>\n",
       "      <td>-0.005463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213867</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.074707</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>0.005798</td>\n",
       "      <td>-0.259766</td>\n",
       "      <td>0.208984</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>-0.064453</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>0.221680</td>\n",
       "      <td>-0.169922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.287109</td>\n",
       "      <td>-0.005554</td>\n",
       "      <td>-0.022705</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>0.332031</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>-0.120605</td>\n",
       "      <td>-0.177734</td>\n",
       "      <td>to leverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>-0.064453</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>0.221680</td>\n",
       "      <td>-0.169922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.287109</td>\n",
       "      <td>-0.005554</td>\n",
       "      <td>-0.022705</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>0.332031</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>-0.120605</td>\n",
       "      <td>-0.177734</td>\n",
       "      <td>to leverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-0.285156</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>-0.226562</td>\n",
       "      <td>-0.020386</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>-0.021729</td>\n",
       "      <td>0.223633</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>-0.139648</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>-0.087402</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>-0.095703</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.104980</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>-0.162109</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>-0.110840</td>\n",
       "      <td>-0.215820</td>\n",
       "      <td>0.051514</td>\n",
       "      <td>0.099121</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>-0.134766</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.043457</td>\n",
       "      <td>-0.058105</td>\n",
       "      <td>-0.059814</td>\n",
       "      <td>0.062988</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.054932</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>0.115723</td>\n",
       "      <td>-0.123047</td>\n",
       "      <td>-0.095215</td>\n",
       "      <td>0.028564</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.059570</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>-0.045654</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>-0.263672</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>-0.009583</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.098633</td>\n",
       "      <td>-0.007996</td>\n",
       "      <td>0.314453</td>\n",
       "      <td>allocates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.059570</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>-0.045654</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>-0.263672</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>-0.009583</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.098633</td>\n",
       "      <td>-0.007996</td>\n",
       "      <td>0.314453</td>\n",
       "      <td>allocates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.059570</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>-0.045654</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>-0.263672</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>-0.009583</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.098633</td>\n",
       "      <td>-0.007996</td>\n",
       "      <td>0.314453</td>\n",
       "      <td>allocates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.380859</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>0.028564</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.060791</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.373047</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>0.048340</td>\n",
       "      <td>-0.174805</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061035</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>0.558594</td>\n",
       "      <td>-0.006989</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>prefers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-0.186523</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>-0.144531</td>\n",
       "      <td>-0.045166</td>\n",
       "      <td>-0.034668</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196289</td>\n",
       "      <td>-0.107422</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>-0.079102</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.022705</td>\n",
       "      <td>0.213867</td>\n",
       "      <td>Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-0.227539</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>-0.117188</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>-0.038574</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.085938</td>\n",
       "      <td>-0.507812</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>-0.002609</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>-0.222656</td>\n",
       "      <td>achieve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.144531</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.030518</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.095215</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.084961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>-0.047852</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>-0.019287</td>\n",
       "      <td>to apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-0.198242</td>\n",
       "      <td>-0.185547</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>-0.056396</td>\n",
       "      <td>-0.026123</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>-0.049561</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>-0.044922</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-0.208984</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>0.122559</td>\n",
       "      <td>-0.151367</td>\n",
       "      <td>-0.002731</td>\n",
       "      <td>-0.182617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003967</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>0.025024</td>\n",
       "      <td>0.061768</td>\n",
       "      <td>-0.094727</td>\n",
       "      <td>explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.008728</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>-0.056885</td>\n",
       "      <td>0.141602</td>\n",
       "      <td>-0.099121</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>-0.031494</td>\n",
       "      <td>-0.010986</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308594</td>\n",
       "      <td>0.077637</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>-0.131836</td>\n",
       "      <td>-0.209961</td>\n",
       "      <td>0.121582</td>\n",
       "      <td>-0.066895</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>-0.245117</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.134766</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>-0.060059</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>-0.051025</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091309</td>\n",
       "      <td>-0.061768</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>-0.013611</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>-0.162109</td>\n",
       "      <td>-0.380859</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>-0.014343</td>\n",
       "      <td>-0.199219</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.127930</td>\n",
       "      <td>0.186523</td>\n",
       "      <td>-0.294922</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.047119</td>\n",
       "      <td>-0.292969</td>\n",
       "      <td>tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.144531</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.030518</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.095215</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.084961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>-0.047852</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>-0.019287</td>\n",
       "      <td>apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.067749</td>\n",
       "      <td>0.122559</td>\n",
       "      <td>-0.197266</td>\n",
       "      <td>0.132690</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>0.190674</td>\n",
       "      <td>0.320801</td>\n",
       "      <td>0.091064</td>\n",
       "      <td>0.043396</td>\n",
       "      <td>-0.166748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198486</td>\n",
       "      <td>-0.214355</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>-0.163574</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.342773</td>\n",
       "      <td>-0.196365</td>\n",
       "      <td>0.061035</td>\n",
       "      <td>0.133301</td>\n",
       "      <td>can distinguish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-0.037354</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>-0.162109</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>0.028442</td>\n",
       "      <td>-0.100586</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>-0.246094</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>-0.191406</td>\n",
       "      <td>0.192383</td>\n",
       "      <td>-0.050537</td>\n",
       "      <td>0.005646</td>\n",
       "      <td>0.046631</td>\n",
       "      <td>-0.015442</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>shows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.378906</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>-0.208008</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>0.056396</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>-0.201172</td>\n",
       "      <td>0.018311</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>-0.174805</td>\n",
       "      <td>-0.074219</td>\n",
       "      <td>-0.287109</td>\n",
       "      <td>0.088379</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>struggles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>-0.037354</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>-0.162109</td>\n",
       "      <td>0.119141</td>\n",
       "      <td>0.028442</td>\n",
       "      <td>-0.100586</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>-0.246094</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>-0.191406</td>\n",
       "      <td>0.192383</td>\n",
       "      <td>-0.050537</td>\n",
       "      <td>0.005646</td>\n",
       "      <td>0.046631</td>\n",
       "      <td>-0.015442</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>shows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.055908</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>0.098145</td>\n",
       "      <td>-0.139648</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>-0.156250</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>-0.073730</td>\n",
       "      <td>-0.015381</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>0.062256</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>Is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.043457</td>\n",
       "      <td>-0.058105</td>\n",
       "      <td>-0.059814</td>\n",
       "      <td>0.062988</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.054932</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>0.115723</td>\n",
       "      <td>-0.123047</td>\n",
       "      <td>-0.095215</td>\n",
       "      <td>0.028564</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.242188</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>-0.275391</td>\n",
       "      <td>0.388672</td>\n",
       "      <td>-0.013855</td>\n",
       "      <td>-0.328125</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>-0.054199</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>-0.392578</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>-0.104492</td>\n",
       "      <td>tuning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>-0.285156</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>-0.226562</td>\n",
       "      <td>-0.020386</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>-0.021729</td>\n",
       "      <td>0.223633</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100586</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>-0.139648</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>-0.087402</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>-0.095703</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.242188</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>-0.275391</td>\n",
       "      <td>0.388672</td>\n",
       "      <td>-0.013855</td>\n",
       "      <td>-0.328125</td>\n",
       "      <td>0.064941</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>-0.054199</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>-0.392578</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>-0.104492</td>\n",
       "      <td>tuning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.186523</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>-0.144531</td>\n",
       "      <td>-0.045166</td>\n",
       "      <td>-0.034668</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196289</td>\n",
       "      <td>-0.107422</td>\n",
       "      <td>-0.142578</td>\n",
       "      <td>-0.079102</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.022705</td>\n",
       "      <td>0.213867</td>\n",
       "      <td>Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-0.106445</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.209961</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.001236</td>\n",
       "      <td>-0.044434</td>\n",
       "      <td>-0.010315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>-0.279297</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.057861</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.023071</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.259766</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>-0.223633</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>-0.042236</td>\n",
       "      <td>-0.202148</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>-0.148438</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.423828</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.259766</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.191406</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>-0.211914</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>appeared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.097656</td>\n",
       "      <td>-0.035645</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>-0.199219</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>-0.061279</td>\n",
       "      <td>-0.013733</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047119</td>\n",
       "      <td>-0.058105</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>-0.092773</td>\n",
       "      <td>0.236328</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>-0.193359</td>\n",
       "      <td>-0.117676</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-0.018677</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>-0.004578</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>0.042480</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>-0.037354</td>\n",
       "      <td>-0.022217</td>\n",
       "      <td>-0.018921</td>\n",
       "      <td>0.072754</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>-0.042969</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.007263</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129883</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-0.037354</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>Starting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.134766</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>-0.060059</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>-0.051025</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091309</td>\n",
       "      <td>-0.061768</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>-0.013611</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.010437</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>-0.004059</td>\n",
       "      <td>0.166992</td>\n",
       "      <td>-0.066895</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>0.019287</td>\n",
       "      <td>0.116699</td>\n",
       "      <td>0.025146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>-0.027710</td>\n",
       "      <td>-0.052002</td>\n",
       "      <td>-0.271484</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>-0.191406</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.213867</td>\n",
       "      <td>argue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.098145</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>0.207031</td>\n",
       "      <td>0.285156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320312</td>\n",
       "      <td>-0.236328</td>\n",
       "      <td>0.102539</td>\n",
       "      <td>-0.029053</td>\n",
       "      <td>-0.011108</td>\n",
       "      <td>-0.184570</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>-0.103027</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>drops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>-0.106445</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.209961</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.001236</td>\n",
       "      <td>-0.044434</td>\n",
       "      <td>-0.010315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>-0.279297</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.057861</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.023071</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>-0.106445</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.209961</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.001236</td>\n",
       "      <td>-0.044434</td>\n",
       "      <td>-0.010315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018921</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>-0.279297</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.057861</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.023071</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>-0.018677</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>-0.004578</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>0.042480</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>-0.037354</td>\n",
       "      <td>-0.022217</td>\n",
       "      <td>-0.018921</td>\n",
       "      <td>0.072754</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-0.051025</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>-0.084961</td>\n",
       "      <td>0.141602</td>\n",
       "      <td>-0.029663</td>\n",
       "      <td>0.026978</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267578</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>-0.098145</td>\n",
       "      <td>0.051514</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>-0.018677</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>-0.004578</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>0.042480</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>-0.037354</td>\n",
       "      <td>-0.022217</td>\n",
       "      <td>-0.018921</td>\n",
       "      <td>0.072754</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.007050</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036377</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.106934</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>-0.102051</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>-0.004944</td>\n",
       "      <td>-0.017456</td>\n",
       "      <td>-0.059082</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162109</td>\n",
       "      <td>-0.210938</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>-0.009155</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>0.049072</td>\n",
       "      <td>-0.085449</td>\n",
       "      <td>produced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-0.237793</td>\n",
       "      <td>-0.122070</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>-0.009766</td>\n",
       "      <td>-0.106201</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>0.425293</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>-0.018311</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.003418</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>-0.044922</td>\n",
       "      <td>-0.108398</td>\n",
       "      <td>0.337891</td>\n",
       "      <td>-0.161377</td>\n",
       "      <td>be explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.032471</td>\n",
       "      <td>0.267578</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>-0.043213</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>0.277344</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.333984</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>-0.507812</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>-0.123535</td>\n",
       "      <td>-0.009460</td>\n",
       "      <td>0.116211</td>\n",
       "      <td>Comparing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.147461 -0.022949  0.169922  0.023682 -0.071289 -0.121094  0.159180   \n",
       "1   -0.074493  0.200195  0.302734 -0.024536 -0.278320  0.155518  0.194824   \n",
       "3    0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
       "4   -0.061768  0.017456 -0.184570 -0.081543 -0.151367 -0.152344  0.207031   \n",
       "8   -0.040039 -0.022095  0.003967  0.063965 -0.183594 -0.047119  0.294922   \n",
       "9   -0.006958 -0.043701 -0.167969  0.176758 -0.143555  0.045898  0.069336   \n",
       "10  -0.098145 -0.091309 -0.250000  0.073730  0.102051 -0.138672  0.067383   \n",
       "16   0.030151  0.128906  0.304688 -0.092285 -0.140625 -0.023804  0.025879   \n",
       "17  -0.308594  0.056152 -0.054443 -0.029541 -0.069336  0.044678  0.062988   \n",
       "18   0.123535  0.029297 -0.130859 -0.095215 -0.242188 -0.130859  0.039795   \n",
       "23   0.077148  0.160156  0.070801  0.010986 -0.168945  0.056641  0.248047   \n",
       "24  -0.079102 -0.141602  0.306641 -0.102539 -0.092285  0.071777  0.172852   \n",
       "25  -0.015625  0.204346 -0.158691  0.264648 -0.403320  0.029785  0.108643   \n",
       "27  -0.197754  0.023438 -0.040405  0.179365  0.068115 -0.149780 -0.036163   \n",
       "28   0.146484  0.154297  0.221680 -0.181641 -0.110840  0.044922  0.159180   \n",
       "29  -0.197754  0.023438 -0.040405  0.179365  0.068115 -0.149780 -0.036163   \n",
       "31  -0.078613 -0.144531  0.005493  0.030518  0.031494  0.010742  0.095215   \n",
       "35   0.027802  0.323242  0.207764 -0.164917 -0.218750 -0.077148 -0.016113   \n",
       "39   0.109375 -0.115723  0.061279  0.002869 -0.080566  0.125000  0.146484   \n",
       "40   0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
       "43  -0.205078  0.061035 -0.084961 -0.136719  0.121094  0.101562  0.012024   \n",
       "44   0.126953  0.180664 -0.148438 -0.219727 -0.198242  0.124512  0.049805   \n",
       "47  -0.083008 -0.089844  0.161133 -0.033447  0.086914  0.041504 -0.076660   \n",
       "48  -0.050293 -0.002747 -0.171875 -0.277344  0.257812  0.094238  0.167969   \n",
       "49   0.003632  0.090820  0.066895  0.089844  0.283203  0.215820  0.031982   \n",
       "50  -0.006958 -0.043701 -0.167969  0.176758 -0.143555  0.045898  0.069336   \n",
       "51  -0.382812 -0.156250  0.177734 -0.437500  0.028198  0.018311  0.037842   \n",
       "52   0.195312 -0.071289 -0.045898  0.034912 -0.176758  0.214844  0.024048   \n",
       "54   0.010742  0.079102  0.043457 -0.058105 -0.059814  0.062988 -0.171875   \n",
       "57  -0.094482 -0.045410  0.308105  0.336670 -0.514648 -0.099854  0.142578   \n",
       "58   0.096191 -0.059570  0.137695  0.162109 -0.312500  0.086914 -0.069824   \n",
       "59  -0.120605  0.175781 -0.069824  0.016113 -0.230469 -0.067383  0.114746   \n",
       "60  -0.120605  0.175781 -0.069824  0.016113 -0.230469 -0.067383  0.114746   \n",
       "61   0.131348  0.243652 -0.042969 -0.094727 -0.321289 -0.008057  0.126465   \n",
       "62  -0.363281 -0.062500 -0.225586 -0.046631  0.175781  0.051514  0.224609   \n",
       "63   0.131348  0.243652 -0.042969 -0.094727 -0.321289 -0.008057  0.126465   \n",
       "64   0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
       "66   0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
       "68  -0.186523  0.163940 -0.051117  0.162354  0.127686 -0.186768  0.036621   \n",
       "69   0.109375 -0.115723  0.061279  0.002869 -0.080566  0.125000  0.146484   \n",
       "77  -0.208984 -0.053467 -0.046631  0.036865  0.135742  0.142578  0.122559   \n",
       "78  -0.310547  0.243164 -0.004700 -0.183594 -0.022705  0.236328 -0.001732   \n",
       "79  -0.208984 -0.053467 -0.046631  0.036865  0.135742  0.142578  0.122559   \n",
       "80   0.027954  0.145264 -0.048340  0.037842  0.069092  0.253235  0.426758   \n",
       "81   0.027954  0.145264 -0.048340  0.037842  0.069092  0.253235  0.426758   \n",
       "82  -0.285156  0.005951 -0.226562 -0.020386  0.052490 -0.021729  0.223633   \n",
       "86  -0.141602  0.194336 -0.053467  0.168945  0.039795 -0.219727  0.130859   \n",
       "87  -0.382812 -0.156250  0.177734 -0.437500  0.028198  0.018311  0.037842   \n",
       "88  -0.198242 -0.185547 -0.112793 -0.056396 -0.026123  0.082031  0.151367   \n",
       "89  -0.198242 -0.185547 -0.112793 -0.056396 -0.026123  0.082031  0.151367   \n",
       "92  -0.018677 -0.060791 -0.140625  0.010254  0.033447 -0.004578  0.122070   \n",
       "93   0.138672 -0.066406 -0.067383 -0.129883  0.119629 -0.030884  0.065430   \n",
       "94   0.146484  0.019287  0.030640 -0.122070 -0.316406 -0.091309  0.005615   \n",
       "95  -0.125977 -0.038574 -0.045166 -0.001534 -0.071289  0.085938  0.091309   \n",
       "96  -0.007477  0.423828 -0.021484 -0.039062  0.193359  0.144531 -0.080078   \n",
       "97  -0.044922 -0.030396  0.002350 -0.006592  0.087891  0.032959 -0.094238   \n",
       "98   0.378906  0.078125 -0.221680  0.244141 -0.152344  0.363281 -0.217773   \n",
       "103 -0.014099  0.035156 -0.035400 -0.008728  0.016968  0.060059 -0.069824   \n",
       "104 -0.109863  0.257812  0.037598 -0.025146 -0.195312 -0.194336 -0.007812   \n",
       "105  0.062988  0.322266  0.135742 -0.026611  0.016235  0.123047  0.055908   \n",
       "106 -0.109375 -0.015625  0.052246 -0.120605 -0.144531 -0.101074  0.062988   \n",
       "110 -0.210938  0.006592  0.002930  0.102539 -0.119141  0.181641  0.247070   \n",
       "111  0.077148  0.160156  0.070801  0.010986 -0.168945  0.056641  0.248047   \n",
       "112  0.096191 -0.059570  0.137695  0.162109 -0.312500  0.086914 -0.069824   \n",
       "115  0.123535  0.203125  0.154297 -0.224609 -0.032715  0.057373 -0.098633   \n",
       "116  0.185547  0.099609 -0.193359 -0.006470  0.119629  0.093750  0.376953   \n",
       "118 -0.000488 -0.103516 -0.138214 -0.056641 -0.033447 -0.263428 -0.425293   \n",
       "119  0.061523  0.162109 -0.102539 -0.004761 -0.014343  0.056152  0.144531   \n",
       "121 -0.253296  0.225647  0.299805 -0.088867 -0.039795  0.128418 -0.127808   \n",
       "122 -0.198242 -0.185547 -0.112793 -0.056396 -0.026123  0.082031  0.151367   \n",
       "123 -0.465607 -0.584961  0.127441  0.095825 -0.254883  0.253418  0.114624   \n",
       "125 -0.412109  0.013123 -0.028809 -0.033447  0.044434 -0.030640  0.105469   \n",
       "126 -0.412109  0.013123 -0.028809 -0.033447  0.044434 -0.030640  0.105469   \n",
       "130 -0.188477 -0.244141  0.112305  0.143555  0.065430 -0.169922 -0.183594   \n",
       "131 -0.106445  0.016357  0.208008  0.217773 -0.209961 -0.166016  0.250000   \n",
       "133 -0.013916  0.054443 -0.032959  0.097168 -0.006531 -0.267578 -0.443359   \n",
       "134 -0.088867  0.104980 -0.074219 -0.126953  0.009399 -0.201172  0.085938   \n",
       "142 -0.072266  0.129883 -0.046631  0.031006  0.130859  0.191406 -0.009399   \n",
       "143  0.103027 -0.008728  0.239258 -0.077148  0.082520  0.183594  0.079102   \n",
       "144  0.103027 -0.008728  0.239258 -0.077148  0.082520  0.183594  0.079102   \n",
       "145 -0.110352  0.053223  0.005798 -0.042236  0.027344 -0.050293  0.373047   \n",
       "151 -0.047607  0.316406  0.125977  0.131836 -0.067871 -0.186523  0.026489   \n",
       "155 -0.188477 -0.244141  0.112305  0.143555  0.065430 -0.169922 -0.183594   \n",
       "156 -0.071289  0.109375 -0.122559  0.253906  0.233398  0.055664  0.369141   \n",
       "157  0.087402  0.035156 -0.146484 -0.064453  0.043701  0.233398  0.014954   \n",
       "158  0.087402  0.035156 -0.146484 -0.064453  0.043701  0.233398  0.014954   \n",
       "159 -0.285156  0.005951 -0.226562 -0.020386  0.052490 -0.021729  0.223633   \n",
       "160  0.104980  0.083984  0.006714 -0.100098 -0.113281  0.088867 -0.162109   \n",
       "161  0.010742  0.079102  0.043457 -0.058105 -0.059814  0.062988 -0.171875   \n",
       "162  0.059570 -0.008362  0.133789 -0.045654  0.054443 -0.263672  0.024902   \n",
       "163  0.059570 -0.008362  0.133789 -0.045654  0.054443 -0.263672  0.024902   \n",
       "164  0.059570 -0.008362  0.133789 -0.045654  0.054443 -0.263672  0.024902   \n",
       "165  0.380859  0.229492  0.028564  0.052734  0.060791  0.074219  0.373047   \n",
       "166 -0.186523 -0.165039 -0.144531 -0.045166 -0.034668  0.012390  0.111328   \n",
       "169 -0.227539  0.060547 -0.117188  0.056641  0.096191 -0.038574  0.251953   \n",
       "170 -0.078613 -0.144531  0.005493  0.030518  0.031494  0.010742  0.095215   \n",
       "171 -0.198242 -0.185547 -0.112793 -0.056396 -0.026123  0.082031  0.151367   \n",
       "176 -0.208984 -0.053467 -0.046631  0.036865  0.135742  0.142578  0.122559   \n",
       "177 -0.008728  0.101562 -0.056885  0.141602 -0.099121  0.011963  0.128906   \n",
       "179  0.134766 -0.001419  0.085449  0.058350 -0.060059  0.175781 -0.051025   \n",
       "180  0.087402  0.242188  0.203125 -0.162109 -0.380859  0.184570 -0.014343   \n",
       "184 -0.078613 -0.144531  0.005493  0.030518  0.031494  0.010742  0.095215   \n",
       "185  0.067749  0.122559 -0.197266  0.132690 -0.505859  0.190674  0.320801   \n",
       "186 -0.037354  0.083008 -0.162109  0.119141  0.028442 -0.100586  0.083496   \n",
       "187  0.378906  0.289062 -0.208008  0.132812  0.004395  0.056396  0.082520   \n",
       "188 -0.037354  0.083008 -0.162109  0.119141  0.028442 -0.100586  0.083496   \n",
       "189  0.127930 -0.055908  0.011230  0.283203 -0.031250  0.209961  0.098145   \n",
       "190  0.010742  0.079102  0.043457 -0.058105 -0.059814  0.062988 -0.171875   \n",
       "192  0.242188  0.239258  0.031494 -0.277344 -0.275391  0.388672 -0.013855   \n",
       "194 -0.285156  0.005951 -0.226562 -0.020386  0.052490 -0.021729  0.223633   \n",
       "195  0.242188  0.239258  0.031494 -0.277344 -0.275391  0.388672 -0.013855   \n",
       "197 -0.186523 -0.165039 -0.144531 -0.045166 -0.034668  0.012390  0.111328   \n",
       "201 -0.106445  0.016357  0.208008  0.217773 -0.209961 -0.166016  0.250000   \n",
       "204  0.259766  0.146484 -0.223633  0.104980 -0.042236 -0.202148  0.067871   \n",
       "207  0.097656 -0.035645  0.091797  0.151367 -0.199219  0.033447  0.199219   \n",
       "208 -0.018677 -0.060791 -0.140625  0.010254  0.033447 -0.004578  0.122070   \n",
       "211  0.054688  0.168945 -0.042969  0.235352  0.087402  0.007263 -0.080078   \n",
       "215  0.134766 -0.001419  0.085449  0.058350 -0.060059  0.175781 -0.051025   \n",
       "224  0.010437  0.007721 -0.004059  0.166992 -0.066895  0.150391  0.279297   \n",
       "226  0.049805  0.098145 -0.154297  0.011353 -0.080078  0.001221 -0.308594   \n",
       "227 -0.106445  0.016357  0.208008  0.217773 -0.209961 -0.166016  0.250000   \n",
       "228 -0.106445  0.016357  0.208008  0.217773 -0.209961 -0.166016  0.250000   \n",
       "231 -0.018677 -0.060791 -0.140625  0.010254  0.033447 -0.004578  0.122070   \n",
       "232 -0.051025  0.004150  0.024902 -0.035156  0.054443 -0.084961  0.141602   \n",
       "233 -0.018677 -0.060791 -0.140625  0.010254  0.033447 -0.004578  0.122070   \n",
       "234  0.007050 -0.073242  0.171875  0.022583 -0.132812  0.198242  0.112793   \n",
       "236 -0.102051  0.063477  0.080078 -0.004944 -0.017456 -0.059082 -0.053467   \n",
       "241 -0.237793 -0.122070  0.055664 -0.009766 -0.106201  0.124512  0.425293   \n",
       "243  0.032471  0.267578  0.124512 -0.043213  0.263672  0.156250  0.117188   \n",
       "\n",
       "            7         8         9  ...       291       292       293  \\\n",
       "0   -0.109375  0.075684  0.010925  ... -0.080566 -0.023560  0.115723   \n",
       "1   -0.141113 -0.006836 -0.109985  ... -0.025208 -0.250977  0.134521   \n",
       "3   -0.107910  0.071777  0.020874  ... -0.036377 -0.093750  0.182617   \n",
       "4    0.167969  0.132812 -0.169922  ...  0.004395 -0.283203  0.414062   \n",
       "8   -0.032471  0.100098  0.029663  ... -0.044434 -0.032715  0.122559   \n",
       "9    0.147461  0.073242 -0.030518  ...  0.135742  0.021484  0.144531   \n",
       "10  -0.075684  0.449219  0.053467  ... -0.333984 -0.198242  0.239258   \n",
       "16  -0.122070 -0.051514  0.040771  ...  0.006287 -0.124023  0.058838   \n",
       "17   0.152344  0.195312  0.021973  ...  0.243164 -0.072266  0.060547   \n",
       "18   0.010010 -0.169922  0.032471  ...  0.363281 -0.095703  0.058838   \n",
       "23  -0.265625  0.014954 -0.017944  ...  0.203125 -0.164062 -0.072266   \n",
       "24  -0.273438  0.209961  0.189453  ... -0.172852 -0.046387  0.267578   \n",
       "25  -0.110840  0.001007 -0.386719  ...  0.346191 -0.107178  0.050781   \n",
       "27  -0.199707  0.019714  0.484375  ...  0.052490 -0.041992  0.246094   \n",
       "28   0.171875  0.044189 -0.050781  ...  0.173828 -0.021484 -0.104492   \n",
       "29  -0.199707  0.019714  0.484375  ...  0.052490 -0.041992  0.246094   \n",
       "31   0.062500 -0.125000 -0.084961  ...  0.075195  0.087402  0.054443   \n",
       "35  -0.063477  0.162109  0.235718  ... -0.141357 -0.039795  0.141113   \n",
       "39  -0.077637  0.277344 -0.147461  ... -0.052734 -0.121582  0.019043   \n",
       "40  -0.107910  0.071777  0.020874  ... -0.036377 -0.093750  0.182617   \n",
       "43  -0.232422  0.343750 -0.490234  ...  0.294922  0.023560  0.092285   \n",
       "44  -0.155273  0.231445 -0.167969  ...  0.083984 -0.345703  0.419922   \n",
       "47  -0.047607 -0.069336 -0.022339  ... -0.147461  0.121094  0.151367   \n",
       "48  -0.128906  0.031250 -0.271484  ... -0.296875 -0.337891  0.086426   \n",
       "49  -0.142578  0.320312 -0.075684  ...  0.359375 -0.031006  0.373047   \n",
       "50   0.147461  0.073242 -0.030518  ...  0.135742  0.021484  0.144531   \n",
       "51  -0.182617  0.088867 -0.150391  ...  0.090332  0.013611  0.218750   \n",
       "52   0.043213  0.107910 -0.275391  ... -0.029541 -0.092285  0.081055   \n",
       "54  -0.054932  0.147461 -0.094238  ... -0.056641 -0.132812  0.205078   \n",
       "57  -0.057861  0.228210  0.196533  ...  0.323975 -0.081543  0.170654   \n",
       "58   0.257812  0.076660  0.146484  ...  0.277344  0.037598  0.042725   \n",
       "59  -0.094727  0.058838  0.153320  ...  0.142578 -0.022095 -0.106445   \n",
       "60  -0.094727  0.058838  0.153320  ...  0.142578 -0.022095 -0.106445   \n",
       "61  -0.038330  0.120056  0.143799  ...  0.193604 -0.023193  0.077026   \n",
       "62  -0.062256  0.101562 -0.023071  ...  0.181641 -0.015991  0.110352   \n",
       "63  -0.038330  0.120056  0.143799  ...  0.193604 -0.023193  0.077026   \n",
       "64  -0.107910  0.071777  0.020874  ... -0.036377 -0.093750  0.182617   \n",
       "66  -0.107910  0.071777  0.020874  ... -0.036377 -0.093750  0.182617   \n",
       "68  -0.348633  0.315674  0.523438  ...  0.184570 -0.184082  0.204285   \n",
       "69  -0.077637  0.277344 -0.147461  ... -0.052734 -0.121582  0.019043   \n",
       "77  -0.151367 -0.002731 -0.182617  ... -0.003967 -0.113281 -0.075684   \n",
       "78   0.031738 -0.079590  0.002365  ...  0.324219 -0.152344 -0.059082   \n",
       "79  -0.151367 -0.002731 -0.182617  ... -0.003967 -0.113281 -0.075684   \n",
       "80   0.045898 -0.157898  0.233948  ...  0.044434 -0.431641  0.131836   \n",
       "81   0.045898 -0.157898  0.233948  ...  0.044434 -0.431641  0.131836   \n",
       "82  -0.129883  0.039062  0.103516  ...  0.100586 -0.248047  0.115234   \n",
       "86  -0.107422  0.373047  0.296875  ...  0.243164 -0.074219  0.013855   \n",
       "87  -0.182617  0.088867 -0.150391  ...  0.090332  0.013611  0.218750   \n",
       "88   0.086426  0.079590  0.036865  ...  0.074219  0.215820  0.071777   \n",
       "89   0.086426  0.079590  0.036865  ...  0.074219  0.215820  0.071777   \n",
       "92  -0.090820  0.009094  0.249023  ... -0.075195 -0.207031  0.042480   \n",
       "93  -0.187500 -0.009766  0.123535  ...  0.016479 -0.067383  0.089355   \n",
       "94   0.022949  0.067383  0.193359  ...  0.312500 -0.074707  0.021973   \n",
       "95  -0.049561 -0.056396  0.174805  ...  0.097656 -0.130859  0.229492   \n",
       "96  -0.203125  0.310547  0.166016  ... -0.119141 -0.015564  0.376953   \n",
       "97  -0.241211 -0.057373  0.226562  ... -0.058594 -0.109863  0.190430   \n",
       "98  -0.048828 -0.143555 -0.018555  ...  0.217773 -0.353516  0.271484   \n",
       "103 -0.116699 -0.302734  0.025879  ...  0.110352 -0.152344  0.201172   \n",
       "104  0.044678  0.085449  0.222656  ... -0.112305 -0.047119  0.183594   \n",
       "105 -0.042480  0.212891  0.178711  ... -0.134766  0.042725  0.141602   \n",
       "106 -0.076660  0.129883 -0.048828  ... -0.102051 -0.075684  0.098145   \n",
       "110 -0.052490  0.219727  0.079590  ...  0.283203  0.176758  0.099609   \n",
       "111 -0.265625  0.014954 -0.017944  ...  0.203125 -0.164062 -0.072266   \n",
       "112  0.257812  0.076660  0.146484  ...  0.277344  0.037598  0.042725   \n",
       "115 -0.197266 -0.012085  0.104004  ...  0.099121 -0.005035  0.057373   \n",
       "116  0.059326  0.003876  0.165039  ...  0.010376 -0.193359  0.051514   \n",
       "118 -0.293945  0.262878  0.474609  ... -0.170654 -0.167053  0.448730   \n",
       "119  0.240234  0.267578  0.363281  ...  0.206055  0.044678  0.239258   \n",
       "121 -0.310059 -0.008301 -0.227051  ... -0.097656 -0.452148  0.418945   \n",
       "122  0.086426  0.079590  0.036865  ...  0.074219  0.215820  0.071777   \n",
       "123 -0.009277  0.452637  0.022072  ... -0.027283 -0.001465  0.491211   \n",
       "125 -0.104492  0.239258 -0.233398  ... -0.044189  0.060791  0.249023   \n",
       "126 -0.104492  0.239258 -0.233398  ... -0.044189  0.060791  0.249023   \n",
       "130 -0.116699 -0.341797  0.048096  ... -0.162109  0.196289  0.183594   \n",
       "131 -0.001236 -0.044434 -0.010315  ...  0.018921  0.251953  0.124512   \n",
       "133 -0.163086  0.166016  0.269531  ...  0.010742  0.056641 -0.174805   \n",
       "134 -0.195312  0.065918  0.227539  ... -0.198242 -0.060791  0.271484   \n",
       "142 -0.145508  0.021729 -0.147461  ... -0.170898 -0.115723 -0.040771   \n",
       "143 -0.085938 -0.029663  0.281250  ...  0.144531 -0.161133  0.265625   \n",
       "144 -0.085938 -0.029663  0.281250  ...  0.144531 -0.161133  0.265625   \n",
       "145  0.032227  0.384766  0.094238  ...  0.184570 -0.263672  0.142578   \n",
       "151 -0.090820  0.148438  0.242188  ...  0.447266 -0.292969 -0.055176   \n",
       "155 -0.116699 -0.341797  0.048096  ... -0.162109  0.196289  0.183594   \n",
       "156 -0.316406 -0.015747 -0.005463  ... -0.213867  0.110352  0.074707   \n",
       "157 -0.200195  0.221680 -0.169922  ...  0.500000 -0.287109 -0.005554   \n",
       "158 -0.200195  0.221680 -0.169922  ...  0.500000 -0.287109 -0.005554   \n",
       "159 -0.129883  0.039062  0.103516  ...  0.100586 -0.248047  0.115234   \n",
       "160 -0.080078  0.144531  0.153320  ...  0.071289 -0.110840 -0.215820   \n",
       "161 -0.054932  0.147461 -0.094238  ... -0.056641 -0.132812  0.205078   \n",
       "162 -0.250000  0.209961 -0.308594  ...  0.010193  0.168945 -0.009583   \n",
       "163 -0.250000  0.209961 -0.308594  ...  0.010193  0.168945 -0.009583   \n",
       "164 -0.250000  0.209961 -0.308594  ...  0.010193  0.168945 -0.009583   \n",
       "165 -0.310547  0.048340 -0.174805  ... -0.061035  0.074219  0.046875   \n",
       "166 -0.073242  0.035156 -0.194336  ... -0.196289 -0.107422 -0.142578   \n",
       "169  0.017822  0.180664  0.083496  ...  0.353516 -0.125000 -0.085938   \n",
       "170  0.062500 -0.125000 -0.084961  ...  0.075195  0.087402  0.054443   \n",
       "171  0.086426  0.079590  0.036865  ...  0.074219  0.215820  0.071777   \n",
       "176 -0.151367 -0.002731 -0.182617  ... -0.003967 -0.113281 -0.075684   \n",
       "177 -0.031494 -0.010986  0.265625  ...  0.308594  0.077637  0.146484   \n",
       "179 -0.080078 -0.080566  0.187500  ...  0.091309 -0.061768  0.063477   \n",
       "180 -0.199219  0.146484  0.177734  ...  0.146484 -0.289062 -0.127930   \n",
       "184  0.062500 -0.125000 -0.084961  ...  0.075195  0.087402  0.054443   \n",
       "185  0.091064  0.043396 -0.166748  ...  0.198486 -0.214355  0.061035   \n",
       "186 -0.246094  0.083496  0.138672  ... -0.200195 -0.191406  0.192383   \n",
       "187 -0.201172  0.018311 -0.132812  ... -0.145508 -0.174805 -0.074219   \n",
       "188 -0.246094  0.083496  0.138672  ... -0.200195 -0.191406  0.192383   \n",
       "189 -0.139648  0.021729  0.003204  ...  0.251953 -0.156250  0.253906   \n",
       "190 -0.054932  0.147461 -0.094238  ... -0.056641 -0.132812  0.205078   \n",
       "192 -0.328125  0.064941  0.127930  ...  0.206055 -0.113281 -0.054199   \n",
       "194 -0.129883  0.039062  0.103516  ...  0.100586 -0.248047  0.115234   \n",
       "195 -0.328125  0.064941  0.127930  ...  0.206055 -0.113281 -0.054199   \n",
       "197 -0.073242  0.035156 -0.194336  ... -0.196289 -0.107422 -0.142578   \n",
       "201 -0.001236 -0.044434 -0.010315  ...  0.018921  0.251953  0.124512   \n",
       "204 -0.148438  0.145508 -0.082520  ... -0.423828  0.078125  0.259766   \n",
       "207 -0.061279 -0.013733 -0.020264  ...  0.047119 -0.058105  0.070801   \n",
       "208 -0.090820  0.009094  0.249023  ... -0.075195 -0.207031  0.042480   \n",
       "211  0.144531  0.018066  0.020996  ...  0.129883  0.026245  0.115234   \n",
       "215 -0.080078 -0.080566  0.187500  ...  0.091309 -0.061768  0.063477   \n",
       "224  0.019287  0.116699  0.025146  ...  0.119629  0.096680 -0.027710   \n",
       "226  0.139648  0.207031  0.285156  ... -0.320312 -0.236328  0.102539   \n",
       "227 -0.001236 -0.044434 -0.010315  ...  0.018921  0.251953  0.124512   \n",
       "228 -0.001236 -0.044434 -0.010315  ...  0.018921  0.251953  0.124512   \n",
       "231 -0.090820  0.009094  0.249023  ... -0.075195 -0.207031  0.042480   \n",
       "232 -0.029663  0.026978  0.040039  ...  0.267578  0.013062  0.235352   \n",
       "233 -0.090820  0.009094  0.249023  ... -0.075195 -0.207031  0.042480   \n",
       "234 -0.107910  0.071777  0.020874  ... -0.036377 -0.093750  0.182617   \n",
       "236 -0.207031  0.201172  0.283203  ... -0.162109 -0.210938  0.210938   \n",
       "241 -0.030273 -0.018311 -0.065430  ...  0.006348 -0.003418  0.082520   \n",
       "243 -0.086914  0.277344  0.163086  ...  0.175781  0.083984  0.333984   \n",
       "\n",
       "          294       295       296       297       298       299  \\\n",
       "0    0.065430  0.162109  0.001762  0.076172  0.023071  0.132812   \n",
       "1   -0.034668  0.257812  0.081177  0.179199  0.129883  0.062256   \n",
       "3    0.027100  0.127930 -0.024780  0.011230  0.164062  0.106934   \n",
       "4   -0.160156  0.408203  0.263672 -0.240234  0.017334 -0.088867   \n",
       "8   -0.035156 -0.069336 -0.150391 -0.000477  0.120605 -0.163086   \n",
       "9   -0.148438 -0.143555  0.161133  0.138672  0.233398  0.057617   \n",
       "10   0.090820 -0.006927 -0.011414  0.081543 -0.027710 -0.096680   \n",
       "16  -0.028564 -0.191406  0.079102  0.133789  0.140625  0.039062   \n",
       "17  -0.388672  0.082520 -0.131836  0.001602 -0.120117  0.029419   \n",
       "18  -0.188477 -0.032959  0.086426 -0.253906  0.109375  0.099121   \n",
       "23   0.061523 -0.310547  0.211914  0.145508  0.187500 -0.136719   \n",
       "24  -0.152344 -0.034912  0.151367 -0.028320  0.277344  0.083984   \n",
       "25  -0.266724  0.324951  0.382324  0.126465 -0.080582 -0.017822   \n",
       "27   0.031738  0.010010 -0.132324 -0.292969 -0.025528 -0.115723   \n",
       "28  -0.302734  0.019531  0.132812  0.045898  0.099609  0.359375   \n",
       "29   0.031738  0.010010 -0.132324 -0.292969 -0.025528 -0.115723   \n",
       "31   0.087402  0.185547  0.263672 -0.047852  0.193359 -0.019287   \n",
       "35  -0.134033 -0.108398  0.116821 -0.381348  0.367188  0.020996   \n",
       "39  -0.125000 -0.417969  0.291016  0.018921  0.099121 -0.045898   \n",
       "40   0.027100  0.127930 -0.024780  0.011230  0.164062  0.106934   \n",
       "43  -0.211914 -0.007629 -0.097168 -0.018555 -0.062012 -0.188477   \n",
       "44  -0.263672  0.562500  0.345703  0.088867  0.176758  0.196289   \n",
       "47  -0.048096  0.036865  0.039062 -0.004486  0.082031  0.188477   \n",
       "48  -0.382812 -0.209961  0.184570  0.012634  0.155273  0.283203   \n",
       "49  -0.228516 -0.214844  0.218750 -0.168945 -0.192383  0.010132   \n",
       "50  -0.148438 -0.143555  0.161133  0.138672  0.233398  0.057617   \n",
       "51  -0.253906 -0.190430 -0.012390 -0.190430  0.015991 -0.026855   \n",
       "52  -0.184570 -0.021973  0.173828 -0.169922  0.069824 -0.074219   \n",
       "54   0.115723 -0.123047 -0.095215  0.028564  0.014832 -0.171875   \n",
       "57   0.409180  0.455078  0.112305 -0.660156  0.187500 -0.098160   \n",
       "58   0.105469 -0.001770 -0.166016 -0.410156 -0.042480  0.292969   \n",
       "59  -0.369141 -0.073242  0.128906  0.227539  0.079102 -0.108887   \n",
       "60  -0.369141 -0.073242  0.128906  0.227539  0.079102 -0.108887   \n",
       "61  -0.293945  0.131836  0.291016 -0.478516  0.049316 -0.058594   \n",
       "62   0.015747  0.215820 -0.310547 -0.152344  0.065918  0.176758   \n",
       "63  -0.293945  0.131836  0.291016 -0.478516  0.049316 -0.058594   \n",
       "64   0.027100  0.127930 -0.024780  0.011230  0.164062  0.106934   \n",
       "66   0.027100  0.127930 -0.024780  0.011230  0.164062  0.106934   \n",
       "68  -0.359863 -0.129150  0.124268  0.043762  0.017578 -0.289062   \n",
       "69  -0.125000 -0.417969  0.291016  0.018921  0.099121 -0.045898   \n",
       "77  -0.289062 -0.104004  0.163086  0.025024  0.061768 -0.094727   \n",
       "78  -0.106934 -0.033691 -0.074219 -0.180664 -0.098633 -0.014893   \n",
       "79  -0.289062 -0.104004  0.163086  0.025024  0.061768 -0.094727   \n",
       "80  -0.069946 -0.167236 -0.022842  0.060669  0.030029 -0.553711   \n",
       "81  -0.069946 -0.167236 -0.022842  0.060669  0.030029 -0.553711   \n",
       "82  -0.139648 -0.104004  0.113770 -0.087402  0.289062 -0.095703   \n",
       "86  -0.429688 -0.147461  0.096680  0.031494  0.105957 -0.273438   \n",
       "87  -0.253906 -0.190430 -0.012390 -0.190430  0.015991 -0.026855   \n",
       "88  -0.049561  0.145508  0.086426 -0.044922  0.148438 -0.248047   \n",
       "89  -0.049561  0.145508  0.086426 -0.044922  0.148438 -0.248047   \n",
       "92  -0.048340 -0.037354 -0.022217 -0.018921  0.072754 -0.310547   \n",
       "93  -0.294922  0.210938  0.076660  0.072266 -0.056641  0.060303   \n",
       "94  -0.216797 -0.012817  0.173828 -0.294922  0.132812 -0.322266   \n",
       "95  -0.118164 -0.006683 -0.014893 -0.220703  0.213867 -0.098145   \n",
       "96  -0.085938 -0.222656 -0.066406  0.010559 -0.094238  0.140625   \n",
       "97   0.069824  0.018311  0.027588  0.012268 -0.088379 -0.015625   \n",
       "98  -0.132812 -0.304688 -0.110840 -0.494141  0.033691  0.092773   \n",
       "103  0.092285  0.304688  0.349609  0.061768  0.025146  0.148438   \n",
       "104  0.002548  0.090332  0.013611 -0.292969  0.048584 -0.102539   \n",
       "105 -0.188477 -0.044678 -0.029541  0.018433  0.174805  0.243164   \n",
       "106  0.159180  0.016357 -0.132812 -0.181641 -0.185547 -0.214844   \n",
       "110 -0.165039  0.023438  0.001549 -0.177734  0.296875 -0.113770   \n",
       "111  0.061523 -0.310547  0.211914  0.145508  0.187500 -0.136719   \n",
       "112  0.105469 -0.001770 -0.166016 -0.410156 -0.042480  0.292969   \n",
       "115 -0.229492 -0.009644  0.199219 -0.082520 -0.006500  0.179688   \n",
       "116 -0.310547 -0.562500  0.130859  0.071777  0.083984  0.048828   \n",
       "118  0.141479 -0.033691  0.150635 -0.033142 -0.098785 -0.283447   \n",
       "119 -0.168945 -0.143555  0.251953 -0.275391  0.123535 -0.032471   \n",
       "121 -0.203979  0.036133 -0.238281 -0.272949  0.289551 -0.136230   \n",
       "122 -0.049561  0.145508  0.086426 -0.044922  0.148438 -0.248047   \n",
       "123  0.011597  0.181641 -0.079224 -0.371582 -0.064453 -0.089355   \n",
       "125 -0.217773 -0.194336  0.068848 -0.175781 -0.062500 -0.097656   \n",
       "126 -0.217773 -0.194336  0.068848 -0.175781 -0.062500 -0.097656   \n",
       "130 -0.550781 -0.178711 -0.141602  0.126953  0.031250 -0.314453   \n",
       "131 -0.279297 -0.065918  0.057861  0.167969  0.025757  0.023071   \n",
       "133 -0.180664 -0.152344  0.019653  0.148438 -0.084473 -0.044189   \n",
       "134 -0.168945 -0.055420  0.022583 -0.210938  0.257812 -0.091309   \n",
       "142 -0.170898 -0.064453  0.320312  0.093262  0.087402 -0.002548   \n",
       "143  0.292969 -0.204102  0.169922  0.046387  0.110352 -0.168945   \n",
       "144  0.292969 -0.204102  0.169922  0.046387  0.110352 -0.168945   \n",
       "145 -0.605469  0.147461  0.059570  0.062500  0.100586  0.184570   \n",
       "151 -0.034180 -0.109375 -0.083496 -0.157227 -0.030151 -0.235352   \n",
       "155 -0.550781 -0.178711 -0.141602  0.126953  0.031250 -0.314453   \n",
       "156  0.235352  0.046387  0.005798 -0.259766  0.208984 -0.019531   \n",
       "157 -0.022705 -0.055420  0.332031  0.099609 -0.120605 -0.177734   \n",
       "158 -0.022705 -0.055420  0.332031  0.099609 -0.120605 -0.177734   \n",
       "159 -0.139648 -0.104004  0.113770 -0.087402  0.289062 -0.095703   \n",
       "160  0.051514  0.099121  0.007507 -0.145508 -0.134766  0.156250   \n",
       "161  0.115723 -0.123047 -0.095215  0.028564  0.014832 -0.171875   \n",
       "162 -0.137695  0.227539  0.127930 -0.098633 -0.007996  0.314453   \n",
       "163 -0.137695  0.227539  0.127930 -0.098633 -0.007996  0.314453   \n",
       "164 -0.137695  0.227539  0.127930 -0.098633 -0.007996  0.314453   \n",
       "165 -0.277344  0.089355  0.558594 -0.006989 -0.034180  0.343750   \n",
       "166 -0.079102 -0.161133 -0.267578  0.062988  0.022705  0.213867   \n",
       "169 -0.507812 -0.155273 -0.002609  0.068359  0.136719 -0.222656   \n",
       "170  0.087402  0.185547  0.263672 -0.047852  0.193359 -0.019287   \n",
       "171 -0.049561  0.145508  0.086426 -0.044922  0.148438 -0.248047   \n",
       "176 -0.289062 -0.104004  0.163086  0.025024  0.061768 -0.094727   \n",
       "177 -0.131836 -0.209961  0.121582 -0.066895  0.093750 -0.245117   \n",
       "179 -0.013611 -0.055420  0.224609 -0.056641 -0.267578  0.108398   \n",
       "180  0.186523 -0.294922 -0.161133  0.032227  0.047119 -0.292969   \n",
       "184  0.087402  0.185547  0.263672 -0.047852  0.193359 -0.019287   \n",
       "185 -0.163574  0.109375  0.342773 -0.196365  0.061035  0.133301   \n",
       "186 -0.050537  0.005646  0.046631 -0.015442  0.058594 -0.080078   \n",
       "187 -0.287109  0.088379  0.113770  0.043945  0.209961  0.064941   \n",
       "188 -0.050537  0.005646  0.046631 -0.015442  0.058594 -0.080078   \n",
       "189  0.024414 -0.073730 -0.015381 -0.147461  0.062256  0.185547   \n",
       "190  0.115723 -0.123047 -0.095215  0.028564  0.014832 -0.171875   \n",
       "192  0.200195 -0.392578 -0.165039  0.257812 -0.015869 -0.104492   \n",
       "194 -0.139648 -0.104004  0.113770 -0.087402  0.289062 -0.095703   \n",
       "195  0.200195 -0.392578 -0.165039  0.257812 -0.015869 -0.104492   \n",
       "197 -0.079102 -0.161133 -0.267578  0.062988  0.022705  0.213867   \n",
       "201 -0.279297 -0.065918  0.057861  0.167969  0.025757  0.023071   \n",
       "204  0.216797  0.191406  0.175781 -0.211914  0.188477 -0.019775   \n",
       "207 -0.092773  0.236328  0.063477 -0.193359 -0.117676  0.050781   \n",
       "208 -0.048340 -0.037354 -0.022217 -0.018921  0.072754 -0.310547   \n",
       "211  0.082520 -0.187500 -0.037354  0.081543  0.139648  0.117676   \n",
       "215 -0.013611 -0.055420  0.224609 -0.056641 -0.267578  0.108398   \n",
       "224 -0.052002 -0.271484  0.043945 -0.191406  0.269531  0.213867   \n",
       "226 -0.029053 -0.011108 -0.184570  0.100098 -0.103027  0.149414   \n",
       "227 -0.279297 -0.065918  0.057861  0.167969  0.025757  0.023071   \n",
       "228 -0.279297 -0.065918  0.057861  0.167969  0.025757  0.023071   \n",
       "231 -0.048340 -0.037354 -0.022217 -0.018921  0.072754 -0.310547   \n",
       "232  0.005768  0.066406  0.179688 -0.098145  0.051514  0.012329   \n",
       "233 -0.048340 -0.037354 -0.022217 -0.018921  0.072754 -0.310547   \n",
       "234  0.027100  0.127930 -0.024780  0.011230  0.164062  0.106934   \n",
       "236 -0.194336 -0.140625 -0.009155 -0.244141  0.049072 -0.085449   \n",
       "241  0.015625  0.121094 -0.044922 -0.108398  0.337891 -0.161377   \n",
       "243 -0.248047 -0.507812  0.094727 -0.123535 -0.009460  0.116211   \n",
       "\n",
       "                   word  \n",
       "0                stands  \n",
       "1           is designed  \n",
       "3                    is  \n",
       "4               obtains  \n",
       "8               present  \n",
       "9                  find  \n",
       "10            published  \n",
       "16               called  \n",
       "17            to reduce  \n",
       "18            retaining  \n",
       "23                focus  \n",
       "24           represents  \n",
       "25          does adjust  \n",
       "27             have had  \n",
       "28           motivating  \n",
       "29             have had  \n",
       "31                apply  \n",
       "35          is captured  \n",
       "39             describe  \n",
       "40                   is  \n",
       "43               assess  \n",
       "44             performs  \n",
       "47             includes  \n",
       "48             explores  \n",
       "49              compare  \n",
       "50                 find  \n",
       "51          investigate  \n",
       "52          generalizes  \n",
       "54                  pre  \n",
       "57   can be transferred  \n",
       "58              encoded  \n",
       "59            introduce  \n",
       "60            introduce  \n",
       "61          can capture  \n",
       "62              ensures  \n",
       "63          can capture  \n",
       "64                   is  \n",
       "66                   is  \n",
       "68         has achieved  \n",
       "69             describe  \n",
       "77              explore  \n",
       "78              enhance  \n",
       "79              explore  \n",
       "80              To show  \n",
       "81              To show  \n",
       "82          demonstrate  \n",
       "86             achieved  \n",
       "87       to investigate  \n",
       "88              provide  \n",
       "89              provide  \n",
       "92                 show  \n",
       "93                gives  \n",
       "94             generate  \n",
       "95              produce  \n",
       "96             Compared  \n",
       "97                  has  \n",
       "98                 Must  \n",
       "103            applying  \n",
       "104               posed  \n",
       "105         confronting  \n",
       "106             address  \n",
       "110          contribute  \n",
       "111               focus  \n",
       "112             encoded  \n",
       "115               leads  \n",
       "116           Revealing  \n",
       "118   has been released  \n",
       "119             masking  \n",
       "121         was trained  \n",
       "122          to provide  \n",
       "123         is verified  \n",
       "125             examine  \n",
       "126             examine  \n",
       "130                BERT  \n",
       "131             propose  \n",
       "133               gains  \n",
       "134              showed  \n",
       "142           exploring  \n",
       "143               based  \n",
       "144               based  \n",
       "145            achieves  \n",
       "151               built  \n",
       "155                BERT  \n",
       "156             studies  \n",
       "157         to leverage  \n",
       "158         to leverage  \n",
       "159         demonstrate  \n",
       "160           answering  \n",
       "161                 pre  \n",
       "162           allocates  \n",
       "163           allocates  \n",
       "164           allocates  \n",
       "165             prefers  \n",
       "166       Understanding  \n",
       "169             achieve  \n",
       "170            to apply  \n",
       "171             provide  \n",
       "176             explore  \n",
       "177                 add  \n",
       "179               using  \n",
       "180               tuned  \n",
       "184               apply  \n",
       "185     can distinguish  \n",
       "186               shows  \n",
       "187           struggles  \n",
       "188               shows  \n",
       "189                  Is  \n",
       "190                 pre  \n",
       "192              tuning  \n",
       "194         demonstrate  \n",
       "195              tuning  \n",
       "197       Understanding  \n",
       "201             propose  \n",
       "204            appeared  \n",
       "207                 can  \n",
       "208                show  \n",
       "211            Starting  \n",
       "215               using  \n",
       "224               argue  \n",
       "226               drops  \n",
       "227             propose  \n",
       "228             propose  \n",
       "231                show  \n",
       "232                take  \n",
       "233                show  \n",
       "234                  is  \n",
       "236            produced  \n",
       "241        be explained  \n",
       "243           Comparing  \n",
       "\n",
       "[129 rows x 301 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_w2v = csv.groupby(\n",
    "    csv.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: get_phrase_embed_word2vec(\n",
    "        word2vec,\n",
    "        group.iloc[0]['averb']\n",
    "    )\n",
    ").reset_index(level=1, drop=True)\n",
    "output_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.564514</td>\n",
       "      <td>12.797974</td>\n",
       "      <td>-1.618530</td>\n",
       "      <td>14.102966</td>\n",
       "      <td>-6.553894</td>\n",
       "      <td>3.161926</td>\n",
       "      <td>-11.673126</td>\n",
       "      <td>-5.858887</td>\n",
       "      <td>-6.630981</td>\n",
       "      <td>2.254150</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.626862</td>\n",
       "      <td>9.635315</td>\n",
       "      <td>-3.750214</td>\n",
       "      <td>-16.891113</td>\n",
       "      <td>-11.451591</td>\n",
       "      <td>-1.493896</td>\n",
       "      <td>-12.604256</td>\n",
       "      <td>16.875305</td>\n",
       "      <td>stands</td>\n",
       "      <td>[ ' W e ' ,   ' i n t r o d u c e ' ,   ' a ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26.618805</td>\n",
       "      <td>17.501648</td>\n",
       "      <td>-2.511353</td>\n",
       "      <td>22.447968</td>\n",
       "      <td>-10.001831</td>\n",
       "      <td>4.959259</td>\n",
       "      <td>-13.350616</td>\n",
       "      <td>-5.756470</td>\n",
       "      <td>-8.808350</td>\n",
       "      <td>3.552795</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.356293</td>\n",
       "      <td>13.561127</td>\n",
       "      <td>-4.474716</td>\n",
       "      <td>-22.738983</td>\n",
       "      <td>-15.406860</td>\n",
       "      <td>-4.236084</td>\n",
       "      <td>-19.165714</td>\n",
       "      <td>23.652893</td>\n",
       "      <td>is designed</td>\n",
       "      <td>[ ' U n l i k e ' ,   ' r e c e n t ' ,   ' l ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.121796</td>\n",
       "      <td>4.982178</td>\n",
       "      <td>-0.262756</td>\n",
       "      <td>5.423309</td>\n",
       "      <td>-3.170593</td>\n",
       "      <td>0.648132</td>\n",
       "      <td>-4.601532</td>\n",
       "      <td>-1.795898</td>\n",
       "      <td>-3.434326</td>\n",
       "      <td>0.802368</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.315826</td>\n",
       "      <td>3.010101</td>\n",
       "      <td>-1.143051</td>\n",
       "      <td>-7.082275</td>\n",
       "      <td>-4.895142</td>\n",
       "      <td>-0.239502</td>\n",
       "      <td>-4.034000</td>\n",
       "      <td>6.109619</td>\n",
       "      <td>is</td>\n",
       "      <td>[ ' B E R T ' ,   ' i s ' ,   ' c o n c e p t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37.079895</td>\n",
       "      <td>22.969543</td>\n",
       "      <td>0.478394</td>\n",
       "      <td>34.478333</td>\n",
       "      <td>-19.502655</td>\n",
       "      <td>1.859482</td>\n",
       "      <td>-18.557220</td>\n",
       "      <td>-18.446533</td>\n",
       "      <td>-3.720032</td>\n",
       "      <td>8.637268</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.007492</td>\n",
       "      <td>21.116119</td>\n",
       "      <td>-7.979095</td>\n",
       "      <td>-38.809570</td>\n",
       "      <td>-29.577148</td>\n",
       "      <td>-6.821167</td>\n",
       "      <td>-21.500793</td>\n",
       "      <td>35.932312</td>\n",
       "      <td>obtains</td>\n",
       "      <td>[ ' I t ' ,   ' o b t a i n s ' ,   ' n e w ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-18.480042</td>\n",
       "      <td>12.646484</td>\n",
       "      <td>-1.222900</td>\n",
       "      <td>16.144135</td>\n",
       "      <td>-8.207855</td>\n",
       "      <td>2.669991</td>\n",
       "      <td>-9.901489</td>\n",
       "      <td>-5.524902</td>\n",
       "      <td>-3.801575</td>\n",
       "      <td>2.894897</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.571838</td>\n",
       "      <td>10.078217</td>\n",
       "      <td>-3.113632</td>\n",
       "      <td>-16.996826</td>\n",
       "      <td>-9.999634</td>\n",
       "      <td>-2.942638</td>\n",
       "      <td>-11.624210</td>\n",
       "      <td>17.545715</td>\n",
       "      <td>present</td>\n",
       "      <td>[ ' W e ' ,   ' p r e s e n t ' ,   ' a ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-15.864868</td>\n",
       "      <td>11.572815</td>\n",
       "      <td>-0.282776</td>\n",
       "      <td>12.601898</td>\n",
       "      <td>-6.314911</td>\n",
       "      <td>2.672180</td>\n",
       "      <td>-10.844482</td>\n",
       "      <td>-5.187988</td>\n",
       "      <td>-3.691284</td>\n",
       "      <td>2.134644</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.168411</td>\n",
       "      <td>9.096649</td>\n",
       "      <td>-1.934433</td>\n",
       "      <td>-14.271896</td>\n",
       "      <td>-9.888062</td>\n",
       "      <td>-2.036133</td>\n",
       "      <td>-11.999577</td>\n",
       "      <td>13.343445</td>\n",
       "      <td>find</td>\n",
       "      <td>[ ' W e ' ,   ' f i n d ' ,   ' t h a t ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-15.773682</td>\n",
       "      <td>11.620422</td>\n",
       "      <td>-0.200745</td>\n",
       "      <td>12.704926</td>\n",
       "      <td>-6.560516</td>\n",
       "      <td>2.856750</td>\n",
       "      <td>-10.842529</td>\n",
       "      <td>-4.964844</td>\n",
       "      <td>-4.067261</td>\n",
       "      <td>2.050659</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.948685</td>\n",
       "      <td>9.001923</td>\n",
       "      <td>-2.173691</td>\n",
       "      <td>-14.408524</td>\n",
       "      <td>-9.715515</td>\n",
       "      <td>-1.979004</td>\n",
       "      <td>-11.738468</td>\n",
       "      <td>13.497742</td>\n",
       "      <td>published</td>\n",
       "      <td>[ ' W e ' ,   ' f i n d ' ,   ' t h a t ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-29.919312</td>\n",
       "      <td>20.295380</td>\n",
       "      <td>-3.821838</td>\n",
       "      <td>24.857269</td>\n",
       "      <td>-8.687439</td>\n",
       "      <td>4.432343</td>\n",
       "      <td>-16.652893</td>\n",
       "      <td>-7.327148</td>\n",
       "      <td>-9.575928</td>\n",
       "      <td>2.441956</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.414566</td>\n",
       "      <td>16.144867</td>\n",
       "      <td>-6.087997</td>\n",
       "      <td>-27.226807</td>\n",
       "      <td>-18.794922</td>\n",
       "      <td>-4.572205</td>\n",
       "      <td>-19.284611</td>\n",
       "      <td>27.399597</td>\n",
       "      <td>called</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' w o r k ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-41.623886</td>\n",
       "      <td>24.787170</td>\n",
       "      <td>-0.309204</td>\n",
       "      <td>34.174652</td>\n",
       "      <td>-14.402130</td>\n",
       "      <td>5.907837</td>\n",
       "      <td>-20.556000</td>\n",
       "      <td>-14.488525</td>\n",
       "      <td>-7.624512</td>\n",
       "      <td>3.679321</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.036240</td>\n",
       "      <td>23.256195</td>\n",
       "      <td>-7.562119</td>\n",
       "      <td>-37.737793</td>\n",
       "      <td>-27.979370</td>\n",
       "      <td>-7.339249</td>\n",
       "      <td>-27.271549</td>\n",
       "      <td>36.026733</td>\n",
       "      <td>to reduce</td>\n",
       "      <td>[ ' W h i l e ' ,   ' m o s t ' ,   ' p r i o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-42.056015</td>\n",
       "      <td>24.814026</td>\n",
       "      <td>-0.232788</td>\n",
       "      <td>34.240326</td>\n",
       "      <td>-14.229279</td>\n",
       "      <td>6.083374</td>\n",
       "      <td>-20.532806</td>\n",
       "      <td>-14.346191</td>\n",
       "      <td>-7.259277</td>\n",
       "      <td>3.668823</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.012802</td>\n",
       "      <td>23.257904</td>\n",
       "      <td>-7.762314</td>\n",
       "      <td>-37.622314</td>\n",
       "      <td>-28.197632</td>\n",
       "      <td>-7.083740</td>\n",
       "      <td>-27.501041</td>\n",
       "      <td>35.957031</td>\n",
       "      <td>retaining</td>\n",
       "      <td>[ ' W h i l e ' ,   ' m o s t ' ,   ' p r i o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-16.140564</td>\n",
       "      <td>8.632385</td>\n",
       "      <td>-0.911560</td>\n",
       "      <td>11.992279</td>\n",
       "      <td>-5.607605</td>\n",
       "      <td>1.470551</td>\n",
       "      <td>-9.514160</td>\n",
       "      <td>-5.128906</td>\n",
       "      <td>-3.585632</td>\n",
       "      <td>2.457153</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.624451</td>\n",
       "      <td>7.037567</td>\n",
       "      <td>-3.634262</td>\n",
       "      <td>-14.338623</td>\n",
       "      <td>-11.374878</td>\n",
       "      <td>-1.979370</td>\n",
       "      <td>-11.031559</td>\n",
       "      <td>11.758545</td>\n",
       "      <td>focus</td>\n",
       "      <td>[ ' W e ' ,   ' f o c u s ' ,   ' o n ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-32.741333</td>\n",
       "      <td>24.290955</td>\n",
       "      <td>-4.827332</td>\n",
       "      <td>28.839905</td>\n",
       "      <td>-12.710571</td>\n",
       "      <td>5.171631</td>\n",
       "      <td>-19.425659</td>\n",
       "      <td>-5.860352</td>\n",
       "      <td>-8.954224</td>\n",
       "      <td>2.302048</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.694016</td>\n",
       "      <td>17.333160</td>\n",
       "      <td>-1.945679</td>\n",
       "      <td>-29.189011</td>\n",
       "      <td>-18.688232</td>\n",
       "      <td>-2.861084</td>\n",
       "      <td>-22.936985</td>\n",
       "      <td>30.397095</td>\n",
       "      <td>represents</td>\n",
       "      <td>[ ' W e ' ,   ' f i n d ' ,   ' t h a t ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-26.419312</td>\n",
       "      <td>18.648071</td>\n",
       "      <td>0.428040</td>\n",
       "      <td>21.434570</td>\n",
       "      <td>-7.940582</td>\n",
       "      <td>5.626343</td>\n",
       "      <td>-12.810211</td>\n",
       "      <td>-6.352173</td>\n",
       "      <td>-7.039093</td>\n",
       "      <td>4.630676</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.506470</td>\n",
       "      <td>14.236450</td>\n",
       "      <td>-3.606934</td>\n",
       "      <td>-26.051514</td>\n",
       "      <td>-16.459839</td>\n",
       "      <td>-3.626709</td>\n",
       "      <td>-18.509384</td>\n",
       "      <td>22.535339</td>\n",
       "      <td>does adjust</td>\n",
       "      <td>[ ' Q u a l i t a t i v e ' ,   ' a n a l y s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-27.210266</td>\n",
       "      <td>17.053894</td>\n",
       "      <td>0.045837</td>\n",
       "      <td>19.657640</td>\n",
       "      <td>-10.012238</td>\n",
       "      <td>4.330933</td>\n",
       "      <td>-14.760315</td>\n",
       "      <td>-10.131836</td>\n",
       "      <td>-6.890564</td>\n",
       "      <td>1.315025</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.803665</td>\n",
       "      <td>13.704437</td>\n",
       "      <td>-4.142075</td>\n",
       "      <td>-22.876953</td>\n",
       "      <td>-14.836914</td>\n",
       "      <td>-3.104248</td>\n",
       "      <td>-16.658375</td>\n",
       "      <td>24.918701</td>\n",
       "      <td>have had</td>\n",
       "      <td>[ ' L a r g e ' ,   ' p r e ' ,   ' - ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-27.554504</td>\n",
       "      <td>16.923035</td>\n",
       "      <td>-0.216248</td>\n",
       "      <td>20.018646</td>\n",
       "      <td>-9.833282</td>\n",
       "      <td>4.136230</td>\n",
       "      <td>-14.955658</td>\n",
       "      <td>-10.503418</td>\n",
       "      <td>-6.915039</td>\n",
       "      <td>1.850182</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.824173</td>\n",
       "      <td>14.055023</td>\n",
       "      <td>-3.807602</td>\n",
       "      <td>-22.886475</td>\n",
       "      <td>-15.102051</td>\n",
       "      <td>-3.443115</td>\n",
       "      <td>-16.783512</td>\n",
       "      <td>24.443604</td>\n",
       "      <td>motivating</td>\n",
       "      <td>[ ' L a r g e ' ,   ' p r e ' ,   ' - ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-27.210266</td>\n",
       "      <td>17.053894</td>\n",
       "      <td>0.045837</td>\n",
       "      <td>19.657640</td>\n",
       "      <td>-10.012238</td>\n",
       "      <td>4.330933</td>\n",
       "      <td>-14.760315</td>\n",
       "      <td>-10.131836</td>\n",
       "      <td>-6.890564</td>\n",
       "      <td>1.315025</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.803665</td>\n",
       "      <td>13.704437</td>\n",
       "      <td>-4.142075</td>\n",
       "      <td>-22.876953</td>\n",
       "      <td>-14.836914</td>\n",
       "      <td>-3.104248</td>\n",
       "      <td>-16.658375</td>\n",
       "      <td>24.918701</td>\n",
       "      <td>have had</td>\n",
       "      <td>[ ' L a r g e ' ,   ' p r e ' ,   ' - ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-17.088257</td>\n",
       "      <td>13.383362</td>\n",
       "      <td>-3.311829</td>\n",
       "      <td>14.775482</td>\n",
       "      <td>-4.919800</td>\n",
       "      <td>1.271149</td>\n",
       "      <td>-10.479156</td>\n",
       "      <td>-2.124023</td>\n",
       "      <td>-4.677673</td>\n",
       "      <td>2.616821</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.841644</td>\n",
       "      <td>10.736542</td>\n",
       "      <td>-3.086899</td>\n",
       "      <td>-15.902100</td>\n",
       "      <td>-10.765747</td>\n",
       "      <td>-1.415161</td>\n",
       "      <td>-10.333073</td>\n",
       "      <td>14.574707</td>\n",
       "      <td>apply</td>\n",
       "      <td>[ ' C o m p l e m e n t a r y ' ,   ' t o ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-24.017365</td>\n",
       "      <td>13.794373</td>\n",
       "      <td>-3.786926</td>\n",
       "      <td>17.983368</td>\n",
       "      <td>-10.059387</td>\n",
       "      <td>1.062927</td>\n",
       "      <td>-9.469360</td>\n",
       "      <td>-6.723633</td>\n",
       "      <td>-2.862549</td>\n",
       "      <td>1.288147</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.645859</td>\n",
       "      <td>10.773041</td>\n",
       "      <td>-4.168808</td>\n",
       "      <td>-21.222412</td>\n",
       "      <td>-12.601440</td>\n",
       "      <td>-3.776123</td>\n",
       "      <td>-11.202702</td>\n",
       "      <td>18.309204</td>\n",
       "      <td>is captured</td>\n",
       "      <td>[ ' L a s t l y ' ,   ' , ' ,   ' w e ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-11.860046</td>\n",
       "      <td>8.162201</td>\n",
       "      <td>0.072571</td>\n",
       "      <td>11.234528</td>\n",
       "      <td>-3.331543</td>\n",
       "      <td>1.602356</td>\n",
       "      <td>-7.585846</td>\n",
       "      <td>-2.352051</td>\n",
       "      <td>-3.459656</td>\n",
       "      <td>1.011963</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.611084</td>\n",
       "      <td>5.576630</td>\n",
       "      <td>-2.387680</td>\n",
       "      <td>-10.306885</td>\n",
       "      <td>-6.929810</td>\n",
       "      <td>-0.575623</td>\n",
       "      <td>-7.184635</td>\n",
       "      <td>11.963135</td>\n",
       "      <td>describe</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-26.668564</td>\n",
       "      <td>18.939453</td>\n",
       "      <td>-3.189209</td>\n",
       "      <td>19.383026</td>\n",
       "      <td>-10.086395</td>\n",
       "      <td>1.344368</td>\n",
       "      <td>-15.461945</td>\n",
       "      <td>-10.622803</td>\n",
       "      <td>-1.254639</td>\n",
       "      <td>3.393372</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.253662</td>\n",
       "      <td>15.468475</td>\n",
       "      <td>-1.790024</td>\n",
       "      <td>-22.317993</td>\n",
       "      <td>-14.784851</td>\n",
       "      <td>-3.555176</td>\n",
       "      <td>-13.525684</td>\n",
       "      <td>23.728333</td>\n",
       "      <td>is</td>\n",
       "      <td>[ ' O u r ' ,   ' s y s t e m ' ,   ' i s ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-58.717773</td>\n",
       "      <td>39.977875</td>\n",
       "      <td>-4.239319</td>\n",
       "      <td>45.616547</td>\n",
       "      <td>-23.015320</td>\n",
       "      <td>9.800423</td>\n",
       "      <td>-31.304047</td>\n",
       "      <td>-15.926147</td>\n",
       "      <td>-16.549805</td>\n",
       "      <td>7.006226</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.775269</td>\n",
       "      <td>28.026245</td>\n",
       "      <td>-8.117172</td>\n",
       "      <td>-52.828766</td>\n",
       "      <td>-32.068604</td>\n",
       "      <td>-8.351013</td>\n",
       "      <td>-36.738712</td>\n",
       "      <td>52.848572</td>\n",
       "      <td>assess</td>\n",
       "      <td>[ ' I ' ,   ' a s s e s s ' ,   ' t h e ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-5.816833</td>\n",
       "      <td>5.465698</td>\n",
       "      <td>0.049377</td>\n",
       "      <td>4.310181</td>\n",
       "      <td>-0.921082</td>\n",
       "      <td>1.067657</td>\n",
       "      <td>-5.046875</td>\n",
       "      <td>-1.714844</td>\n",
       "      <td>-3.527893</td>\n",
       "      <td>0.919800</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.014786</td>\n",
       "      <td>2.838165</td>\n",
       "      <td>-1.312103</td>\n",
       "      <td>-6.244385</td>\n",
       "      <td>-4.129272</td>\n",
       "      <td>0.329346</td>\n",
       "      <td>-4.360477</td>\n",
       "      <td>6.594360</td>\n",
       "      <td>performs</td>\n",
       "      <td>[ ' T h e ' ,   ' B E R T ' ,   ' m o d e l ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-22.944275</td>\n",
       "      <td>15.527039</td>\n",
       "      <td>1.014221</td>\n",
       "      <td>18.212250</td>\n",
       "      <td>-8.559814</td>\n",
       "      <td>4.103493</td>\n",
       "      <td>-11.886505</td>\n",
       "      <td>-8.235107</td>\n",
       "      <td>-8.179932</td>\n",
       "      <td>5.442566</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.501434</td>\n",
       "      <td>11.574677</td>\n",
       "      <td>-6.837265</td>\n",
       "      <td>-22.693848</td>\n",
       "      <td>-14.971191</td>\n",
       "      <td>-3.704987</td>\n",
       "      <td>-17.487247</td>\n",
       "      <td>23.723328</td>\n",
       "      <td>includes</td>\n",
       "      <td>[ ' A ' ,   ' n e w ' ,   ' r e l e a s e ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-33.469421</td>\n",
       "      <td>20.792816</td>\n",
       "      <td>3.540649</td>\n",
       "      <td>24.772156</td>\n",
       "      <td>-12.921936</td>\n",
       "      <td>2.215637</td>\n",
       "      <td>-19.198608</td>\n",
       "      <td>-11.840332</td>\n",
       "      <td>-10.833008</td>\n",
       "      <td>5.405685</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.399918</td>\n",
       "      <td>17.153137</td>\n",
       "      <td>-8.223114</td>\n",
       "      <td>-30.312302</td>\n",
       "      <td>-23.728271</td>\n",
       "      <td>-2.714478</td>\n",
       "      <td>-21.808693</td>\n",
       "      <td>28.343628</td>\n",
       "      <td>explores</td>\n",
       "      <td>[ ' T h i s ' ,   ' p a p e r ' ,   ' e x p l ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-17.875397</td>\n",
       "      <td>12.215271</td>\n",
       "      <td>-1.037720</td>\n",
       "      <td>12.781189</td>\n",
       "      <td>-6.438690</td>\n",
       "      <td>0.112518</td>\n",
       "      <td>-11.815155</td>\n",
       "      <td>-5.845703</td>\n",
       "      <td>-3.861938</td>\n",
       "      <td>1.363220</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.703613</td>\n",
       "      <td>9.435852</td>\n",
       "      <td>-2.585785</td>\n",
       "      <td>-15.835449</td>\n",
       "      <td>-12.064819</td>\n",
       "      <td>-0.270874</td>\n",
       "      <td>-9.351204</td>\n",
       "      <td>15.373413</td>\n",
       "      <td>compare</td>\n",
       "      <td>[ ' W e ' ,   ' c o m p a r e ' ,   ' m B E R ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-17.864807</td>\n",
       "      <td>12.349792</td>\n",
       "      <td>-0.802856</td>\n",
       "      <td>12.694275</td>\n",
       "      <td>-6.011932</td>\n",
       "      <td>0.282440</td>\n",
       "      <td>-11.852509</td>\n",
       "      <td>-6.135742</td>\n",
       "      <td>-3.614868</td>\n",
       "      <td>1.318054</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.756104</td>\n",
       "      <td>9.664368</td>\n",
       "      <td>-2.665863</td>\n",
       "      <td>-15.906738</td>\n",
       "      <td>-12.007202</td>\n",
       "      <td>-0.578491</td>\n",
       "      <td>-9.776985</td>\n",
       "      <td>15.325928</td>\n",
       "      <td>find</td>\n",
       "      <td>[ ' W e ' ,   ' c o m p a r e ' ,   ' m B E R ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-32.652771</td>\n",
       "      <td>20.464417</td>\n",
       "      <td>-1.756836</td>\n",
       "      <td>23.768005</td>\n",
       "      <td>-11.483154</td>\n",
       "      <td>5.177704</td>\n",
       "      <td>-17.253052</td>\n",
       "      <td>-9.235352</td>\n",
       "      <td>-6.131958</td>\n",
       "      <td>4.572632</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.404236</td>\n",
       "      <td>12.608582</td>\n",
       "      <td>-5.018890</td>\n",
       "      <td>-27.133957</td>\n",
       "      <td>-16.355652</td>\n",
       "      <td>-3.688110</td>\n",
       "      <td>-20.807137</td>\n",
       "      <td>26.344360</td>\n",
       "      <td>investigate</td>\n",
       "      <td>[ ' A d d i t i o n a l l y ' ,   ' , ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-33.230896</td>\n",
       "      <td>20.379456</td>\n",
       "      <td>-1.533203</td>\n",
       "      <td>23.295593</td>\n",
       "      <td>-11.278198</td>\n",
       "      <td>4.981171</td>\n",
       "      <td>-17.239258</td>\n",
       "      <td>-9.461182</td>\n",
       "      <td>-6.151001</td>\n",
       "      <td>4.697632</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.298340</td>\n",
       "      <td>12.746277</td>\n",
       "      <td>-5.088226</td>\n",
       "      <td>-27.302414</td>\n",
       "      <td>-16.541870</td>\n",
       "      <td>-3.708618</td>\n",
       "      <td>-20.860970</td>\n",
       "      <td>26.391724</td>\n",
       "      <td>generalizes</td>\n",
       "      <td>[ ' A d d i t i o n a l l y ' ,   ' , ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-17.517090</td>\n",
       "      <td>9.776184</td>\n",
       "      <td>1.405457</td>\n",
       "      <td>13.137299</td>\n",
       "      <td>-6.659760</td>\n",
       "      <td>0.753265</td>\n",
       "      <td>-8.480072</td>\n",
       "      <td>-5.401611</td>\n",
       "      <td>-5.038269</td>\n",
       "      <td>2.191956</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.172882</td>\n",
       "      <td>7.833832</td>\n",
       "      <td>-4.939316</td>\n",
       "      <td>-15.392334</td>\n",
       "      <td>-10.591553</td>\n",
       "      <td>-1.506348</td>\n",
       "      <td>-9.916508</td>\n",
       "      <td>15.242615</td>\n",
       "      <td>pre</td>\n",
       "      <td>[ ' L a n g u a g e ' ,   ' m o d e l ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-16.745483</td>\n",
       "      <td>13.092041</td>\n",
       "      <td>-1.563354</td>\n",
       "      <td>13.112274</td>\n",
       "      <td>-6.980377</td>\n",
       "      <td>3.700165</td>\n",
       "      <td>-11.461639</td>\n",
       "      <td>-6.673584</td>\n",
       "      <td>-6.694519</td>\n",
       "      <td>1.320923</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.144089</td>\n",
       "      <td>10.094727</td>\n",
       "      <td>-3.235306</td>\n",
       "      <td>-15.268677</td>\n",
       "      <td>-10.504395</td>\n",
       "      <td>-0.827026</td>\n",
       "      <td>-13.090645</td>\n",
       "      <td>16.727066</td>\n",
       "      <td>can be transferred</td>\n",
       "      <td>[ ' B y ' ,   ' l e v e r a g i n g ' ,   ' t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-16.936157</td>\n",
       "      <td>13.106201</td>\n",
       "      <td>-1.392944</td>\n",
       "      <td>13.286835</td>\n",
       "      <td>-7.182526</td>\n",
       "      <td>3.513397</td>\n",
       "      <td>-11.249237</td>\n",
       "      <td>-6.989258</td>\n",
       "      <td>-6.542969</td>\n",
       "      <td>1.370972</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.263229</td>\n",
       "      <td>10.222656</td>\n",
       "      <td>-2.931595</td>\n",
       "      <td>-14.811829</td>\n",
       "      <td>-10.226074</td>\n",
       "      <td>-1.077026</td>\n",
       "      <td>-12.860664</td>\n",
       "      <td>16.335938</td>\n",
       "      <td>encoded</td>\n",
       "      <td>[ ' B y ' ,   ' l e v e r a g i n g ' ,   ' t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-24.661499</td>\n",
       "      <td>14.191833</td>\n",
       "      <td>-2.275085</td>\n",
       "      <td>19.448608</td>\n",
       "      <td>-6.168427</td>\n",
       "      <td>4.372253</td>\n",
       "      <td>-12.663208</td>\n",
       "      <td>-7.353516</td>\n",
       "      <td>-5.276611</td>\n",
       "      <td>2.058167</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.533340</td>\n",
       "      <td>12.440216</td>\n",
       "      <td>-4.313568</td>\n",
       "      <td>-21.688232</td>\n",
       "      <td>-14.205444</td>\n",
       "      <td>-3.282349</td>\n",
       "      <td>-15.591679</td>\n",
       "      <td>20.280273</td>\n",
       "      <td>introduce</td>\n",
       "      <td>[ ' M o r e o v e r ' ,   ' , ' ,   ' w e ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-24.661499</td>\n",
       "      <td>14.191833</td>\n",
       "      <td>-2.275085</td>\n",
       "      <td>19.448608</td>\n",
       "      <td>-6.168427</td>\n",
       "      <td>4.372253</td>\n",
       "      <td>-12.663208</td>\n",
       "      <td>-7.353516</td>\n",
       "      <td>-5.276611</td>\n",
       "      <td>2.058167</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.533340</td>\n",
       "      <td>12.440216</td>\n",
       "      <td>-4.313568</td>\n",
       "      <td>-21.688232</td>\n",
       "      <td>-14.205444</td>\n",
       "      <td>-3.282349</td>\n",
       "      <td>-15.591679</td>\n",
       "      <td>20.280273</td>\n",
       "      <td>introduce</td>\n",
       "      <td>[ ' M o r e o v e r ' ,   ' , ' ,   ' w e ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-16.544861</td>\n",
       "      <td>10.374756</td>\n",
       "      <td>-0.804810</td>\n",
       "      <td>12.581787</td>\n",
       "      <td>-5.533020</td>\n",
       "      <td>0.750946</td>\n",
       "      <td>-10.841797</td>\n",
       "      <td>-6.043213</td>\n",
       "      <td>-3.155579</td>\n",
       "      <td>0.118225</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.032486</td>\n",
       "      <td>8.544128</td>\n",
       "      <td>-3.040100</td>\n",
       "      <td>-14.062012</td>\n",
       "      <td>-10.526123</td>\n",
       "      <td>0.137207</td>\n",
       "      <td>-10.204353</td>\n",
       "      <td>14.955505</td>\n",
       "      <td>can capture</td>\n",
       "      <td>[ ' T h i s ' ,   ' f r a m e w o r k ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-16.050232</td>\n",
       "      <td>10.680908</td>\n",
       "      <td>-0.622192</td>\n",
       "      <td>12.533691</td>\n",
       "      <td>-6.030090</td>\n",
       "      <td>0.691376</td>\n",
       "      <td>-10.939941</td>\n",
       "      <td>-6.019287</td>\n",
       "      <td>-3.137085</td>\n",
       "      <td>0.285095</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.039688</td>\n",
       "      <td>8.510803</td>\n",
       "      <td>-3.349792</td>\n",
       "      <td>-14.145996</td>\n",
       "      <td>-9.924561</td>\n",
       "      <td>-0.188965</td>\n",
       "      <td>-10.220955</td>\n",
       "      <td>14.720154</td>\n",
       "      <td>ensures</td>\n",
       "      <td>[ ' T h i s ' ,   ' f r a m e w o r k ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-16.544861</td>\n",
       "      <td>10.374756</td>\n",
       "      <td>-0.804810</td>\n",
       "      <td>12.581787</td>\n",
       "      <td>-5.533020</td>\n",
       "      <td>0.750946</td>\n",
       "      <td>-10.841797</td>\n",
       "      <td>-6.043213</td>\n",
       "      <td>-3.155579</td>\n",
       "      <td>0.118225</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.032486</td>\n",
       "      <td>8.544128</td>\n",
       "      <td>-3.040100</td>\n",
       "      <td>-14.062012</td>\n",
       "      <td>-10.526123</td>\n",
       "      <td>0.137207</td>\n",
       "      <td>-10.204353</td>\n",
       "      <td>14.955505</td>\n",
       "      <td>can capture</td>\n",
       "      <td>[ ' T h i s ' ,   ' f r a m e w o r k ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-17.257172</td>\n",
       "      <td>12.434509</td>\n",
       "      <td>0.869751</td>\n",
       "      <td>13.580902</td>\n",
       "      <td>-6.711487</td>\n",
       "      <td>3.521088</td>\n",
       "      <td>-11.200500</td>\n",
       "      <td>-6.827393</td>\n",
       "      <td>-3.454285</td>\n",
       "      <td>1.955933</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.283478</td>\n",
       "      <td>8.199371</td>\n",
       "      <td>-2.344559</td>\n",
       "      <td>-15.488861</td>\n",
       "      <td>-11.182617</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>-12.172310</td>\n",
       "      <td>16.699402</td>\n",
       "      <td>is</td>\n",
       "      <td>[ ' T i n y B E R T ' ,   ' i s ' ,   ' e m p ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-17.421326</td>\n",
       "      <td>11.021545</td>\n",
       "      <td>-1.648010</td>\n",
       "      <td>14.441162</td>\n",
       "      <td>-6.832306</td>\n",
       "      <td>2.705910</td>\n",
       "      <td>-8.198120</td>\n",
       "      <td>-4.444336</td>\n",
       "      <td>-1.673401</td>\n",
       "      <td>2.370850</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.639832</td>\n",
       "      <td>7.550690</td>\n",
       "      <td>-2.694305</td>\n",
       "      <td>-15.214600</td>\n",
       "      <td>-8.951538</td>\n",
       "      <td>-2.486816</td>\n",
       "      <td>-11.285282</td>\n",
       "      <td>15.590698</td>\n",
       "      <td>is</td>\n",
       "      <td>[ ' T i n y B E R T ' ,   ' i s ' ,   ' a l s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-12.120178</td>\n",
       "      <td>8.826050</td>\n",
       "      <td>1.143097</td>\n",
       "      <td>10.252319</td>\n",
       "      <td>-4.345734</td>\n",
       "      <td>1.444244</td>\n",
       "      <td>-9.386871</td>\n",
       "      <td>-4.368652</td>\n",
       "      <td>-4.024902</td>\n",
       "      <td>0.808739</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.289597</td>\n",
       "      <td>7.164520</td>\n",
       "      <td>-3.093353</td>\n",
       "      <td>-11.542480</td>\n",
       "      <td>-8.287598</td>\n",
       "      <td>-0.039490</td>\n",
       "      <td>-8.432499</td>\n",
       "      <td>11.741699</td>\n",
       "      <td>has achieved</td>\n",
       "      <td>[ ' B E R T ' ,   ' , ' ,   ' a ' ,   ' p r e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-11.164246</td>\n",
       "      <td>7.125702</td>\n",
       "      <td>0.346924</td>\n",
       "      <td>9.028076</td>\n",
       "      <td>-4.170105</td>\n",
       "      <td>0.918610</td>\n",
       "      <td>-8.278412</td>\n",
       "      <td>-5.033691</td>\n",
       "      <td>-1.923096</td>\n",
       "      <td>1.223450</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.678207</td>\n",
       "      <td>5.167542</td>\n",
       "      <td>-0.848602</td>\n",
       "      <td>-9.650192</td>\n",
       "      <td>-7.712036</td>\n",
       "      <td>0.817566</td>\n",
       "      <td>-5.392677</td>\n",
       "      <td>9.776917</td>\n",
       "      <td>describe</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-27.786560</td>\n",
       "      <td>19.217712</td>\n",
       "      <td>-1.426880</td>\n",
       "      <td>23.495667</td>\n",
       "      <td>-12.569214</td>\n",
       "      <td>3.669861</td>\n",
       "      <td>-18.426636</td>\n",
       "      <td>-6.473145</td>\n",
       "      <td>-9.966690</td>\n",
       "      <td>5.306641</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.577286</td>\n",
       "      <td>17.438293</td>\n",
       "      <td>-4.972137</td>\n",
       "      <td>-26.354340</td>\n",
       "      <td>-21.299927</td>\n",
       "      <td>-1.053589</td>\n",
       "      <td>-19.801636</td>\n",
       "      <td>26.499634</td>\n",
       "      <td>explore</td>\n",
       "      <td>[ ' S i n c e ' ,   ' R e v i e w R C ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-27.684998</td>\n",
       "      <td>18.921082</td>\n",
       "      <td>-1.468811</td>\n",
       "      <td>23.716125</td>\n",
       "      <td>-12.410767</td>\n",
       "      <td>3.576111</td>\n",
       "      <td>-18.302345</td>\n",
       "      <td>-6.656250</td>\n",
       "      <td>-9.889832</td>\n",
       "      <td>5.121658</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.538223</td>\n",
       "      <td>17.421692</td>\n",
       "      <td>-5.154266</td>\n",
       "      <td>-26.424652</td>\n",
       "      <td>-21.062622</td>\n",
       "      <td>-0.847900</td>\n",
       "      <td>-19.641235</td>\n",
       "      <td>26.419800</td>\n",
       "      <td>enhance</td>\n",
       "      <td>[ ' S i n c e ' ,   ' R e v i e w R C ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-27.786560</td>\n",
       "      <td>19.217712</td>\n",
       "      <td>-1.426880</td>\n",
       "      <td>23.495667</td>\n",
       "      <td>-12.569214</td>\n",
       "      <td>3.669861</td>\n",
       "      <td>-18.426636</td>\n",
       "      <td>-6.473145</td>\n",
       "      <td>-9.966690</td>\n",
       "      <td>5.306641</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.577286</td>\n",
       "      <td>17.438293</td>\n",
       "      <td>-4.972137</td>\n",
       "      <td>-26.354340</td>\n",
       "      <td>-21.299927</td>\n",
       "      <td>-1.053589</td>\n",
       "      <td>-19.801636</td>\n",
       "      <td>26.499634</td>\n",
       "      <td>explore</td>\n",
       "      <td>[ ' S i n c e ' ,   ' R e v i e w R C ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-29.659180</td>\n",
       "      <td>19.578613</td>\n",
       "      <td>-4.045288</td>\n",
       "      <td>24.432404</td>\n",
       "      <td>-9.010162</td>\n",
       "      <td>1.003540</td>\n",
       "      <td>-12.515778</td>\n",
       "      <td>-4.442383</td>\n",
       "      <td>-4.560303</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.175171</td>\n",
       "      <td>16.141296</td>\n",
       "      <td>-3.614365</td>\n",
       "      <td>-27.639572</td>\n",
       "      <td>-16.417709</td>\n",
       "      <td>-3.521729</td>\n",
       "      <td>-13.417297</td>\n",
       "      <td>26.571167</td>\n",
       "      <td>To show</td>\n",
       "      <td>[ ' T o ' ,   ' s h o w ' ,   ' t h e ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-29.659180</td>\n",
       "      <td>19.578613</td>\n",
       "      <td>-4.045288</td>\n",
       "      <td>24.432404</td>\n",
       "      <td>-9.010162</td>\n",
       "      <td>1.003540</td>\n",
       "      <td>-12.515778</td>\n",
       "      <td>-4.442383</td>\n",
       "      <td>-4.560303</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.175171</td>\n",
       "      <td>16.141296</td>\n",
       "      <td>-3.614365</td>\n",
       "      <td>-27.639572</td>\n",
       "      <td>-16.417709</td>\n",
       "      <td>-3.521729</td>\n",
       "      <td>-13.417297</td>\n",
       "      <td>26.571167</td>\n",
       "      <td>To show</td>\n",
       "      <td>[ ' T o ' ,   ' s h o w ' ,   ' t h e ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-12.938354</td>\n",
       "      <td>9.510162</td>\n",
       "      <td>-1.703674</td>\n",
       "      <td>9.741089</td>\n",
       "      <td>-4.209869</td>\n",
       "      <td>1.348450</td>\n",
       "      <td>-6.227966</td>\n",
       "      <td>-2.209473</td>\n",
       "      <td>-1.920288</td>\n",
       "      <td>0.983032</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.310364</td>\n",
       "      <td>6.529816</td>\n",
       "      <td>-1.057617</td>\n",
       "      <td>-11.082321</td>\n",
       "      <td>-7.444092</td>\n",
       "      <td>-1.914551</td>\n",
       "      <td>-7.020020</td>\n",
       "      <td>10.668579</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>[ ' E x p e r i m e n t a l ' ,   ' r e s u l ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-24.360291</td>\n",
       "      <td>15.665710</td>\n",
       "      <td>0.056152</td>\n",
       "      <td>17.821228</td>\n",
       "      <td>-9.274017</td>\n",
       "      <td>3.022980</td>\n",
       "      <td>-13.115387</td>\n",
       "      <td>-7.661133</td>\n",
       "      <td>-7.141907</td>\n",
       "      <td>3.064514</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.344879</td>\n",
       "      <td>11.876831</td>\n",
       "      <td>-5.743866</td>\n",
       "      <td>-21.073730</td>\n",
       "      <td>-13.820312</td>\n",
       "      <td>-2.474121</td>\n",
       "      <td>-15.236458</td>\n",
       "      <td>21.105713</td>\n",
       "      <td>achieved</td>\n",
       "      <td>[ ' A s ' ,   ' a ' ,   ' s t a t e ' ,   ' - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-24.113403</td>\n",
       "      <td>14.638153</td>\n",
       "      <td>-0.347534</td>\n",
       "      <td>19.985291</td>\n",
       "      <td>-8.984650</td>\n",
       "      <td>2.465454</td>\n",
       "      <td>-12.561523</td>\n",
       "      <td>-6.426270</td>\n",
       "      <td>-3.990173</td>\n",
       "      <td>3.387573</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.194153</td>\n",
       "      <td>11.977478</td>\n",
       "      <td>-5.106781</td>\n",
       "      <td>-22.004898</td>\n",
       "      <td>-16.092834</td>\n",
       "      <td>-2.616150</td>\n",
       "      <td>-15.873055</td>\n",
       "      <td>19.543152</td>\n",
       "      <td>to investigate</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-24.297974</td>\n",
       "      <td>14.667450</td>\n",
       "      <td>-0.057007</td>\n",
       "      <td>19.604187</td>\n",
       "      <td>-8.930328</td>\n",
       "      <td>2.401733</td>\n",
       "      <td>-12.675049</td>\n",
       "      <td>-6.695312</td>\n",
       "      <td>-3.980896</td>\n",
       "      <td>3.200317</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.396362</td>\n",
       "      <td>12.124451</td>\n",
       "      <td>-5.311127</td>\n",
       "      <td>-22.340836</td>\n",
       "      <td>-16.191650</td>\n",
       "      <td>-2.761658</td>\n",
       "      <td>-16.005501</td>\n",
       "      <td>19.764343</td>\n",
       "      <td>provide</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-24.297974</td>\n",
       "      <td>14.667450</td>\n",
       "      <td>-0.057007</td>\n",
       "      <td>19.604187</td>\n",
       "      <td>-8.930328</td>\n",
       "      <td>2.401733</td>\n",
       "      <td>-12.675049</td>\n",
       "      <td>-6.695312</td>\n",
       "      <td>-3.980896</td>\n",
       "      <td>3.200317</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.396362</td>\n",
       "      <td>12.124451</td>\n",
       "      <td>-5.311127</td>\n",
       "      <td>-22.340836</td>\n",
       "      <td>-16.191650</td>\n",
       "      <td>-2.761658</td>\n",
       "      <td>-16.005501</td>\n",
       "      <td>19.764343</td>\n",
       "      <td>provide</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-6.950256</td>\n",
       "      <td>5.559448</td>\n",
       "      <td>0.824280</td>\n",
       "      <td>5.714203</td>\n",
       "      <td>-2.806274</td>\n",
       "      <td>1.353767</td>\n",
       "      <td>-5.453522</td>\n",
       "      <td>-4.686523</td>\n",
       "      <td>-2.784851</td>\n",
       "      <td>1.656250</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.710541</td>\n",
       "      <td>5.933929</td>\n",
       "      <td>-1.904526</td>\n",
       "      <td>-7.341797</td>\n",
       "      <td>-6.720581</td>\n",
       "      <td>-0.606079</td>\n",
       "      <td>-6.292423</td>\n",
       "      <td>7.551941</td>\n",
       "      <td>show</td>\n",
       "      <td>[ ' W e ' ,   ' s h o w ' ,   ' t h a t ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-10.905273</td>\n",
       "      <td>7.385925</td>\n",
       "      <td>-0.374207</td>\n",
       "      <td>7.490356</td>\n",
       "      <td>-3.840363</td>\n",
       "      <td>0.837250</td>\n",
       "      <td>-6.273285</td>\n",
       "      <td>-3.083984</td>\n",
       "      <td>-3.044617</td>\n",
       "      <td>1.378601</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.000946</td>\n",
       "      <td>4.715607</td>\n",
       "      <td>-2.007050</td>\n",
       "      <td>-9.928467</td>\n",
       "      <td>-6.566040</td>\n",
       "      <td>-0.855469</td>\n",
       "      <td>-5.467899</td>\n",
       "      <td>9.439392</td>\n",
       "      <td>gives</td>\n",
       "      <td>[ ' T h i s ' ,   ' f o r m u l a t i o n ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-11.623108</td>\n",
       "      <td>6.956726</td>\n",
       "      <td>-0.458435</td>\n",
       "      <td>8.305756</td>\n",
       "      <td>-5.376282</td>\n",
       "      <td>1.330231</td>\n",
       "      <td>-6.821320</td>\n",
       "      <td>-4.317871</td>\n",
       "      <td>-3.089661</td>\n",
       "      <td>1.717285</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.875336</td>\n",
       "      <td>5.056854</td>\n",
       "      <td>-1.876083</td>\n",
       "      <td>-9.387817</td>\n",
       "      <td>-7.557007</td>\n",
       "      <td>-1.296753</td>\n",
       "      <td>-8.488102</td>\n",
       "      <td>9.338806</td>\n",
       "      <td>generate</td>\n",
       "      <td>[ ' W e ' ,   ' g e n e r a t e ' ,   ' f r o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-11.350647</td>\n",
       "      <td>7.014587</td>\n",
       "      <td>-0.382629</td>\n",
       "      <td>8.185219</td>\n",
       "      <td>-5.621399</td>\n",
       "      <td>1.152985</td>\n",
       "      <td>-6.907013</td>\n",
       "      <td>-4.245361</td>\n",
       "      <td>-2.965881</td>\n",
       "      <td>1.735840</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.819183</td>\n",
       "      <td>4.849335</td>\n",
       "      <td>-1.974716</td>\n",
       "      <td>-9.393951</td>\n",
       "      <td>-7.368286</td>\n",
       "      <td>-1.370972</td>\n",
       "      <td>-8.569157</td>\n",
       "      <td>9.114685</td>\n",
       "      <td>produce</td>\n",
       "      <td>[ ' W e ' ,   ' g e n e r a t e ' ,   ' f r o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-21.748322</td>\n",
       "      <td>14.339233</td>\n",
       "      <td>-2.007874</td>\n",
       "      <td>14.595306</td>\n",
       "      <td>-7.022064</td>\n",
       "      <td>3.101471</td>\n",
       "      <td>-10.873810</td>\n",
       "      <td>-5.840332</td>\n",
       "      <td>-6.072388</td>\n",
       "      <td>2.385864</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.014694</td>\n",
       "      <td>9.435516</td>\n",
       "      <td>-2.815659</td>\n",
       "      <td>-16.048584</td>\n",
       "      <td>-11.387817</td>\n",
       "      <td>-3.335876</td>\n",
       "      <td>-12.920719</td>\n",
       "      <td>18.357727</td>\n",
       "      <td>Compared</td>\n",
       "      <td>[ ' C o m p a r e d ' ,   ' t o ' ,   ' t h e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-7.286316</td>\n",
       "      <td>5.377045</td>\n",
       "      <td>1.584198</td>\n",
       "      <td>6.503174</td>\n",
       "      <td>-3.670929</td>\n",
       "      <td>-1.558167</td>\n",
       "      <td>-7.629791</td>\n",
       "      <td>-6.945312</td>\n",
       "      <td>-2.280701</td>\n",
       "      <td>0.411499</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.315338</td>\n",
       "      <td>5.422913</td>\n",
       "      <td>-2.253265</td>\n",
       "      <td>-7.855225</td>\n",
       "      <td>-7.793091</td>\n",
       "      <td>1.540161</td>\n",
       "      <td>-4.549175</td>\n",
       "      <td>7.670105</td>\n",
       "      <td>has</td>\n",
       "      <td>[ ' B E R T ' ,   ' h a s ' ,   ' a ' ,   ' M ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-7.710144</td>\n",
       "      <td>5.268524</td>\n",
       "      <td>1.808228</td>\n",
       "      <td>6.252441</td>\n",
       "      <td>-3.430695</td>\n",
       "      <td>-1.888489</td>\n",
       "      <td>-7.506256</td>\n",
       "      <td>-7.137695</td>\n",
       "      <td>-2.194519</td>\n",
       "      <td>0.656616</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.071686</td>\n",
       "      <td>5.341858</td>\n",
       "      <td>-2.050629</td>\n",
       "      <td>-7.532227</td>\n",
       "      <td>-7.654663</td>\n",
       "      <td>2.046570</td>\n",
       "      <td>-4.671246</td>\n",
       "      <td>7.561707</td>\n",
       "      <td>Must</td>\n",
       "      <td>[ ' B E R T ' ,   ' h a s ' ,   ' a ' ,   ' M ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-16.435669</td>\n",
       "      <td>10.573669</td>\n",
       "      <td>-1.176819</td>\n",
       "      <td>14.317169</td>\n",
       "      <td>-6.098297</td>\n",
       "      <td>2.453094</td>\n",
       "      <td>-8.295929</td>\n",
       "      <td>-3.776367</td>\n",
       "      <td>-6.324402</td>\n",
       "      <td>1.172791</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.018158</td>\n",
       "      <td>8.225433</td>\n",
       "      <td>-2.726059</td>\n",
       "      <td>-16.652756</td>\n",
       "      <td>-11.861816</td>\n",
       "      <td>-1.335083</td>\n",
       "      <td>-10.069889</td>\n",
       "      <td>15.688904</td>\n",
       "      <td>applying</td>\n",
       "      <td>[ ' F o l l o w i n g ' ,   ' r e c e n t ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-19.106628</td>\n",
       "      <td>13.482849</td>\n",
       "      <td>-1.748108</td>\n",
       "      <td>13.946167</td>\n",
       "      <td>-9.014771</td>\n",
       "      <td>2.410522</td>\n",
       "      <td>-10.395660</td>\n",
       "      <td>-5.960693</td>\n",
       "      <td>-6.491943</td>\n",
       "      <td>2.084106</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.621994</td>\n",
       "      <td>10.476593</td>\n",
       "      <td>-2.686966</td>\n",
       "      <td>-16.627197</td>\n",
       "      <td>-12.627258</td>\n",
       "      <td>-2.717041</td>\n",
       "      <td>-13.489872</td>\n",
       "      <td>16.462402</td>\n",
       "      <td>posed</td>\n",
       "      <td>[ ' T h i s ' ,   ' r e q u i r e d ' ,   ' c ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-19.279480</td>\n",
       "      <td>13.418396</td>\n",
       "      <td>-1.846252</td>\n",
       "      <td>13.947632</td>\n",
       "      <td>-9.226318</td>\n",
       "      <td>2.093140</td>\n",
       "      <td>-10.459381</td>\n",
       "      <td>-5.873535</td>\n",
       "      <td>-6.619385</td>\n",
       "      <td>2.128052</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.711838</td>\n",
       "      <td>10.518585</td>\n",
       "      <td>-2.495941</td>\n",
       "      <td>-16.492188</td>\n",
       "      <td>-12.584106</td>\n",
       "      <td>-3.028442</td>\n",
       "      <td>-13.616093</td>\n",
       "      <td>16.116699</td>\n",
       "      <td>confronting</td>\n",
       "      <td>[ ' T h i s ' ,   ' r e q u i r e d ' ,   ' c ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-18.681335</td>\n",
       "      <td>11.769470</td>\n",
       "      <td>0.577759</td>\n",
       "      <td>16.133545</td>\n",
       "      <td>-7.027069</td>\n",
       "      <td>2.627014</td>\n",
       "      <td>-8.658234</td>\n",
       "      <td>-4.540039</td>\n",
       "      <td>-5.875366</td>\n",
       "      <td>1.168335</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.323654</td>\n",
       "      <td>9.834106</td>\n",
       "      <td>-3.500366</td>\n",
       "      <td>-16.794189</td>\n",
       "      <td>-9.244995</td>\n",
       "      <td>-2.450317</td>\n",
       "      <td>-11.563477</td>\n",
       "      <td>18.877075</td>\n",
       "      <td>address</td>\n",
       "      <td>[ ' W e ' ,   ' a d d r e s s ' ,   ' t h i s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-24.814148</td>\n",
       "      <td>14.957947</td>\n",
       "      <td>-2.545349</td>\n",
       "      <td>16.592377</td>\n",
       "      <td>-8.958282</td>\n",
       "      <td>0.550507</td>\n",
       "      <td>-12.553101</td>\n",
       "      <td>-8.396240</td>\n",
       "      <td>-2.192200</td>\n",
       "      <td>0.143028</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.643311</td>\n",
       "      <td>9.994965</td>\n",
       "      <td>-3.376083</td>\n",
       "      <td>-20.929611</td>\n",
       "      <td>-11.547935</td>\n",
       "      <td>-2.362061</td>\n",
       "      <td>-10.841129</td>\n",
       "      <td>20.631531</td>\n",
       "      <td>contribute</td>\n",
       "      <td>[ ' B E R T ' ,   ' - ' ,   ' b a s e d ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-18.163452</td>\n",
       "      <td>11.247711</td>\n",
       "      <td>-2.643860</td>\n",
       "      <td>14.358490</td>\n",
       "      <td>-6.200439</td>\n",
       "      <td>1.847504</td>\n",
       "      <td>-10.291626</td>\n",
       "      <td>-4.196289</td>\n",
       "      <td>-4.373535</td>\n",
       "      <td>3.560486</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.524231</td>\n",
       "      <td>8.915863</td>\n",
       "      <td>-4.584213</td>\n",
       "      <td>-16.816650</td>\n",
       "      <td>-11.859131</td>\n",
       "      <td>-2.873230</td>\n",
       "      <td>-13.596500</td>\n",
       "      <td>14.514771</td>\n",
       "      <td>focus</td>\n",
       "      <td>[ ' I n ' ,   ' t h e ' ,   ' c u r r e n t ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-28.307739</td>\n",
       "      <td>19.517334</td>\n",
       "      <td>1.154907</td>\n",
       "      <td>20.389008</td>\n",
       "      <td>-10.265289</td>\n",
       "      <td>1.595642</td>\n",
       "      <td>-15.131287</td>\n",
       "      <td>-11.554199</td>\n",
       "      <td>-4.442261</td>\n",
       "      <td>3.130981</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.171173</td>\n",
       "      <td>15.532349</td>\n",
       "      <td>-6.198471</td>\n",
       "      <td>-25.394348</td>\n",
       "      <td>-17.104248</td>\n",
       "      <td>-3.582397</td>\n",
       "      <td>-16.803532</td>\n",
       "      <td>23.847656</td>\n",
       "      <td>encoded</td>\n",
       "      <td>[ ' U s i n g ' ,   ' a ' ,   ' s u b s e t ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-16.959961</td>\n",
       "      <td>12.376709</td>\n",
       "      <td>-1.872375</td>\n",
       "      <td>13.400726</td>\n",
       "      <td>-6.553406</td>\n",
       "      <td>3.250580</td>\n",
       "      <td>-10.256775</td>\n",
       "      <td>-4.614746</td>\n",
       "      <td>-5.225464</td>\n",
       "      <td>2.893250</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.509140</td>\n",
       "      <td>9.019501</td>\n",
       "      <td>-2.487167</td>\n",
       "      <td>-15.332397</td>\n",
       "      <td>-10.432251</td>\n",
       "      <td>-1.952881</td>\n",
       "      <td>-11.788486</td>\n",
       "      <td>14.017273</td>\n",
       "      <td>leads</td>\n",
       "      <td>[ ' W e ' ,   ' s h o w ' ,   ' t h a t ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-4.102844</td>\n",
       "      <td>2.765442</td>\n",
       "      <td>0.744934</td>\n",
       "      <td>2.893341</td>\n",
       "      <td>-1.945770</td>\n",
       "      <td>0.441498</td>\n",
       "      <td>-3.713318</td>\n",
       "      <td>-2.679443</td>\n",
       "      <td>-1.027924</td>\n",
       "      <td>-0.005310</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.597656</td>\n",
       "      <td>2.653656</td>\n",
       "      <td>-0.294662</td>\n",
       "      <td>-2.705811</td>\n",
       "      <td>-2.793457</td>\n",
       "      <td>0.653564</td>\n",
       "      <td>-2.618324</td>\n",
       "      <td>4.759705</td>\n",
       "      <td>Revealing</td>\n",
       "      <td>[ ' R e v e a l i n g ' ,   ' t h e ' ,   ' D ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-22.401855</td>\n",
       "      <td>12.963196</td>\n",
       "      <td>-0.108612</td>\n",
       "      <td>19.047852</td>\n",
       "      <td>-6.000519</td>\n",
       "      <td>2.818451</td>\n",
       "      <td>-15.047607</td>\n",
       "      <td>-9.354492</td>\n",
       "      <td>-5.827576</td>\n",
       "      <td>0.496117</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.149139</td>\n",
       "      <td>13.249451</td>\n",
       "      <td>-3.951019</td>\n",
       "      <td>-20.121582</td>\n",
       "      <td>-15.788086</td>\n",
       "      <td>-0.290588</td>\n",
       "      <td>-15.049297</td>\n",
       "      <td>21.129150</td>\n",
       "      <td>has been released</td>\n",
       "      <td>[ ' R e c e n t l y ' ,   ' , ' ,   ' a n ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-22.463867</td>\n",
       "      <td>12.697571</td>\n",
       "      <td>-0.144287</td>\n",
       "      <td>18.995972</td>\n",
       "      <td>-6.019623</td>\n",
       "      <td>2.498871</td>\n",
       "      <td>-15.617432</td>\n",
       "      <td>-9.888672</td>\n",
       "      <td>-5.832275</td>\n",
       "      <td>0.607445</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.360870</td>\n",
       "      <td>13.458923</td>\n",
       "      <td>-3.640594</td>\n",
       "      <td>-20.011719</td>\n",
       "      <td>-15.889404</td>\n",
       "      <td>-0.048340</td>\n",
       "      <td>-15.271618</td>\n",
       "      <td>20.878174</td>\n",
       "      <td>masking</td>\n",
       "      <td>[ ' R e c e n t l y ' ,   ' , ' ,   ' a n ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-7.130432</td>\n",
       "      <td>5.434753</td>\n",
       "      <td>-1.262939</td>\n",
       "      <td>6.814728</td>\n",
       "      <td>-2.382690</td>\n",
       "      <td>0.633545</td>\n",
       "      <td>-5.013916</td>\n",
       "      <td>-1.445801</td>\n",
       "      <td>-1.567810</td>\n",
       "      <td>0.496094</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.258362</td>\n",
       "      <td>4.210510</td>\n",
       "      <td>-0.789658</td>\n",
       "      <td>-6.663086</td>\n",
       "      <td>-5.030029</td>\n",
       "      <td>-0.448486</td>\n",
       "      <td>-5.352112</td>\n",
       "      <td>6.095398</td>\n",
       "      <td>was trained</td>\n",
       "      <td>[ ' T h e ' ,   ' m o d e l ' ,   ' w a s ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-19.400574</td>\n",
       "      <td>12.850464</td>\n",
       "      <td>-1.066833</td>\n",
       "      <td>15.983246</td>\n",
       "      <td>-6.810486</td>\n",
       "      <td>3.994507</td>\n",
       "      <td>-11.629333</td>\n",
       "      <td>-4.908203</td>\n",
       "      <td>-4.655090</td>\n",
       "      <td>2.438904</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.724762</td>\n",
       "      <td>8.990204</td>\n",
       "      <td>-1.122665</td>\n",
       "      <td>-15.611252</td>\n",
       "      <td>-10.208862</td>\n",
       "      <td>-1.856079</td>\n",
       "      <td>-12.459538</td>\n",
       "      <td>18.087830</td>\n",
       "      <td>to provide</td>\n",
       "      <td>[ ' W e ' ,   ' a i m ' ,   ' t o ' ,   ' p r ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-35.591461</td>\n",
       "      <td>23.093506</td>\n",
       "      <td>4.189392</td>\n",
       "      <td>34.265106</td>\n",
       "      <td>-18.556458</td>\n",
       "      <td>3.809608</td>\n",
       "      <td>-27.027039</td>\n",
       "      <td>-16.777344</td>\n",
       "      <td>-12.221069</td>\n",
       "      <td>5.933907</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.461441</td>\n",
       "      <td>20.702362</td>\n",
       "      <td>-5.355698</td>\n",
       "      <td>-35.503662</td>\n",
       "      <td>-26.683960</td>\n",
       "      <td>2.121613</td>\n",
       "      <td>-27.306995</td>\n",
       "      <td>32.626526</td>\n",
       "      <td>is verified</td>\n",
       "      <td>[ ' T h e ' ,   ' m o d e l ' ,   ' i s ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-10.118469</td>\n",
       "      <td>8.828766</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>8.975830</td>\n",
       "      <td>-2.204590</td>\n",
       "      <td>2.771301</td>\n",
       "      <td>-9.895660</td>\n",
       "      <td>-3.518066</td>\n",
       "      <td>-3.448608</td>\n",
       "      <td>1.502014</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.100616</td>\n",
       "      <td>5.810669</td>\n",
       "      <td>-0.837616</td>\n",
       "      <td>-9.333420</td>\n",
       "      <td>-7.104614</td>\n",
       "      <td>1.888489</td>\n",
       "      <td>-7.868053</td>\n",
       "      <td>11.166504</td>\n",
       "      <td>examine</td>\n",
       "      <td>[ ' M o r e o v e r ' ,   ' , ' ,   ' w e ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-10.118469</td>\n",
       "      <td>8.828766</td>\n",
       "      <td>-0.147461</td>\n",
       "      <td>8.975830</td>\n",
       "      <td>-2.204590</td>\n",
       "      <td>2.771301</td>\n",
       "      <td>-9.895660</td>\n",
       "      <td>-3.518066</td>\n",
       "      <td>-3.448608</td>\n",
       "      <td>1.502014</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.100616</td>\n",
       "      <td>5.810669</td>\n",
       "      <td>-0.837616</td>\n",
       "      <td>-9.333420</td>\n",
       "      <td>-7.104614</td>\n",
       "      <td>1.888489</td>\n",
       "      <td>-7.868053</td>\n",
       "      <td>11.166504</td>\n",
       "      <td>examine</td>\n",
       "      <td>[ ' M o r e o v e r ' ,   ' , ' ,   ' w e ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-27.429626</td>\n",
       "      <td>16.273865</td>\n",
       "      <td>0.138489</td>\n",
       "      <td>22.957489</td>\n",
       "      <td>-7.153412</td>\n",
       "      <td>3.921722</td>\n",
       "      <td>-12.789062</td>\n",
       "      <td>-6.779297</td>\n",
       "      <td>-5.886902</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.161224</td>\n",
       "      <td>13.943451</td>\n",
       "      <td>-5.345200</td>\n",
       "      <td>-25.906250</td>\n",
       "      <td>-15.066406</td>\n",
       "      <td>-2.639893</td>\n",
       "      <td>-15.224674</td>\n",
       "      <td>26.455261</td>\n",
       "      <td>BERT</td>\n",
       "      <td>[ ' H o w e v e r ' ,   ' , ' ,   ' p r e v i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>-29.502136</td>\n",
       "      <td>20.062500</td>\n",
       "      <td>-0.502380</td>\n",
       "      <td>21.204956</td>\n",
       "      <td>-7.370850</td>\n",
       "      <td>2.294617</td>\n",
       "      <td>-15.410095</td>\n",
       "      <td>-6.933334</td>\n",
       "      <td>-8.956055</td>\n",
       "      <td>1.992371</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.637695</td>\n",
       "      <td>13.013947</td>\n",
       "      <td>-5.282928</td>\n",
       "      <td>-26.817627</td>\n",
       "      <td>-16.822754</td>\n",
       "      <td>-2.698486</td>\n",
       "      <td>-15.718754</td>\n",
       "      <td>28.949341</td>\n",
       "      <td>propose</td>\n",
       "      <td>[ ' T o ' ,   ' t a c k l e ' ,   ' t h i s ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-12.943497</td>\n",
       "      <td>7.160950</td>\n",
       "      <td>1.324890</td>\n",
       "      <td>9.366058</td>\n",
       "      <td>-4.995209</td>\n",
       "      <td>0.956764</td>\n",
       "      <td>-5.002533</td>\n",
       "      <td>-4.685059</td>\n",
       "      <td>-3.330933</td>\n",
       "      <td>0.100769</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.204376</td>\n",
       "      <td>6.235443</td>\n",
       "      <td>-2.753159</td>\n",
       "      <td>-10.480103</td>\n",
       "      <td>-8.104492</td>\n",
       "      <td>-1.603394</td>\n",
       "      <td>-5.929996</td>\n",
       "      <td>12.464050</td>\n",
       "      <td>gains</td>\n",
       "      <td>[ ' B y ' ,   ' l e v e r a g i n g ' ,   ' a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-18.413330</td>\n",
       "      <td>13.753967</td>\n",
       "      <td>-1.645264</td>\n",
       "      <td>12.156097</td>\n",
       "      <td>-5.250671</td>\n",
       "      <td>0.531586</td>\n",
       "      <td>-11.118835</td>\n",
       "      <td>-5.291504</td>\n",
       "      <td>-4.247131</td>\n",
       "      <td>1.728149</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.034943</td>\n",
       "      <td>9.419556</td>\n",
       "      <td>-4.891830</td>\n",
       "      <td>-17.094040</td>\n",
       "      <td>-11.737183</td>\n",
       "      <td>-2.062866</td>\n",
       "      <td>-9.680973</td>\n",
       "      <td>16.015259</td>\n",
       "      <td>showed</td>\n",
       "      <td>[ ' E x p e r i m e n t s ' ,   ' o n ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-12.509338</td>\n",
       "      <td>7.786987</td>\n",
       "      <td>-0.025085</td>\n",
       "      <td>8.333466</td>\n",
       "      <td>-5.079315</td>\n",
       "      <td>1.652802</td>\n",
       "      <td>-7.685028</td>\n",
       "      <td>-4.121582</td>\n",
       "      <td>-4.790833</td>\n",
       "      <td>3.090454</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.041428</td>\n",
       "      <td>5.872040</td>\n",
       "      <td>-2.727646</td>\n",
       "      <td>-10.237961</td>\n",
       "      <td>-8.912476</td>\n",
       "      <td>-1.980103</td>\n",
       "      <td>-9.695866</td>\n",
       "      <td>11.496445</td>\n",
       "      <td>exploring</td>\n",
       "      <td>[ ' H o w e v e r ' ,   ' , ' ,   ' t h e r e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>-11.570557</td>\n",
       "      <td>7.048431</td>\n",
       "      <td>-1.155396</td>\n",
       "      <td>10.035492</td>\n",
       "      <td>-3.611938</td>\n",
       "      <td>0.761230</td>\n",
       "      <td>-5.488647</td>\n",
       "      <td>-2.931763</td>\n",
       "      <td>-3.451477</td>\n",
       "      <td>0.962952</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.872330</td>\n",
       "      <td>6.692596</td>\n",
       "      <td>-3.575668</td>\n",
       "      <td>-11.831787</td>\n",
       "      <td>-8.802368</td>\n",
       "      <td>-1.094421</td>\n",
       "      <td>-7.605900</td>\n",
       "      <td>9.610107</td>\n",
       "      <td>based</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' w o r k ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>-11.570557</td>\n",
       "      <td>7.048431</td>\n",
       "      <td>-1.155396</td>\n",
       "      <td>10.035492</td>\n",
       "      <td>-3.611938</td>\n",
       "      <td>0.761230</td>\n",
       "      <td>-5.488647</td>\n",
       "      <td>-2.931763</td>\n",
       "      <td>-3.451477</td>\n",
       "      <td>0.962952</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.872330</td>\n",
       "      <td>6.692596</td>\n",
       "      <td>-3.575668</td>\n",
       "      <td>-11.831787</td>\n",
       "      <td>-8.802368</td>\n",
       "      <td>-1.094421</td>\n",
       "      <td>-7.605900</td>\n",
       "      <td>9.610107</td>\n",
       "      <td>based</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' w o r k ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-39.643555</td>\n",
       "      <td>28.672302</td>\n",
       "      <td>-3.305908</td>\n",
       "      <td>33.084961</td>\n",
       "      <td>-14.234741</td>\n",
       "      <td>6.201477</td>\n",
       "      <td>-21.345428</td>\n",
       "      <td>-10.508301</td>\n",
       "      <td>-11.517212</td>\n",
       "      <td>3.598145</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.971695</td>\n",
       "      <td>21.160309</td>\n",
       "      <td>-6.620361</td>\n",
       "      <td>-38.599899</td>\n",
       "      <td>-21.519653</td>\n",
       "      <td>-5.080200</td>\n",
       "      <td>-24.934082</td>\n",
       "      <td>35.591125</td>\n",
       "      <td>achieves</td>\n",
       "      <td>[ ' E x p e r i m e n t a l ' ,   ' r e s u l ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-24.001831</td>\n",
       "      <td>14.939545</td>\n",
       "      <td>-0.501465</td>\n",
       "      <td>19.128845</td>\n",
       "      <td>-7.469513</td>\n",
       "      <td>4.210815</td>\n",
       "      <td>-12.021484</td>\n",
       "      <td>-6.784180</td>\n",
       "      <td>-7.369080</td>\n",
       "      <td>4.144287</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.513580</td>\n",
       "      <td>11.668274</td>\n",
       "      <td>-5.301971</td>\n",
       "      <td>-22.914062</td>\n",
       "      <td>-14.204590</td>\n",
       "      <td>-1.180725</td>\n",
       "      <td>-15.644051</td>\n",
       "      <td>21.639954</td>\n",
       "      <td>built</td>\n",
       "      <td>[ ' I t ' ,   ' e n a b l e s ' ,   ' s e a m ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-10.580261</td>\n",
       "      <td>6.464355</td>\n",
       "      <td>-0.132690</td>\n",
       "      <td>7.699677</td>\n",
       "      <td>-3.559418</td>\n",
       "      <td>1.780304</td>\n",
       "      <td>-6.098267</td>\n",
       "      <td>-4.370117</td>\n",
       "      <td>-2.158020</td>\n",
       "      <td>1.837463</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.035233</td>\n",
       "      <td>5.159424</td>\n",
       "      <td>-1.359116</td>\n",
       "      <td>-9.778931</td>\n",
       "      <td>-6.791626</td>\n",
       "      <td>-0.828979</td>\n",
       "      <td>-6.765690</td>\n",
       "      <td>9.156250</td>\n",
       "      <td>BERT</td>\n",
       "      <td>[ ' B E R T ' ,   ' w i t h ' ,   ' H i s t o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-9.895691</td>\n",
       "      <td>6.088440</td>\n",
       "      <td>0.981750</td>\n",
       "      <td>8.065063</td>\n",
       "      <td>-3.336578</td>\n",
       "      <td>-0.167725</td>\n",
       "      <td>-5.829742</td>\n",
       "      <td>-3.093262</td>\n",
       "      <td>-1.066833</td>\n",
       "      <td>0.175507</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.164108</td>\n",
       "      <td>5.784210</td>\n",
       "      <td>-3.281219</td>\n",
       "      <td>-9.730713</td>\n",
       "      <td>-6.632629</td>\n",
       "      <td>0.034302</td>\n",
       "      <td>-5.355595</td>\n",
       "      <td>9.519958</td>\n",
       "      <td>studies</td>\n",
       "      <td>[ ' T h i s ' ,   ' p a p e r ' ,   ' s t u d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-20.503174</td>\n",
       "      <td>12.440491</td>\n",
       "      <td>0.598877</td>\n",
       "      <td>18.543243</td>\n",
       "      <td>-6.401611</td>\n",
       "      <td>2.478577</td>\n",
       "      <td>-13.930695</td>\n",
       "      <td>-8.824707</td>\n",
       "      <td>-5.226746</td>\n",
       "      <td>1.518738</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.880295</td>\n",
       "      <td>13.295227</td>\n",
       "      <td>-5.029007</td>\n",
       "      <td>-18.625778</td>\n",
       "      <td>-13.343140</td>\n",
       "      <td>-0.846313</td>\n",
       "      <td>-14.691814</td>\n",
       "      <td>20.756531</td>\n",
       "      <td>to leverage</td>\n",
       "      <td>[ ' W e ' ,   ' e x p l o r e ' ,   ' s e v e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-20.503174</td>\n",
       "      <td>12.440491</td>\n",
       "      <td>0.598877</td>\n",
       "      <td>18.543243</td>\n",
       "      <td>-6.401611</td>\n",
       "      <td>2.478577</td>\n",
       "      <td>-13.930695</td>\n",
       "      <td>-8.824707</td>\n",
       "      <td>-5.226746</td>\n",
       "      <td>1.518738</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.880295</td>\n",
       "      <td>13.295227</td>\n",
       "      <td>-5.029007</td>\n",
       "      <td>-18.625778</td>\n",
       "      <td>-13.343140</td>\n",
       "      <td>-0.846313</td>\n",
       "      <td>-14.691814</td>\n",
       "      <td>20.756531</td>\n",
       "      <td>to leverage</td>\n",
       "      <td>[ ' W e ' ,   ' e x p l o r e ' ,   ' s e v e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-29.300842</td>\n",
       "      <td>18.569122</td>\n",
       "      <td>-0.888733</td>\n",
       "      <td>21.407104</td>\n",
       "      <td>-9.447601</td>\n",
       "      <td>1.691017</td>\n",
       "      <td>-15.228485</td>\n",
       "      <td>-9.979980</td>\n",
       "      <td>-5.248474</td>\n",
       "      <td>1.870911</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.077744</td>\n",
       "      <td>13.485321</td>\n",
       "      <td>-6.441742</td>\n",
       "      <td>-26.295456</td>\n",
       "      <td>-16.593506</td>\n",
       "      <td>-1.632935</td>\n",
       "      <td>-16.443462</td>\n",
       "      <td>26.553711</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>[ ' E x p e r i m e n t a l ' ,   ' r e s u l ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-29.690979</td>\n",
       "      <td>18.491089</td>\n",
       "      <td>-1.122009</td>\n",
       "      <td>21.486816</td>\n",
       "      <td>-9.281830</td>\n",
       "      <td>1.580421</td>\n",
       "      <td>-14.842743</td>\n",
       "      <td>-10.029785</td>\n",
       "      <td>-5.353943</td>\n",
       "      <td>1.821106</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.214951</td>\n",
       "      <td>13.816376</td>\n",
       "      <td>-6.632904</td>\n",
       "      <td>-26.498581</td>\n",
       "      <td>-16.487244</td>\n",
       "      <td>-1.574829</td>\n",
       "      <td>-16.019634</td>\n",
       "      <td>26.301758</td>\n",
       "      <td>answering</td>\n",
       "      <td>[ ' E x p e r i m e n t a l ' ,   ' r e s u l ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>-18.528137</td>\n",
       "      <td>12.523010</td>\n",
       "      <td>-2.069519</td>\n",
       "      <td>13.907166</td>\n",
       "      <td>-7.528381</td>\n",
       "      <td>1.637390</td>\n",
       "      <td>-10.878113</td>\n",
       "      <td>-5.381104</td>\n",
       "      <td>-5.273865</td>\n",
       "      <td>1.710632</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.680008</td>\n",
       "      <td>10.400116</td>\n",
       "      <td>-4.000702</td>\n",
       "      <td>-15.750824</td>\n",
       "      <td>-12.227295</td>\n",
       "      <td>-2.147705</td>\n",
       "      <td>-11.680244</td>\n",
       "      <td>16.623169</td>\n",
       "      <td>pre</td>\n",
       "      <td>[ ' E x p e r i m e n t a l ' ,   ' r e s u l ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-36.281067</td>\n",
       "      <td>25.299988</td>\n",
       "      <td>-6.570129</td>\n",
       "      <td>27.676880</td>\n",
       "      <td>-10.968445</td>\n",
       "      <td>5.279022</td>\n",
       "      <td>-17.821747</td>\n",
       "      <td>-9.395996</td>\n",
       "      <td>-6.045105</td>\n",
       "      <td>2.433899</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.825699</td>\n",
       "      <td>17.579071</td>\n",
       "      <td>-6.528534</td>\n",
       "      <td>-33.747314</td>\n",
       "      <td>-17.536499</td>\n",
       "      <td>-5.545532</td>\n",
       "      <td>-20.790470</td>\n",
       "      <td>31.788391</td>\n",
       "      <td>allocates</td>\n",
       "      <td>[ ' A n a l y s e s ' ,   ' i l l u s t r a t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-36.281067</td>\n",
       "      <td>25.299988</td>\n",
       "      <td>-6.570129</td>\n",
       "      <td>27.676880</td>\n",
       "      <td>-10.968445</td>\n",
       "      <td>5.279022</td>\n",
       "      <td>-17.821747</td>\n",
       "      <td>-9.395996</td>\n",
       "      <td>-6.045105</td>\n",
       "      <td>2.433899</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.825699</td>\n",
       "      <td>17.579071</td>\n",
       "      <td>-6.528534</td>\n",
       "      <td>-33.747314</td>\n",
       "      <td>-17.536499</td>\n",
       "      <td>-5.545532</td>\n",
       "      <td>-20.790470</td>\n",
       "      <td>31.788391</td>\n",
       "      <td>allocates</td>\n",
       "      <td>[ ' A n a l y s e s ' ,   ' i l l u s t r a t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-36.281067</td>\n",
       "      <td>25.299988</td>\n",
       "      <td>-6.570129</td>\n",
       "      <td>27.676880</td>\n",
       "      <td>-10.968445</td>\n",
       "      <td>5.279022</td>\n",
       "      <td>-17.821747</td>\n",
       "      <td>-9.395996</td>\n",
       "      <td>-6.045105</td>\n",
       "      <td>2.433899</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.825699</td>\n",
       "      <td>17.579071</td>\n",
       "      <td>-6.528534</td>\n",
       "      <td>-33.747314</td>\n",
       "      <td>-17.536499</td>\n",
       "      <td>-5.545532</td>\n",
       "      <td>-20.790470</td>\n",
       "      <td>31.788391</td>\n",
       "      <td>allocates</td>\n",
       "      <td>[ ' A n a l y s e s ' ,   ' i l l u s t r a t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-36.602356</td>\n",
       "      <td>25.062134</td>\n",
       "      <td>-6.464905</td>\n",
       "      <td>27.578491</td>\n",
       "      <td>-10.974792</td>\n",
       "      <td>4.941132</td>\n",
       "      <td>-18.169891</td>\n",
       "      <td>-9.335449</td>\n",
       "      <td>-5.883484</td>\n",
       "      <td>2.300110</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.730972</td>\n",
       "      <td>17.522614</td>\n",
       "      <td>-6.388885</td>\n",
       "      <td>-33.609131</td>\n",
       "      <td>-17.967163</td>\n",
       "      <td>-5.637177</td>\n",
       "      <td>-20.764286</td>\n",
       "      <td>31.759094</td>\n",
       "      <td>prefers</td>\n",
       "      <td>[ ' A n a l y s e s ' ,   ' i l l u s t r a t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-5.619995</td>\n",
       "      <td>3.188232</td>\n",
       "      <td>1.303040</td>\n",
       "      <td>4.800690</td>\n",
       "      <td>-2.971283</td>\n",
       "      <td>0.298950</td>\n",
       "      <td>-3.972107</td>\n",
       "      <td>-3.342285</td>\n",
       "      <td>-1.381714</td>\n",
       "      <td>1.179382</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.441406</td>\n",
       "      <td>3.896088</td>\n",
       "      <td>-2.223129</td>\n",
       "      <td>-4.984863</td>\n",
       "      <td>-4.556152</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>-4.294502</td>\n",
       "      <td>5.176758</td>\n",
       "      <td>Understanding</td>\n",
       "      <td>[ ' U n d e r s t a n d i n g ' ,   ' t h e ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-24.410461</td>\n",
       "      <td>17.546906</td>\n",
       "      <td>-3.723206</td>\n",
       "      <td>18.370209</td>\n",
       "      <td>-6.500427</td>\n",
       "      <td>2.537476</td>\n",
       "      <td>-12.766785</td>\n",
       "      <td>-4.510010</td>\n",
       "      <td>-2.957703</td>\n",
       "      <td>1.534607</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.396011</td>\n",
       "      <td>12.344818</td>\n",
       "      <td>-2.689560</td>\n",
       "      <td>-21.560318</td>\n",
       "      <td>-13.105789</td>\n",
       "      <td>-3.334900</td>\n",
       "      <td>-12.713200</td>\n",
       "      <td>21.806702</td>\n",
       "      <td>achieve</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-10.201294</td>\n",
       "      <td>7.022034</td>\n",
       "      <td>-0.415344</td>\n",
       "      <td>7.199097</td>\n",
       "      <td>-3.519043</td>\n",
       "      <td>0.784637</td>\n",
       "      <td>-5.905396</td>\n",
       "      <td>-3.546387</td>\n",
       "      <td>-2.822144</td>\n",
       "      <td>1.008240</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.260010</td>\n",
       "      <td>4.735626</td>\n",
       "      <td>-2.619720</td>\n",
       "      <td>-9.732178</td>\n",
       "      <td>-6.727173</td>\n",
       "      <td>-1.080078</td>\n",
       "      <td>-6.137577</td>\n",
       "      <td>9.851990</td>\n",
       "      <td>to apply</td>\n",
       "      <td>[ ' T o ' ,   ' o u r ' ,   ' k n o w l e d g ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-8.494995</td>\n",
       "      <td>6.399597</td>\n",
       "      <td>0.983276</td>\n",
       "      <td>5.681396</td>\n",
       "      <td>-2.042145</td>\n",
       "      <td>0.884644</td>\n",
       "      <td>-5.249939</td>\n",
       "      <td>-3.049805</td>\n",
       "      <td>-2.293884</td>\n",
       "      <td>1.325378</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.622635</td>\n",
       "      <td>3.718262</td>\n",
       "      <td>-1.862183</td>\n",
       "      <td>-7.676270</td>\n",
       "      <td>-4.863892</td>\n",
       "      <td>-1.439575</td>\n",
       "      <td>-5.183105</td>\n",
       "      <td>8.905640</td>\n",
       "      <td>provide</td>\n",
       "      <td>[ ' O u r ' ,   ' m o d e l s ' ,   ' p r o v ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-30.001404</td>\n",
       "      <td>20.372620</td>\n",
       "      <td>-3.838074</td>\n",
       "      <td>23.846130</td>\n",
       "      <td>-8.426453</td>\n",
       "      <td>3.110657</td>\n",
       "      <td>-18.966461</td>\n",
       "      <td>-9.458008</td>\n",
       "      <td>-5.180557</td>\n",
       "      <td>0.863953</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.609207</td>\n",
       "      <td>16.274750</td>\n",
       "      <td>-4.639740</td>\n",
       "      <td>-24.144455</td>\n",
       "      <td>-17.261108</td>\n",
       "      <td>-2.163330</td>\n",
       "      <td>-17.949104</td>\n",
       "      <td>27.209656</td>\n",
       "      <td>explore</td>\n",
       "      <td>[ ' W e ' ,   ' e x p l o r e ' ,   ' t h e ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-30.201660</td>\n",
       "      <td>20.217590</td>\n",
       "      <td>-3.827820</td>\n",
       "      <td>23.741394</td>\n",
       "      <td>-8.191589</td>\n",
       "      <td>3.241272</td>\n",
       "      <td>-18.972809</td>\n",
       "      <td>-9.577881</td>\n",
       "      <td>-5.172302</td>\n",
       "      <td>0.415710</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.800125</td>\n",
       "      <td>16.052582</td>\n",
       "      <td>-4.796967</td>\n",
       "      <td>-24.038498</td>\n",
       "      <td>-17.219604</td>\n",
       "      <td>-2.071411</td>\n",
       "      <td>-17.981087</td>\n",
       "      <td>27.360046</td>\n",
       "      <td>add</td>\n",
       "      <td>[ ' W e ' ,   ' e x p l o r e ' ,   ' t h e ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-31.413574</td>\n",
       "      <td>21.168411</td>\n",
       "      <td>-3.449890</td>\n",
       "      <td>21.318146</td>\n",
       "      <td>-12.414856</td>\n",
       "      <td>3.035889</td>\n",
       "      <td>-19.190552</td>\n",
       "      <td>-11.126709</td>\n",
       "      <td>-5.826599</td>\n",
       "      <td>3.718712</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.603851</td>\n",
       "      <td>13.560974</td>\n",
       "      <td>-4.646011</td>\n",
       "      <td>-25.516281</td>\n",
       "      <td>-18.364990</td>\n",
       "      <td>-2.260254</td>\n",
       "      <td>-18.496288</td>\n",
       "      <td>25.611450</td>\n",
       "      <td>using</td>\n",
       "      <td>[ ' B y ' ,   ' u s i n g ' ,   ' P A L s ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>-31.366211</td>\n",
       "      <td>20.924805</td>\n",
       "      <td>-3.567566</td>\n",
       "      <td>21.538605</td>\n",
       "      <td>-12.094055</td>\n",
       "      <td>3.027100</td>\n",
       "      <td>-19.227234</td>\n",
       "      <td>-11.007568</td>\n",
       "      <td>-6.053650</td>\n",
       "      <td>3.728477</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.376556</td>\n",
       "      <td>13.752380</td>\n",
       "      <td>-4.846146</td>\n",
       "      <td>-25.276779</td>\n",
       "      <td>-17.979248</td>\n",
       "      <td>-2.349121</td>\n",
       "      <td>-18.810986</td>\n",
       "      <td>26.012817</td>\n",
       "      <td>tuned</td>\n",
       "      <td>[ ' B y ' ,   ' u s i n g ' ,   ' P A L s ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>-59.601196</td>\n",
       "      <td>38.280151</td>\n",
       "      <td>-2.738342</td>\n",
       "      <td>45.096771</td>\n",
       "      <td>-22.937592</td>\n",
       "      <td>9.173981</td>\n",
       "      <td>-25.945221</td>\n",
       "      <td>-17.168457</td>\n",
       "      <td>-14.075989</td>\n",
       "      <td>6.334473</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.020065</td>\n",
       "      <td>31.113007</td>\n",
       "      <td>-8.470932</td>\n",
       "      <td>-54.159103</td>\n",
       "      <td>-34.700195</td>\n",
       "      <td>-10.048462</td>\n",
       "      <td>-34.693913</td>\n",
       "      <td>49.258362</td>\n",
       "      <td>apply</td>\n",
       "      <td>[ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-59.747559</td>\n",
       "      <td>38.013062</td>\n",
       "      <td>-2.535583</td>\n",
       "      <td>44.994598</td>\n",
       "      <td>-22.400238</td>\n",
       "      <td>8.994049</td>\n",
       "      <td>-26.170807</td>\n",
       "      <td>-17.197021</td>\n",
       "      <td>-14.244385</td>\n",
       "      <td>6.416260</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.718307</td>\n",
       "      <td>31.106415</td>\n",
       "      <td>-8.219955</td>\n",
       "      <td>-54.082932</td>\n",
       "      <td>-34.779297</td>\n",
       "      <td>-9.899948</td>\n",
       "      <td>-34.561588</td>\n",
       "      <td>49.105774</td>\n",
       "      <td>can distinguish</td>\n",
       "      <td>[ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-59.642456</td>\n",
       "      <td>38.052612</td>\n",
       "      <td>-2.570740</td>\n",
       "      <td>45.008148</td>\n",
       "      <td>-22.934540</td>\n",
       "      <td>9.285309</td>\n",
       "      <td>-25.933502</td>\n",
       "      <td>-16.859863</td>\n",
       "      <td>-14.284485</td>\n",
       "      <td>6.110840</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.741257</td>\n",
       "      <td>30.975067</td>\n",
       "      <td>-8.332993</td>\n",
       "      <td>-53.979202</td>\n",
       "      <td>-34.483154</td>\n",
       "      <td>-10.080872</td>\n",
       "      <td>-34.559147</td>\n",
       "      <td>49.319153</td>\n",
       "      <td>shows</td>\n",
       "      <td>[ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>-60.058716</td>\n",
       "      <td>37.846558</td>\n",
       "      <td>-2.524841</td>\n",
       "      <td>44.994476</td>\n",
       "      <td>-22.910492</td>\n",
       "      <td>9.128326</td>\n",
       "      <td>-25.932526</td>\n",
       "      <td>-16.904785</td>\n",
       "      <td>-14.219299</td>\n",
       "      <td>6.382324</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.757858</td>\n",
       "      <td>31.241669</td>\n",
       "      <td>-8.096420</td>\n",
       "      <td>-54.061935</td>\n",
       "      <td>-34.550293</td>\n",
       "      <td>-10.140259</td>\n",
       "      <td>-34.710514</td>\n",
       "      <td>49.174133</td>\n",
       "      <td>struggles</td>\n",
       "      <td>[ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>-59.642456</td>\n",
       "      <td>38.052612</td>\n",
       "      <td>-2.570740</td>\n",
       "      <td>45.008148</td>\n",
       "      <td>-22.934540</td>\n",
       "      <td>9.285309</td>\n",
       "      <td>-25.933502</td>\n",
       "      <td>-16.859863</td>\n",
       "      <td>-14.284485</td>\n",
       "      <td>6.110840</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.741257</td>\n",
       "      <td>30.975067</td>\n",
       "      <td>-8.332993</td>\n",
       "      <td>-53.979202</td>\n",
       "      <td>-34.483154</td>\n",
       "      <td>-10.080872</td>\n",
       "      <td>-34.559147</td>\n",
       "      <td>49.319153</td>\n",
       "      <td>shows</td>\n",
       "      <td>[ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>-14.459595</td>\n",
       "      <td>7.500946</td>\n",
       "      <td>1.956238</td>\n",
       "      <td>8.723724</td>\n",
       "      <td>-4.317200</td>\n",
       "      <td>-1.071106</td>\n",
       "      <td>-8.571869</td>\n",
       "      <td>-6.311035</td>\n",
       "      <td>-3.033264</td>\n",
       "      <td>1.308769</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.169342</td>\n",
       "      <td>5.906952</td>\n",
       "      <td>-2.949936</td>\n",
       "      <td>-12.323486</td>\n",
       "      <td>-9.637329</td>\n",
       "      <td>0.403503</td>\n",
       "      <td>-7.265293</td>\n",
       "      <td>10.973022</td>\n",
       "      <td>Is</td>\n",
       "      <td>[ ' W h a t ' ,   ' B E R T ' ,   ' I s ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>-11.334229</td>\n",
       "      <td>7.240662</td>\n",
       "      <td>1.569519</td>\n",
       "      <td>8.343109</td>\n",
       "      <td>-4.176910</td>\n",
       "      <td>0.490051</td>\n",
       "      <td>-7.111481</td>\n",
       "      <td>-5.263428</td>\n",
       "      <td>-2.570007</td>\n",
       "      <td>0.593040</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.632431</td>\n",
       "      <td>5.239227</td>\n",
       "      <td>-3.176376</td>\n",
       "      <td>-9.960693</td>\n",
       "      <td>-6.509399</td>\n",
       "      <td>0.099365</td>\n",
       "      <td>-6.373295</td>\n",
       "      <td>10.736450</td>\n",
       "      <td>pre</td>\n",
       "      <td>[ ' L a n g u a g e ' ,   ' m o d e l ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-16.969971</td>\n",
       "      <td>9.780304</td>\n",
       "      <td>-0.846680</td>\n",
       "      <td>15.231049</td>\n",
       "      <td>-5.841400</td>\n",
       "      <td>-0.311737</td>\n",
       "      <td>-7.410980</td>\n",
       "      <td>-3.185181</td>\n",
       "      <td>-3.656555</td>\n",
       "      <td>0.798218</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.724762</td>\n",
       "      <td>9.234100</td>\n",
       "      <td>-3.045151</td>\n",
       "      <td>-16.721436</td>\n",
       "      <td>-11.015991</td>\n",
       "      <td>-1.571594</td>\n",
       "      <td>-8.315861</td>\n",
       "      <td>15.221375</td>\n",
       "      <td>tuning</td>\n",
       "      <td>[ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>-21.352356</td>\n",
       "      <td>13.702179</td>\n",
       "      <td>-0.880188</td>\n",
       "      <td>15.209686</td>\n",
       "      <td>-6.514069</td>\n",
       "      <td>2.187836</td>\n",
       "      <td>-11.445862</td>\n",
       "      <td>-6.838379</td>\n",
       "      <td>-3.620483</td>\n",
       "      <td>2.271423</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.561417</td>\n",
       "      <td>11.048798</td>\n",
       "      <td>-2.970078</td>\n",
       "      <td>-17.683838</td>\n",
       "      <td>-12.578979</td>\n",
       "      <td>-2.822754</td>\n",
       "      <td>-12.271305</td>\n",
       "      <td>18.311157</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>[ ' W e ' ,   ' a l s o ' ,   ' d e m o n s t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-30.676392</td>\n",
       "      <td>18.686157</td>\n",
       "      <td>-2.856140</td>\n",
       "      <td>23.710297</td>\n",
       "      <td>-12.331635</td>\n",
       "      <td>4.306915</td>\n",
       "      <td>-15.154205</td>\n",
       "      <td>-8.854980</td>\n",
       "      <td>-6.253113</td>\n",
       "      <td>2.606873</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.396561</td>\n",
       "      <td>13.792694</td>\n",
       "      <td>-3.643295</td>\n",
       "      <td>-25.189697</td>\n",
       "      <td>-15.602417</td>\n",
       "      <td>-4.768921</td>\n",
       "      <td>-20.514927</td>\n",
       "      <td>26.353943</td>\n",
       "      <td>tuning</td>\n",
       "      <td>[ ' S e c o n d ' ,   ' , ' ,   ' t h e ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-7.239868</td>\n",
       "      <td>4.581299</td>\n",
       "      <td>1.753540</td>\n",
       "      <td>5.662201</td>\n",
       "      <td>-3.809540</td>\n",
       "      <td>0.344055</td>\n",
       "      <td>-4.761627</td>\n",
       "      <td>-4.071777</td>\n",
       "      <td>-1.361633</td>\n",
       "      <td>1.200256</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.563568</td>\n",
       "      <td>3.976685</td>\n",
       "      <td>-2.011032</td>\n",
       "      <td>-6.695801</td>\n",
       "      <td>-4.965576</td>\n",
       "      <td>-0.228638</td>\n",
       "      <td>-5.398258</td>\n",
       "      <td>6.729065</td>\n",
       "      <td>Understanding</td>\n",
       "      <td>[ ' V i s u a l i z i n g ' ,   ' a n d ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-13.371948</td>\n",
       "      <td>10.108765</td>\n",
       "      <td>-2.011414</td>\n",
       "      <td>10.665863</td>\n",
       "      <td>-5.692596</td>\n",
       "      <td>2.029388</td>\n",
       "      <td>-8.696228</td>\n",
       "      <td>-3.301010</td>\n",
       "      <td>-5.945251</td>\n",
       "      <td>2.254028</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.261612</td>\n",
       "      <td>7.791473</td>\n",
       "      <td>-2.049057</td>\n",
       "      <td>-12.405197</td>\n",
       "      <td>-9.358032</td>\n",
       "      <td>-1.626343</td>\n",
       "      <td>-10.055363</td>\n",
       "      <td>12.161987</td>\n",
       "      <td>propose</td>\n",
       "      <td>[ ' W e ' ,   ' p r o p o s e ' ,   ' a ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-31.259155</td>\n",
       "      <td>19.737305</td>\n",
       "      <td>-0.449402</td>\n",
       "      <td>26.184387</td>\n",
       "      <td>-12.463226</td>\n",
       "      <td>4.892242</td>\n",
       "      <td>-19.857941</td>\n",
       "      <td>-9.925293</td>\n",
       "      <td>-12.440613</td>\n",
       "      <td>3.841248</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.812668</td>\n",
       "      <td>17.458099</td>\n",
       "      <td>-6.898621</td>\n",
       "      <td>-27.533615</td>\n",
       "      <td>-22.563721</td>\n",
       "      <td>-1.646240</td>\n",
       "      <td>-23.475109</td>\n",
       "      <td>25.542175</td>\n",
       "      <td>appeared</td>\n",
       "      <td>[ ' W e ' ,   ' r e t r o f i t ' ,   ' B E R ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-9.751770</td>\n",
       "      <td>6.848083</td>\n",
       "      <td>-2.038147</td>\n",
       "      <td>8.106323</td>\n",
       "      <td>-5.536377</td>\n",
       "      <td>1.727600</td>\n",
       "      <td>-6.409943</td>\n",
       "      <td>-2.534424</td>\n",
       "      <td>-4.257263</td>\n",
       "      <td>1.244202</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.193039</td>\n",
       "      <td>5.183624</td>\n",
       "      <td>-0.999603</td>\n",
       "      <td>-9.083908</td>\n",
       "      <td>-7.314819</td>\n",
       "      <td>-0.672241</td>\n",
       "      <td>-7.765507</td>\n",
       "      <td>7.946716</td>\n",
       "      <td>can</td>\n",
       "      <td>[ ' T h e ' ,   ' w e l l ' ,   ' t r a i n e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-26.963135</td>\n",
       "      <td>17.125366</td>\n",
       "      <td>-1.756775</td>\n",
       "      <td>20.301758</td>\n",
       "      <td>-6.608002</td>\n",
       "      <td>3.490631</td>\n",
       "      <td>-12.683807</td>\n",
       "      <td>-6.164551</td>\n",
       "      <td>-5.036926</td>\n",
       "      <td>2.754395</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.384659</td>\n",
       "      <td>13.237091</td>\n",
       "      <td>-4.801880</td>\n",
       "      <td>-25.109268</td>\n",
       "      <td>-16.275879</td>\n",
       "      <td>-3.802124</td>\n",
       "      <td>-15.486084</td>\n",
       "      <td>22.717163</td>\n",
       "      <td>show</td>\n",
       "      <td>[ ' E x p e r i m e n t s ' ,   ' o n ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>-21.575989</td>\n",
       "      <td>13.605591</td>\n",
       "      <td>-0.184937</td>\n",
       "      <td>13.470551</td>\n",
       "      <td>-9.672211</td>\n",
       "      <td>2.169197</td>\n",
       "      <td>-11.700104</td>\n",
       "      <td>-8.276855</td>\n",
       "      <td>-5.426086</td>\n",
       "      <td>3.664856</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.495972</td>\n",
       "      <td>8.838593</td>\n",
       "      <td>-4.620590</td>\n",
       "      <td>-19.024750</td>\n",
       "      <td>-15.384399</td>\n",
       "      <td>-3.500122</td>\n",
       "      <td>-14.089390</td>\n",
       "      <td>16.301819</td>\n",
       "      <td>Starting</td>\n",
       "      <td>[ ' S t a r t i n g ' ,   ' f r o m ' ,   ' a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>-34.672852</td>\n",
       "      <td>23.654770</td>\n",
       "      <td>0.727112</td>\n",
       "      <td>29.176971</td>\n",
       "      <td>-10.905273</td>\n",
       "      <td>6.346863</td>\n",
       "      <td>-19.559998</td>\n",
       "      <td>-10.356445</td>\n",
       "      <td>-7.655518</td>\n",
       "      <td>4.289307</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.472580</td>\n",
       "      <td>19.507050</td>\n",
       "      <td>-5.974899</td>\n",
       "      <td>-32.193848</td>\n",
       "      <td>-20.097290</td>\n",
       "      <td>-3.867981</td>\n",
       "      <td>-23.562572</td>\n",
       "      <td>32.339417</td>\n",
       "      <td>using</td>\n",
       "      <td>[ ' R e c e n t l y ' ,   ' , ' ,   ' a ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-30.006775</td>\n",
       "      <td>17.424347</td>\n",
       "      <td>-2.857452</td>\n",
       "      <td>22.608978</td>\n",
       "      <td>-9.902161</td>\n",
       "      <td>1.631439</td>\n",
       "      <td>-14.242554</td>\n",
       "      <td>-7.511475</td>\n",
       "      <td>-4.585022</td>\n",
       "      <td>3.370605</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.054520</td>\n",
       "      <td>12.717743</td>\n",
       "      <td>-6.645004</td>\n",
       "      <td>-25.176025</td>\n",
       "      <td>-16.747559</td>\n",
       "      <td>-4.677734</td>\n",
       "      <td>-16.792301</td>\n",
       "      <td>23.433899</td>\n",
       "      <td>argue</td>\n",
       "      <td>[ ' W e ' ,   ' t a k e ' ,   ' i s s u e ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>-15.874451</td>\n",
       "      <td>10.309753</td>\n",
       "      <td>-1.547913</td>\n",
       "      <td>11.627411</td>\n",
       "      <td>-4.328186</td>\n",
       "      <td>1.915802</td>\n",
       "      <td>-7.821564</td>\n",
       "      <td>-4.999512</td>\n",
       "      <td>-3.649719</td>\n",
       "      <td>-0.026855</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.924896</td>\n",
       "      <td>7.611053</td>\n",
       "      <td>-1.410751</td>\n",
       "      <td>-14.663940</td>\n",
       "      <td>-8.496460</td>\n",
       "      <td>-1.657837</td>\n",
       "      <td>-7.922184</td>\n",
       "      <td>14.178040</td>\n",
       "      <td>drops</td>\n",
       "      <td>[ ' M o r e ' ,   ' s p e c i f i c a l l y ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>-14.773438</td>\n",
       "      <td>10.396057</td>\n",
       "      <td>-1.906067</td>\n",
       "      <td>12.019470</td>\n",
       "      <td>-4.921692</td>\n",
       "      <td>2.040924</td>\n",
       "      <td>-9.056122</td>\n",
       "      <td>-2.819565</td>\n",
       "      <td>-3.095093</td>\n",
       "      <td>1.448608</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.372681</td>\n",
       "      <td>7.520111</td>\n",
       "      <td>-1.505341</td>\n",
       "      <td>-12.344528</td>\n",
       "      <td>-7.812500</td>\n",
       "      <td>-0.440063</td>\n",
       "      <td>-8.150154</td>\n",
       "      <td>12.913391</td>\n",
       "      <td>propose</td>\n",
       "      <td>[ ' A s ' ,   ' a ' ,   ' r e m e d y ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>-14.773438</td>\n",
       "      <td>10.396057</td>\n",
       "      <td>-1.906067</td>\n",
       "      <td>12.019470</td>\n",
       "      <td>-4.921692</td>\n",
       "      <td>2.040924</td>\n",
       "      <td>-9.056122</td>\n",
       "      <td>-2.819565</td>\n",
       "      <td>-3.095093</td>\n",
       "      <td>1.448608</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.372681</td>\n",
       "      <td>7.520111</td>\n",
       "      <td>-1.505341</td>\n",
       "      <td>-12.344528</td>\n",
       "      <td>-7.812500</td>\n",
       "      <td>-0.440063</td>\n",
       "      <td>-8.150154</td>\n",
       "      <td>12.913391</td>\n",
       "      <td>propose</td>\n",
       "      <td>[ ' A s ' ,   ' a ' ,   ' r e m e d y ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>-14.114807</td>\n",
       "      <td>9.352295</td>\n",
       "      <td>-0.671204</td>\n",
       "      <td>10.589630</td>\n",
       "      <td>-4.832184</td>\n",
       "      <td>1.997528</td>\n",
       "      <td>-10.239807</td>\n",
       "      <td>-6.954102</td>\n",
       "      <td>-3.367249</td>\n",
       "      <td>-0.229797</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.102310</td>\n",
       "      <td>8.616180</td>\n",
       "      <td>-2.888016</td>\n",
       "      <td>-12.803955</td>\n",
       "      <td>-9.961304</td>\n",
       "      <td>0.274048</td>\n",
       "      <td>-9.394726</td>\n",
       "      <td>13.853516</td>\n",
       "      <td>show</td>\n",
       "      <td>[ ' W e ' ,   ' t a k e ' ,   ' t h i s ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-14.082458</td>\n",
       "      <td>9.287354</td>\n",
       "      <td>-0.836731</td>\n",
       "      <td>10.635040</td>\n",
       "      <td>-4.853180</td>\n",
       "      <td>2.077911</td>\n",
       "      <td>-10.259338</td>\n",
       "      <td>-7.015259</td>\n",
       "      <td>-3.385132</td>\n",
       "      <td>-0.020813</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.322403</td>\n",
       "      <td>8.423309</td>\n",
       "      <td>-2.942123</td>\n",
       "      <td>-12.907715</td>\n",
       "      <td>-10.163208</td>\n",
       "      <td>0.353271</td>\n",
       "      <td>-9.373486</td>\n",
       "      <td>13.530640</td>\n",
       "      <td>take</td>\n",
       "      <td>[ ' W e ' ,   ' t a k e ' ,   ' t h i s ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>-14.114807</td>\n",
       "      <td>9.352295</td>\n",
       "      <td>-0.671204</td>\n",
       "      <td>10.589630</td>\n",
       "      <td>-4.832184</td>\n",
       "      <td>1.997528</td>\n",
       "      <td>-10.239807</td>\n",
       "      <td>-6.954102</td>\n",
       "      <td>-3.367249</td>\n",
       "      <td>-0.229797</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.102310</td>\n",
       "      <td>8.616180</td>\n",
       "      <td>-2.888016</td>\n",
       "      <td>-12.803955</td>\n",
       "      <td>-9.961304</td>\n",
       "      <td>0.274048</td>\n",
       "      <td>-9.394726</td>\n",
       "      <td>13.853516</td>\n",
       "      <td>show</td>\n",
       "      <td>[ ' W e ' ,   ' t a k e ' ,   ' t h i s ' ,   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>-9.630707</td>\n",
       "      <td>7.085632</td>\n",
       "      <td>2.065857</td>\n",
       "      <td>9.120636</td>\n",
       "      <td>-4.833191</td>\n",
       "      <td>0.641571</td>\n",
       "      <td>-8.413727</td>\n",
       "      <td>-5.578491</td>\n",
       "      <td>-3.540771</td>\n",
       "      <td>0.352722</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.003632</td>\n",
       "      <td>5.686371</td>\n",
       "      <td>-2.156235</td>\n",
       "      <td>-11.187134</td>\n",
       "      <td>-7.997559</td>\n",
       "      <td>1.297119</td>\n",
       "      <td>-7.287666</td>\n",
       "      <td>11.016357</td>\n",
       "      <td>is</td>\n",
       "      <td>[ ' B E R T ' ,   ' i s ' ,   ' N o t ' ,   ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>-14.844055</td>\n",
       "      <td>10.549438</td>\n",
       "      <td>-1.165710</td>\n",
       "      <td>10.190948</td>\n",
       "      <td>-5.454498</td>\n",
       "      <td>0.673492</td>\n",
       "      <td>-9.431458</td>\n",
       "      <td>-5.494263</td>\n",
       "      <td>-4.789246</td>\n",
       "      <td>0.945923</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.565750</td>\n",
       "      <td>7.851807</td>\n",
       "      <td>-1.014145</td>\n",
       "      <td>-12.943817</td>\n",
       "      <td>-9.991455</td>\n",
       "      <td>-1.111328</td>\n",
       "      <td>-8.372990</td>\n",
       "      <td>14.842041</td>\n",
       "      <td>produced</td>\n",
       "      <td>[ ' H o w e v e r ' ,   ' , ' ,   ' j u s t ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-31.616165</td>\n",
       "      <td>20.896515</td>\n",
       "      <td>-0.153625</td>\n",
       "      <td>25.523560</td>\n",
       "      <td>-10.797577</td>\n",
       "      <td>3.786079</td>\n",
       "      <td>-17.493195</td>\n",
       "      <td>-10.038452</td>\n",
       "      <td>-6.956177</td>\n",
       "      <td>3.597740</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.408585</td>\n",
       "      <td>17.893127</td>\n",
       "      <td>-4.643402</td>\n",
       "      <td>-29.401505</td>\n",
       "      <td>-19.960938</td>\n",
       "      <td>-3.248718</td>\n",
       "      <td>-20.830326</td>\n",
       "      <td>28.885010</td>\n",
       "      <td>be explained</td>\n",
       "      <td>[ ' I n ' ,   ' a l l ' ,   ' l a y e r s ' , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>-7.031555</td>\n",
       "      <td>4.685791</td>\n",
       "      <td>1.159851</td>\n",
       "      <td>5.458374</td>\n",
       "      <td>-3.195740</td>\n",
       "      <td>-0.129906</td>\n",
       "      <td>-6.937927</td>\n",
       "      <td>-4.434082</td>\n",
       "      <td>-2.077454</td>\n",
       "      <td>0.870506</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.475052</td>\n",
       "      <td>4.236420</td>\n",
       "      <td>-1.089203</td>\n",
       "      <td>-4.090576</td>\n",
       "      <td>-5.547241</td>\n",
       "      <td>1.768799</td>\n",
       "      <td>-4.327091</td>\n",
       "      <td>5.065430</td>\n",
       "      <td>Comparing</td>\n",
       "      <td>[ ' C o m p a r i n g ' ,   ' t h e ' ,   ' G ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2          3          4         5  \\\n",
       "0   -18.564514  12.797974 -1.618530  14.102966  -6.553894  3.161926   \n",
       "1   -26.618805  17.501648 -2.511353  22.447968 -10.001831  4.959259   \n",
       "3    -6.121796   4.982178 -0.262756   5.423309  -3.170593  0.648132   \n",
       "4   -37.079895  22.969543  0.478394  34.478333 -19.502655  1.859482   \n",
       "8   -18.480042  12.646484 -1.222900  16.144135  -8.207855  2.669991   \n",
       "9   -15.864868  11.572815 -0.282776  12.601898  -6.314911  2.672180   \n",
       "10  -15.773682  11.620422 -0.200745  12.704926  -6.560516  2.856750   \n",
       "16  -29.919312  20.295380 -3.821838  24.857269  -8.687439  4.432343   \n",
       "17  -41.623886  24.787170 -0.309204  34.174652 -14.402130  5.907837   \n",
       "18  -42.056015  24.814026 -0.232788  34.240326 -14.229279  6.083374   \n",
       "23  -16.140564   8.632385 -0.911560  11.992279  -5.607605  1.470551   \n",
       "24  -32.741333  24.290955 -4.827332  28.839905 -12.710571  5.171631   \n",
       "25  -26.419312  18.648071  0.428040  21.434570  -7.940582  5.626343   \n",
       "27  -27.210266  17.053894  0.045837  19.657640 -10.012238  4.330933   \n",
       "28  -27.554504  16.923035 -0.216248  20.018646  -9.833282  4.136230   \n",
       "29  -27.210266  17.053894  0.045837  19.657640 -10.012238  4.330933   \n",
       "31  -17.088257  13.383362 -3.311829  14.775482  -4.919800  1.271149   \n",
       "35  -24.017365  13.794373 -3.786926  17.983368 -10.059387  1.062927   \n",
       "39  -11.860046   8.162201  0.072571  11.234528  -3.331543  1.602356   \n",
       "40  -26.668564  18.939453 -3.189209  19.383026 -10.086395  1.344368   \n",
       "43  -58.717773  39.977875 -4.239319  45.616547 -23.015320  9.800423   \n",
       "44   -5.816833   5.465698  0.049377   4.310181  -0.921082  1.067657   \n",
       "47  -22.944275  15.527039  1.014221  18.212250  -8.559814  4.103493   \n",
       "48  -33.469421  20.792816  3.540649  24.772156 -12.921936  2.215637   \n",
       "49  -17.875397  12.215271 -1.037720  12.781189  -6.438690  0.112518   \n",
       "50  -17.864807  12.349792 -0.802856  12.694275  -6.011932  0.282440   \n",
       "51  -32.652771  20.464417 -1.756836  23.768005 -11.483154  5.177704   \n",
       "52  -33.230896  20.379456 -1.533203  23.295593 -11.278198  4.981171   \n",
       "54  -17.517090   9.776184  1.405457  13.137299  -6.659760  0.753265   \n",
       "57  -16.745483  13.092041 -1.563354  13.112274  -6.980377  3.700165   \n",
       "58  -16.936157  13.106201 -1.392944  13.286835  -7.182526  3.513397   \n",
       "59  -24.661499  14.191833 -2.275085  19.448608  -6.168427  4.372253   \n",
       "60  -24.661499  14.191833 -2.275085  19.448608  -6.168427  4.372253   \n",
       "61  -16.544861  10.374756 -0.804810  12.581787  -5.533020  0.750946   \n",
       "62  -16.050232  10.680908 -0.622192  12.533691  -6.030090  0.691376   \n",
       "63  -16.544861  10.374756 -0.804810  12.581787  -5.533020  0.750946   \n",
       "64  -17.257172  12.434509  0.869751  13.580902  -6.711487  3.521088   \n",
       "66  -17.421326  11.021545 -1.648010  14.441162  -6.832306  2.705910   \n",
       "68  -12.120178   8.826050  1.143097  10.252319  -4.345734  1.444244   \n",
       "69  -11.164246   7.125702  0.346924   9.028076  -4.170105  0.918610   \n",
       "77  -27.786560  19.217712 -1.426880  23.495667 -12.569214  3.669861   \n",
       "78  -27.684998  18.921082 -1.468811  23.716125 -12.410767  3.576111   \n",
       "79  -27.786560  19.217712 -1.426880  23.495667 -12.569214  3.669861   \n",
       "80  -29.659180  19.578613 -4.045288  24.432404  -9.010162  1.003540   \n",
       "81  -29.659180  19.578613 -4.045288  24.432404  -9.010162  1.003540   \n",
       "82  -12.938354   9.510162 -1.703674   9.741089  -4.209869  1.348450   \n",
       "86  -24.360291  15.665710  0.056152  17.821228  -9.274017  3.022980   \n",
       "87  -24.113403  14.638153 -0.347534  19.985291  -8.984650  2.465454   \n",
       "88  -24.297974  14.667450 -0.057007  19.604187  -8.930328  2.401733   \n",
       "89  -24.297974  14.667450 -0.057007  19.604187  -8.930328  2.401733   \n",
       "92   -6.950256   5.559448  0.824280   5.714203  -2.806274  1.353767   \n",
       "93  -10.905273   7.385925 -0.374207   7.490356  -3.840363  0.837250   \n",
       "94  -11.623108   6.956726 -0.458435   8.305756  -5.376282  1.330231   \n",
       "95  -11.350647   7.014587 -0.382629   8.185219  -5.621399  1.152985   \n",
       "96  -21.748322  14.339233 -2.007874  14.595306  -7.022064  3.101471   \n",
       "97   -7.286316   5.377045  1.584198   6.503174  -3.670929 -1.558167   \n",
       "98   -7.710144   5.268524  1.808228   6.252441  -3.430695 -1.888489   \n",
       "103 -16.435669  10.573669 -1.176819  14.317169  -6.098297  2.453094   \n",
       "104 -19.106628  13.482849 -1.748108  13.946167  -9.014771  2.410522   \n",
       "105 -19.279480  13.418396 -1.846252  13.947632  -9.226318  2.093140   \n",
       "106 -18.681335  11.769470  0.577759  16.133545  -7.027069  2.627014   \n",
       "110 -24.814148  14.957947 -2.545349  16.592377  -8.958282  0.550507   \n",
       "111 -18.163452  11.247711 -2.643860  14.358490  -6.200439  1.847504   \n",
       "112 -28.307739  19.517334  1.154907  20.389008 -10.265289  1.595642   \n",
       "115 -16.959961  12.376709 -1.872375  13.400726  -6.553406  3.250580   \n",
       "116  -4.102844   2.765442  0.744934   2.893341  -1.945770  0.441498   \n",
       "118 -22.401855  12.963196 -0.108612  19.047852  -6.000519  2.818451   \n",
       "119 -22.463867  12.697571 -0.144287  18.995972  -6.019623  2.498871   \n",
       "121  -7.130432   5.434753 -1.262939   6.814728  -2.382690  0.633545   \n",
       "122 -19.400574  12.850464 -1.066833  15.983246  -6.810486  3.994507   \n",
       "123 -35.591461  23.093506  4.189392  34.265106 -18.556458  3.809608   \n",
       "125 -10.118469   8.828766 -0.147461   8.975830  -2.204590  2.771301   \n",
       "126 -10.118469   8.828766 -0.147461   8.975830  -2.204590  2.771301   \n",
       "130 -27.429626  16.273865  0.138489  22.957489  -7.153412  3.921722   \n",
       "131 -29.502136  20.062500 -0.502380  21.204956  -7.370850  2.294617   \n",
       "133 -12.943497   7.160950  1.324890   9.366058  -4.995209  0.956764   \n",
       "134 -18.413330  13.753967 -1.645264  12.156097  -5.250671  0.531586   \n",
       "142 -12.509338   7.786987 -0.025085   8.333466  -5.079315  1.652802   \n",
       "143 -11.570557   7.048431 -1.155396  10.035492  -3.611938  0.761230   \n",
       "144 -11.570557   7.048431 -1.155396  10.035492  -3.611938  0.761230   \n",
       "145 -39.643555  28.672302 -3.305908  33.084961 -14.234741  6.201477   \n",
       "151 -24.001831  14.939545 -0.501465  19.128845  -7.469513  4.210815   \n",
       "155 -10.580261   6.464355 -0.132690   7.699677  -3.559418  1.780304   \n",
       "156  -9.895691   6.088440  0.981750   8.065063  -3.336578 -0.167725   \n",
       "157 -20.503174  12.440491  0.598877  18.543243  -6.401611  2.478577   \n",
       "158 -20.503174  12.440491  0.598877  18.543243  -6.401611  2.478577   \n",
       "159 -29.300842  18.569122 -0.888733  21.407104  -9.447601  1.691017   \n",
       "160 -29.690979  18.491089 -1.122009  21.486816  -9.281830  1.580421   \n",
       "161 -18.528137  12.523010 -2.069519  13.907166  -7.528381  1.637390   \n",
       "162 -36.281067  25.299988 -6.570129  27.676880 -10.968445  5.279022   \n",
       "163 -36.281067  25.299988 -6.570129  27.676880 -10.968445  5.279022   \n",
       "164 -36.281067  25.299988 -6.570129  27.676880 -10.968445  5.279022   \n",
       "165 -36.602356  25.062134 -6.464905  27.578491 -10.974792  4.941132   \n",
       "166  -5.619995   3.188232  1.303040   4.800690  -2.971283  0.298950   \n",
       "169 -24.410461  17.546906 -3.723206  18.370209  -6.500427  2.537476   \n",
       "170 -10.201294   7.022034 -0.415344   7.199097  -3.519043  0.784637   \n",
       "171  -8.494995   6.399597  0.983276   5.681396  -2.042145  0.884644   \n",
       "176 -30.001404  20.372620 -3.838074  23.846130  -8.426453  3.110657   \n",
       "177 -30.201660  20.217590 -3.827820  23.741394  -8.191589  3.241272   \n",
       "179 -31.413574  21.168411 -3.449890  21.318146 -12.414856  3.035889   \n",
       "180 -31.366211  20.924805 -3.567566  21.538605 -12.094055  3.027100   \n",
       "184 -59.601196  38.280151 -2.738342  45.096771 -22.937592  9.173981   \n",
       "185 -59.747559  38.013062 -2.535583  44.994598 -22.400238  8.994049   \n",
       "186 -59.642456  38.052612 -2.570740  45.008148 -22.934540  9.285309   \n",
       "187 -60.058716  37.846558 -2.524841  44.994476 -22.910492  9.128326   \n",
       "188 -59.642456  38.052612 -2.570740  45.008148 -22.934540  9.285309   \n",
       "189 -14.459595   7.500946  1.956238   8.723724  -4.317200 -1.071106   \n",
       "190 -11.334229   7.240662  1.569519   8.343109  -4.176910  0.490051   \n",
       "192 -16.969971   9.780304 -0.846680  15.231049  -5.841400 -0.311737   \n",
       "194 -21.352356  13.702179 -0.880188  15.209686  -6.514069  2.187836   \n",
       "195 -30.676392  18.686157 -2.856140  23.710297 -12.331635  4.306915   \n",
       "197  -7.239868   4.581299  1.753540   5.662201  -3.809540  0.344055   \n",
       "201 -13.371948  10.108765 -2.011414  10.665863  -5.692596  2.029388   \n",
       "204 -31.259155  19.737305 -0.449402  26.184387 -12.463226  4.892242   \n",
       "207  -9.751770   6.848083 -2.038147   8.106323  -5.536377  1.727600   \n",
       "208 -26.963135  17.125366 -1.756775  20.301758  -6.608002  3.490631   \n",
       "211 -21.575989  13.605591 -0.184937  13.470551  -9.672211  2.169197   \n",
       "215 -34.672852  23.654770  0.727112  29.176971 -10.905273  6.346863   \n",
       "224 -30.006775  17.424347 -2.857452  22.608978  -9.902161  1.631439   \n",
       "226 -15.874451  10.309753 -1.547913  11.627411  -4.328186  1.915802   \n",
       "227 -14.773438  10.396057 -1.906067  12.019470  -4.921692  2.040924   \n",
       "228 -14.773438  10.396057 -1.906067  12.019470  -4.921692  2.040924   \n",
       "231 -14.114807   9.352295 -0.671204  10.589630  -4.832184  1.997528   \n",
       "232 -14.082458   9.287354 -0.836731  10.635040  -4.853180  2.077911   \n",
       "233 -14.114807   9.352295 -0.671204  10.589630  -4.832184  1.997528   \n",
       "234  -9.630707   7.085632  2.065857   9.120636  -4.833191  0.641571   \n",
       "236 -14.844055  10.549438 -1.165710  10.190948  -5.454498  0.673492   \n",
       "241 -31.616165  20.896515 -0.153625  25.523560 -10.797577  3.786079   \n",
       "243  -7.031555   4.685791  1.159851   5.458374  -3.195740 -0.129906   \n",
       "\n",
       "             6          7          8         9  ...        292        293  \\\n",
       "0   -11.673126  -5.858887  -6.630981  2.254150  ... -10.626862   9.635315   \n",
       "1   -13.350616  -5.756470  -8.808350  3.552795  ... -12.356293  13.561127   \n",
       "3    -4.601532  -1.795898  -3.434326  0.802368  ...  -6.315826   3.010101   \n",
       "4   -18.557220 -18.446533  -3.720032  8.637268  ... -29.007492  21.116119   \n",
       "8    -9.901489  -5.524902  -3.801575  2.894897  ... -12.571838  10.078217   \n",
       "9   -10.844482  -5.187988  -3.691284  2.134644  ...  -9.168411   9.096649   \n",
       "10  -10.842529  -4.964844  -4.067261  2.050659  ...  -8.948685   9.001923   \n",
       "16  -16.652893  -7.327148  -9.575928  2.441956  ... -17.414566  16.144867   \n",
       "17  -20.556000 -14.488525  -7.624512  3.679321  ... -22.036240  23.256195   \n",
       "18  -20.532806 -14.346191  -7.259277  3.668823  ... -22.012802  23.257904   \n",
       "23   -9.514160  -5.128906  -3.585632  2.457153  ...  -7.624451   7.037567   \n",
       "24  -19.425659  -5.860352  -8.954224  2.302048  ... -18.694016  17.333160   \n",
       "25  -12.810211  -6.352173  -7.039093  4.630676  ... -16.506470  14.236450   \n",
       "27  -14.760315 -10.131836  -6.890564  1.315025  ... -12.803665  13.704437   \n",
       "28  -14.955658 -10.503418  -6.915039  1.850182  ... -12.824173  14.055023   \n",
       "29  -14.760315 -10.131836  -6.890564  1.315025  ... -12.803665  13.704437   \n",
       "31  -10.479156  -2.124023  -4.677673  2.616821  ... -10.841644  10.736542   \n",
       "35   -9.469360  -6.723633  -2.862549  1.288147  ... -10.645859  10.773041   \n",
       "39   -7.585846  -2.352051  -3.459656  1.011963  ...  -8.611084   5.576630   \n",
       "40  -15.461945 -10.622803  -1.254639  3.393372  ... -14.253662  15.468475   \n",
       "43  -31.304047 -15.926147 -16.549805  7.006226  ... -33.775269  28.026245   \n",
       "44   -5.046875  -1.714844  -3.527893  0.919800  ...  -5.014786   2.838165   \n",
       "47  -11.886505  -8.235107  -8.179932  5.442566  ... -16.501434  11.574677   \n",
       "48  -19.198608 -11.840332 -10.833008  5.405685  ... -18.399918  17.153137   \n",
       "49  -11.815155  -5.845703  -3.861938  1.363220  ... -10.703613   9.435852   \n",
       "50  -11.852509  -6.135742  -3.614868  1.318054  ... -10.756104   9.664368   \n",
       "51  -17.253052  -9.235352  -6.131958  4.572632  ... -16.404236  12.608582   \n",
       "52  -17.239258  -9.461182  -6.151001  4.697632  ... -16.298340  12.746277   \n",
       "54   -8.480072  -5.401611  -5.038269  2.191956  ...  -9.172882   7.833832   \n",
       "57  -11.461639  -6.673584  -6.694519  1.320923  ...  -9.144089  10.094727   \n",
       "58  -11.249237  -6.989258  -6.542969  1.370972  ...  -9.263229  10.222656   \n",
       "59  -12.663208  -7.353516  -5.276611  2.058167  ... -11.533340  12.440216   \n",
       "60  -12.663208  -7.353516  -5.276611  2.058167  ... -11.533340  12.440216   \n",
       "61  -10.841797  -6.043213  -3.155579  0.118225  ...  -8.032486   8.544128   \n",
       "62  -10.939941  -6.019287  -3.137085  0.285095  ...  -8.039688   8.510803   \n",
       "63  -10.841797  -6.043213  -3.155579  0.118225  ...  -8.032486   8.544128   \n",
       "64  -11.200500  -6.827393  -3.454285  1.955933  ... -12.283478   8.199371   \n",
       "66   -8.198120  -4.444336  -1.673401  2.370850  ...  -9.639832   7.550690   \n",
       "68   -9.386871  -4.368652  -4.024902  0.808739  ...  -8.289597   7.164520   \n",
       "69   -8.278412  -5.033691  -1.923096  1.223450  ...  -8.678207   5.167542   \n",
       "77  -18.426636  -6.473145  -9.966690  5.306641  ... -18.577286  17.438293   \n",
       "78  -18.302345  -6.656250  -9.889832  5.121658  ... -18.538223  17.421692   \n",
       "79  -18.426636  -6.473145  -9.966690  5.306641  ... -18.577286  17.438293   \n",
       "80  -12.515778  -4.442383  -4.560303 -0.171875  ... -16.175171  16.141296   \n",
       "81  -12.515778  -4.442383  -4.560303 -0.171875  ... -16.175171  16.141296   \n",
       "82   -6.227966  -2.209473  -1.920288  0.983032  ...  -7.310364   6.529816   \n",
       "86  -13.115387  -7.661133  -7.141907  3.064514  ... -12.344879  11.876831   \n",
       "87  -12.561523  -6.426270  -3.990173  3.387573  ... -13.194153  11.977478   \n",
       "88  -12.675049  -6.695312  -3.980896  3.200317  ... -13.396362  12.124451   \n",
       "89  -12.675049  -6.695312  -3.980896  3.200317  ... -13.396362  12.124451   \n",
       "92   -5.453522  -4.686523  -2.784851  1.656250  ...  -5.710541   5.933929   \n",
       "93   -6.273285  -3.083984  -3.044617  1.378601  ...  -7.000946   4.715607   \n",
       "94   -6.821320  -4.317871  -3.089661  1.717285  ...  -4.875336   5.056854   \n",
       "95   -6.907013  -4.245361  -2.965881  1.735840  ...  -4.819183   4.849335   \n",
       "96  -10.873810  -5.840332  -6.072388  2.385864  ... -10.014694   9.435516   \n",
       "97   -7.629791  -6.945312  -2.280701  0.411499  ...  -5.315338   5.422913   \n",
       "98   -7.506256  -7.137695  -2.194519  0.656616  ...  -5.071686   5.341858   \n",
       "103  -8.295929  -3.776367  -6.324402  1.172791  ... -11.018158   8.225433   \n",
       "104 -10.395660  -5.960693  -6.491943  2.084106  ...  -8.621994  10.476593   \n",
       "105 -10.459381  -5.873535  -6.619385  2.128052  ...  -8.711838  10.518585   \n",
       "106  -8.658234  -4.540039  -5.875366  1.168335  ...  -9.323654   9.834106   \n",
       "110 -12.553101  -8.396240  -2.192200  0.143028  ... -11.643311   9.994965   \n",
       "111 -10.291626  -4.196289  -4.373535  3.560486  ...  -8.524231   8.915863   \n",
       "112 -15.131287 -11.554199  -4.442261  3.130981  ... -14.171173  15.532349   \n",
       "115 -10.256775  -4.614746  -5.225464  2.893250  ... -10.509140   9.019501   \n",
       "116  -3.713318  -2.679443  -1.027924 -0.005310  ...  -2.597656   2.653656   \n",
       "118 -15.047607  -9.354492  -5.827576  0.496117  ... -11.149139  13.249451   \n",
       "119 -15.617432  -9.888672  -5.832275  0.607445  ... -11.360870  13.458923   \n",
       "121  -5.013916  -1.445801  -1.567810  0.496094  ...  -4.258362   4.210510   \n",
       "122 -11.629333  -4.908203  -4.655090  2.438904  ... -11.724762   8.990204   \n",
       "123 -27.027039 -16.777344 -12.221069  5.933907  ... -24.461441  20.702362   \n",
       "125  -9.895660  -3.518066  -3.448608  1.502014  ...  -9.100616   5.810669   \n",
       "126  -9.895660  -3.518066  -3.448608  1.502014  ...  -9.100616   5.810669   \n",
       "130 -12.789062  -6.779297  -5.886902  1.937500  ... -16.161224  13.943451   \n",
       "131 -15.410095  -6.933334  -8.956055  1.992371  ... -17.637695  13.013947   \n",
       "133  -5.002533  -4.685059  -3.330933  0.100769  ...  -7.204376   6.235443   \n",
       "134 -11.118835  -5.291504  -4.247131  1.728149  ... -11.034943   9.419556   \n",
       "142  -7.685028  -4.121582  -4.790833  3.090454  ...  -6.041428   5.872040   \n",
       "143  -5.488647  -2.931763  -3.451477  0.962952  ...  -5.872330   6.692596   \n",
       "144  -5.488647  -2.931763  -3.451477  0.962952  ...  -5.872330   6.692596   \n",
       "145 -21.345428 -10.508301 -11.517212  3.598145  ... -24.971695  21.160309   \n",
       "151 -12.021484  -6.784180  -7.369080  4.144287  ... -13.513580  11.668274   \n",
       "155  -6.098267  -4.370117  -2.158020  1.837463  ...  -5.035233   5.159424   \n",
       "156  -5.829742  -3.093262  -1.066833  0.175507  ...  -6.164108   5.784210   \n",
       "157 -13.930695  -8.824707  -5.226746  1.518738  ...  -9.880295  13.295227   \n",
       "158 -13.930695  -8.824707  -5.226746  1.518738  ...  -9.880295  13.295227   \n",
       "159 -15.228485  -9.979980  -5.248474  1.870911  ... -14.077744  13.485321   \n",
       "160 -14.842743 -10.029785  -5.353943  1.821106  ... -14.214951  13.816376   \n",
       "161 -10.878113  -5.381104  -5.273865  1.710632  ...  -8.680008  10.400116   \n",
       "162 -17.821747  -9.395996  -6.045105  2.433899  ... -18.825699  17.579071   \n",
       "163 -17.821747  -9.395996  -6.045105  2.433899  ... -18.825699  17.579071   \n",
       "164 -17.821747  -9.395996  -6.045105  2.433899  ... -18.825699  17.579071   \n",
       "165 -18.169891  -9.335449  -5.883484  2.300110  ... -18.730972  17.522614   \n",
       "166  -3.972107  -3.342285  -1.381714  1.179382  ...  -2.441406   3.896088   \n",
       "169 -12.766785  -4.510010  -2.957703  1.534607  ... -14.396011  12.344818   \n",
       "170  -5.905396  -3.546387  -2.822144  1.008240  ...  -6.260010   4.735626   \n",
       "171  -5.249939  -3.049805  -2.293884  1.325378  ...  -5.622635   3.718262   \n",
       "176 -18.966461  -9.458008  -5.180557  0.863953  ... -14.609207  16.274750   \n",
       "177 -18.972809  -9.577881  -5.172302  0.415710  ... -14.800125  16.052582   \n",
       "179 -19.190552 -11.126709  -5.826599  3.718712  ... -15.603851  13.560974   \n",
       "180 -19.227234 -11.007568  -6.053650  3.728477  ... -15.376556  13.752380   \n",
       "184 -25.945221 -17.168457 -14.075989  6.334473  ... -31.020065  31.113007   \n",
       "185 -26.170807 -17.197021 -14.244385  6.416260  ... -30.718307  31.106415   \n",
       "186 -25.933502 -16.859863 -14.284485  6.110840  ... -30.741257  30.975067   \n",
       "187 -25.932526 -16.904785 -14.219299  6.382324  ... -30.757858  31.241669   \n",
       "188 -25.933502 -16.859863 -14.284485  6.110840  ... -30.741257  30.975067   \n",
       "189  -8.571869  -6.311035  -3.033264  1.308769  ...  -6.169342   5.906952   \n",
       "190  -7.111481  -5.263428  -2.570007  0.593040  ...  -6.632431   5.239227   \n",
       "192  -7.410980  -3.185181  -3.656555  0.798218  ... -10.724762   9.234100   \n",
       "194 -11.445862  -6.838379  -3.620483  2.271423  ... -10.561417  11.048798   \n",
       "195 -15.154205  -8.854980  -6.253113  2.606873  ... -13.396561  13.792694   \n",
       "197  -4.761627  -4.071777  -1.361633  1.200256  ...  -3.563568   3.976685   \n",
       "201  -8.696228  -3.301010  -5.945251  2.254028  ...  -8.261612   7.791473   \n",
       "204 -19.857941  -9.925293 -12.440613  3.841248  ... -16.812668  17.458099   \n",
       "207  -6.409943  -2.534424  -4.257263  1.244202  ...  -5.193039   5.183624   \n",
       "208 -12.683807  -6.164551  -5.036926  2.754395  ... -15.384659  13.237091   \n",
       "211 -11.700104  -8.276855  -5.426086  3.664856  ... -12.495972   8.838593   \n",
       "215 -19.559998 -10.356445  -7.655518  4.289307  ... -21.472580  19.507050   \n",
       "224 -14.242554  -7.511475  -4.585022  3.370605  ... -14.054520  12.717743   \n",
       "226  -7.821564  -4.999512  -3.649719 -0.026855  ...  -8.924896   7.611053   \n",
       "227  -9.056122  -2.819565  -3.095093  1.448608  ...  -8.372681   7.520111   \n",
       "228  -9.056122  -2.819565  -3.095093  1.448608  ...  -8.372681   7.520111   \n",
       "231 -10.239807  -6.954102  -3.367249 -0.229797  ...  -7.102310   8.616180   \n",
       "232 -10.259338  -7.015259  -3.385132 -0.020813  ...  -7.322403   8.423309   \n",
       "233 -10.239807  -6.954102  -3.367249 -0.229797  ...  -7.102310   8.616180   \n",
       "234  -8.413727  -5.578491  -3.540771  0.352722  ...  -7.003632   5.686371   \n",
       "236  -9.431458  -5.494263  -4.789246  0.945923  ...  -8.565750   7.851807   \n",
       "241 -17.493195 -10.038452  -6.956177  3.597740  ... -17.408585  17.893127   \n",
       "243  -6.937927  -4.434082  -2.077454  0.870506  ...  -3.475052   4.236420   \n",
       "\n",
       "          294        295        296        297        298        299  \\\n",
       "0   -3.750214 -16.891113 -11.451591  -1.493896 -12.604256  16.875305   \n",
       "1   -4.474716 -22.738983 -15.406860  -4.236084 -19.165714  23.652893   \n",
       "3   -1.143051  -7.082275  -4.895142  -0.239502  -4.034000   6.109619   \n",
       "4   -7.979095 -38.809570 -29.577148  -6.821167 -21.500793  35.932312   \n",
       "8   -3.113632 -16.996826  -9.999634  -2.942638 -11.624210  17.545715   \n",
       "9   -1.934433 -14.271896  -9.888062  -2.036133 -11.999577  13.343445   \n",
       "10  -2.173691 -14.408524  -9.715515  -1.979004 -11.738468  13.497742   \n",
       "16  -6.087997 -27.226807 -18.794922  -4.572205 -19.284611  27.399597   \n",
       "17  -7.562119 -37.737793 -27.979370  -7.339249 -27.271549  36.026733   \n",
       "18  -7.762314 -37.622314 -28.197632  -7.083740 -27.501041  35.957031   \n",
       "23  -3.634262 -14.338623 -11.374878  -1.979370 -11.031559  11.758545   \n",
       "24  -1.945679 -29.189011 -18.688232  -2.861084 -22.936985  30.397095   \n",
       "25  -3.606934 -26.051514 -16.459839  -3.626709 -18.509384  22.535339   \n",
       "27  -4.142075 -22.876953 -14.836914  -3.104248 -16.658375  24.918701   \n",
       "28  -3.807602 -22.886475 -15.102051  -3.443115 -16.783512  24.443604   \n",
       "29  -4.142075 -22.876953 -14.836914  -3.104248 -16.658375  24.918701   \n",
       "31  -3.086899 -15.902100 -10.765747  -1.415161 -10.333073  14.574707   \n",
       "35  -4.168808 -21.222412 -12.601440  -3.776123 -11.202702  18.309204   \n",
       "39  -2.387680 -10.306885  -6.929810  -0.575623  -7.184635  11.963135   \n",
       "40  -1.790024 -22.317993 -14.784851  -3.555176 -13.525684  23.728333   \n",
       "43  -8.117172 -52.828766 -32.068604  -8.351013 -36.738712  52.848572   \n",
       "44  -1.312103  -6.244385  -4.129272   0.329346  -4.360477   6.594360   \n",
       "47  -6.837265 -22.693848 -14.971191  -3.704987 -17.487247  23.723328   \n",
       "48  -8.223114 -30.312302 -23.728271  -2.714478 -21.808693  28.343628   \n",
       "49  -2.585785 -15.835449 -12.064819  -0.270874  -9.351204  15.373413   \n",
       "50  -2.665863 -15.906738 -12.007202  -0.578491  -9.776985  15.325928   \n",
       "51  -5.018890 -27.133957 -16.355652  -3.688110 -20.807137  26.344360   \n",
       "52  -5.088226 -27.302414 -16.541870  -3.708618 -20.860970  26.391724   \n",
       "54  -4.939316 -15.392334 -10.591553  -1.506348  -9.916508  15.242615   \n",
       "57  -3.235306 -15.268677 -10.504395  -0.827026 -13.090645  16.727066   \n",
       "58  -2.931595 -14.811829 -10.226074  -1.077026 -12.860664  16.335938   \n",
       "59  -4.313568 -21.688232 -14.205444  -3.282349 -15.591679  20.280273   \n",
       "60  -4.313568 -21.688232 -14.205444  -3.282349 -15.591679  20.280273   \n",
       "61  -3.040100 -14.062012 -10.526123   0.137207 -10.204353  14.955505   \n",
       "62  -3.349792 -14.145996  -9.924561  -0.188965 -10.220955  14.720154   \n",
       "63  -3.040100 -14.062012 -10.526123   0.137207 -10.204353  14.955505   \n",
       "64  -2.344559 -15.488861 -11.182617  -0.002075 -12.172310  16.699402   \n",
       "66  -2.694305 -15.214600  -8.951538  -2.486816 -11.285282  15.590698   \n",
       "68  -3.093353 -11.542480  -8.287598  -0.039490  -8.432499  11.741699   \n",
       "69  -0.848602  -9.650192  -7.712036   0.817566  -5.392677   9.776917   \n",
       "77  -4.972137 -26.354340 -21.299927  -1.053589 -19.801636  26.499634   \n",
       "78  -5.154266 -26.424652 -21.062622  -0.847900 -19.641235  26.419800   \n",
       "79  -4.972137 -26.354340 -21.299927  -1.053589 -19.801636  26.499634   \n",
       "80  -3.614365 -27.639572 -16.417709  -3.521729 -13.417297  26.571167   \n",
       "81  -3.614365 -27.639572 -16.417709  -3.521729 -13.417297  26.571167   \n",
       "82  -1.057617 -11.082321  -7.444092  -1.914551  -7.020020  10.668579   \n",
       "86  -5.743866 -21.073730 -13.820312  -2.474121 -15.236458  21.105713   \n",
       "87  -5.106781 -22.004898 -16.092834  -2.616150 -15.873055  19.543152   \n",
       "88  -5.311127 -22.340836 -16.191650  -2.761658 -16.005501  19.764343   \n",
       "89  -5.311127 -22.340836 -16.191650  -2.761658 -16.005501  19.764343   \n",
       "92  -1.904526  -7.341797  -6.720581  -0.606079  -6.292423   7.551941   \n",
       "93  -2.007050  -9.928467  -6.566040  -0.855469  -5.467899   9.439392   \n",
       "94  -1.876083  -9.387817  -7.557007  -1.296753  -8.488102   9.338806   \n",
       "95  -1.974716  -9.393951  -7.368286  -1.370972  -8.569157   9.114685   \n",
       "96  -2.815659 -16.048584 -11.387817  -3.335876 -12.920719  18.357727   \n",
       "97  -2.253265  -7.855225  -7.793091   1.540161  -4.549175   7.670105   \n",
       "98  -2.050629  -7.532227  -7.654663   2.046570  -4.671246   7.561707   \n",
       "103 -2.726059 -16.652756 -11.861816  -1.335083 -10.069889  15.688904   \n",
       "104 -2.686966 -16.627197 -12.627258  -2.717041 -13.489872  16.462402   \n",
       "105 -2.495941 -16.492188 -12.584106  -3.028442 -13.616093  16.116699   \n",
       "106 -3.500366 -16.794189  -9.244995  -2.450317 -11.563477  18.877075   \n",
       "110 -3.376083 -20.929611 -11.547935  -2.362061 -10.841129  20.631531   \n",
       "111 -4.584213 -16.816650 -11.859131  -2.873230 -13.596500  14.514771   \n",
       "112 -6.198471 -25.394348 -17.104248  -3.582397 -16.803532  23.847656   \n",
       "115 -2.487167 -15.332397 -10.432251  -1.952881 -11.788486  14.017273   \n",
       "116 -0.294662  -2.705811  -2.793457   0.653564  -2.618324   4.759705   \n",
       "118 -3.951019 -20.121582 -15.788086  -0.290588 -15.049297  21.129150   \n",
       "119 -3.640594 -20.011719 -15.889404  -0.048340 -15.271618  20.878174   \n",
       "121 -0.789658  -6.663086  -5.030029  -0.448486  -5.352112   6.095398   \n",
       "122 -1.122665 -15.611252 -10.208862  -1.856079 -12.459538  18.087830   \n",
       "123 -5.355698 -35.503662 -26.683960   2.121613 -27.306995  32.626526   \n",
       "125 -0.837616  -9.333420  -7.104614   1.888489  -7.868053  11.166504   \n",
       "126 -0.837616  -9.333420  -7.104614   1.888489  -7.868053  11.166504   \n",
       "130 -5.345200 -25.906250 -15.066406  -2.639893 -15.224674  26.455261   \n",
       "131 -5.282928 -26.817627 -16.822754  -2.698486 -15.718754  28.949341   \n",
       "133 -2.753159 -10.480103  -8.104492  -1.603394  -5.929996  12.464050   \n",
       "134 -4.891830 -17.094040 -11.737183  -2.062866  -9.680973  16.015259   \n",
       "142 -2.727646 -10.237961  -8.912476  -1.980103  -9.695866  11.496445   \n",
       "143 -3.575668 -11.831787  -8.802368  -1.094421  -7.605900   9.610107   \n",
       "144 -3.575668 -11.831787  -8.802368  -1.094421  -7.605900   9.610107   \n",
       "145 -6.620361 -38.599899 -21.519653  -5.080200 -24.934082  35.591125   \n",
       "151 -5.301971 -22.914062 -14.204590  -1.180725 -15.644051  21.639954   \n",
       "155 -1.359116  -9.778931  -6.791626  -0.828979  -6.765690   9.156250   \n",
       "156 -3.281219  -9.730713  -6.632629   0.034302  -5.355595   9.519958   \n",
       "157 -5.029007 -18.625778 -13.343140  -0.846313 -14.691814  20.756531   \n",
       "158 -5.029007 -18.625778 -13.343140  -0.846313 -14.691814  20.756531   \n",
       "159 -6.441742 -26.295456 -16.593506  -1.632935 -16.443462  26.553711   \n",
       "160 -6.632904 -26.498581 -16.487244  -1.574829 -16.019634  26.301758   \n",
       "161 -4.000702 -15.750824 -12.227295  -2.147705 -11.680244  16.623169   \n",
       "162 -6.528534 -33.747314 -17.536499  -5.545532 -20.790470  31.788391   \n",
       "163 -6.528534 -33.747314 -17.536499  -5.545532 -20.790470  31.788391   \n",
       "164 -6.528534 -33.747314 -17.536499  -5.545532 -20.790470  31.788391   \n",
       "165 -6.388885 -33.609131 -17.967163  -5.637177 -20.764286  31.759094   \n",
       "166 -2.223129  -4.984863  -4.556152   0.175781  -4.294502   5.176758   \n",
       "169 -2.689560 -21.560318 -13.105789  -3.334900 -12.713200  21.806702   \n",
       "170 -2.619720  -9.732178  -6.727173  -1.080078  -6.137577   9.851990   \n",
       "171 -1.862183  -7.676270  -4.863892  -1.439575  -5.183105   8.905640   \n",
       "176 -4.639740 -24.144455 -17.261108  -2.163330 -17.949104  27.209656   \n",
       "177 -4.796967 -24.038498 -17.219604  -2.071411 -17.981087  27.360046   \n",
       "179 -4.646011 -25.516281 -18.364990  -2.260254 -18.496288  25.611450   \n",
       "180 -4.846146 -25.276779 -17.979248  -2.349121 -18.810986  26.012817   \n",
       "184 -8.470932 -54.159103 -34.700195 -10.048462 -34.693913  49.258362   \n",
       "185 -8.219955 -54.082932 -34.779297  -9.899948 -34.561588  49.105774   \n",
       "186 -8.332993 -53.979202 -34.483154 -10.080872 -34.559147  49.319153   \n",
       "187 -8.096420 -54.061935 -34.550293 -10.140259 -34.710514  49.174133   \n",
       "188 -8.332993 -53.979202 -34.483154 -10.080872 -34.559147  49.319153   \n",
       "189 -2.949936 -12.323486  -9.637329   0.403503  -7.265293  10.973022   \n",
       "190 -3.176376  -9.960693  -6.509399   0.099365  -6.373295  10.736450   \n",
       "192 -3.045151 -16.721436 -11.015991  -1.571594  -8.315861  15.221375   \n",
       "194 -2.970078 -17.683838 -12.578979  -2.822754 -12.271305  18.311157   \n",
       "195 -3.643295 -25.189697 -15.602417  -4.768921 -20.514927  26.353943   \n",
       "197 -2.011032  -6.695801  -4.965576  -0.228638  -5.398258   6.729065   \n",
       "201 -2.049057 -12.405197  -9.358032  -1.626343 -10.055363  12.161987   \n",
       "204 -6.898621 -27.533615 -22.563721  -1.646240 -23.475109  25.542175   \n",
       "207 -0.999603  -9.083908  -7.314819  -0.672241  -7.765507   7.946716   \n",
       "208 -4.801880 -25.109268 -16.275879  -3.802124 -15.486084  22.717163   \n",
       "211 -4.620590 -19.024750 -15.384399  -3.500122 -14.089390  16.301819   \n",
       "215 -5.974899 -32.193848 -20.097290  -3.867981 -23.562572  32.339417   \n",
       "224 -6.645004 -25.176025 -16.747559  -4.677734 -16.792301  23.433899   \n",
       "226 -1.410751 -14.663940  -8.496460  -1.657837  -7.922184  14.178040   \n",
       "227 -1.505341 -12.344528  -7.812500  -0.440063  -8.150154  12.913391   \n",
       "228 -1.505341 -12.344528  -7.812500  -0.440063  -8.150154  12.913391   \n",
       "231 -2.888016 -12.803955  -9.961304   0.274048  -9.394726  13.853516   \n",
       "232 -2.942123 -12.907715 -10.163208   0.353271  -9.373486  13.530640   \n",
       "233 -2.888016 -12.803955  -9.961304   0.274048  -9.394726  13.853516   \n",
       "234 -2.156235 -11.187134  -7.997559   1.297119  -7.287666  11.016357   \n",
       "236 -1.014145 -12.943817  -9.991455  -1.111328  -8.372990  14.842041   \n",
       "241 -4.643402 -29.401505 -19.960938  -3.248718 -20.830326  28.885010   \n",
       "243 -1.089203  -4.090576  -5.547241   1.768799  -4.327091   5.065430   \n",
       "\n",
       "                   word                                           sentence  \n",
       "0                stands  [ ' W e ' ,   ' i n t r o d u c e ' ,   ' a ' ...  \n",
       "1           is designed  [ ' U n l i k e ' ,   ' r e c e n t ' ,   ' l ...  \n",
       "3                    is  [ ' B E R T ' ,   ' i s ' ,   ' c o n c e p t ...  \n",
       "4               obtains  [ ' I t ' ,   ' o b t a i n s ' ,   ' n e w ' ...  \n",
       "8               present  [ ' W e ' ,   ' p r e s e n t ' ,   ' a ' ,   ...  \n",
       "9                  find  [ ' W e ' ,   ' f i n d ' ,   ' t h a t ' ,   ...  \n",
       "10            published  [ ' W e ' ,   ' f i n d ' ,   ' t h a t ' ,   ...  \n",
       "16               called  [ ' I n ' ,   ' t h i s ' ,   ' w o r k ' ,   ...  \n",
       "17            to reduce  [ ' W h i l e ' ,   ' m o s t ' ,   ' p r i o ...  \n",
       "18            retaining  [ ' W h i l e ' ,   ' m o s t ' ,   ' p r i o ...  \n",
       "23                focus  [ ' W e ' ,   ' f o c u s ' ,   ' o n ' ,   ' ...  \n",
       "24           represents  [ ' W e ' ,   ' f i n d ' ,   ' t h a t ' ,   ...  \n",
       "25          does adjust  [ ' Q u a l i t a t i v e ' ,   ' a n a l y s ...  \n",
       "27             have had  [ ' L a r g e ' ,   ' p r e ' ,   ' - ' ,   ' ...  \n",
       "28           motivating  [ ' L a r g e ' ,   ' p r e ' ,   ' - ' ,   ' ...  \n",
       "29             have had  [ ' L a r g e ' ,   ' p r e ' ,   ' - ' ,   ' ...  \n",
       "31                apply  [ ' C o m p l e m e n t a r y ' ,   ' t o ' , ...  \n",
       "35          is captured  [ ' L a s t l y ' ,   ' , ' ,   ' w e ' ,   ' ...  \n",
       "39             describe  [ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...  \n",
       "40                   is  [ ' O u r ' ,   ' s y s t e m ' ,   ' i s ' , ...  \n",
       "43               assess  [ ' I ' ,   ' a s s e s s ' ,   ' t h e ' ,   ...  \n",
       "44             performs  [ ' T h e ' ,   ' B E R T ' ,   ' m o d e l ' ...  \n",
       "47             includes  [ ' A ' ,   ' n e w ' ,   ' r e l e a s e ' , ...  \n",
       "48             explores  [ ' T h i s ' ,   ' p a p e r ' ,   ' e x p l ...  \n",
       "49              compare  [ ' W e ' ,   ' c o m p a r e ' ,   ' m B E R ...  \n",
       "50                 find  [ ' W e ' ,   ' c o m p a r e ' ,   ' m B E R ...  \n",
       "51          investigate  [ ' A d d i t i o n a l l y ' ,   ' , ' ,   ' ...  \n",
       "52          generalizes  [ ' A d d i t i o n a l l y ' ,   ' , ' ,   ' ...  \n",
       "54                  pre  [ ' L a n g u a g e ' ,   ' m o d e l ' ,   ' ...  \n",
       "57   can be transferred  [ ' B y ' ,   ' l e v e r a g i n g ' ,   ' t ...  \n",
       "58              encoded  [ ' B y ' ,   ' l e v e r a g i n g ' ,   ' t ...  \n",
       "59            introduce  [ ' M o r e o v e r ' ,   ' , ' ,   ' w e ' , ...  \n",
       "60            introduce  [ ' M o r e o v e r ' ,   ' , ' ,   ' w e ' , ...  \n",
       "61          can capture  [ ' T h i s ' ,   ' f r a m e w o r k ' ,   ' ...  \n",
       "62              ensures  [ ' T h i s ' ,   ' f r a m e w o r k ' ,   ' ...  \n",
       "63          can capture  [ ' T h i s ' ,   ' f r a m e w o r k ' ,   ' ...  \n",
       "64                   is  [ ' T i n y B E R T ' ,   ' i s ' ,   ' e m p ...  \n",
       "66                   is  [ ' T i n y B E R T ' ,   ' i s ' ,   ' a l s ...  \n",
       "68         has achieved  [ ' B E R T ' ,   ' , ' ,   ' a ' ,   ' p r e ...  \n",
       "69             describe  [ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...  \n",
       "77              explore  [ ' S i n c e ' ,   ' R e v i e w R C ' ,   ' ...  \n",
       "78              enhance  [ ' S i n c e ' ,   ' R e v i e w R C ' ,   ' ...  \n",
       "79              explore  [ ' S i n c e ' ,   ' R e v i e w R C ' ,   ' ...  \n",
       "80              To show  [ ' T o ' ,   ' s h o w ' ,   ' t h e ' ,   ' ...  \n",
       "81              To show  [ ' T o ' ,   ' s h o w ' ,   ' t h e ' ,   ' ...  \n",
       "82          demonstrate  [ ' E x p e r i m e n t a l ' ,   ' r e s u l ...  \n",
       "86             achieved  [ ' A s ' ,   ' a ' ,   ' s t a t e ' ,   ' - ...  \n",
       "87       to investigate  [ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...  \n",
       "88              provide  [ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...  \n",
       "89              provide  [ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...  \n",
       "92                 show  [ ' W e ' ,   ' s h o w ' ,   ' t h a t ' ,   ...  \n",
       "93                gives  [ ' T h i s ' ,   ' f o r m u l a t i o n ' , ...  \n",
       "94             generate  [ ' W e ' ,   ' g e n e r a t e ' ,   ' f r o ...  \n",
       "95              produce  [ ' W e ' ,   ' g e n e r a t e ' ,   ' f r o ...  \n",
       "96             Compared  [ ' C o m p a r e d ' ,   ' t o ' ,   ' t h e ...  \n",
       "97                  has  [ ' B E R T ' ,   ' h a s ' ,   ' a ' ,   ' M ...  \n",
       "98                 Must  [ ' B E R T ' ,   ' h a s ' ,   ' a ' ,   ' M ...  \n",
       "103            applying  [ ' F o l l o w i n g ' ,   ' r e c e n t ' , ...  \n",
       "104               posed  [ ' T h i s ' ,   ' r e q u i r e d ' ,   ' c ...  \n",
       "105         confronting  [ ' T h i s ' ,   ' r e q u i r e d ' ,   ' c ...  \n",
       "106             address  [ ' W e ' ,   ' a d d r e s s ' ,   ' t h i s ...  \n",
       "110          contribute  [ ' B E R T ' ,   ' - ' ,   ' b a s e d ' ,   ...  \n",
       "111               focus  [ ' I n ' ,   ' t h e ' ,   ' c u r r e n t ' ...  \n",
       "112             encoded  [ ' U s i n g ' ,   ' a ' ,   ' s u b s e t ' ...  \n",
       "115               leads  [ ' W e ' ,   ' s h o w ' ,   ' t h a t ' ,   ...  \n",
       "116           Revealing  [ ' R e v e a l i n g ' ,   ' t h e ' ,   ' D ...  \n",
       "118   has been released  [ ' R e c e n t l y ' ,   ' , ' ,   ' a n ' , ...  \n",
       "119             masking  [ ' R e c e n t l y ' ,   ' , ' ,   ' a n ' , ...  \n",
       "121         was trained  [ ' T h e ' ,   ' m o d e l ' ,   ' w a s ' , ...  \n",
       "122          to provide  [ ' W e ' ,   ' a i m ' ,   ' t o ' ,   ' p r ...  \n",
       "123         is verified  [ ' T h e ' ,   ' m o d e l ' ,   ' i s ' ,   ...  \n",
       "125             examine  [ ' M o r e o v e r ' ,   ' , ' ,   ' w e ' , ...  \n",
       "126             examine  [ ' M o r e o v e r ' ,   ' , ' ,   ' w e ' , ...  \n",
       "130                BERT  [ ' H o w e v e r ' ,   ' , ' ,   ' p r e v i ...  \n",
       "131             propose  [ ' T o ' ,   ' t a c k l e ' ,   ' t h i s ' ...  \n",
       "133               gains  [ ' B y ' ,   ' l e v e r a g i n g ' ,   ' a ...  \n",
       "134              showed  [ ' E x p e r i m e n t s ' ,   ' o n ' ,   ' ...  \n",
       "142           exploring  [ ' H o w e v e r ' ,   ' , ' ,   ' t h e r e ...  \n",
       "143               based  [ ' I n ' ,   ' t h i s ' ,   ' w o r k ' ,   ...  \n",
       "144               based  [ ' I n ' ,   ' t h i s ' ,   ' w o r k ' ,   ...  \n",
       "145            achieves  [ ' E x p e r i m e n t a l ' ,   ' r e s u l ...  \n",
       "151               built  [ ' I t ' ,   ' e n a b l e s ' ,   ' s e a m ...  \n",
       "155                BERT  [ ' B E R T ' ,   ' w i t h ' ,   ' H i s t o ...  \n",
       "156             studies  [ ' T h i s ' ,   ' p a p e r ' ,   ' s t u d ...  \n",
       "157         to leverage  [ ' W e ' ,   ' e x p l o r e ' ,   ' s e v e ...  \n",
       "158         to leverage  [ ' W e ' ,   ' e x p l o r e ' ,   ' s e v e ...  \n",
       "159         demonstrate  [ ' E x p e r i m e n t a l ' ,   ' r e s u l ...  \n",
       "160           answering  [ ' E x p e r i m e n t a l ' ,   ' r e s u l ...  \n",
       "161                 pre  [ ' E x p e r i m e n t a l ' ,   ' r e s u l ...  \n",
       "162           allocates  [ ' A n a l y s e s ' ,   ' i l l u s t r a t ...  \n",
       "163           allocates  [ ' A n a l y s e s ' ,   ' i l l u s t r a t ...  \n",
       "164           allocates  [ ' A n a l y s e s ' ,   ' i l l u s t r a t ...  \n",
       "165             prefers  [ ' A n a l y s e s ' ,   ' i l l u s t r a t ...  \n",
       "166       Understanding  [ ' U n d e r s t a n d i n g ' ,   ' t h e ' ...  \n",
       "169             achieve  [ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...  \n",
       "170            to apply  [ ' T o ' ,   ' o u r ' ,   ' k n o w l e d g ...  \n",
       "171             provide  [ ' O u r ' ,   ' m o d e l s ' ,   ' p r o v ...  \n",
       "176             explore  [ ' W e ' ,   ' e x p l o r e ' ,   ' t h e ' ...  \n",
       "177                 add  [ ' W e ' ,   ' e x p l o r e ' ,   ' t h e ' ...  \n",
       "179               using  [ ' B y ' ,   ' u s i n g ' ,   ' P A L s ' , ...  \n",
       "180               tuned  [ ' B y ' ,   ' u s i n g ' ,   ' P A L s ' , ...  \n",
       "184               apply  [ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...  \n",
       "185     can distinguish  [ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...  \n",
       "186               shows  [ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...  \n",
       "187           struggles  [ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...  \n",
       "188               shows  [ ' A s ' ,   ' a ' ,   ' c a s e ' ,   ' s t ...  \n",
       "189                  Is  [ ' W h a t ' ,   ' B E R T ' ,   ' I s ' ,   ...  \n",
       "190                 pre  [ ' L a n g u a g e ' ,   ' m o d e l ' ,   ' ...  \n",
       "192              tuning  [ ' I n ' ,   ' t h i s ' ,   ' p a p e r ' , ...  \n",
       "194         demonstrate  [ ' W e ' ,   ' a l s o ' ,   ' d e m o n s t ...  \n",
       "195              tuning  [ ' S e c o n d ' ,   ' , ' ,   ' t h e ' ,   ...  \n",
       "197       Understanding  [ ' V i s u a l i z i n g ' ,   ' a n d ' ,   ...  \n",
       "201             propose  [ ' W e ' ,   ' p r o p o s e ' ,   ' a ' ,   ...  \n",
       "204            appeared  [ ' W e ' ,   ' r e t r o f i t ' ,   ' B E R ...  \n",
       "207                 can  [ ' T h e ' ,   ' w e l l ' ,   ' t r a i n e ...  \n",
       "208                show  [ ' E x p e r i m e n t s ' ,   ' o n ' ,   ' ...  \n",
       "211            Starting  [ ' S t a r t i n g ' ,   ' f r o m ' ,   ' a ...  \n",
       "215               using  [ ' R e c e n t l y ' ,   ' , ' ,   ' a ' ,   ...  \n",
       "224               argue  [ ' W e ' ,   ' t a k e ' ,   ' i s s u e ' , ...  \n",
       "226               drops  [ ' M o r e ' ,   ' s p e c i f i c a l l y ' ...  \n",
       "227             propose  [ ' A s ' ,   ' a ' ,   ' r e m e d y ' ,   ' ...  \n",
       "228             propose  [ ' A s ' ,   ' a ' ,   ' r e m e d y ' ,   ' ...  \n",
       "231                show  [ ' W e ' ,   ' t a k e ' ,   ' t h i s ' ,   ...  \n",
       "232                take  [ ' W e ' ,   ' t a k e ' ,   ' t h i s ' ,   ...  \n",
       "233                show  [ ' W e ' ,   ' t a k e ' ,   ' t h i s ' ,   ...  \n",
       "234                  is  [ ' B E R T ' ,   ' i s ' ,   ' N o t ' ,   ' ...  \n",
       "236            produced  [ ' H o w e v e r ' ,   ' , ' ,   ' j u s t ' ...  \n",
       "241        be explained  [ ' I n ' ,   ' a l l ' ,   ' l a y e r s ' , ...  \n",
       "243           Comparing  [ ' C o m p a r i n g ' ,   ' t h e ' ,   ' G ...  \n",
       "\n",
       "[129 rows x 302 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_w2v_inv = csv.groupby(\n",
    "    csv.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: embed_subtract(\n",
    "        get_phrase_embed_word2vec(\n",
    "            word2vec,\n",
    "            group.iloc[0]['averb']\n",
    "        ), \n",
    "        get_phrase_embed_word2vec(\n",
    "            word2vec,\n",
    "            ' '.join(group.iloc[0]['split_tokens'])\n",
    "        ), 300)\n",
    ").reset_index(level=1, drop=True)\n",
    "output_w2v_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid is the arithmetic mean position of all points in the figure\n",
    "output_w2v_c = pd.DataFrame(\n",
    "    [np.mean(output_w2v.iloc[:, 0:300])])\n",
    "output_w2v_c['word'] = \"[CENTROID]\"\n",
    "output_w2v_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import ast\n",
    "\n",
    "# save ELMo outputs so rerunning doesn't take forever...\n",
    "all_sentences = [' '.join(ast.literal_eval(r)) for r in csv['split_tokens']]\n",
    "embeddings = elmo(\n",
    "    all_sentences, \n",
    "    signature='default', \n",
    "    as_dict=True)['elmo']\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    x = sess.run(embeddings)\n",
    "\n",
    "x = dict(zip(all_sentences, x))\n",
    "pickle.dump(x, open(\"temp/BERT_ELMo.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059835</td>\n",
       "      <td>0.240861</td>\n",
       "      <td>0.275318</td>\n",
       "      <td>-0.019283</td>\n",
       "      <td>0.400894</td>\n",
       "      <td>-0.494091</td>\n",
       "      <td>-0.582469</td>\n",
       "      <td>0.771119</td>\n",
       "      <td>0.678693</td>\n",
       "      <td>0.157774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239020</td>\n",
       "      <td>-0.453651</td>\n",
       "      <td>0.498861</td>\n",
       "      <td>-0.247483</td>\n",
       "      <td>0.007910</td>\n",
       "      <td>0.221942</td>\n",
       "      <td>-0.369337</td>\n",
       "      <td>0.173195</td>\n",
       "      <td>1.074753</td>\n",
       "      <td>stands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.479267</td>\n",
       "      <td>-0.340685</td>\n",
       "      <td>-0.083469</td>\n",
       "      <td>0.484785</td>\n",
       "      <td>-0.378456</td>\n",
       "      <td>-0.478947</td>\n",
       "      <td>-1.161890</td>\n",
       "      <td>0.293845</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>1.032229</td>\n",
       "      <td>...</td>\n",
       "      <td>1.752333</td>\n",
       "      <td>-0.459487</td>\n",
       "      <td>-0.223691</td>\n",
       "      <td>-0.280019</td>\n",
       "      <td>-0.589877</td>\n",
       "      <td>1.815206</td>\n",
       "      <td>-1.060352</td>\n",
       "      <td>1.315524</td>\n",
       "      <td>-0.427330</td>\n",
       "      <td>is designed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.551758</td>\n",
       "      <td>-0.601709</td>\n",
       "      <td>-0.185830</td>\n",
       "      <td>0.249082</td>\n",
       "      <td>0.240786</td>\n",
       "      <td>0.193926</td>\n",
       "      <td>0.498678</td>\n",
       "      <td>1.359404</td>\n",
       "      <td>-0.046613</td>\n",
       "      <td>-0.063560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605742</td>\n",
       "      <td>-0.658780</td>\n",
       "      <td>0.295447</td>\n",
       "      <td>-0.233592</td>\n",
       "      <td>-0.069840</td>\n",
       "      <td>0.409337</td>\n",
       "      <td>-0.145462</td>\n",
       "      <td>0.840502</td>\n",
       "      <td>0.370108</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.014832</td>\n",
       "      <td>0.090976</td>\n",
       "      <td>0.263633</td>\n",
       "      <td>0.249120</td>\n",
       "      <td>-0.170383</td>\n",
       "      <td>-0.548515</td>\n",
       "      <td>0.152994</td>\n",
       "      <td>-0.380535</td>\n",
       "      <td>-0.093677</td>\n",
       "      <td>0.559947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620792</td>\n",
       "      <td>-0.345686</td>\n",
       "      <td>-0.029003</td>\n",
       "      <td>0.367554</td>\n",
       "      <td>-0.847556</td>\n",
       "      <td>0.339698</td>\n",
       "      <td>0.421686</td>\n",
       "      <td>1.098213</td>\n",
       "      <td>-0.169227</td>\n",
       "      <td>obtains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.181994</td>\n",
       "      <td>0.605189</td>\n",
       "      <td>-0.997008</td>\n",
       "      <td>-0.302269</td>\n",
       "      <td>-0.015252</td>\n",
       "      <td>-0.242133</td>\n",
       "      <td>0.398710</td>\n",
       "      <td>0.620097</td>\n",
       "      <td>-0.426464</td>\n",
       "      <td>-0.437437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248234</td>\n",
       "      <td>0.209224</td>\n",
       "      <td>-0.108337</td>\n",
       "      <td>0.851365</td>\n",
       "      <td>0.451605</td>\n",
       "      <td>0.213644</td>\n",
       "      <td>-0.125964</td>\n",
       "      <td>-0.765226</td>\n",
       "      <td>-0.007932</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.315933</td>\n",
       "      <td>-0.017035</td>\n",
       "      <td>-0.117964</td>\n",
       "      <td>-0.383910</td>\n",
       "      <td>-0.521591</td>\n",
       "      <td>0.075804</td>\n",
       "      <td>0.260689</td>\n",
       "      <td>0.050643</td>\n",
       "      <td>0.095864</td>\n",
       "      <td>0.354527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181811</td>\n",
       "      <td>-0.030457</td>\n",
       "      <td>0.430391</td>\n",
       "      <td>0.794675</td>\n",
       "      <td>0.130590</td>\n",
       "      <td>0.387605</td>\n",
       "      <td>0.500386</td>\n",
       "      <td>0.055710</td>\n",
       "      <td>0.285135</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.439284</td>\n",
       "      <td>-0.402886</td>\n",
       "      <td>0.087319</td>\n",
       "      <td>-0.258886</td>\n",
       "      <td>0.390618</td>\n",
       "      <td>0.289037</td>\n",
       "      <td>-0.358386</td>\n",
       "      <td>0.335580</td>\n",
       "      <td>-0.254189</td>\n",
       "      <td>0.151227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485583</td>\n",
       "      <td>0.324492</td>\n",
       "      <td>0.135045</td>\n",
       "      <td>-0.480554</td>\n",
       "      <td>0.113316</td>\n",
       "      <td>-0.687692</td>\n",
       "      <td>0.051682</td>\n",
       "      <td>0.121813</td>\n",
       "      <td>0.081608</td>\n",
       "      <td>published</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.084465</td>\n",
       "      <td>-0.285254</td>\n",
       "      <td>-0.678433</td>\n",
       "      <td>-0.661452</td>\n",
       "      <td>0.258676</td>\n",
       "      <td>-0.566749</td>\n",
       "      <td>0.016577</td>\n",
       "      <td>-0.123753</td>\n",
       "      <td>0.014325</td>\n",
       "      <td>0.238596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470453</td>\n",
       "      <td>-0.487008</td>\n",
       "      <td>0.392476</td>\n",
       "      <td>-0.356934</td>\n",
       "      <td>-0.595192</td>\n",
       "      <td>-0.263980</td>\n",
       "      <td>-0.218496</td>\n",
       "      <td>-0.197325</td>\n",
       "      <td>0.190303</td>\n",
       "      <td>called</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.815184</td>\n",
       "      <td>0.874137</td>\n",
       "      <td>-0.904312</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>-0.145967</td>\n",
       "      <td>-1.167747</td>\n",
       "      <td>-0.479527</td>\n",
       "      <td>0.078829</td>\n",
       "      <td>0.937768</td>\n",
       "      <td>-0.005589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.682849</td>\n",
       "      <td>-0.757245</td>\n",
       "      <td>0.783076</td>\n",
       "      <td>0.242215</td>\n",
       "      <td>-0.745963</td>\n",
       "      <td>0.686173</td>\n",
       "      <td>0.456795</td>\n",
       "      <td>-0.594572</td>\n",
       "      <td>-0.332893</td>\n",
       "      <td>to reduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.498545</td>\n",
       "      <td>0.375550</td>\n",
       "      <td>-0.521380</td>\n",
       "      <td>-0.145479</td>\n",
       "      <td>0.318801</td>\n",
       "      <td>-0.570060</td>\n",
       "      <td>-0.201639</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>0.423595</td>\n",
       "      <td>-0.227575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.718149</td>\n",
       "      <td>0.397363</td>\n",
       "      <td>0.408168</td>\n",
       "      <td>0.042329</td>\n",
       "      <td>-0.874830</td>\n",
       "      <td>0.505904</td>\n",
       "      <td>-0.435902</td>\n",
       "      <td>-0.058884</td>\n",
       "      <td>0.048013</td>\n",
       "      <td>retaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.220900</td>\n",
       "      <td>-0.208086</td>\n",
       "      <td>-0.592484</td>\n",
       "      <td>-0.644049</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>-0.630416</td>\n",
       "      <td>0.668904</td>\n",
       "      <td>0.465420</td>\n",
       "      <td>-0.401957</td>\n",
       "      <td>1.085995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088196</td>\n",
       "      <td>0.072950</td>\n",
       "      <td>-0.040444</td>\n",
       "      <td>0.675150</td>\n",
       "      <td>-0.484231</td>\n",
       "      <td>-0.085576</td>\n",
       "      <td>-0.377170</td>\n",
       "      <td>-0.293063</td>\n",
       "      <td>-0.034907</td>\n",
       "      <td>focus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.252087</td>\n",
       "      <td>-0.110759</td>\n",
       "      <td>0.149693</td>\n",
       "      <td>-0.030479</td>\n",
       "      <td>0.573090</td>\n",
       "      <td>-0.059328</td>\n",
       "      <td>-0.027973</td>\n",
       "      <td>-0.060144</td>\n",
       "      <td>-0.739402</td>\n",
       "      <td>-0.121876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501332</td>\n",
       "      <td>-0.351525</td>\n",
       "      <td>0.612804</td>\n",
       "      <td>0.455139</td>\n",
       "      <td>-0.259600</td>\n",
       "      <td>0.212947</td>\n",
       "      <td>-0.156704</td>\n",
       "      <td>0.725402</td>\n",
       "      <td>0.219973</td>\n",
       "      <td>represents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.031056</td>\n",
       "      <td>0.511771</td>\n",
       "      <td>-0.402305</td>\n",
       "      <td>0.392806</td>\n",
       "      <td>1.072279</td>\n",
       "      <td>-1.198256</td>\n",
       "      <td>0.544827</td>\n",
       "      <td>-0.259022</td>\n",
       "      <td>-0.309486</td>\n",
       "      <td>-0.354982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043094</td>\n",
       "      <td>-0.937673</td>\n",
       "      <td>-0.259689</td>\n",
       "      <td>1.527750</td>\n",
       "      <td>-0.508610</td>\n",
       "      <td>0.901813</td>\n",
       "      <td>0.892514</td>\n",
       "      <td>-0.107142</td>\n",
       "      <td>0.167213</td>\n",
       "      <td>does adjust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.122050</td>\n",
       "      <td>0.949302</td>\n",
       "      <td>-0.864618</td>\n",
       "      <td>0.426060</td>\n",
       "      <td>2.344748</td>\n",
       "      <td>-0.409799</td>\n",
       "      <td>-0.064119</td>\n",
       "      <td>0.421269</td>\n",
       "      <td>-0.476417</td>\n",
       "      <td>-0.644752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271764</td>\n",
       "      <td>-0.605495</td>\n",
       "      <td>0.335029</td>\n",
       "      <td>-0.237850</td>\n",
       "      <td>0.670713</td>\n",
       "      <td>1.014461</td>\n",
       "      <td>-0.409830</td>\n",
       "      <td>0.442895</td>\n",
       "      <td>-0.383798</td>\n",
       "      <td>have had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.461630</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>-0.511341</td>\n",
       "      <td>0.364487</td>\n",
       "      <td>0.381569</td>\n",
       "      <td>0.259681</td>\n",
       "      <td>0.038859</td>\n",
       "      <td>0.290896</td>\n",
       "      <td>0.433663</td>\n",
       "      <td>0.678046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.436564</td>\n",
       "      <td>0.330171</td>\n",
       "      <td>-0.354160</td>\n",
       "      <td>-0.238221</td>\n",
       "      <td>0.177976</td>\n",
       "      <td>-0.224470</td>\n",
       "      <td>0.346975</td>\n",
       "      <td>-0.132699</td>\n",
       "      <td>-0.541793</td>\n",
       "      <td>motivating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.122050</td>\n",
       "      <td>0.949302</td>\n",
       "      <td>-0.864618</td>\n",
       "      <td>0.426060</td>\n",
       "      <td>2.344748</td>\n",
       "      <td>-0.409799</td>\n",
       "      <td>-0.064119</td>\n",
       "      <td>0.421269</td>\n",
       "      <td>-0.476417</td>\n",
       "      <td>-0.644752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271764</td>\n",
       "      <td>-0.605495</td>\n",
       "      <td>0.335029</td>\n",
       "      <td>-0.237850</td>\n",
       "      <td>0.670713</td>\n",
       "      <td>1.014461</td>\n",
       "      <td>-0.409830</td>\n",
       "      <td>0.442895</td>\n",
       "      <td>-0.383798</td>\n",
       "      <td>have had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.755990</td>\n",
       "      <td>0.733870</td>\n",
       "      <td>0.350174</td>\n",
       "      <td>-0.242800</td>\n",
       "      <td>0.857030</td>\n",
       "      <td>0.161440</td>\n",
       "      <td>-0.314395</td>\n",
       "      <td>-0.238194</td>\n",
       "      <td>-0.492384</td>\n",
       "      <td>0.025187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096688</td>\n",
       "      <td>-0.139346</td>\n",
       "      <td>-0.121755</td>\n",
       "      <td>0.893739</td>\n",
       "      <td>-0.041012</td>\n",
       "      <td>0.497487</td>\n",
       "      <td>0.272676</td>\n",
       "      <td>-0.097849</td>\n",
       "      <td>-0.120186</td>\n",
       "      <td>apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.355967</td>\n",
       "      <td>0.024432</td>\n",
       "      <td>0.499512</td>\n",
       "      <td>0.470141</td>\n",
       "      <td>0.026586</td>\n",
       "      <td>-0.896237</td>\n",
       "      <td>-0.924498</td>\n",
       "      <td>0.011559</td>\n",
       "      <td>-1.753619</td>\n",
       "      <td>0.198732</td>\n",
       "      <td>...</td>\n",
       "      <td>1.433819</td>\n",
       "      <td>-0.251986</td>\n",
       "      <td>-0.020722</td>\n",
       "      <td>0.320408</td>\n",
       "      <td>0.399718</td>\n",
       "      <td>-0.774725</td>\n",
       "      <td>-0.447722</td>\n",
       "      <td>0.926539</td>\n",
       "      <td>0.192881</td>\n",
       "      <td>is captured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.382333</td>\n",
       "      <td>0.323978</td>\n",
       "      <td>-0.130768</td>\n",
       "      <td>-0.194236</td>\n",
       "      <td>0.576638</td>\n",
       "      <td>0.374112</td>\n",
       "      <td>-0.376351</td>\n",
       "      <td>-0.234408</td>\n",
       "      <td>-0.173447</td>\n",
       "      <td>-0.251889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833246</td>\n",
       "      <td>-0.385633</td>\n",
       "      <td>0.127101</td>\n",
       "      <td>-0.034632</td>\n",
       "      <td>0.070954</td>\n",
       "      <td>-0.510118</td>\n",
       "      <td>-0.082089</td>\n",
       "      <td>-0.185954</td>\n",
       "      <td>0.178776</td>\n",
       "      <td>describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.067147</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>-0.092259</td>\n",
       "      <td>-0.159158</td>\n",
       "      <td>0.181222</td>\n",
       "      <td>-0.282149</td>\n",
       "      <td>-0.241155</td>\n",
       "      <td>0.374727</td>\n",
       "      <td>-0.658454</td>\n",
       "      <td>-0.262804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218099</td>\n",
       "      <td>-0.927582</td>\n",
       "      <td>0.520521</td>\n",
       "      <td>0.267276</td>\n",
       "      <td>-0.284520</td>\n",
       "      <td>0.222652</td>\n",
       "      <td>-0.485243</td>\n",
       "      <td>0.633723</td>\n",
       "      <td>0.211128</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.073721</td>\n",
       "      <td>-0.287547</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>-0.238369</td>\n",
       "      <td>0.153739</td>\n",
       "      <td>0.252630</td>\n",
       "      <td>-0.122325</td>\n",
       "      <td>-0.034344</td>\n",
       "      <td>-0.326629</td>\n",
       "      <td>0.457434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164473</td>\n",
       "      <td>-0.661984</td>\n",
       "      <td>0.392214</td>\n",
       "      <td>0.456948</td>\n",
       "      <td>0.391443</td>\n",
       "      <td>0.323856</td>\n",
       "      <td>0.224405</td>\n",
       "      <td>-0.218731</td>\n",
       "      <td>-0.187188</td>\n",
       "      <td>assess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.068286</td>\n",
       "      <td>0.493022</td>\n",
       "      <td>0.123446</td>\n",
       "      <td>0.266689</td>\n",
       "      <td>0.511212</td>\n",
       "      <td>-0.718671</td>\n",
       "      <td>-0.138341</td>\n",
       "      <td>0.254072</td>\n",
       "      <td>0.220373</td>\n",
       "      <td>-0.292399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701825</td>\n",
       "      <td>0.487816</td>\n",
       "      <td>-0.197305</td>\n",
       "      <td>0.244604</td>\n",
       "      <td>0.182160</td>\n",
       "      <td>0.150620</td>\n",
       "      <td>-0.232240</td>\n",
       "      <td>1.072437</td>\n",
       "      <td>1.076384</td>\n",
       "      <td>performs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.108845</td>\n",
       "      <td>0.687509</td>\n",
       "      <td>-0.152748</td>\n",
       "      <td>-0.056645</td>\n",
       "      <td>-0.189407</td>\n",
       "      <td>-0.154639</td>\n",
       "      <td>0.781604</td>\n",
       "      <td>0.434901</td>\n",
       "      <td>-0.007707</td>\n",
       "      <td>0.062158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686036</td>\n",
       "      <td>-0.295823</td>\n",
       "      <td>0.022191</td>\n",
       "      <td>-0.524981</td>\n",
       "      <td>-0.225768</td>\n",
       "      <td>0.725936</td>\n",
       "      <td>-0.513620</td>\n",
       "      <td>0.288689</td>\n",
       "      <td>0.744691</td>\n",
       "      <td>includes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.411049</td>\n",
       "      <td>0.084121</td>\n",
       "      <td>-0.147872</td>\n",
       "      <td>0.031999</td>\n",
       "      <td>0.046202</td>\n",
       "      <td>-0.053003</td>\n",
       "      <td>0.677854</td>\n",
       "      <td>-0.056690</td>\n",
       "      <td>-0.326474</td>\n",
       "      <td>-0.317143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569455</td>\n",
       "      <td>-0.522795</td>\n",
       "      <td>0.505677</td>\n",
       "      <td>0.385698</td>\n",
       "      <td>0.227224</td>\n",
       "      <td>-0.110392</td>\n",
       "      <td>-0.252462</td>\n",
       "      <td>0.358088</td>\n",
       "      <td>0.426464</td>\n",
       "      <td>explores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.330973</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>-0.149374</td>\n",
       "      <td>0.012377</td>\n",
       "      <td>0.510642</td>\n",
       "      <td>-0.182420</td>\n",
       "      <td>0.332414</td>\n",
       "      <td>-0.034646</td>\n",
       "      <td>0.327653</td>\n",
       "      <td>0.097017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195848</td>\n",
       "      <td>-0.608211</td>\n",
       "      <td>0.562728</td>\n",
       "      <td>1.002413</td>\n",
       "      <td>0.343637</td>\n",
       "      <td>0.654133</td>\n",
       "      <td>-0.067831</td>\n",
       "      <td>0.071678</td>\n",
       "      <td>-0.252707</td>\n",
       "      <td>compare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.244245</td>\n",
       "      <td>0.089943</td>\n",
       "      <td>0.221659</td>\n",
       "      <td>-0.075435</td>\n",
       "      <td>-0.312054</td>\n",
       "      <td>0.153455</td>\n",
       "      <td>0.203115</td>\n",
       "      <td>-0.016109</td>\n",
       "      <td>0.321191</td>\n",
       "      <td>0.042987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189089</td>\n",
       "      <td>-0.135420</td>\n",
       "      <td>-0.091386</td>\n",
       "      <td>1.081662</td>\n",
       "      <td>0.191716</td>\n",
       "      <td>0.382711</td>\n",
       "      <td>0.630311</td>\n",
       "      <td>0.016077</td>\n",
       "      <td>-0.210032</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.241720</td>\n",
       "      <td>0.024009</td>\n",
       "      <td>-0.620501</td>\n",
       "      <td>0.120691</td>\n",
       "      <td>-0.006524</td>\n",
       "      <td>-0.164343</td>\n",
       "      <td>0.143863</td>\n",
       "      <td>0.090789</td>\n",
       "      <td>-0.238136</td>\n",
       "      <td>0.386626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232300</td>\n",
       "      <td>-0.224819</td>\n",
       "      <td>0.716208</td>\n",
       "      <td>0.925934</td>\n",
       "      <td>0.663127</td>\n",
       "      <td>0.337111</td>\n",
       "      <td>0.209465</td>\n",
       "      <td>0.085920</td>\n",
       "      <td>-0.198426</td>\n",
       "      <td>investigate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.139784</td>\n",
       "      <td>0.282808</td>\n",
       "      <td>0.098378</td>\n",
       "      <td>0.399734</td>\n",
       "      <td>0.659067</td>\n",
       "      <td>0.015212</td>\n",
       "      <td>0.514658</td>\n",
       "      <td>0.351281</td>\n",
       "      <td>0.019359</td>\n",
       "      <td>0.117071</td>\n",
       "      <td>...</td>\n",
       "      <td>1.121217</td>\n",
       "      <td>-0.712355</td>\n",
       "      <td>0.407688</td>\n",
       "      <td>0.222121</td>\n",
       "      <td>0.211399</td>\n",
       "      <td>0.240220</td>\n",
       "      <td>0.275232</td>\n",
       "      <td>0.383000</td>\n",
       "      <td>0.673816</td>\n",
       "      <td>generalizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.367494</td>\n",
       "      <td>-0.796367</td>\n",
       "      <td>-0.328557</td>\n",
       "      <td>0.440196</td>\n",
       "      <td>0.587401</td>\n",
       "      <td>-0.421285</td>\n",
       "      <td>-0.094993</td>\n",
       "      <td>0.791555</td>\n",
       "      <td>-0.020347</td>\n",
       "      <td>-0.320504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766191</td>\n",
       "      <td>-0.411308</td>\n",
       "      <td>0.110840</td>\n",
       "      <td>0.456917</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>0.133110</td>\n",
       "      <td>-0.543383</td>\n",
       "      <td>0.226422</td>\n",
       "      <td>-0.259586</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-1.697060</td>\n",
       "      <td>2.623034</td>\n",
       "      <td>0.321020</td>\n",
       "      <td>0.414173</td>\n",
       "      <td>2.568970</td>\n",
       "      <td>-0.878483</td>\n",
       "      <td>-1.340148</td>\n",
       "      <td>0.580774</td>\n",
       "      <td>0.174799</td>\n",
       "      <td>-0.558720</td>\n",
       "      <td>...</td>\n",
       "      <td>1.605328</td>\n",
       "      <td>-1.827377</td>\n",
       "      <td>0.584723</td>\n",
       "      <td>0.409072</td>\n",
       "      <td>-0.012171</td>\n",
       "      <td>-1.364160</td>\n",
       "      <td>0.879133</td>\n",
       "      <td>0.811989</td>\n",
       "      <td>0.705468</td>\n",
       "      <td>can be transferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.259860</td>\n",
       "      <td>0.439059</td>\n",
       "      <td>0.398028</td>\n",
       "      <td>1.014346</td>\n",
       "      <td>-0.376176</td>\n",
       "      <td>-0.578750</td>\n",
       "      <td>-0.077481</td>\n",
       "      <td>-0.038502</td>\n",
       "      <td>-0.488136</td>\n",
       "      <td>0.207464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862793</td>\n",
       "      <td>-0.012750</td>\n",
       "      <td>0.423572</td>\n",
       "      <td>0.245964</td>\n",
       "      <td>0.406454</td>\n",
       "      <td>-0.574949</td>\n",
       "      <td>0.132890</td>\n",
       "      <td>0.237464</td>\n",
       "      <td>-0.476773</td>\n",
       "      <td>encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.510059</td>\n",
       "      <td>0.777036</td>\n",
       "      <td>-0.647992</td>\n",
       "      <td>-0.431016</td>\n",
       "      <td>0.293009</td>\n",
       "      <td>0.300757</td>\n",
       "      <td>0.028609</td>\n",
       "      <td>0.200901</td>\n",
       "      <td>-0.264500</td>\n",
       "      <td>-0.314564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476622</td>\n",
       "      <td>0.369570</td>\n",
       "      <td>0.120154</td>\n",
       "      <td>0.742265</td>\n",
       "      <td>-0.042360</td>\n",
       "      <td>0.194151</td>\n",
       "      <td>0.213807</td>\n",
       "      <td>-0.428823</td>\n",
       "      <td>-0.113527</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.510059</td>\n",
       "      <td>0.777036</td>\n",
       "      <td>-0.647992</td>\n",
       "      <td>-0.431016</td>\n",
       "      <td>0.293009</td>\n",
       "      <td>0.300757</td>\n",
       "      <td>0.028609</td>\n",
       "      <td>0.200901</td>\n",
       "      <td>-0.264500</td>\n",
       "      <td>-0.314564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476622</td>\n",
       "      <td>0.369570</td>\n",
       "      <td>0.120154</td>\n",
       "      <td>0.742265</td>\n",
       "      <td>-0.042360</td>\n",
       "      <td>0.194151</td>\n",
       "      <td>0.213807</td>\n",
       "      <td>-0.428823</td>\n",
       "      <td>-0.113527</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.372455</td>\n",
       "      <td>0.313116</td>\n",
       "      <td>-0.421734</td>\n",
       "      <td>-0.302487</td>\n",
       "      <td>0.391502</td>\n",
       "      <td>-2.002126</td>\n",
       "      <td>-0.294954</td>\n",
       "      <td>0.323469</td>\n",
       "      <td>-1.068244</td>\n",
       "      <td>-0.748371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465439</td>\n",
       "      <td>-0.479181</td>\n",
       "      <td>1.058791</td>\n",
       "      <td>0.144505</td>\n",
       "      <td>0.102943</td>\n",
       "      <td>0.351012</td>\n",
       "      <td>0.322578</td>\n",
       "      <td>1.222291</td>\n",
       "      <td>-0.365344</td>\n",
       "      <td>can capture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.038596</td>\n",
       "      <td>0.460821</td>\n",
       "      <td>0.153196</td>\n",
       "      <td>0.114656</td>\n",
       "      <td>0.197212</td>\n",
       "      <td>-0.525422</td>\n",
       "      <td>-0.147961</td>\n",
       "      <td>0.092074</td>\n",
       "      <td>0.362372</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601565</td>\n",
       "      <td>-0.948843</td>\n",
       "      <td>-0.056877</td>\n",
       "      <td>-0.074685</td>\n",
       "      <td>-0.230631</td>\n",
       "      <td>0.501561</td>\n",
       "      <td>-0.357212</td>\n",
       "      <td>0.791067</td>\n",
       "      <td>-0.193788</td>\n",
       "      <td>ensures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.372455</td>\n",
       "      <td>0.313116</td>\n",
       "      <td>-0.421734</td>\n",
       "      <td>-0.302487</td>\n",
       "      <td>0.391502</td>\n",
       "      <td>-2.002126</td>\n",
       "      <td>-0.294954</td>\n",
       "      <td>0.323469</td>\n",
       "      <td>-1.068244</td>\n",
       "      <td>-0.748371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465439</td>\n",
       "      <td>-0.479181</td>\n",
       "      <td>1.058791</td>\n",
       "      <td>0.144505</td>\n",
       "      <td>0.102943</td>\n",
       "      <td>0.351012</td>\n",
       "      <td>0.322578</td>\n",
       "      <td>1.222291</td>\n",
       "      <td>-0.365344</td>\n",
       "      <td>can capture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.133340</td>\n",
       "      <td>-0.244977</td>\n",
       "      <td>-0.066271</td>\n",
       "      <td>0.176615</td>\n",
       "      <td>0.091122</td>\n",
       "      <td>0.293755</td>\n",
       "      <td>0.298214</td>\n",
       "      <td>0.938860</td>\n",
       "      <td>-0.570982</td>\n",
       "      <td>-0.014076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290145</td>\n",
       "      <td>-0.766449</td>\n",
       "      <td>0.170825</td>\n",
       "      <td>-0.114680</td>\n",
       "      <td>-0.555400</td>\n",
       "      <td>0.648216</td>\n",
       "      <td>-0.475290</td>\n",
       "      <td>0.762671</td>\n",
       "      <td>0.340396</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-0.133340</td>\n",
       "      <td>-0.244977</td>\n",
       "      <td>-0.066271</td>\n",
       "      <td>0.176615</td>\n",
       "      <td>0.091122</td>\n",
       "      <td>0.293755</td>\n",
       "      <td>0.298214</td>\n",
       "      <td>0.938860</td>\n",
       "      <td>-0.570982</td>\n",
       "      <td>-0.014076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756402</td>\n",
       "      <td>-0.672986</td>\n",
       "      <td>-0.060730</td>\n",
       "      <td>0.166313</td>\n",
       "      <td>-0.472846</td>\n",
       "      <td>0.481247</td>\n",
       "      <td>-0.753033</td>\n",
       "      <td>0.967856</td>\n",
       "      <td>0.027864</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-1.025773</td>\n",
       "      <td>0.310373</td>\n",
       "      <td>-0.447934</td>\n",
       "      <td>-0.457204</td>\n",
       "      <td>0.679729</td>\n",
       "      <td>-0.192911</td>\n",
       "      <td>-0.144402</td>\n",
       "      <td>1.037015</td>\n",
       "      <td>-0.467973</td>\n",
       "      <td>-0.688231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197316</td>\n",
       "      <td>-1.048957</td>\n",
       "      <td>0.400892</td>\n",
       "      <td>0.348396</td>\n",
       "      <td>-1.342147</td>\n",
       "      <td>0.705789</td>\n",
       "      <td>-0.213839</td>\n",
       "      <td>0.529447</td>\n",
       "      <td>-0.752717</td>\n",
       "      <td>has achieved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-0.382333</td>\n",
       "      <td>0.323978</td>\n",
       "      <td>-0.130768</td>\n",
       "      <td>-0.194236</td>\n",
       "      <td>0.576638</td>\n",
       "      <td>0.374112</td>\n",
       "      <td>-0.376351</td>\n",
       "      <td>-0.234408</td>\n",
       "      <td>-0.173447</td>\n",
       "      <td>-0.251889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651315</td>\n",
       "      <td>-0.195203</td>\n",
       "      <td>0.293907</td>\n",
       "      <td>-0.246816</td>\n",
       "      <td>0.500733</td>\n",
       "      <td>-0.373290</td>\n",
       "      <td>-0.383964</td>\n",
       "      <td>-0.191344</td>\n",
       "      <td>0.048969</td>\n",
       "      <td>describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.020189</td>\n",
       "      <td>-0.007402</td>\n",
       "      <td>-0.099057</td>\n",
       "      <td>-0.233371</td>\n",
       "      <td>-0.424543</td>\n",
       "      <td>-0.118362</td>\n",
       "      <td>0.611347</td>\n",
       "      <td>0.175541</td>\n",
       "      <td>-0.133079</td>\n",
       "      <td>-0.148777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307901</td>\n",
       "      <td>-0.157737</td>\n",
       "      <td>-0.276184</td>\n",
       "      <td>1.076540</td>\n",
       "      <td>0.242981</td>\n",
       "      <td>-0.248841</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>-0.305152</td>\n",
       "      <td>-0.245822</td>\n",
       "      <td>explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.315573</td>\n",
       "      <td>0.449618</td>\n",
       "      <td>-1.079981</td>\n",
       "      <td>-0.171654</td>\n",
       "      <td>0.205883</td>\n",
       "      <td>0.127349</td>\n",
       "      <td>0.271354</td>\n",
       "      <td>0.312264</td>\n",
       "      <td>0.212851</td>\n",
       "      <td>0.099918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485182</td>\n",
       "      <td>0.088553</td>\n",
       "      <td>-0.460196</td>\n",
       "      <td>0.623339</td>\n",
       "      <td>0.064982</td>\n",
       "      <td>0.346466</td>\n",
       "      <td>-0.032952</td>\n",
       "      <td>-0.077567</td>\n",
       "      <td>-0.541435</td>\n",
       "      <td>enhance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.020189</td>\n",
       "      <td>-0.007402</td>\n",
       "      <td>-0.099057</td>\n",
       "      <td>-0.233371</td>\n",
       "      <td>-0.424543</td>\n",
       "      <td>-0.118362</td>\n",
       "      <td>0.611347</td>\n",
       "      <td>0.175541</td>\n",
       "      <td>-0.133079</td>\n",
       "      <td>-0.148777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307901</td>\n",
       "      <td>-0.157737</td>\n",
       "      <td>-0.276184</td>\n",
       "      <td>1.076540</td>\n",
       "      <td>0.242981</td>\n",
       "      <td>-0.248841</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>-0.305152</td>\n",
       "      <td>-0.245822</td>\n",
       "      <td>explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.563688</td>\n",
       "      <td>-0.074106</td>\n",
       "      <td>-0.755131</td>\n",
       "      <td>-0.456024</td>\n",
       "      <td>0.280470</td>\n",
       "      <td>0.357581</td>\n",
       "      <td>-0.740065</td>\n",
       "      <td>-0.832735</td>\n",
       "      <td>-0.339992</td>\n",
       "      <td>1.556885</td>\n",
       "      <td>...</td>\n",
       "      <td>1.071753</td>\n",
       "      <td>-0.562981</td>\n",
       "      <td>0.344128</td>\n",
       "      <td>0.408223</td>\n",
       "      <td>0.607924</td>\n",
       "      <td>0.851186</td>\n",
       "      <td>-0.361726</td>\n",
       "      <td>0.013772</td>\n",
       "      <td>-0.445355</td>\n",
       "      <td>To show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.563688</td>\n",
       "      <td>-0.074106</td>\n",
       "      <td>-0.755131</td>\n",
       "      <td>-0.456024</td>\n",
       "      <td>0.280470</td>\n",
       "      <td>0.357581</td>\n",
       "      <td>-0.740065</td>\n",
       "      <td>-0.832735</td>\n",
       "      <td>-0.339992</td>\n",
       "      <td>1.556885</td>\n",
       "      <td>...</td>\n",
       "      <td>1.071753</td>\n",
       "      <td>-0.562981</td>\n",
       "      <td>0.344128</td>\n",
       "      <td>0.408223</td>\n",
       "      <td>0.607924</td>\n",
       "      <td>0.851186</td>\n",
       "      <td>-0.361726</td>\n",
       "      <td>0.013772</td>\n",
       "      <td>-0.445355</td>\n",
       "      <td>To show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-0.004066</td>\n",
       "      <td>0.832123</td>\n",
       "      <td>-0.456464</td>\n",
       "      <td>0.383548</td>\n",
       "      <td>0.603917</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>-0.676367</td>\n",
       "      <td>-0.021864</td>\n",
       "      <td>0.388104</td>\n",
       "      <td>-0.679269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633375</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.216832</td>\n",
       "      <td>0.708668</td>\n",
       "      <td>0.199695</td>\n",
       "      <td>0.381721</td>\n",
       "      <td>0.077058</td>\n",
       "      <td>-0.190891</td>\n",
       "      <td>-0.094872</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.607313</td>\n",
       "      <td>0.113089</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>-0.084065</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>-0.035418</td>\n",
       "      <td>-0.209196</td>\n",
       "      <td>1.103459</td>\n",
       "      <td>-0.060140</td>\n",
       "      <td>-0.467778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228359</td>\n",
       "      <td>-0.109186</td>\n",
       "      <td>0.187208</td>\n",
       "      <td>0.047805</td>\n",
       "      <td>-0.406115</td>\n",
       "      <td>0.507450</td>\n",
       "      <td>0.075443</td>\n",
       "      <td>0.459482</td>\n",
       "      <td>-0.650638</td>\n",
       "      <td>achieved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.876917</td>\n",
       "      <td>0.387537</td>\n",
       "      <td>-0.582344</td>\n",
       "      <td>-0.594061</td>\n",
       "      <td>-0.014207</td>\n",
       "      <td>-0.174442</td>\n",
       "      <td>-0.194894</td>\n",
       "      <td>0.255090</td>\n",
       "      <td>-0.441897</td>\n",
       "      <td>-0.677492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262748</td>\n",
       "      <td>-0.833305</td>\n",
       "      <td>-0.148131</td>\n",
       "      <td>0.965400</td>\n",
       "      <td>0.534390</td>\n",
       "      <td>0.363053</td>\n",
       "      <td>0.091123</td>\n",
       "      <td>-0.280127</td>\n",
       "      <td>-0.180349</td>\n",
       "      <td>to investigate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.280924</td>\n",
       "      <td>0.502440</td>\n",
       "      <td>-0.709884</td>\n",
       "      <td>-0.146214</td>\n",
       "      <td>0.307728</td>\n",
       "      <td>0.523678</td>\n",
       "      <td>0.321026</td>\n",
       "      <td>0.300733</td>\n",
       "      <td>0.038990</td>\n",
       "      <td>-0.184458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319877</td>\n",
       "      <td>-0.054451</td>\n",
       "      <td>-0.173571</td>\n",
       "      <td>0.533406</td>\n",
       "      <td>-0.052567</td>\n",
       "      <td>0.692147</td>\n",
       "      <td>0.277761</td>\n",
       "      <td>-0.384924</td>\n",
       "      <td>-0.015674</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.280924</td>\n",
       "      <td>0.502440</td>\n",
       "      <td>-0.709884</td>\n",
       "      <td>-0.146214</td>\n",
       "      <td>0.307728</td>\n",
       "      <td>0.523678</td>\n",
       "      <td>0.321026</td>\n",
       "      <td>0.300733</td>\n",
       "      <td>0.038990</td>\n",
       "      <td>-0.184458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319877</td>\n",
       "      <td>-0.054451</td>\n",
       "      <td>-0.173571</td>\n",
       "      <td>0.533406</td>\n",
       "      <td>-0.052567</td>\n",
       "      <td>0.692147</td>\n",
       "      <td>0.277761</td>\n",
       "      <td>-0.384924</td>\n",
       "      <td>-0.015674</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.026602</td>\n",
       "      <td>-0.481717</td>\n",
       "      <td>-0.528872</td>\n",
       "      <td>-0.620290</td>\n",
       "      <td>0.013163</td>\n",
       "      <td>0.556058</td>\n",
       "      <td>-0.062064</td>\n",
       "      <td>-0.274613</td>\n",
       "      <td>-0.156070</td>\n",
       "      <td>0.495878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389084</td>\n",
       "      <td>0.678852</td>\n",
       "      <td>0.317035</td>\n",
       "      <td>0.166892</td>\n",
       "      <td>0.076563</td>\n",
       "      <td>0.092344</td>\n",
       "      <td>-0.162532</td>\n",
       "      <td>-0.259825</td>\n",
       "      <td>0.313861</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.183888</td>\n",
       "      <td>0.372771</td>\n",
       "      <td>-0.206273</td>\n",
       "      <td>0.343829</td>\n",
       "      <td>0.225271</td>\n",
       "      <td>0.131173</td>\n",
       "      <td>0.179302</td>\n",
       "      <td>0.526922</td>\n",
       "      <td>-0.329145</td>\n",
       "      <td>-0.006487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860042</td>\n",
       "      <td>-0.901996</td>\n",
       "      <td>-0.398250</td>\n",
       "      <td>0.082292</td>\n",
       "      <td>-0.006859</td>\n",
       "      <td>0.338475</td>\n",
       "      <td>-0.224042</td>\n",
       "      <td>0.667025</td>\n",
       "      <td>0.434968</td>\n",
       "      <td>gives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.259945</td>\n",
       "      <td>-0.142396</td>\n",
       "      <td>-0.339951</td>\n",
       "      <td>0.022232</td>\n",
       "      <td>0.302048</td>\n",
       "      <td>-0.056249</td>\n",
       "      <td>0.619220</td>\n",
       "      <td>-0.117933</td>\n",
       "      <td>-0.422811</td>\n",
       "      <td>0.629025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067377</td>\n",
       "      <td>-0.016568</td>\n",
       "      <td>0.898584</td>\n",
       "      <td>1.011176</td>\n",
       "      <td>-0.328115</td>\n",
       "      <td>0.079541</td>\n",
       "      <td>0.250277</td>\n",
       "      <td>0.188860</td>\n",
       "      <td>-0.167318</td>\n",
       "      <td>generate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.028216</td>\n",
       "      <td>0.683443</td>\n",
       "      <td>-0.376829</td>\n",
       "      <td>0.232246</td>\n",
       "      <td>0.285615</td>\n",
       "      <td>0.433518</td>\n",
       "      <td>-0.316045</td>\n",
       "      <td>0.127629</td>\n",
       "      <td>-0.247909</td>\n",
       "      <td>-0.092227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455918</td>\n",
       "      <td>0.222376</td>\n",
       "      <td>0.215965</td>\n",
       "      <td>0.476834</td>\n",
       "      <td>0.099480</td>\n",
       "      <td>-0.343889</td>\n",
       "      <td>0.139337</td>\n",
       "      <td>0.620149</td>\n",
       "      <td>-0.219147</td>\n",
       "      <td>produce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.259551</td>\n",
       "      <td>0.412980</td>\n",
       "      <td>0.281104</td>\n",
       "      <td>1.102394</td>\n",
       "      <td>1.177207</td>\n",
       "      <td>-0.287381</td>\n",
       "      <td>-0.432665</td>\n",
       "      <td>-0.558143</td>\n",
       "      <td>0.342117</td>\n",
       "      <td>0.349016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.068888</td>\n",
       "      <td>-1.171590</td>\n",
       "      <td>-0.310252</td>\n",
       "      <td>0.220471</td>\n",
       "      <td>0.571696</td>\n",
       "      <td>0.778376</td>\n",
       "      <td>-0.612661</td>\n",
       "      <td>-0.135189</td>\n",
       "      <td>0.125477</td>\n",
       "      <td>Compared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.269682</td>\n",
       "      <td>0.044014</td>\n",
       "      <td>-0.105810</td>\n",
       "      <td>-0.077238</td>\n",
       "      <td>1.501853</td>\n",
       "      <td>-0.509115</td>\n",
       "      <td>0.388575</td>\n",
       "      <td>0.469875</td>\n",
       "      <td>0.497823</td>\n",
       "      <td>0.476876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426512</td>\n",
       "      <td>-0.233275</td>\n",
       "      <td>0.309767</td>\n",
       "      <td>0.009863</td>\n",
       "      <td>-0.229079</td>\n",
       "      <td>-0.190608</td>\n",
       "      <td>0.344162</td>\n",
       "      <td>0.196488</td>\n",
       "      <td>0.361369</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.786009</td>\n",
       "      <td>-0.788565</td>\n",
       "      <td>-0.140104</td>\n",
       "      <td>-0.291559</td>\n",
       "      <td>0.532405</td>\n",
       "      <td>-0.328127</td>\n",
       "      <td>0.080115</td>\n",
       "      <td>0.161973</td>\n",
       "      <td>-0.616295</td>\n",
       "      <td>0.151344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105938</td>\n",
       "      <td>-0.343113</td>\n",
       "      <td>-0.298812</td>\n",
       "      <td>-0.049000</td>\n",
       "      <td>0.277953</td>\n",
       "      <td>0.019683</td>\n",
       "      <td>0.144155</td>\n",
       "      <td>-0.393033</td>\n",
       "      <td>-0.693077</td>\n",
       "      <td>Must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.216901</td>\n",
       "      <td>0.513081</td>\n",
       "      <td>-0.433099</td>\n",
       "      <td>-0.174096</td>\n",
       "      <td>0.823155</td>\n",
       "      <td>0.221483</td>\n",
       "      <td>-0.227389</td>\n",
       "      <td>0.042357</td>\n",
       "      <td>-0.036590</td>\n",
       "      <td>0.574497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212701</td>\n",
       "      <td>0.129081</td>\n",
       "      <td>0.165361</td>\n",
       "      <td>0.131574</td>\n",
       "      <td>-0.020974</td>\n",
       "      <td>0.321985</td>\n",
       "      <td>0.138003</td>\n",
       "      <td>0.258033</td>\n",
       "      <td>-0.572022</td>\n",
       "      <td>applying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.052707</td>\n",
       "      <td>-0.162361</td>\n",
       "      <td>0.262985</td>\n",
       "      <td>0.618636</td>\n",
       "      <td>1.238630</td>\n",
       "      <td>0.871532</td>\n",
       "      <td>-0.663967</td>\n",
       "      <td>-0.390604</td>\n",
       "      <td>-0.790598</td>\n",
       "      <td>0.497508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.321996</td>\n",
       "      <td>0.119809</td>\n",
       "      <td>-0.012760</td>\n",
       "      <td>0.410180</td>\n",
       "      <td>-0.541516</td>\n",
       "      <td>-0.390635</td>\n",
       "      <td>0.018117</td>\n",
       "      <td>posed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-0.174840</td>\n",
       "      <td>-0.478888</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>0.144196</td>\n",
       "      <td>0.099981</td>\n",
       "      <td>-0.247665</td>\n",
       "      <td>-0.016141</td>\n",
       "      <td>-0.057487</td>\n",
       "      <td>0.183482</td>\n",
       "      <td>-0.276753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100621</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>0.154316</td>\n",
       "      <td>-0.576677</td>\n",
       "      <td>-0.401166</td>\n",
       "      <td>0.155216</td>\n",
       "      <td>0.103805</td>\n",
       "      <td>-0.299826</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>confronting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.070869</td>\n",
       "      <td>-0.356521</td>\n",
       "      <td>-0.100021</td>\n",
       "      <td>-0.095342</td>\n",
       "      <td>0.464404</td>\n",
       "      <td>0.119578</td>\n",
       "      <td>-0.302497</td>\n",
       "      <td>0.087231</td>\n",
       "      <td>0.071933</td>\n",
       "      <td>0.193344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128228</td>\n",
       "      <td>-0.421634</td>\n",
       "      <td>-0.064661</td>\n",
       "      <td>0.494092</td>\n",
       "      <td>0.200832</td>\n",
       "      <td>0.412989</td>\n",
       "      <td>-0.075398</td>\n",
       "      <td>-0.971734</td>\n",
       "      <td>-0.235871</td>\n",
       "      <td>address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.219071</td>\n",
       "      <td>1.008216</td>\n",
       "      <td>-1.086389</td>\n",
       "      <td>0.574497</td>\n",
       "      <td>1.464685</td>\n",
       "      <td>0.422202</td>\n",
       "      <td>0.228203</td>\n",
       "      <td>0.723016</td>\n",
       "      <td>0.527867</td>\n",
       "      <td>0.214996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104654</td>\n",
       "      <td>-0.107193</td>\n",
       "      <td>0.393227</td>\n",
       "      <td>0.889303</td>\n",
       "      <td>0.244363</td>\n",
       "      <td>0.494307</td>\n",
       "      <td>0.051408</td>\n",
       "      <td>0.113009</td>\n",
       "      <td>-0.584796</td>\n",
       "      <td>contribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-0.272068</td>\n",
       "      <td>0.158148</td>\n",
       "      <td>-1.006689</td>\n",
       "      <td>-0.456199</td>\n",
       "      <td>0.171681</td>\n",
       "      <td>-0.577783</td>\n",
       "      <td>0.077261</td>\n",
       "      <td>0.169222</td>\n",
       "      <td>-0.521943</td>\n",
       "      <td>0.930989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021652</td>\n",
       "      <td>-0.161078</td>\n",
       "      <td>-0.217208</td>\n",
       "      <td>0.651561</td>\n",
       "      <td>-0.451000</td>\n",
       "      <td>-0.001198</td>\n",
       "      <td>-0.354049</td>\n",
       "      <td>0.446097</td>\n",
       "      <td>-0.060521</td>\n",
       "      <td>focus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.435460</td>\n",
       "      <td>0.580408</td>\n",
       "      <td>-0.290261</td>\n",
       "      <td>0.471809</td>\n",
       "      <td>-1.222336</td>\n",
       "      <td>-1.222143</td>\n",
       "      <td>0.138435</td>\n",
       "      <td>0.703725</td>\n",
       "      <td>-0.375188</td>\n",
       "      <td>1.090575</td>\n",
       "      <td>...</td>\n",
       "      <td>1.308993</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>-0.045796</td>\n",
       "      <td>-0.201046</td>\n",
       "      <td>-0.369348</td>\n",
       "      <td>-0.261295</td>\n",
       "      <td>-0.140437</td>\n",
       "      <td>0.725859</td>\n",
       "      <td>-0.121904</td>\n",
       "      <td>encoded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.481207</td>\n",
       "      <td>0.164531</td>\n",
       "      <td>-0.489792</td>\n",
       "      <td>0.334123</td>\n",
       "      <td>-0.355504</td>\n",
       "      <td>0.436219</td>\n",
       "      <td>-0.140649</td>\n",
       "      <td>-0.593268</td>\n",
       "      <td>-0.020781</td>\n",
       "      <td>0.235909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583105</td>\n",
       "      <td>-0.664965</td>\n",
       "      <td>-0.163459</td>\n",
       "      <td>0.181102</td>\n",
       "      <td>-0.605254</td>\n",
       "      <td>0.479321</td>\n",
       "      <td>-0.447984</td>\n",
       "      <td>0.304987</td>\n",
       "      <td>0.882621</td>\n",
       "      <td>leads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-0.382651</td>\n",
       "      <td>-0.179681</td>\n",
       "      <td>-0.572773</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.314946</td>\n",
       "      <td>-0.143388</td>\n",
       "      <td>-0.163793</td>\n",
       "      <td>-0.719510</td>\n",
       "      <td>-0.597466</td>\n",
       "      <td>-0.252264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067834</td>\n",
       "      <td>0.326153</td>\n",
       "      <td>0.052696</td>\n",
       "      <td>0.615497</td>\n",
       "      <td>0.221731</td>\n",
       "      <td>-0.429850</td>\n",
       "      <td>-0.355684</td>\n",
       "      <td>0.075178</td>\n",
       "      <td>-0.469998</td>\n",
       "      <td>Revealing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-0.382435</td>\n",
       "      <td>0.858616</td>\n",
       "      <td>-0.746813</td>\n",
       "      <td>-1.393332</td>\n",
       "      <td>1.523503</td>\n",
       "      <td>-0.214236</td>\n",
       "      <td>0.145070</td>\n",
       "      <td>1.799290</td>\n",
       "      <td>0.375392</td>\n",
       "      <td>0.022188</td>\n",
       "      <td>...</td>\n",
       "      <td>1.480488</td>\n",
       "      <td>-0.085819</td>\n",
       "      <td>0.318279</td>\n",
       "      <td>-0.087766</td>\n",
       "      <td>-0.573257</td>\n",
       "      <td>0.159479</td>\n",
       "      <td>-1.581218</td>\n",
       "      <td>0.099891</td>\n",
       "      <td>0.433922</td>\n",
       "      <td>has been released</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-0.476875</td>\n",
       "      <td>1.261953</td>\n",
       "      <td>-1.058676</td>\n",
       "      <td>-0.365043</td>\n",
       "      <td>0.069808</td>\n",
       "      <td>-0.520373</td>\n",
       "      <td>-0.572612</td>\n",
       "      <td>-0.624140</td>\n",
       "      <td>0.321894</td>\n",
       "      <td>0.122681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100138</td>\n",
       "      <td>0.243910</td>\n",
       "      <td>-0.202182</td>\n",
       "      <td>0.168511</td>\n",
       "      <td>-0.047734</td>\n",
       "      <td>-0.121627</td>\n",
       "      <td>-0.891663</td>\n",
       "      <td>-0.205805</td>\n",
       "      <td>0.123384</td>\n",
       "      <td>masking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-0.990069</td>\n",
       "      <td>-1.002584</td>\n",
       "      <td>-0.228493</td>\n",
       "      <td>-0.264751</td>\n",
       "      <td>-0.268350</td>\n",
       "      <td>-0.036842</td>\n",
       "      <td>-0.626979</td>\n",
       "      <td>-0.130726</td>\n",
       "      <td>0.561310</td>\n",
       "      <td>1.079396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854765</td>\n",
       "      <td>-0.999095</td>\n",
       "      <td>0.332998</td>\n",
       "      <td>0.339326</td>\n",
       "      <td>-0.575115</td>\n",
       "      <td>0.136851</td>\n",
       "      <td>-0.775325</td>\n",
       "      <td>0.032501</td>\n",
       "      <td>-0.716508</td>\n",
       "      <td>was trained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-0.539446</td>\n",
       "      <td>-0.165851</td>\n",
       "      <td>-0.314165</td>\n",
       "      <td>-0.600294</td>\n",
       "      <td>0.150964</td>\n",
       "      <td>0.278123</td>\n",
       "      <td>0.484477</td>\n",
       "      <td>0.394713</td>\n",
       "      <td>-0.123819</td>\n",
       "      <td>0.551198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175462</td>\n",
       "      <td>-0.990535</td>\n",
       "      <td>-0.364141</td>\n",
       "      <td>0.252785</td>\n",
       "      <td>-0.161892</td>\n",
       "      <td>1.929286</td>\n",
       "      <td>-0.603728</td>\n",
       "      <td>-0.227155</td>\n",
       "      <td>-0.749955</td>\n",
       "      <td>to provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-0.546252</td>\n",
       "      <td>-0.032759</td>\n",
       "      <td>-0.244539</td>\n",
       "      <td>-0.282502</td>\n",
       "      <td>-0.093734</td>\n",
       "      <td>-0.367365</td>\n",
       "      <td>-1.279347</td>\n",
       "      <td>0.106391</td>\n",
       "      <td>-0.927847</td>\n",
       "      <td>0.694455</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064587</td>\n",
       "      <td>-1.637465</td>\n",
       "      <td>0.636184</td>\n",
       "      <td>-0.148116</td>\n",
       "      <td>-1.106313</td>\n",
       "      <td>0.533086</td>\n",
       "      <td>0.005295</td>\n",
       "      <td>0.591866</td>\n",
       "      <td>-0.697045</td>\n",
       "      <td>is verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.515809</td>\n",
       "      <td>0.106172</td>\n",
       "      <td>-0.495997</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>-0.121481</td>\n",
       "      <td>-0.204168</td>\n",
       "      <td>0.195508</td>\n",
       "      <td>0.119249</td>\n",
       "      <td>-0.153667</td>\n",
       "      <td>0.136960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>-0.186245</td>\n",
       "      <td>0.431865</td>\n",
       "      <td>0.682063</td>\n",
       "      <td>0.321938</td>\n",
       "      <td>0.128866</td>\n",
       "      <td>-0.094373</td>\n",
       "      <td>-0.148446</td>\n",
       "      <td>-0.212645</td>\n",
       "      <td>examine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.515809</td>\n",
       "      <td>0.106172</td>\n",
       "      <td>-0.495997</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>-0.121481</td>\n",
       "      <td>-0.204168</td>\n",
       "      <td>0.195508</td>\n",
       "      <td>0.119249</td>\n",
       "      <td>-0.153667</td>\n",
       "      <td>0.136960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>-0.186245</td>\n",
       "      <td>0.431865</td>\n",
       "      <td>0.682063</td>\n",
       "      <td>0.321938</td>\n",
       "      <td>0.128866</td>\n",
       "      <td>-0.094373</td>\n",
       "      <td>-0.148446</td>\n",
       "      <td>-0.212645</td>\n",
       "      <td>examine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-1.044857</td>\n",
       "      <td>-1.256696</td>\n",
       "      <td>-0.957417</td>\n",
       "      <td>0.458047</td>\n",
       "      <td>1.089027</td>\n",
       "      <td>-0.144280</td>\n",
       "      <td>-0.435878</td>\n",
       "      <td>-0.045810</td>\n",
       "      <td>0.474600</td>\n",
       "      <td>1.863183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707857</td>\n",
       "      <td>-0.041422</td>\n",
       "      <td>0.320756</td>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.391607</td>\n",
       "      <td>0.727173</td>\n",
       "      <td>0.134034</td>\n",
       "      <td>-0.459333</td>\n",
       "      <td>-0.523964</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.022375</td>\n",
       "      <td>0.191903</td>\n",
       "      <td>-0.411201</td>\n",
       "      <td>-0.275466</td>\n",
       "      <td>-0.058894</td>\n",
       "      <td>0.487233</td>\n",
       "      <td>0.296230</td>\n",
       "      <td>0.217108</td>\n",
       "      <td>0.178636</td>\n",
       "      <td>-0.092290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371697</td>\n",
       "      <td>0.360514</td>\n",
       "      <td>-0.203928</td>\n",
       "      <td>0.722203</td>\n",
       "      <td>-0.031995</td>\n",
       "      <td>0.430550</td>\n",
       "      <td>0.576389</td>\n",
       "      <td>-0.448070</td>\n",
       "      <td>-0.374031</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-0.214934</td>\n",
       "      <td>0.376908</td>\n",
       "      <td>0.209861</td>\n",
       "      <td>0.495754</td>\n",
       "      <td>0.314413</td>\n",
       "      <td>-0.274643</td>\n",
       "      <td>0.063896</td>\n",
       "      <td>0.557843</td>\n",
       "      <td>0.377129</td>\n",
       "      <td>-0.032968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778510</td>\n",
       "      <td>0.116582</td>\n",
       "      <td>0.034619</td>\n",
       "      <td>-0.634318</td>\n",
       "      <td>-0.138169</td>\n",
       "      <td>0.845218</td>\n",
       "      <td>0.186309</td>\n",
       "      <td>0.604633</td>\n",
       "      <td>0.140994</td>\n",
       "      <td>gains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-0.312544</td>\n",
       "      <td>0.333999</td>\n",
       "      <td>-0.204261</td>\n",
       "      <td>0.092911</td>\n",
       "      <td>0.753148</td>\n",
       "      <td>0.336927</td>\n",
       "      <td>-0.856925</td>\n",
       "      <td>-0.382683</td>\n",
       "      <td>0.735788</td>\n",
       "      <td>-0.295922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199534</td>\n",
       "      <td>0.105076</td>\n",
       "      <td>0.465326</td>\n",
       "      <td>0.125013</td>\n",
       "      <td>-0.341154</td>\n",
       "      <td>-0.030454</td>\n",
       "      <td>0.314653</td>\n",
       "      <td>0.441275</td>\n",
       "      <td>0.195672</td>\n",
       "      <td>showed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-0.457224</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-0.164742</td>\n",
       "      <td>0.114626</td>\n",
       "      <td>-0.099622</td>\n",
       "      <td>0.503985</td>\n",
       "      <td>0.562962</td>\n",
       "      <td>0.176879</td>\n",
       "      <td>0.360459</td>\n",
       "      <td>-0.045676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276039</td>\n",
       "      <td>0.228639</td>\n",
       "      <td>-0.306417</td>\n",
       "      <td>-0.040134</td>\n",
       "      <td>0.514693</td>\n",
       "      <td>-0.217696</td>\n",
       "      <td>-0.026793</td>\n",
       "      <td>-0.300793</td>\n",
       "      <td>0.187881</td>\n",
       "      <td>exploring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.323992</td>\n",
       "      <td>-0.035124</td>\n",
       "      <td>-0.006809</td>\n",
       "      <td>-0.337739</td>\n",
       "      <td>-0.376882</td>\n",
       "      <td>-0.593107</td>\n",
       "      <td>-0.206207</td>\n",
       "      <td>0.266228</td>\n",
       "      <td>-0.025153</td>\n",
       "      <td>0.531280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688798</td>\n",
       "      <td>0.192031</td>\n",
       "      <td>-0.291331</td>\n",
       "      <td>-0.035325</td>\n",
       "      <td>-0.263105</td>\n",
       "      <td>0.409068</td>\n",
       "      <td>-0.434015</td>\n",
       "      <td>-0.056372</td>\n",
       "      <td>0.084591</td>\n",
       "      <td>based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.323992</td>\n",
       "      <td>-0.035124</td>\n",
       "      <td>-0.006809</td>\n",
       "      <td>-0.337739</td>\n",
       "      <td>-0.376882</td>\n",
       "      <td>-0.593107</td>\n",
       "      <td>-0.206207</td>\n",
       "      <td>0.266228</td>\n",
       "      <td>-0.025153</td>\n",
       "      <td>0.531280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688798</td>\n",
       "      <td>0.192031</td>\n",
       "      <td>-0.291331</td>\n",
       "      <td>-0.035325</td>\n",
       "      <td>-0.263105</td>\n",
       "      <td>0.409068</td>\n",
       "      <td>-0.434015</td>\n",
       "      <td>-0.056372</td>\n",
       "      <td>0.084591</td>\n",
       "      <td>based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-0.397924</td>\n",
       "      <td>0.353230</td>\n",
       "      <td>-0.238537</td>\n",
       "      <td>0.023938</td>\n",
       "      <td>0.401741</td>\n",
       "      <td>-0.336119</td>\n",
       "      <td>-0.133388</td>\n",
       "      <td>0.439974</td>\n",
       "      <td>-0.156474</td>\n",
       "      <td>-0.260964</td>\n",
       "      <td>...</td>\n",
       "      <td>1.135502</td>\n",
       "      <td>-0.675815</td>\n",
       "      <td>0.424360</td>\n",
       "      <td>0.135711</td>\n",
       "      <td>-0.866698</td>\n",
       "      <td>0.703063</td>\n",
       "      <td>-0.151839</td>\n",
       "      <td>0.782931</td>\n",
       "      <td>0.043184</td>\n",
       "      <td>achieves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.135813</td>\n",
       "      <td>0.043795</td>\n",
       "      <td>0.053178</td>\n",
       "      <td>-0.037448</td>\n",
       "      <td>-0.513281</td>\n",
       "      <td>-0.307284</td>\n",
       "      <td>-0.461424</td>\n",
       "      <td>0.220284</td>\n",
       "      <td>-0.201216</td>\n",
       "      <td>0.453648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776984</td>\n",
       "      <td>-0.046215</td>\n",
       "      <td>0.434492</td>\n",
       "      <td>0.128772</td>\n",
       "      <td>-0.542470</td>\n",
       "      <td>0.534018</td>\n",
       "      <td>-0.430055</td>\n",
       "      <td>-0.399740</td>\n",
       "      <td>0.013967</td>\n",
       "      <td>built</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-0.815985</td>\n",
       "      <td>-1.428854</td>\n",
       "      <td>-0.698961</td>\n",
       "      <td>0.228826</td>\n",
       "      <td>1.057089</td>\n",
       "      <td>-0.331962</td>\n",
       "      <td>0.201367</td>\n",
       "      <td>0.658992</td>\n",
       "      <td>0.251297</td>\n",
       "      <td>1.341728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460238</td>\n",
       "      <td>-0.009219</td>\n",
       "      <td>-0.077263</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>-0.205631</td>\n",
       "      <td>-0.082900</td>\n",
       "      <td>-0.413378</td>\n",
       "      <td>-0.428615</td>\n",
       "      <td>-0.216429</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.736893</td>\n",
       "      <td>0.352485</td>\n",
       "      <td>0.098556</td>\n",
       "      <td>0.417768</td>\n",
       "      <td>-0.039069</td>\n",
       "      <td>-0.159275</td>\n",
       "      <td>-0.275430</td>\n",
       "      <td>-0.090387</td>\n",
       "      <td>-0.695369</td>\n",
       "      <td>0.464568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393943</td>\n",
       "      <td>-0.875116</td>\n",
       "      <td>0.477085</td>\n",
       "      <td>0.084233</td>\n",
       "      <td>0.367899</td>\n",
       "      <td>0.013431</td>\n",
       "      <td>0.200616</td>\n",
       "      <td>0.266879</td>\n",
       "      <td>0.114448</td>\n",
       "      <td>studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-0.744695</td>\n",
       "      <td>0.976921</td>\n",
       "      <td>-0.492248</td>\n",
       "      <td>-0.010015</td>\n",
       "      <td>-0.266327</td>\n",
       "      <td>-0.326750</td>\n",
       "      <td>0.099546</td>\n",
       "      <td>1.131829</td>\n",
       "      <td>-0.229543</td>\n",
       "      <td>0.829619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537563</td>\n",
       "      <td>-0.409710</td>\n",
       "      <td>-0.162906</td>\n",
       "      <td>1.319239</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.423026</td>\n",
       "      <td>-0.140935</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.595686</td>\n",
       "      <td>to leverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-0.744695</td>\n",
       "      <td>0.976921</td>\n",
       "      <td>-0.492248</td>\n",
       "      <td>-0.010015</td>\n",
       "      <td>-0.266327</td>\n",
       "      <td>-0.326750</td>\n",
       "      <td>0.099546</td>\n",
       "      <td>1.131829</td>\n",
       "      <td>-0.229543</td>\n",
       "      <td>0.829619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537563</td>\n",
       "      <td>-0.409710</td>\n",
       "      <td>-0.162906</td>\n",
       "      <td>1.319239</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.423026</td>\n",
       "      <td>-0.140935</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.595686</td>\n",
       "      <td>to leverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-0.442201</td>\n",
       "      <td>0.810350</td>\n",
       "      <td>-0.333028</td>\n",
       "      <td>0.150481</td>\n",
       "      <td>0.564273</td>\n",
       "      <td>-0.094420</td>\n",
       "      <td>-0.978193</td>\n",
       "      <td>0.287024</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>-0.745039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647157</td>\n",
       "      <td>-0.306085</td>\n",
       "      <td>0.508288</td>\n",
       "      <td>0.555787</td>\n",
       "      <td>0.214668</td>\n",
       "      <td>0.329127</td>\n",
       "      <td>-0.196993</td>\n",
       "      <td>-0.120132</td>\n",
       "      <td>-0.208324</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.712951</td>\n",
       "      <td>0.444234</td>\n",
       "      <td>-1.100974</td>\n",
       "      <td>0.320230</td>\n",
       "      <td>0.293257</td>\n",
       "      <td>-0.338834</td>\n",
       "      <td>-0.529803</td>\n",
       "      <td>-0.336810</td>\n",
       "      <td>-0.422745</td>\n",
       "      <td>0.291791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278843</td>\n",
       "      <td>-0.015365</td>\n",
       "      <td>-0.109212</td>\n",
       "      <td>-0.473473</td>\n",
       "      <td>-0.385310</td>\n",
       "      <td>0.045476</td>\n",
       "      <td>-0.206303</td>\n",
       "      <td>-0.477562</td>\n",
       "      <td>-0.198813</td>\n",
       "      <td>answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-0.637179</td>\n",
       "      <td>0.809932</td>\n",
       "      <td>-0.746795</td>\n",
       "      <td>-0.034512</td>\n",
       "      <td>0.564196</td>\n",
       "      <td>-0.661429</td>\n",
       "      <td>0.107035</td>\n",
       "      <td>0.597028</td>\n",
       "      <td>0.181911</td>\n",
       "      <td>1.146924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572198</td>\n",
       "      <td>-0.920624</td>\n",
       "      <td>0.287102</td>\n",
       "      <td>0.198521</td>\n",
       "      <td>-0.015174</td>\n",
       "      <td>1.044421</td>\n",
       "      <td>-0.429029</td>\n",
       "      <td>0.549898</td>\n",
       "      <td>0.272856</td>\n",
       "      <td>allocates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-0.637179</td>\n",
       "      <td>0.809932</td>\n",
       "      <td>-0.746795</td>\n",
       "      <td>-0.034512</td>\n",
       "      <td>0.564196</td>\n",
       "      <td>-0.661429</td>\n",
       "      <td>0.107035</td>\n",
       "      <td>0.597028</td>\n",
       "      <td>0.181911</td>\n",
       "      <td>1.146924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572198</td>\n",
       "      <td>-0.920624</td>\n",
       "      <td>0.287102</td>\n",
       "      <td>0.198521</td>\n",
       "      <td>-0.015174</td>\n",
       "      <td>1.044421</td>\n",
       "      <td>-0.429029</td>\n",
       "      <td>0.549898</td>\n",
       "      <td>0.272856</td>\n",
       "      <td>allocates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-0.637179</td>\n",
       "      <td>0.809932</td>\n",
       "      <td>-0.746795</td>\n",
       "      <td>-0.034512</td>\n",
       "      <td>0.564196</td>\n",
       "      <td>-0.661429</td>\n",
       "      <td>0.107035</td>\n",
       "      <td>0.597028</td>\n",
       "      <td>0.181911</td>\n",
       "      <td>1.146924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572198</td>\n",
       "      <td>-0.920624</td>\n",
       "      <td>0.287102</td>\n",
       "      <td>0.198521</td>\n",
       "      <td>-0.015174</td>\n",
       "      <td>1.044421</td>\n",
       "      <td>-0.429029</td>\n",
       "      <td>0.549898</td>\n",
       "      <td>0.272856</td>\n",
       "      <td>allocates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-0.306534</td>\n",
       "      <td>1.028156</td>\n",
       "      <td>-0.645633</td>\n",
       "      <td>0.008846</td>\n",
       "      <td>-0.070073</td>\n",
       "      <td>-0.164579</td>\n",
       "      <td>-0.061679</td>\n",
       "      <td>0.153521</td>\n",
       "      <td>0.701131</td>\n",
       "      <td>0.875699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876721</td>\n",
       "      <td>-0.683474</td>\n",
       "      <td>0.306355</td>\n",
       "      <td>0.433349</td>\n",
       "      <td>0.232320</td>\n",
       "      <td>0.875051</td>\n",
       "      <td>0.071240</td>\n",
       "      <td>0.692731</td>\n",
       "      <td>1.187933</td>\n",
       "      <td>prefers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-0.622821</td>\n",
       "      <td>-0.023114</td>\n",
       "      <td>-0.595264</td>\n",
       "      <td>0.105776</td>\n",
       "      <td>0.038853</td>\n",
       "      <td>-0.620449</td>\n",
       "      <td>0.479517</td>\n",
       "      <td>-0.351034</td>\n",
       "      <td>-0.479981</td>\n",
       "      <td>-0.407538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215935</td>\n",
       "      <td>-0.070316</td>\n",
       "      <td>0.354473</td>\n",
       "      <td>0.592619</td>\n",
       "      <td>-0.305956</td>\n",
       "      <td>-0.107259</td>\n",
       "      <td>0.244101</td>\n",
       "      <td>-0.096341</td>\n",
       "      <td>-0.385758</td>\n",
       "      <td>Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-0.510332</td>\n",
       "      <td>0.324319</td>\n",
       "      <td>-0.423043</td>\n",
       "      <td>0.070147</td>\n",
       "      <td>0.277236</td>\n",
       "      <td>-0.218969</td>\n",
       "      <td>-0.220465</td>\n",
       "      <td>0.371821</td>\n",
       "      <td>-0.195989</td>\n",
       "      <td>-0.466408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088687</td>\n",
       "      <td>0.014089</td>\n",
       "      <td>-0.063682</td>\n",
       "      <td>0.528301</td>\n",
       "      <td>-0.320240</td>\n",
       "      <td>0.252926</td>\n",
       "      <td>-0.073796</td>\n",
       "      <td>0.128380</td>\n",
       "      <td>-0.581874</td>\n",
       "      <td>achieve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-2.382297</td>\n",
       "      <td>1.501762</td>\n",
       "      <td>-0.254505</td>\n",
       "      <td>0.131927</td>\n",
       "      <td>0.557682</td>\n",
       "      <td>-0.805990</td>\n",
       "      <td>-0.836453</td>\n",
       "      <td>1.486825</td>\n",
       "      <td>0.466984</td>\n",
       "      <td>0.280121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340417</td>\n",
       "      <td>-0.205900</td>\n",
       "      <td>0.047145</td>\n",
       "      <td>1.347082</td>\n",
       "      <td>-0.371997</td>\n",
       "      <td>1.545529</td>\n",
       "      <td>-0.039519</td>\n",
       "      <td>-0.645954</td>\n",
       "      <td>-0.779783</td>\n",
       "      <td>to apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-0.106164</td>\n",
       "      <td>0.177298</td>\n",
       "      <td>-0.645414</td>\n",
       "      <td>0.020065</td>\n",
       "      <td>0.327438</td>\n",
       "      <td>0.280174</td>\n",
       "      <td>-0.032412</td>\n",
       "      <td>-0.203636</td>\n",
       "      <td>-0.347379</td>\n",
       "      <td>0.118631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346252</td>\n",
       "      <td>-0.421215</td>\n",
       "      <td>-0.207567</td>\n",
       "      <td>0.548721</td>\n",
       "      <td>0.274218</td>\n",
       "      <td>0.870021</td>\n",
       "      <td>0.233757</td>\n",
       "      <td>-0.149339</td>\n",
       "      <td>0.211811</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-0.491497</td>\n",
       "      <td>-0.265550</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>-0.190314</td>\n",
       "      <td>-0.319147</td>\n",
       "      <td>0.221899</td>\n",
       "      <td>1.154957</td>\n",
       "      <td>0.146642</td>\n",
       "      <td>-0.184705</td>\n",
       "      <td>-0.036220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183958</td>\n",
       "      <td>-0.347498</td>\n",
       "      <td>0.105770</td>\n",
       "      <td>0.752538</td>\n",
       "      <td>0.454219</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>-0.244662</td>\n",
       "      <td>-0.434922</td>\n",
       "      <td>-0.349551</td>\n",
       "      <td>explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.452951</td>\n",
       "      <td>0.566724</td>\n",
       "      <td>-0.610321</td>\n",
       "      <td>-0.125062</td>\n",
       "      <td>0.286878</td>\n",
       "      <td>0.089243</td>\n",
       "      <td>0.177399</td>\n",
       "      <td>0.609697</td>\n",
       "      <td>0.152518</td>\n",
       "      <td>-0.103807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.343522</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>-0.372169</td>\n",
       "      <td>0.808265</td>\n",
       "      <td>0.421602</td>\n",
       "      <td>0.902775</td>\n",
       "      <td>0.097758</td>\n",
       "      <td>-0.920171</td>\n",
       "      <td>-0.426469</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.171325</td>\n",
       "      <td>-0.909733</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>-0.105646</td>\n",
       "      <td>0.302346</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>-0.119290</td>\n",
       "      <td>0.908444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145924</td>\n",
       "      <td>0.096544</td>\n",
       "      <td>0.089728</td>\n",
       "      <td>0.067208</td>\n",
       "      <td>0.076053</td>\n",
       "      <td>0.769554</td>\n",
       "      <td>0.199837</td>\n",
       "      <td>-0.004670</td>\n",
       "      <td>-0.195965</td>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>-0.223500</td>\n",
       "      <td>0.316168</td>\n",
       "      <td>-0.573313</td>\n",
       "      <td>-0.213089</td>\n",
       "      <td>-0.320077</td>\n",
       "      <td>-0.019755</td>\n",
       "      <td>0.037804</td>\n",
       "      <td>0.352678</td>\n",
       "      <td>-0.001870</td>\n",
       "      <td>0.399941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040434</td>\n",
       "      <td>0.677997</td>\n",
       "      <td>0.405590</td>\n",
       "      <td>0.577239</td>\n",
       "      <td>-0.307932</td>\n",
       "      <td>0.472655</td>\n",
       "      <td>0.184877</td>\n",
       "      <td>-0.477090</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.564599</td>\n",
       "      <td>0.794030</td>\n",
       "      <td>-0.024180</td>\n",
       "      <td>-0.530278</td>\n",
       "      <td>0.794951</td>\n",
       "      <td>0.081063</td>\n",
       "      <td>-0.207360</td>\n",
       "      <td>0.101212</td>\n",
       "      <td>-0.933098</td>\n",
       "      <td>0.402112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.288826</td>\n",
       "      <td>-0.400641</td>\n",
       "      <td>0.352166</td>\n",
       "      <td>1.102918</td>\n",
       "      <td>-0.187711</td>\n",
       "      <td>0.754685</td>\n",
       "      <td>0.280691</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>-0.257189</td>\n",
       "      <td>apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-1.521737</td>\n",
       "      <td>2.222839</td>\n",
       "      <td>-0.953666</td>\n",
       "      <td>-0.381553</td>\n",
       "      <td>1.439641</td>\n",
       "      <td>-1.471898</td>\n",
       "      <td>0.230037</td>\n",
       "      <td>-0.325597</td>\n",
       "      <td>0.434378</td>\n",
       "      <td>-0.096491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709743</td>\n",
       "      <td>-1.517585</td>\n",
       "      <td>1.858804</td>\n",
       "      <td>0.477978</td>\n",
       "      <td>0.106701</td>\n",
       "      <td>1.236328</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>2.241943</td>\n",
       "      <td>-0.300628</td>\n",
       "      <td>can distinguish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-0.185942</td>\n",
       "      <td>0.202461</td>\n",
       "      <td>-0.152083</td>\n",
       "      <td>-0.094080</td>\n",
       "      <td>0.437873</td>\n",
       "      <td>0.291210</td>\n",
       "      <td>-0.657938</td>\n",
       "      <td>-0.277459</td>\n",
       "      <td>0.089848</td>\n",
       "      <td>-0.374019</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000594</td>\n",
       "      <td>0.237579</td>\n",
       "      <td>0.306476</td>\n",
       "      <td>-0.084550</td>\n",
       "      <td>0.422182</td>\n",
       "      <td>0.369707</td>\n",
       "      <td>-0.324411</td>\n",
       "      <td>0.439643</td>\n",
       "      <td>0.437833</td>\n",
       "      <td>shows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.244365</td>\n",
       "      <td>0.070699</td>\n",
       "      <td>0.199234</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.367544</td>\n",
       "      <td>-0.085583</td>\n",
       "      <td>-0.257422</td>\n",
       "      <td>0.622567</td>\n",
       "      <td>0.419108</td>\n",
       "      <td>0.702335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430302</td>\n",
       "      <td>-0.687203</td>\n",
       "      <td>0.084277</td>\n",
       "      <td>0.025163</td>\n",
       "      <td>0.060803</td>\n",
       "      <td>0.078199</td>\n",
       "      <td>0.320811</td>\n",
       "      <td>1.301208</td>\n",
       "      <td>0.578158</td>\n",
       "      <td>struggles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>-0.185942</td>\n",
       "      <td>0.202461</td>\n",
       "      <td>-0.152083</td>\n",
       "      <td>-0.094080</td>\n",
       "      <td>0.437873</td>\n",
       "      <td>0.291210</td>\n",
       "      <td>-0.657938</td>\n",
       "      <td>-0.277459</td>\n",
       "      <td>0.089848</td>\n",
       "      <td>-0.374019</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000594</td>\n",
       "      <td>0.237579</td>\n",
       "      <td>0.306476</td>\n",
       "      <td>-0.084550</td>\n",
       "      <td>0.422182</td>\n",
       "      <td>0.369707</td>\n",
       "      <td>-0.324411</td>\n",
       "      <td>0.439643</td>\n",
       "      <td>0.437833</td>\n",
       "      <td>shows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.076216</td>\n",
       "      <td>-0.184452</td>\n",
       "      <td>-0.158902</td>\n",
       "      <td>-0.027487</td>\n",
       "      <td>0.266010</td>\n",
       "      <td>-0.002750</td>\n",
       "      <td>-0.178853</td>\n",
       "      <td>0.678962</td>\n",
       "      <td>-0.220332</td>\n",
       "      <td>0.339723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204747</td>\n",
       "      <td>-0.927405</td>\n",
       "      <td>0.166153</td>\n",
       "      <td>0.476868</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.059544</td>\n",
       "      <td>0.440806</td>\n",
       "      <td>0.071041</td>\n",
       "      <td>0.288301</td>\n",
       "      <td>Is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>-0.367494</td>\n",
       "      <td>-0.796367</td>\n",
       "      <td>-0.328557</td>\n",
       "      <td>0.440196</td>\n",
       "      <td>0.587401</td>\n",
       "      <td>-0.421285</td>\n",
       "      <td>-0.094993</td>\n",
       "      <td>0.791555</td>\n",
       "      <td>-0.020347</td>\n",
       "      <td>-0.320504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783239</td>\n",
       "      <td>-0.560625</td>\n",
       "      <td>0.092044</td>\n",
       "      <td>0.456102</td>\n",
       "      <td>-0.221155</td>\n",
       "      <td>-0.077868</td>\n",
       "      <td>-0.613095</td>\n",
       "      <td>0.159887</td>\n",
       "      <td>-0.251353</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-0.293475</td>\n",
       "      <td>1.080610</td>\n",
       "      <td>-0.032094</td>\n",
       "      <td>0.346921</td>\n",
       "      <td>-0.995904</td>\n",
       "      <td>0.444716</td>\n",
       "      <td>0.089096</td>\n",
       "      <td>0.036606</td>\n",
       "      <td>0.425408</td>\n",
       "      <td>0.374226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168921</td>\n",
       "      <td>0.436939</td>\n",
       "      <td>-0.135292</td>\n",
       "      <td>0.395321</td>\n",
       "      <td>0.416342</td>\n",
       "      <td>0.553579</td>\n",
       "      <td>0.241204</td>\n",
       "      <td>-0.839277</td>\n",
       "      <td>-0.528782</td>\n",
       "      <td>tuning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>-0.115650</td>\n",
       "      <td>0.308205</td>\n",
       "      <td>-0.178497</td>\n",
       "      <td>-0.211281</td>\n",
       "      <td>0.311753</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>-0.308651</td>\n",
       "      <td>-0.122757</td>\n",
       "      <td>-0.159068</td>\n",
       "      <td>-0.256250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476192</td>\n",
       "      <td>-0.024006</td>\n",
       "      <td>0.482596</td>\n",
       "      <td>0.874799</td>\n",
       "      <td>0.241520</td>\n",
       "      <td>0.299062</td>\n",
       "      <td>-0.021645</td>\n",
       "      <td>0.094385</td>\n",
       "      <td>-0.005442</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-0.441149</td>\n",
       "      <td>1.283116</td>\n",
       "      <td>-0.159065</td>\n",
       "      <td>-0.027061</td>\n",
       "      <td>-0.972555</td>\n",
       "      <td>0.282964</td>\n",
       "      <td>0.150320</td>\n",
       "      <td>0.323244</td>\n",
       "      <td>0.502255</td>\n",
       "      <td>0.210881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.258051</td>\n",
       "      <td>0.376371</td>\n",
       "      <td>0.577945</td>\n",
       "      <td>0.654155</td>\n",
       "      <td>0.901190</td>\n",
       "      <td>0.433413</td>\n",
       "      <td>-0.665444</td>\n",
       "      <td>-0.187893</td>\n",
       "      <td>tuning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.333929</td>\n",
       "      <td>-0.137962</td>\n",
       "      <td>-0.828017</td>\n",
       "      <td>0.304083</td>\n",
       "      <td>0.033157</td>\n",
       "      <td>-0.302081</td>\n",
       "      <td>0.653135</td>\n",
       "      <td>-0.243234</td>\n",
       "      <td>-0.401150</td>\n",
       "      <td>-0.190340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424297</td>\n",
       "      <td>0.134926</td>\n",
       "      <td>0.361010</td>\n",
       "      <td>0.545954</td>\n",
       "      <td>-0.501078</td>\n",
       "      <td>0.119009</td>\n",
       "      <td>0.215204</td>\n",
       "      <td>-0.078726</td>\n",
       "      <td>-0.447719</td>\n",
       "      <td>Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-0.155043</td>\n",
       "      <td>0.021688</td>\n",
       "      <td>-0.118369</td>\n",
       "      <td>-0.400168</td>\n",
       "      <td>-0.025100</td>\n",
       "      <td>0.345643</td>\n",
       "      <td>0.335597</td>\n",
       "      <td>0.431523</td>\n",
       "      <td>0.174753</td>\n",
       "      <td>0.026218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241310</td>\n",
       "      <td>0.590665</td>\n",
       "      <td>-0.382804</td>\n",
       "      <td>0.486396</td>\n",
       "      <td>0.453194</td>\n",
       "      <td>0.536334</td>\n",
       "      <td>0.439166</td>\n",
       "      <td>-0.742241</td>\n",
       "      <td>-0.258627</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.207421</td>\n",
       "      <td>-0.004642</td>\n",
       "      <td>0.682881</td>\n",
       "      <td>-0.500220</td>\n",
       "      <td>-0.260086</td>\n",
       "      <td>0.487780</td>\n",
       "      <td>-0.337118</td>\n",
       "      <td>0.755221</td>\n",
       "      <td>0.034336</td>\n",
       "      <td>0.192336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540372</td>\n",
       "      <td>-0.008147</td>\n",
       "      <td>0.021521</td>\n",
       "      <td>-0.095862</td>\n",
       "      <td>0.539939</td>\n",
       "      <td>0.277078</td>\n",
       "      <td>-0.774834</td>\n",
       "      <td>0.986001</td>\n",
       "      <td>0.178261</td>\n",
       "      <td>appeared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-0.279398</td>\n",
       "      <td>0.321832</td>\n",
       "      <td>-0.506279</td>\n",
       "      <td>-0.333663</td>\n",
       "      <td>0.175019</td>\n",
       "      <td>-0.805464</td>\n",
       "      <td>-0.554689</td>\n",
       "      <td>0.136162</td>\n",
       "      <td>0.141380</td>\n",
       "      <td>-0.248368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311025</td>\n",
       "      <td>-0.438615</td>\n",
       "      <td>0.284868</td>\n",
       "      <td>-0.177979</td>\n",
       "      <td>-0.033754</td>\n",
       "      <td>0.670074</td>\n",
       "      <td>-0.380049</td>\n",
       "      <td>0.155138</td>\n",
       "      <td>0.152966</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-0.313869</td>\n",
       "      <td>0.491609</td>\n",
       "      <td>-0.546574</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.575175</td>\n",
       "      <td>0.376803</td>\n",
       "      <td>-0.678005</td>\n",
       "      <td>-0.107280</td>\n",
       "      <td>0.224484</td>\n",
       "      <td>-0.634121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203104</td>\n",
       "      <td>0.450005</td>\n",
       "      <td>0.390723</td>\n",
       "      <td>0.420562</td>\n",
       "      <td>0.418868</td>\n",
       "      <td>0.242043</td>\n",
       "      <td>0.088068</td>\n",
       "      <td>-0.224804</td>\n",
       "      <td>-0.064666</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.926681</td>\n",
       "      <td>0.415704</td>\n",
       "      <td>0.822869</td>\n",
       "      <td>0.571611</td>\n",
       "      <td>0.354067</td>\n",
       "      <td>-0.627812</td>\n",
       "      <td>0.767652</td>\n",
       "      <td>1.050904</td>\n",
       "      <td>0.610913</td>\n",
       "      <td>-0.009384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309215</td>\n",
       "      <td>-0.268491</td>\n",
       "      <td>0.114303</td>\n",
       "      <td>0.120640</td>\n",
       "      <td>0.652090</td>\n",
       "      <td>1.117427</td>\n",
       "      <td>-0.621829</td>\n",
       "      <td>-0.113080</td>\n",
       "      <td>-0.019751</td>\n",
       "      <td>Starting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>-0.195453</td>\n",
       "      <td>0.273537</td>\n",
       "      <td>-1.238211</td>\n",
       "      <td>-0.044912</td>\n",
       "      <td>0.045494</td>\n",
       "      <td>-0.534641</td>\n",
       "      <td>0.260857</td>\n",
       "      <td>0.202678</td>\n",
       "      <td>-0.340133</td>\n",
       "      <td>0.748079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556610</td>\n",
       "      <td>-0.116709</td>\n",
       "      <td>0.087752</td>\n",
       "      <td>-0.169133</td>\n",
       "      <td>-0.226868</td>\n",
       "      <td>0.747890</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.214938</td>\n",
       "      <td>-0.306645</td>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-0.775546</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.468646</td>\n",
       "      <td>-0.621496</td>\n",
       "      <td>0.395246</td>\n",
       "      <td>-0.095464</td>\n",
       "      <td>-0.445617</td>\n",
       "      <td>0.345592</td>\n",
       "      <td>0.109316</td>\n",
       "      <td>0.595509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082740</td>\n",
       "      <td>-0.229096</td>\n",
       "      <td>-0.259973</td>\n",
       "      <td>-0.071673</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>0.257417</td>\n",
       "      <td>0.618628</td>\n",
       "      <td>-0.088082</td>\n",
       "      <td>-0.446665</td>\n",
       "      <td>argue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>-0.367639</td>\n",
       "      <td>0.870716</td>\n",
       "      <td>0.384394</td>\n",
       "      <td>0.151159</td>\n",
       "      <td>0.164806</td>\n",
       "      <td>-0.762566</td>\n",
       "      <td>-0.199744</td>\n",
       "      <td>0.159498</td>\n",
       "      <td>0.119781</td>\n",
       "      <td>0.294016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477889</td>\n",
       "      <td>-0.094701</td>\n",
       "      <td>-0.237239</td>\n",
       "      <td>0.474289</td>\n",
       "      <td>0.092879</td>\n",
       "      <td>0.080902</td>\n",
       "      <td>0.147815</td>\n",
       "      <td>0.820764</td>\n",
       "      <td>-0.008964</td>\n",
       "      <td>drops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.210166</td>\n",
       "      <td>-0.299578</td>\n",
       "      <td>-0.163822</td>\n",
       "      <td>0.067021</td>\n",
       "      <td>0.544443</td>\n",
       "      <td>0.290994</td>\n",
       "      <td>0.419907</td>\n",
       "      <td>0.181920</td>\n",
       "      <td>0.041746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085592</td>\n",
       "      <td>0.564838</td>\n",
       "      <td>-0.191597</td>\n",
       "      <td>0.610405</td>\n",
       "      <td>-0.247212</td>\n",
       "      <td>0.154659</td>\n",
       "      <td>0.126074</td>\n",
       "      <td>-0.277974</td>\n",
       "      <td>-0.354286</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.210166</td>\n",
       "      <td>-0.299578</td>\n",
       "      <td>-0.163822</td>\n",
       "      <td>0.067021</td>\n",
       "      <td>0.544443</td>\n",
       "      <td>0.290994</td>\n",
       "      <td>0.419907</td>\n",
       "      <td>0.181920</td>\n",
       "      <td>0.041746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085592</td>\n",
       "      <td>0.564838</td>\n",
       "      <td>-0.191597</td>\n",
       "      <td>0.610405</td>\n",
       "      <td>-0.247212</td>\n",
       "      <td>0.154659</td>\n",
       "      <td>0.126074</td>\n",
       "      <td>-0.277974</td>\n",
       "      <td>-0.354286</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.136322</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>-0.417185</td>\n",
       "      <td>-0.448869</td>\n",
       "      <td>0.657620</td>\n",
       "      <td>0.729416</td>\n",
       "      <td>-0.218409</td>\n",
       "      <td>0.249544</td>\n",
       "      <td>-0.409935</td>\n",
       "      <td>0.190494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461168</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.478554</td>\n",
       "      <td>0.280043</td>\n",
       "      <td>0.108729</td>\n",
       "      <td>0.099748</td>\n",
       "      <td>-0.453225</td>\n",
       "      <td>-0.184486</td>\n",
       "      <td>0.346713</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-0.200802</td>\n",
       "      <td>-0.576773</td>\n",
       "      <td>0.039280</td>\n",
       "      <td>-0.248180</td>\n",
       "      <td>-0.347001</td>\n",
       "      <td>0.187511</td>\n",
       "      <td>0.340412</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>-0.306738</td>\n",
       "      <td>0.332596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040463</td>\n",
       "      <td>-0.084452</td>\n",
       "      <td>0.257912</td>\n",
       "      <td>0.952511</td>\n",
       "      <td>-0.132371</td>\n",
       "      <td>0.176657</td>\n",
       "      <td>0.162941</td>\n",
       "      <td>0.347501</td>\n",
       "      <td>0.215831</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.136322</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>-0.417185</td>\n",
       "      <td>-0.448869</td>\n",
       "      <td>0.657620</td>\n",
       "      <td>0.729416</td>\n",
       "      <td>-0.218409</td>\n",
       "      <td>0.249544</td>\n",
       "      <td>-0.409935</td>\n",
       "      <td>0.190494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461168</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.478554</td>\n",
       "      <td>0.280043</td>\n",
       "      <td>0.108729</td>\n",
       "      <td>0.099748</td>\n",
       "      <td>-0.453225</td>\n",
       "      <td>-0.184486</td>\n",
       "      <td>0.346713</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>-0.551758</td>\n",
       "      <td>-0.601709</td>\n",
       "      <td>-0.185830</td>\n",
       "      <td>0.249082</td>\n",
       "      <td>0.240786</td>\n",
       "      <td>0.193926</td>\n",
       "      <td>0.498678</td>\n",
       "      <td>1.359404</td>\n",
       "      <td>-0.046613</td>\n",
       "      <td>-0.063560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374104</td>\n",
       "      <td>-0.463241</td>\n",
       "      <td>0.101950</td>\n",
       "      <td>0.150720</td>\n",
       "      <td>-0.371247</td>\n",
       "      <td>0.304828</td>\n",
       "      <td>-0.008086</td>\n",
       "      <td>0.564784</td>\n",
       "      <td>0.491858</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>-0.069362</td>\n",
       "      <td>0.168038</td>\n",
       "      <td>0.526058</td>\n",
       "      <td>0.380979</td>\n",
       "      <td>0.189340</td>\n",
       "      <td>0.464952</td>\n",
       "      <td>-0.957528</td>\n",
       "      <td>0.415063</td>\n",
       "      <td>-0.442072</td>\n",
       "      <td>0.904541</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021371</td>\n",
       "      <td>0.441891</td>\n",
       "      <td>0.424356</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>-0.314881</td>\n",
       "      <td>-0.136070</td>\n",
       "      <td>-0.893684</td>\n",
       "      <td>0.031063</td>\n",
       "      <td>0.045445</td>\n",
       "      <td>produced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.217825</td>\n",
       "      <td>0.135323</td>\n",
       "      <td>0.089498</td>\n",
       "      <td>0.233386</td>\n",
       "      <td>0.476492</td>\n",
       "      <td>0.921903</td>\n",
       "      <td>-1.108064</td>\n",
       "      <td>-0.507780</td>\n",
       "      <td>-0.079257</td>\n",
       "      <td>-0.745102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853719</td>\n",
       "      <td>-0.580739</td>\n",
       "      <td>-0.354759</td>\n",
       "      <td>0.797989</td>\n",
       "      <td>-0.795937</td>\n",
       "      <td>-0.743815</td>\n",
       "      <td>0.087255</td>\n",
       "      <td>-0.108192</td>\n",
       "      <td>0.181438</td>\n",
       "      <td>be explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>-0.390515</td>\n",
       "      <td>0.386091</td>\n",
       "      <td>-0.348924</td>\n",
       "      <td>0.649216</td>\n",
       "      <td>0.783944</td>\n",
       "      <td>-0.154428</td>\n",
       "      <td>0.101172</td>\n",
       "      <td>-0.059748</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>0.261134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165821</td>\n",
       "      <td>-0.454197</td>\n",
       "      <td>-0.188265</td>\n",
       "      <td>0.266283</td>\n",
       "      <td>0.282501</td>\n",
       "      <td>0.700711</td>\n",
       "      <td>-0.265215</td>\n",
       "      <td>-0.178640</td>\n",
       "      <td>-0.254985</td>\n",
       "      <td>Comparing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.059835  0.240861  0.275318 -0.019283  0.400894 -0.494091 -0.582469   \n",
       "1   -0.479267 -0.340685 -0.083469  0.484785 -0.378456 -0.478947 -1.161890   \n",
       "3   -0.551758 -0.601709 -0.185830  0.249082  0.240786  0.193926  0.498678   \n",
       "4   -0.014832  0.090976  0.263633  0.249120 -0.170383 -0.548515  0.152994   \n",
       "8    0.181994  0.605189 -0.997008 -0.302269 -0.015252 -0.242133  0.398710   \n",
       "9   -0.315933 -0.017035 -0.117964 -0.383910 -0.521591  0.075804  0.260689   \n",
       "10   0.439284 -0.402886  0.087319 -0.258886  0.390618  0.289037 -0.358386   \n",
       "16   0.084465 -0.285254 -0.678433 -0.661452  0.258676 -0.566749  0.016577   \n",
       "17  -0.815184  0.874137 -0.904312  0.096941 -0.145967 -1.167747 -0.479527   \n",
       "18  -0.498545  0.375550 -0.521380 -0.145479  0.318801 -0.570060 -0.201639   \n",
       "23  -0.220900 -0.208086 -0.592484 -0.644049  0.029968 -0.630416  0.668904   \n",
       "24  -0.252087 -0.110759  0.149693 -0.030479  0.573090 -0.059328 -0.027973   \n",
       "25  -0.031056  0.511771 -0.402305  0.392806  1.072279 -1.198256  0.544827   \n",
       "27   0.122050  0.949302 -0.864618  0.426060  2.344748 -0.409799 -0.064119   \n",
       "28  -0.461630  0.006685 -0.511341  0.364487  0.381569  0.259681  0.038859   \n",
       "29   0.122050  0.949302 -0.864618  0.426060  2.344748 -0.409799 -0.064119   \n",
       "31   0.755990  0.733870  0.350174 -0.242800  0.857030  0.161440 -0.314395   \n",
       "35  -0.355967  0.024432  0.499512  0.470141  0.026586 -0.896237 -0.924498   \n",
       "39  -0.382333  0.323978 -0.130768 -0.194236  0.576638  0.374112 -0.376351   \n",
       "40  -0.067147  0.009191 -0.092259 -0.159158  0.181222 -0.282149 -0.241155   \n",
       "43  -0.073721 -0.287547  0.375200 -0.238369  0.153739  0.252630 -0.122325   \n",
       "44  -0.068286  0.493022  0.123446  0.266689  0.511212 -0.718671 -0.138341   \n",
       "47   0.108845  0.687509 -0.152748 -0.056645 -0.189407 -0.154639  0.781604   \n",
       "48  -0.411049  0.084121 -0.147872  0.031999  0.046202 -0.053003  0.677854   \n",
       "49  -0.330973  0.033278 -0.149374  0.012377  0.510642 -0.182420  0.332414   \n",
       "50   0.244245  0.089943  0.221659 -0.075435 -0.312054  0.153455  0.203115   \n",
       "51  -0.241720  0.024009 -0.620501  0.120691 -0.006524 -0.164343  0.143863   \n",
       "52  -0.139784  0.282808  0.098378  0.399734  0.659067  0.015212  0.514658   \n",
       "54  -0.367494 -0.796367 -0.328557  0.440196  0.587401 -0.421285 -0.094993   \n",
       "57  -1.697060  2.623034  0.321020  0.414173  2.568970 -0.878483 -1.340148   \n",
       "58  -0.259860  0.439059  0.398028  1.014346 -0.376176 -0.578750 -0.077481   \n",
       "59   0.510059  0.777036 -0.647992 -0.431016  0.293009  0.300757  0.028609   \n",
       "60   0.510059  0.777036 -0.647992 -0.431016  0.293009  0.300757  0.028609   \n",
       "61  -0.372455  0.313116 -0.421734 -0.302487  0.391502 -2.002126 -0.294954   \n",
       "62  -0.038596  0.460821  0.153196  0.114656  0.197212 -0.525422 -0.147961   \n",
       "63  -0.372455  0.313116 -0.421734 -0.302487  0.391502 -2.002126 -0.294954   \n",
       "64  -0.133340 -0.244977 -0.066271  0.176615  0.091122  0.293755  0.298214   \n",
       "66  -0.133340 -0.244977 -0.066271  0.176615  0.091122  0.293755  0.298214   \n",
       "68  -1.025773  0.310373 -0.447934 -0.457204  0.679729 -0.192911 -0.144402   \n",
       "69  -0.382333  0.323978 -0.130768 -0.194236  0.576638  0.374112 -0.376351   \n",
       "77   0.020189 -0.007402 -0.099057 -0.233371 -0.424543 -0.118362  0.611347   \n",
       "78  -0.315573  0.449618 -1.079981 -0.171654  0.205883  0.127349  0.271354   \n",
       "79   0.020189 -0.007402 -0.099057 -0.233371 -0.424543 -0.118362  0.611347   \n",
       "80  -0.563688 -0.074106 -0.755131 -0.456024  0.280470  0.357581 -0.740065   \n",
       "81  -0.563688 -0.074106 -0.755131 -0.456024  0.280470  0.357581 -0.740065   \n",
       "82  -0.004066  0.832123 -0.456464  0.383548  0.603917  0.009833 -0.676367   \n",
       "86  -0.607313  0.113089  0.073252 -0.084065  0.004822 -0.035418 -0.209196   \n",
       "87  -0.876917  0.387537 -0.582344 -0.594061 -0.014207 -0.174442 -0.194894   \n",
       "88   0.280924  0.502440 -0.709884 -0.146214  0.307728  0.523678  0.321026   \n",
       "89   0.280924  0.502440 -0.709884 -0.146214  0.307728  0.523678  0.321026   \n",
       "92   0.026602 -0.481717 -0.528872 -0.620290  0.013163  0.556058 -0.062064   \n",
       "93  -0.183888  0.372771 -0.206273  0.343829  0.225271  0.131173  0.179302   \n",
       "94   0.259945 -0.142396 -0.339951  0.022232  0.302048 -0.056249  0.619220   \n",
       "95   0.028216  0.683443 -0.376829  0.232246  0.285615  0.433518 -0.316045   \n",
       "96  -0.259551  0.412980  0.281104  1.102394  1.177207 -0.287381 -0.432665   \n",
       "97  -0.269682  0.044014 -0.105810 -0.077238  1.501853 -0.509115  0.388575   \n",
       "98  -0.786009 -0.788565 -0.140104 -0.291559  0.532405 -0.328127  0.080115   \n",
       "103  0.216901  0.513081 -0.433099 -0.174096  0.823155  0.221483 -0.227389   \n",
       "104  0.052707 -0.162361  0.262985  0.618636  1.238630  0.871532 -0.663967   \n",
       "105 -0.174840 -0.478888 -0.055353  0.144196  0.099981 -0.247665 -0.016141   \n",
       "106  0.070869 -0.356521 -0.100021 -0.095342  0.464404  0.119578 -0.302497   \n",
       "110 -0.219071  1.008216 -1.086389  0.574497  1.464685  0.422202  0.228203   \n",
       "111 -0.272068  0.158148 -1.006689 -0.456199  0.171681 -0.577783  0.077261   \n",
       "112  0.435460  0.580408 -0.290261  0.471809 -1.222336 -1.222143  0.138435   \n",
       "115  0.481207  0.164531 -0.489792  0.334123 -0.355504  0.436219 -0.140649   \n",
       "116 -0.382651 -0.179681 -0.572773  0.209791  0.314946 -0.143388 -0.163793   \n",
       "118 -0.382435  0.858616 -0.746813 -1.393332  1.523503 -0.214236  0.145070   \n",
       "119 -0.476875  1.261953 -1.058676 -0.365043  0.069808 -0.520373 -0.572612   \n",
       "121 -0.990069 -1.002584 -0.228493 -0.264751 -0.268350 -0.036842 -0.626979   \n",
       "122 -0.539446 -0.165851 -0.314165 -0.600294  0.150964  0.278123  0.484477   \n",
       "123 -0.546252 -0.032759 -0.244539 -0.282502 -0.093734 -0.367365 -1.279347   \n",
       "125 -0.515809  0.106172 -0.495997  0.025402 -0.121481 -0.204168  0.195508   \n",
       "126 -0.515809  0.106172 -0.495997  0.025402 -0.121481 -0.204168  0.195508   \n",
       "130 -1.044857 -1.256696 -0.957417  0.458047  1.089027 -0.144280 -0.435878   \n",
       "131  0.022375  0.191903 -0.411201 -0.275466 -0.058894  0.487233  0.296230   \n",
       "133 -0.214934  0.376908  0.209861  0.495754  0.314413 -0.274643  0.063896   \n",
       "134 -0.312544  0.333999 -0.204261  0.092911  0.753148  0.336927 -0.856925   \n",
       "142 -0.457224  0.000885 -0.164742  0.114626 -0.099622  0.503985  0.562962   \n",
       "143  0.323992 -0.035124 -0.006809 -0.337739 -0.376882 -0.593107 -0.206207   \n",
       "144  0.323992 -0.035124 -0.006809 -0.337739 -0.376882 -0.593107 -0.206207   \n",
       "145 -0.397924  0.353230 -0.238537  0.023938  0.401741 -0.336119 -0.133388   \n",
       "151  0.135813  0.043795  0.053178 -0.037448 -0.513281 -0.307284 -0.461424   \n",
       "155 -0.815985 -1.428854 -0.698961  0.228826  1.057089 -0.331962  0.201367   \n",
       "156 -0.736893  0.352485  0.098556  0.417768 -0.039069 -0.159275 -0.275430   \n",
       "157 -0.744695  0.976921 -0.492248 -0.010015 -0.266327 -0.326750  0.099546   \n",
       "158 -0.744695  0.976921 -0.492248 -0.010015 -0.266327 -0.326750  0.099546   \n",
       "159 -0.442201  0.810350 -0.333028  0.150481  0.564273 -0.094420 -0.978193   \n",
       "160 -0.712951  0.444234 -1.100974  0.320230  0.293257 -0.338834 -0.529803   \n",
       "161  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "162 -0.637179  0.809932 -0.746795 -0.034512  0.564196 -0.661429  0.107035   \n",
       "163 -0.637179  0.809932 -0.746795 -0.034512  0.564196 -0.661429  0.107035   \n",
       "164 -0.637179  0.809932 -0.746795 -0.034512  0.564196 -0.661429  0.107035   \n",
       "165 -0.306534  1.028156 -0.645633  0.008846 -0.070073 -0.164579 -0.061679   \n",
       "166 -0.622821 -0.023114 -0.595264  0.105776  0.038853 -0.620449  0.479517   \n",
       "169 -0.510332  0.324319 -0.423043  0.070147  0.277236 -0.218969 -0.220465   \n",
       "170 -2.382297  1.501762 -0.254505  0.131927  0.557682 -0.805990 -0.836453   \n",
       "171 -0.106164  0.177298 -0.645414  0.020065  0.327438  0.280174 -0.032412   \n",
       "176 -0.491497 -0.265550  0.047296 -0.190314 -0.319147  0.221899  1.154957   \n",
       "177  0.452951  0.566724 -0.610321 -0.125062  0.286878  0.089243  0.177399   \n",
       "179 -0.021112  0.171325 -0.909733  0.013434  0.034247 -0.105646  0.302346   \n",
       "180 -0.223500  0.316168 -0.573313 -0.213089 -0.320077 -0.019755  0.037804   \n",
       "184  0.564599  0.794030 -0.024180 -0.530278  0.794951  0.081063 -0.207360   \n",
       "185 -1.521737  2.222839 -0.953666 -0.381553  1.439641 -1.471898  0.230037   \n",
       "186 -0.185942  0.202461 -0.152083 -0.094080  0.437873  0.291210 -0.657938   \n",
       "187  0.244365  0.070699  0.199234  0.569431  0.367544 -0.085583 -0.257422   \n",
       "188 -0.185942  0.202461 -0.152083 -0.094080  0.437873  0.291210 -0.657938   \n",
       "189  0.076216 -0.184452 -0.158902 -0.027487  0.266010 -0.002750 -0.178853   \n",
       "190 -0.367494 -0.796367 -0.328557  0.440196  0.587401 -0.421285 -0.094993   \n",
       "192 -0.293475  1.080610 -0.032094  0.346921 -0.995904  0.444716  0.089096   \n",
       "194 -0.115650  0.308205 -0.178497 -0.211281  0.311753  0.015071 -0.308651   \n",
       "195 -0.441149  1.283116 -0.159065 -0.027061 -0.972555  0.282964  0.150320   \n",
       "197 -0.333929 -0.137962 -0.828017  0.304083  0.033157 -0.302081  0.653135   \n",
       "201 -0.155043  0.021688 -0.118369 -0.400168 -0.025100  0.345643  0.335597   \n",
       "204  0.207421 -0.004642  0.682881 -0.500220 -0.260086  0.487780 -0.337118   \n",
       "207 -0.279398  0.321832 -0.506279 -0.333663  0.175019 -0.805464 -0.554689   \n",
       "208 -0.313869  0.491609 -0.546574 -0.000684  0.575175  0.376803 -0.678005   \n",
       "211  0.926681  0.415704  0.822869  0.571611  0.354067 -0.627812  0.767652   \n",
       "215 -0.195453  0.273537 -1.238211 -0.044912  0.045494 -0.534641  0.260857   \n",
       "224 -0.775546 -0.002334  0.468646 -0.621496  0.395246 -0.095464 -0.445617   \n",
       "226 -0.367639  0.870716  0.384394  0.151159  0.164806 -0.762566 -0.199744   \n",
       "227  0.006100  0.210166 -0.299578 -0.163822  0.067021  0.544443  0.290994   \n",
       "228  0.006100  0.210166 -0.299578 -0.163822  0.067021  0.544443  0.290994   \n",
       "231  0.136322  0.003662 -0.417185 -0.448869  0.657620  0.729416 -0.218409   \n",
       "232 -0.200802 -0.576773  0.039280 -0.248180 -0.347001  0.187511  0.340412   \n",
       "233  0.136322  0.003662 -0.417185 -0.448869  0.657620  0.729416 -0.218409   \n",
       "234 -0.551758 -0.601709 -0.185830  0.249082  0.240786  0.193926  0.498678   \n",
       "236 -0.069362  0.168038  0.526058  0.380979  0.189340  0.464952 -0.957528   \n",
       "241  0.217825  0.135323  0.089498  0.233386  0.476492  0.921903 -1.108064   \n",
       "243 -0.390515  0.386091 -0.348924  0.649216  0.783944 -0.154428  0.101172   \n",
       "\n",
       "            7         8         9  ...      1015      1016      1017  \\\n",
       "0    0.771119  0.678693  0.157774  ...  0.239020 -0.453651  0.498861   \n",
       "1    0.293845  0.188119  1.032229  ...  1.752333 -0.459487 -0.223691   \n",
       "3    1.359404 -0.046613 -0.063560  ...  0.605742 -0.658780  0.295447   \n",
       "4   -0.380535 -0.093677  0.559947  ...  0.620792 -0.345686 -0.029003   \n",
       "8    0.620097 -0.426464 -0.437437  ...  0.248234  0.209224 -0.108337   \n",
       "9    0.050643  0.095864  0.354527  ... -0.181811 -0.030457  0.430391   \n",
       "10   0.335580 -0.254189  0.151227  ...  0.485583  0.324492  0.135045   \n",
       "16  -0.123753  0.014325  0.238596  ...  0.470453 -0.487008  0.392476   \n",
       "17   0.078829  0.937768 -0.005589  ... -0.682849 -0.757245  0.783076   \n",
       "18   0.149462  0.423595 -0.227575  ... -0.718149  0.397363  0.408168   \n",
       "23   0.465420 -0.401957  1.085995  ...  0.088196  0.072950 -0.040444   \n",
       "24  -0.060144 -0.739402 -0.121876  ...  0.501332 -0.351525  0.612804   \n",
       "25  -0.259022 -0.309486 -0.354982  ... -0.043094 -0.937673 -0.259689   \n",
       "27   0.421269 -0.476417 -0.644752  ...  0.271764 -0.605495  0.335029   \n",
       "28   0.290896  0.433663  0.678046  ... -0.436564  0.330171 -0.354160   \n",
       "29   0.421269 -0.476417 -0.644752  ...  0.271764 -0.605495  0.335029   \n",
       "31  -0.238194 -0.492384  0.025187  ... -0.096688 -0.139346 -0.121755   \n",
       "35   0.011559 -1.753619  0.198732  ...  1.433819 -0.251986 -0.020722   \n",
       "39  -0.234408 -0.173447 -0.251889  ...  0.833246 -0.385633  0.127101   \n",
       "40   0.374727 -0.658454 -0.262804  ...  0.218099 -0.927582  0.520521   \n",
       "43  -0.034344 -0.326629  0.457434  ...  0.164473 -0.661984  0.392214   \n",
       "44   0.254072  0.220373 -0.292399  ...  0.701825  0.487816 -0.197305   \n",
       "47   0.434901 -0.007707  0.062158  ...  0.686036 -0.295823  0.022191   \n",
       "48  -0.056690 -0.326474 -0.317143  ...  0.569455 -0.522795  0.505677   \n",
       "49  -0.034646  0.327653  0.097017  ...  0.195848 -0.608211  0.562728   \n",
       "50  -0.016109  0.321191  0.042987  ... -0.189089 -0.135420 -0.091386   \n",
       "51   0.090789 -0.238136  0.386626  ... -0.232300 -0.224819  0.716208   \n",
       "52   0.351281  0.019359  0.117071  ...  1.121217 -0.712355  0.407688   \n",
       "54   0.791555 -0.020347 -0.320504  ...  0.766191 -0.411308  0.110840   \n",
       "57   0.580774  0.174799 -0.558720  ...  1.605328 -1.827377  0.584723   \n",
       "58  -0.038502 -0.488136  0.207464  ...  0.862793 -0.012750  0.423572   \n",
       "59   0.200901 -0.264500 -0.314564  ...  0.476622  0.369570  0.120154   \n",
       "60   0.200901 -0.264500 -0.314564  ...  0.476622  0.369570  0.120154   \n",
       "61   0.323469 -1.068244 -0.748371  ...  0.465439 -0.479181  1.058791   \n",
       "62   0.092074  0.362372  0.004563  ...  0.601565 -0.948843 -0.056877   \n",
       "63   0.323469 -1.068244 -0.748371  ...  0.465439 -0.479181  1.058791   \n",
       "64   0.938860 -0.570982 -0.014076  ...  0.290145 -0.766449  0.170825   \n",
       "66   0.938860 -0.570982 -0.014076  ...  0.756402 -0.672986 -0.060730   \n",
       "68   1.037015 -0.467973 -0.688231  ...  0.197316 -1.048957  0.400892   \n",
       "69  -0.234408 -0.173447 -0.251889  ...  0.651315 -0.195203  0.293907   \n",
       "77   0.175541 -0.133079 -0.148777  ...  0.307901 -0.157737 -0.276184   \n",
       "78   0.312264  0.212851  0.099918  ...  0.485182  0.088553 -0.460196   \n",
       "79   0.175541 -0.133079 -0.148777  ...  0.307901 -0.157737 -0.276184   \n",
       "80  -0.832735 -0.339992  1.556885  ...  1.071753 -0.562981  0.344128   \n",
       "81  -0.832735 -0.339992  1.556885  ...  1.071753 -0.562981  0.344128   \n",
       "82  -0.021864  0.388104 -0.679269  ...  0.633375  0.000403  0.216832   \n",
       "86   1.103459 -0.060140 -0.467778  ...  0.228359 -0.109186  0.187208   \n",
       "87   0.255090 -0.441897 -0.677492  ...  0.262748 -0.833305 -0.148131   \n",
       "88   0.300733  0.038990 -0.184458  ...  0.319877 -0.054451 -0.173571   \n",
       "89   0.300733  0.038990 -0.184458  ...  0.319877 -0.054451 -0.173571   \n",
       "92  -0.274613 -0.156070  0.495878  ...  0.389084  0.678852  0.317035   \n",
       "93   0.526922 -0.329145 -0.006487  ...  0.860042 -0.901996 -0.398250   \n",
       "94  -0.117933 -0.422811  0.629025  ...  0.067377 -0.016568  0.898584   \n",
       "95   0.127629 -0.247909 -0.092227  ...  0.455918  0.222376  0.215965   \n",
       "96  -0.558143  0.342117  0.349016  ...  1.068888 -1.171590 -0.310252   \n",
       "97   0.469875  0.497823  0.476876  ...  0.426512 -0.233275  0.309767   \n",
       "98   0.161973 -0.616295  0.151344  ... -0.105938 -0.343113 -0.298812   \n",
       "103  0.042357 -0.036590  0.574497  ...  0.212701  0.129081  0.165361   \n",
       "104 -0.390604 -0.790598  0.497508  ...  0.524841  0.189800  0.321996   \n",
       "105 -0.057487  0.183482 -0.276753  ... -0.100621  0.105851  0.154316   \n",
       "106  0.087231  0.071933  0.193344  ... -0.128228 -0.421634 -0.064661   \n",
       "110  0.723016  0.527867  0.214996  ...  0.104654 -0.107193  0.393227   \n",
       "111  0.169222 -0.521943  0.930989  ... -0.021652 -0.161078 -0.217208   \n",
       "112  0.703725 -0.375188  1.090575  ...  1.308993  0.006627 -0.045796   \n",
       "115 -0.593268 -0.020781  0.235909  ...  0.583105 -0.664965 -0.163459   \n",
       "116 -0.719510 -0.597466 -0.252264  ...  0.067834  0.326153  0.052696   \n",
       "118  1.799290  0.375392  0.022188  ...  1.480488 -0.085819  0.318279   \n",
       "119 -0.624140  0.321894  0.122681  ... -0.100138  0.243910 -0.202182   \n",
       "121 -0.130726  0.561310  1.079396  ...  0.854765 -0.999095  0.332998   \n",
       "122  0.394713 -0.123819  0.551198  ...  0.175462 -0.990535 -0.364141   \n",
       "123  0.106391 -0.927847  0.694455  ...  1.064587 -1.637465  0.636184   \n",
       "125  0.119249 -0.153667  0.136960  ...  0.184843 -0.186245  0.431865   \n",
       "126  0.119249 -0.153667  0.136960  ...  0.184843 -0.186245  0.431865   \n",
       "130 -0.045810  0.474600  1.863183  ...  0.707857 -0.041422  0.320756   \n",
       "131  0.217108  0.178636 -0.092290  ...  0.371697  0.360514 -0.203928   \n",
       "133  0.557843  0.377129 -0.032968  ...  0.778510  0.116582  0.034619   \n",
       "134 -0.382683  0.735788 -0.295922  ...  0.199534  0.105076  0.465326   \n",
       "142  0.176879  0.360459 -0.045676  ...  0.276039  0.228639 -0.306417   \n",
       "143  0.266228 -0.025153  0.531280  ...  0.688798  0.192031 -0.291331   \n",
       "144  0.266228 -0.025153  0.531280  ...  0.688798  0.192031 -0.291331   \n",
       "145  0.439974 -0.156474 -0.260964  ...  1.135502 -0.675815  0.424360   \n",
       "151  0.220284 -0.201216  0.453648  ...  0.776984 -0.046215  0.434492   \n",
       "155  0.658992  0.251297  1.341728  ...  0.460238 -0.009219 -0.077263   \n",
       "156 -0.090387 -0.695369  0.464568  ...  0.393943 -0.875116  0.477085   \n",
       "157  1.131829 -0.229543  0.829619  ...  0.537563 -0.409710 -0.162906   \n",
       "158  1.131829 -0.229543  0.829619  ...  0.537563 -0.409710 -0.162906   \n",
       "159  0.287024  0.225000 -0.745039  ...  0.647157 -0.306085  0.508288   \n",
       "160 -0.336810 -0.422745  0.291791  ... -0.278843 -0.015365 -0.109212   \n",
       "161  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "162  0.597028  0.181911  1.146924  ...  0.572198 -0.920624  0.287102   \n",
       "163  0.597028  0.181911  1.146924  ...  0.572198 -0.920624  0.287102   \n",
       "164  0.597028  0.181911  1.146924  ...  0.572198 -0.920624  0.287102   \n",
       "165  0.153521  0.701131  0.875699  ...  0.876721 -0.683474  0.306355   \n",
       "166 -0.351034 -0.479981 -0.407538  ... -0.215935 -0.070316  0.354473   \n",
       "169  0.371821 -0.195989 -0.466408  ...  0.088687  0.014089 -0.063682   \n",
       "170  1.486825  0.466984  0.280121  ... -0.340417 -0.205900  0.047145   \n",
       "171 -0.203636 -0.347379  0.118631  ...  0.346252 -0.421215 -0.207567   \n",
       "176  0.146642 -0.184705 -0.036220  ... -0.183958 -0.347498  0.105770   \n",
       "177  0.609697  0.152518 -0.103807  ... -0.343522  0.059616 -0.372169   \n",
       "179  0.083100 -0.119290  0.908444  ...  0.145924  0.096544  0.089728   \n",
       "180  0.352678 -0.001870  0.399941  ...  0.040434  0.677997  0.405590   \n",
       "184  0.101212 -0.933098  0.402112  ... -0.288826 -0.400641  0.352166   \n",
       "185 -0.325597  0.434378 -0.096491  ... -0.709743 -1.517585  1.858804   \n",
       "186 -0.277459  0.089848 -0.374019  ...  1.000594  0.237579  0.306476   \n",
       "187  0.622567  0.419108  0.702335  ...  0.430302 -0.687203  0.084277   \n",
       "188 -0.277459  0.089848 -0.374019  ...  1.000594  0.237579  0.306476   \n",
       "189  0.678962 -0.220332  0.339723  ...  0.204747 -0.927405  0.166153   \n",
       "190  0.791555 -0.020347 -0.320504  ...  0.783239 -0.560625  0.092044   \n",
       "192  0.036606  0.425408  0.374226  ... -0.168921  0.436939 -0.135292   \n",
       "194 -0.122757 -0.159068 -0.256250  ...  0.476192 -0.024006  0.482596   \n",
       "195  0.323244  0.502255  0.210881  ...  0.009614  0.258051  0.376371   \n",
       "197 -0.243234 -0.401150 -0.190340  ... -0.424297  0.134926  0.361010   \n",
       "201  0.431523  0.174753  0.026218  ...  0.241310  0.590665 -0.382804   \n",
       "204  0.755221  0.034336  0.192336  ...  0.540372 -0.008147  0.021521   \n",
       "207  0.136162  0.141380 -0.248368  ...  0.311025 -0.438615  0.284868   \n",
       "208 -0.107280  0.224484 -0.634121  ...  0.203104  0.450005  0.390723   \n",
       "211  1.050904  0.610913 -0.009384  ...  0.309215 -0.268491  0.114303   \n",
       "215  0.202678 -0.340133  0.748079  ...  0.556610 -0.116709  0.087752   \n",
       "224  0.345592  0.109316  0.595509  ...  0.082740 -0.229096 -0.259973   \n",
       "226  0.159498  0.119781  0.294016  ...  0.477889 -0.094701 -0.237239   \n",
       "227  0.419907  0.181920  0.041746  ...  0.085592  0.564838 -0.191597   \n",
       "228  0.419907  0.181920  0.041746  ...  0.085592  0.564838 -0.191597   \n",
       "231  0.249544 -0.409935  0.190494  ...  0.461168  0.625715  0.478554   \n",
       "232  0.013036 -0.306738  0.332596  ...  0.040463 -0.084452  0.257912   \n",
       "233  0.249544 -0.409935  0.190494  ...  0.461168  0.625715  0.478554   \n",
       "234  1.359404 -0.046613 -0.063560  ...  0.374104 -0.463241  0.101950   \n",
       "236  0.415063 -0.442072  0.904541  ...  1.021371  0.441891  0.424356   \n",
       "241 -0.507780 -0.079257 -0.745102  ...  0.853719 -0.580739 -0.354759   \n",
       "243 -0.059748  0.032813  0.261134  ...  0.165821 -0.454197 -0.188265   \n",
       "\n",
       "         1018      1019      1020      1021      1022      1023  \\\n",
       "0   -0.247483  0.007910  0.221942 -0.369337  0.173195  1.074753   \n",
       "1   -0.280019 -0.589877  1.815206 -1.060352  1.315524 -0.427330   \n",
       "3   -0.233592 -0.069840  0.409337 -0.145462  0.840502  0.370108   \n",
       "4    0.367554 -0.847556  0.339698  0.421686  1.098213 -0.169227   \n",
       "8    0.851365  0.451605  0.213644 -0.125964 -0.765226 -0.007932   \n",
       "9    0.794675  0.130590  0.387605  0.500386  0.055710  0.285135   \n",
       "10  -0.480554  0.113316 -0.687692  0.051682  0.121813  0.081608   \n",
       "16  -0.356934 -0.595192 -0.263980 -0.218496 -0.197325  0.190303   \n",
       "17   0.242215 -0.745963  0.686173  0.456795 -0.594572 -0.332893   \n",
       "18   0.042329 -0.874830  0.505904 -0.435902 -0.058884  0.048013   \n",
       "23   0.675150 -0.484231 -0.085576 -0.377170 -0.293063 -0.034907   \n",
       "24   0.455139 -0.259600  0.212947 -0.156704  0.725402  0.219973   \n",
       "25   1.527750 -0.508610  0.901813  0.892514 -0.107142  0.167213   \n",
       "27  -0.237850  0.670713  1.014461 -0.409830  0.442895 -0.383798   \n",
       "28  -0.238221  0.177976 -0.224470  0.346975 -0.132699 -0.541793   \n",
       "29  -0.237850  0.670713  1.014461 -0.409830  0.442895 -0.383798   \n",
       "31   0.893739 -0.041012  0.497487  0.272676 -0.097849 -0.120186   \n",
       "35   0.320408  0.399718 -0.774725 -0.447722  0.926539  0.192881   \n",
       "39  -0.034632  0.070954 -0.510118 -0.082089 -0.185954  0.178776   \n",
       "40   0.267276 -0.284520  0.222652 -0.485243  0.633723  0.211128   \n",
       "43   0.456948  0.391443  0.323856  0.224405 -0.218731 -0.187188   \n",
       "44   0.244604  0.182160  0.150620 -0.232240  1.072437  1.076384   \n",
       "47  -0.524981 -0.225768  0.725936 -0.513620  0.288689  0.744691   \n",
       "48   0.385698  0.227224 -0.110392 -0.252462  0.358088  0.426464   \n",
       "49   1.002413  0.343637  0.654133 -0.067831  0.071678 -0.252707   \n",
       "50   1.081662  0.191716  0.382711  0.630311  0.016077 -0.210032   \n",
       "51   0.925934  0.663127  0.337111  0.209465  0.085920 -0.198426   \n",
       "52   0.222121  0.211399  0.240220  0.275232  0.383000  0.673816   \n",
       "54   0.456917 -0.034821  0.133110 -0.543383  0.226422 -0.259586   \n",
       "57   0.409072 -0.012171 -1.364160  0.879133  0.811989  0.705468   \n",
       "58   0.245964  0.406454 -0.574949  0.132890  0.237464 -0.476773   \n",
       "59   0.742265 -0.042360  0.194151  0.213807 -0.428823 -0.113527   \n",
       "60   0.742265 -0.042360  0.194151  0.213807 -0.428823 -0.113527   \n",
       "61   0.144505  0.102943  0.351012  0.322578  1.222291 -0.365344   \n",
       "62  -0.074685 -0.230631  0.501561 -0.357212  0.791067 -0.193788   \n",
       "63   0.144505  0.102943  0.351012  0.322578  1.222291 -0.365344   \n",
       "64  -0.114680 -0.555400  0.648216 -0.475290  0.762671  0.340396   \n",
       "66   0.166313 -0.472846  0.481247 -0.753033  0.967856  0.027864   \n",
       "68   0.348396 -1.342147  0.705789 -0.213839  0.529447 -0.752717   \n",
       "69  -0.246816  0.500733 -0.373290 -0.383964 -0.191344  0.048969   \n",
       "77   1.076540  0.242981 -0.248841  0.015883 -0.305152 -0.245822   \n",
       "78   0.623339  0.064982  0.346466 -0.032952 -0.077567 -0.541435   \n",
       "79   1.076540  0.242981 -0.248841  0.015883 -0.305152 -0.245822   \n",
       "80   0.408223  0.607924  0.851186 -0.361726  0.013772 -0.445355   \n",
       "81   0.408223  0.607924  0.851186 -0.361726  0.013772 -0.445355   \n",
       "82   0.708668  0.199695  0.381721  0.077058 -0.190891 -0.094872   \n",
       "86   0.047805 -0.406115  0.507450  0.075443  0.459482 -0.650638   \n",
       "87   0.965400  0.534390  0.363053  0.091123 -0.280127 -0.180349   \n",
       "88   0.533406 -0.052567  0.692147  0.277761 -0.384924 -0.015674   \n",
       "89   0.533406 -0.052567  0.692147  0.277761 -0.384924 -0.015674   \n",
       "92   0.166892  0.076563  0.092344 -0.162532 -0.259825  0.313861   \n",
       "93   0.082292 -0.006859  0.338475 -0.224042  0.667025  0.434968   \n",
       "94   1.011176 -0.328115  0.079541  0.250277  0.188860 -0.167318   \n",
       "95   0.476834  0.099480 -0.343889  0.139337  0.620149 -0.219147   \n",
       "96   0.220471  0.571696  0.778376 -0.612661 -0.135189  0.125477   \n",
       "97   0.009863 -0.229079 -0.190608  0.344162  0.196488  0.361369   \n",
       "98  -0.049000  0.277953  0.019683  0.144155 -0.393033 -0.693077   \n",
       "103  0.131574 -0.020974  0.321985  0.138003  0.258033 -0.572022   \n",
       "104  0.119809 -0.012760  0.410180 -0.541516 -0.390635  0.018117   \n",
       "105 -0.576677 -0.401166  0.155216  0.103805 -0.299826  0.009541   \n",
       "106  0.494092  0.200832  0.412989 -0.075398 -0.971734 -0.235871   \n",
       "110  0.889303  0.244363  0.494307  0.051408  0.113009 -0.584796   \n",
       "111  0.651561 -0.451000 -0.001198 -0.354049  0.446097 -0.060521   \n",
       "112 -0.201046 -0.369348 -0.261295 -0.140437  0.725859 -0.121904   \n",
       "115  0.181102 -0.605254  0.479321 -0.447984  0.304987  0.882621   \n",
       "116  0.615497  0.221731 -0.429850 -0.355684  0.075178 -0.469998   \n",
       "118 -0.087766 -0.573257  0.159479 -1.581218  0.099891  0.433922   \n",
       "119  0.168511 -0.047734 -0.121627 -0.891663 -0.205805  0.123384   \n",
       "121  0.339326 -0.575115  0.136851 -0.775325  0.032501 -0.716508   \n",
       "122  0.252785 -0.161892  1.929286 -0.603728 -0.227155 -0.749955   \n",
       "123 -0.148116 -1.106313  0.533086  0.005295  0.591866 -0.697045   \n",
       "125  0.682063  0.321938  0.128866 -0.094373 -0.148446 -0.212645   \n",
       "126  0.682063  0.321938  0.128866 -0.094373 -0.148446 -0.212645   \n",
       "130 -0.001872 -0.391607  0.727173  0.134034 -0.459333 -0.523964   \n",
       "131  0.722203 -0.031995  0.430550  0.576389 -0.448070 -0.374031   \n",
       "133 -0.634318 -0.138169  0.845218  0.186309  0.604633  0.140994   \n",
       "134  0.125013 -0.341154 -0.030454  0.314653  0.441275  0.195672   \n",
       "142 -0.040134  0.514693 -0.217696 -0.026793 -0.300793  0.187881   \n",
       "143 -0.035325 -0.263105  0.409068 -0.434015 -0.056372  0.084591   \n",
       "144 -0.035325 -0.263105  0.409068 -0.434015 -0.056372  0.084591   \n",
       "145  0.135711 -0.866698  0.703063 -0.151839  0.782931  0.043184   \n",
       "151  0.128772 -0.542470  0.534018 -0.430055 -0.399740  0.013967   \n",
       "155  0.009958 -0.205631 -0.082900 -0.413378 -0.428615 -0.216429   \n",
       "156  0.084233  0.367899  0.013431  0.200616  0.266879  0.114448   \n",
       "157  1.319239 -0.206765 -0.423026 -0.140935 -0.095076 -0.595686   \n",
       "158  1.319239 -0.206765 -0.423026 -0.140935 -0.095076 -0.595686   \n",
       "159  0.555787  0.214668  0.329127 -0.196993 -0.120132 -0.208324   \n",
       "160 -0.473473 -0.385310  0.045476 -0.206303 -0.477562 -0.198813   \n",
       "161  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "162  0.198521 -0.015174  1.044421 -0.429029  0.549898  0.272856   \n",
       "163  0.198521 -0.015174  1.044421 -0.429029  0.549898  0.272856   \n",
       "164  0.198521 -0.015174  1.044421 -0.429029  0.549898  0.272856   \n",
       "165  0.433349  0.232320  0.875051  0.071240  0.692731  1.187933   \n",
       "166  0.592619 -0.305956 -0.107259  0.244101 -0.096341 -0.385758   \n",
       "169  0.528301 -0.320240  0.252926 -0.073796  0.128380 -0.581874   \n",
       "170  1.347082 -0.371997  1.545529 -0.039519 -0.645954 -0.779783   \n",
       "171  0.548721  0.274218  0.870021  0.233757 -0.149339  0.211811   \n",
       "176  0.752538  0.454219  0.003193 -0.244662 -0.434922 -0.349551   \n",
       "177  0.808265  0.421602  0.902775  0.097758 -0.920171 -0.426469   \n",
       "179  0.067208  0.076053  0.769554  0.199837 -0.004670 -0.195965   \n",
       "180  0.577239 -0.307932  0.472655  0.184877 -0.477090  0.215669   \n",
       "184  1.102918 -0.187711  0.754685  0.280691  0.005094 -0.257189   \n",
       "185  0.477978  0.106701  1.236328  0.033561  2.241943 -0.300628   \n",
       "186 -0.084550  0.422182  0.369707 -0.324411  0.439643  0.437833   \n",
       "187  0.025163  0.060803  0.078199  0.320811  1.301208  0.578158   \n",
       "188 -0.084550  0.422182  0.369707 -0.324411  0.439643  0.437833   \n",
       "189  0.476868  0.005264  0.059544  0.440806  0.071041  0.288301   \n",
       "190  0.456102 -0.221155 -0.077868 -0.613095  0.159887 -0.251353   \n",
       "192  0.395321  0.416342  0.553579  0.241204 -0.839277 -0.528782   \n",
       "194  0.874799  0.241520  0.299062 -0.021645  0.094385 -0.005442   \n",
       "195  0.577945  0.654155  0.901190  0.433413 -0.665444 -0.187893   \n",
       "197  0.545954 -0.501078  0.119009  0.215204 -0.078726 -0.447719   \n",
       "201  0.486396  0.453194  0.536334  0.439166 -0.742241 -0.258627   \n",
       "204 -0.095862  0.539939  0.277078 -0.774834  0.986001  0.178261   \n",
       "207 -0.177979 -0.033754  0.670074 -0.380049  0.155138  0.152966   \n",
       "208  0.420562  0.418868  0.242043  0.088068 -0.224804 -0.064666   \n",
       "211  0.120640  0.652090  1.117427 -0.621829 -0.113080 -0.019751   \n",
       "215 -0.169133 -0.226868  0.747890  0.171800  0.214938 -0.306645   \n",
       "224 -0.071673  0.024601  0.257417  0.618628 -0.088082 -0.446665   \n",
       "226  0.474289  0.092879  0.080902  0.147815  0.820764 -0.008964   \n",
       "227  0.610405 -0.247212  0.154659  0.126074 -0.277974 -0.354286   \n",
       "228  0.610405 -0.247212  0.154659  0.126074 -0.277974 -0.354286   \n",
       "231  0.280043  0.108729  0.099748 -0.453225 -0.184486  0.346713   \n",
       "232  0.952511 -0.132371  0.176657  0.162941  0.347501  0.215831   \n",
       "233  0.280043  0.108729  0.099748 -0.453225 -0.184486  0.346713   \n",
       "234  0.150720 -0.371247  0.304828 -0.008086  0.564784  0.491858   \n",
       "236 -0.003356 -0.314881 -0.136070 -0.893684  0.031063  0.045445   \n",
       "241  0.797989 -0.795937 -0.743815  0.087255 -0.108192  0.181438   \n",
       "243  0.266283  0.282501  0.700711 -0.265215 -0.178640 -0.254985   \n",
       "\n",
       "                   word  \n",
       "0                stands  \n",
       "1           is designed  \n",
       "3                    is  \n",
       "4               obtains  \n",
       "8               present  \n",
       "9                  find  \n",
       "10            published  \n",
       "16               called  \n",
       "17            to reduce  \n",
       "18            retaining  \n",
       "23                focus  \n",
       "24           represents  \n",
       "25          does adjust  \n",
       "27             have had  \n",
       "28           motivating  \n",
       "29             have had  \n",
       "31                apply  \n",
       "35          is captured  \n",
       "39             describe  \n",
       "40                   is  \n",
       "43               assess  \n",
       "44             performs  \n",
       "47             includes  \n",
       "48             explores  \n",
       "49              compare  \n",
       "50                 find  \n",
       "51          investigate  \n",
       "52          generalizes  \n",
       "54                  pre  \n",
       "57   can be transferred  \n",
       "58              encoded  \n",
       "59            introduce  \n",
       "60            introduce  \n",
       "61          can capture  \n",
       "62              ensures  \n",
       "63          can capture  \n",
       "64                   is  \n",
       "66                   is  \n",
       "68         has achieved  \n",
       "69             describe  \n",
       "77              explore  \n",
       "78              enhance  \n",
       "79              explore  \n",
       "80              To show  \n",
       "81              To show  \n",
       "82          demonstrate  \n",
       "86             achieved  \n",
       "87       to investigate  \n",
       "88              provide  \n",
       "89              provide  \n",
       "92                 show  \n",
       "93                gives  \n",
       "94             generate  \n",
       "95              produce  \n",
       "96             Compared  \n",
       "97                  has  \n",
       "98                 Must  \n",
       "103            applying  \n",
       "104               posed  \n",
       "105         confronting  \n",
       "106             address  \n",
       "110          contribute  \n",
       "111               focus  \n",
       "112             encoded  \n",
       "115               leads  \n",
       "116           Revealing  \n",
       "118   has been released  \n",
       "119             masking  \n",
       "121         was trained  \n",
       "122          to provide  \n",
       "123         is verified  \n",
       "125             examine  \n",
       "126             examine  \n",
       "130                BERT  \n",
       "131             propose  \n",
       "133               gains  \n",
       "134              showed  \n",
       "142           exploring  \n",
       "143               based  \n",
       "144               based  \n",
       "145            achieves  \n",
       "151               built  \n",
       "155                BERT  \n",
       "156             studies  \n",
       "157         to leverage  \n",
       "158         to leverage  \n",
       "159         demonstrate  \n",
       "160           answering  \n",
       "161                 pre  \n",
       "162           allocates  \n",
       "163           allocates  \n",
       "164           allocates  \n",
       "165             prefers  \n",
       "166       Understanding  \n",
       "169             achieve  \n",
       "170            to apply  \n",
       "171             provide  \n",
       "176             explore  \n",
       "177                 add  \n",
       "179               using  \n",
       "180               tuned  \n",
       "184               apply  \n",
       "185     can distinguish  \n",
       "186               shows  \n",
       "187           struggles  \n",
       "188               shows  \n",
       "189                  Is  \n",
       "190                 pre  \n",
       "192              tuning  \n",
       "194         demonstrate  \n",
       "195              tuning  \n",
       "197       Understanding  \n",
       "201             propose  \n",
       "204            appeared  \n",
       "207                 can  \n",
       "208                show  \n",
       "211            Starting  \n",
       "215               using  \n",
       "224               argue  \n",
       "226               drops  \n",
       "227             propose  \n",
       "228             propose  \n",
       "231                show  \n",
       "232                take  \n",
       "233                show  \n",
       "234                  is  \n",
       "236            produced  \n",
       "241        be explained  \n",
       "243           Comparing  \n",
       "\n",
       "[129 rows x 1025 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "x = pickle.load(open(\"temp/BERT_ELMo.pkl\", \"rb\"))\n",
    "output_elmo = csv.groupby(\n",
    "    csv.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: get_phrase_embed_elmo(\n",
    "        elmo,\n",
    "        ' '.join(ast.literal_eval(group.iloc[0]['split_tokens'])),\n",
    "        group.iloc[0]['averb_span0'],\n",
    "        group.iloc[0]['averb_span1'],\n",
    "        group.iloc[0]['averb'], pregenerated=x\n",
    "    )[0]\n",
    ").reset_index(level=1, drop=True)\n",
    "\n",
    "pickle.dump(output_elmo, open(f'temp/BERT_anchorverb_elmo.pkl', \"wb\"))\n",
    "output_elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid is the arithmetic mean position of all points in the figure\n",
    "output_elmo_c = pd.DataFrame(\n",
    "    [np.mean(output_elmo.iloc[:, 0:300])])\n",
    "output_elmo_c['word'] = \"[CENTROID]\"\n",
    "output_elmo_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.859465</td>\n",
       "      <td>-3.911501</td>\n",
       "      <td>0.685157</td>\n",
       "      <td>-5.027377</td>\n",
       "      <td>1.997894</td>\n",
       "      <td>0.340094</td>\n",
       "      <td>3.416013</td>\n",
       "      <td>10.158601</td>\n",
       "      <td>-2.106146</td>\n",
       "      <td>-8.207319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919644</td>\n",
       "      <td>14.184185</td>\n",
       "      <td>2.516548</td>\n",
       "      <td>-0.972502</td>\n",
       "      <td>6.017331</td>\n",
       "      <td>-2.040474</td>\n",
       "      <td>-5.436984</td>\n",
       "      <td>-2.138046</td>\n",
       "      <td>stands</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.728350</td>\n",
       "      <td>1.564678</td>\n",
       "      <td>1.657149</td>\n",
       "      <td>-1.960320</td>\n",
       "      <td>2.319930</td>\n",
       "      <td>-0.572749</td>\n",
       "      <td>-7.824029</td>\n",
       "      <td>9.078582</td>\n",
       "      <td>-3.022813</td>\n",
       "      <td>-9.243394</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.103853</td>\n",
       "      <td>11.668791</td>\n",
       "      <td>-0.148851</td>\n",
       "      <td>6.478394</td>\n",
       "      <td>22.855887</td>\n",
       "      <td>-2.133500</td>\n",
       "      <td>3.163807</td>\n",
       "      <td>-2.175767</td>\n",
       "      <td>is designed</td>\n",
       "      <td>Unlike recent language representation models ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.126984</td>\n",
       "      <td>-4.240447</td>\n",
       "      <td>3.539152</td>\n",
       "      <td>0.015390</td>\n",
       "      <td>0.509718</td>\n",
       "      <td>-2.326005</td>\n",
       "      <td>4.007223</td>\n",
       "      <td>3.939335</td>\n",
       "      <td>-3.068178</td>\n",
       "      <td>-5.500858</td>\n",
       "      <td>...</td>\n",
       "      <td>2.193373</td>\n",
       "      <td>8.026586</td>\n",
       "      <td>6.021397</td>\n",
       "      <td>2.223149</td>\n",
       "      <td>3.006229</td>\n",
       "      <td>2.246306</td>\n",
       "      <td>-0.630127</td>\n",
       "      <td>-0.570627</td>\n",
       "      <td>is</td>\n",
       "      <td>BERT is conceptually simple and empirically po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.498888</td>\n",
       "      <td>15.824597</td>\n",
       "      <td>18.073197</td>\n",
       "      <td>-11.127961</td>\n",
       "      <td>9.263360</td>\n",
       "      <td>1.104467</td>\n",
       "      <td>-25.863521</td>\n",
       "      <td>26.202192</td>\n",
       "      <td>22.393852</td>\n",
       "      <td>-10.348825</td>\n",
       "      <td>...</td>\n",
       "      <td>8.796367</td>\n",
       "      <td>20.908768</td>\n",
       "      <td>4.457648</td>\n",
       "      <td>-34.910531</td>\n",
       "      <td>22.006920</td>\n",
       "      <td>-7.382068</td>\n",
       "      <td>17.523363</td>\n",
       "      <td>11.469820</td>\n",
       "      <td>obtains</td>\n",
       "      <td>It obtains new state - of - the - art results ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-10.205494</td>\n",
       "      <td>-0.681300</td>\n",
       "      <td>-4.486080</td>\n",
       "      <td>-2.441828</td>\n",
       "      <td>0.308743</td>\n",
       "      <td>-12.273580</td>\n",
       "      <td>-1.296075</td>\n",
       "      <td>12.359570</td>\n",
       "      <td>3.727988</td>\n",
       "      <td>4.990989</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.120051</td>\n",
       "      <td>13.107285</td>\n",
       "      <td>4.218679</td>\n",
       "      <td>-1.619001</td>\n",
       "      <td>22.665985</td>\n",
       "      <td>-4.506110</td>\n",
       "      <td>3.464262</td>\n",
       "      <td>-1.422632</td>\n",
       "      <td>present</td>\n",
       "      <td>We present a replication study of BERT pretrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-8.658046</td>\n",
       "      <td>-8.063972</td>\n",
       "      <td>-2.774598</td>\n",
       "      <td>-5.325392</td>\n",
       "      <td>4.084332</td>\n",
       "      <td>-3.396239</td>\n",
       "      <td>-1.856154</td>\n",
       "      <td>7.292558</td>\n",
       "      <td>-0.900050</td>\n",
       "      <td>-1.248224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993280</td>\n",
       "      <td>8.935133</td>\n",
       "      <td>4.858590</td>\n",
       "      <td>-0.444038</td>\n",
       "      <td>11.929569</td>\n",
       "      <td>-4.004733</td>\n",
       "      <td>-3.337786</td>\n",
       "      <td>-0.312336</td>\n",
       "      <td>find</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-9.413263</td>\n",
       "      <td>-7.678121</td>\n",
       "      <td>-2.979881</td>\n",
       "      <td>-5.450416</td>\n",
       "      <td>3.172122</td>\n",
       "      <td>-3.609473</td>\n",
       "      <td>-1.237080</td>\n",
       "      <td>7.007622</td>\n",
       "      <td>-0.549997</td>\n",
       "      <td>-1.044924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638331</td>\n",
       "      <td>9.230479</td>\n",
       "      <td>6.133819</td>\n",
       "      <td>-0.426764</td>\n",
       "      <td>13.004866</td>\n",
       "      <td>-3.556028</td>\n",
       "      <td>-3.403889</td>\n",
       "      <td>-0.108809</td>\n",
       "      <td>published</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-4.707697</td>\n",
       "      <td>-1.030785</td>\n",
       "      <td>-3.864956</td>\n",
       "      <td>-4.009549</td>\n",
       "      <td>-0.104628</td>\n",
       "      <td>-1.701529</td>\n",
       "      <td>7.102601</td>\n",
       "      <td>12.099053</td>\n",
       "      <td>-0.310520</td>\n",
       "      <td>-9.456530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581415</td>\n",
       "      <td>16.120759</td>\n",
       "      <td>0.921296</td>\n",
       "      <td>-3.696960</td>\n",
       "      <td>20.956018</td>\n",
       "      <td>-8.554245</td>\n",
       "      <td>-5.580938</td>\n",
       "      <td>-3.900606</td>\n",
       "      <td>called</td>\n",
       "      <td>In this work , we propose a method to pre-trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-12.279747</td>\n",
       "      <td>13.149659</td>\n",
       "      <td>-9.941004</td>\n",
       "      <td>2.849923</td>\n",
       "      <td>-2.146318</td>\n",
       "      <td>-17.102795</td>\n",
       "      <td>-13.786130</td>\n",
       "      <td>9.165139</td>\n",
       "      <td>12.048118</td>\n",
       "      <td>-3.275484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854651</td>\n",
       "      <td>25.979131</td>\n",
       "      <td>5.340115</td>\n",
       "      <td>-15.328704</td>\n",
       "      <td>32.994659</td>\n",
       "      <td>-14.922404</td>\n",
       "      <td>7.904891</td>\n",
       "      <td>-3.531432</td>\n",
       "      <td>to reduce</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-12.596385</td>\n",
       "      <td>13.648245</td>\n",
       "      <td>-10.323936</td>\n",
       "      <td>3.092344</td>\n",
       "      <td>-2.611085</td>\n",
       "      <td>-17.700482</td>\n",
       "      <td>-14.064017</td>\n",
       "      <td>9.094506</td>\n",
       "      <td>12.562291</td>\n",
       "      <td>-3.053498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299957</td>\n",
       "      <td>26.354039</td>\n",
       "      <td>5.540001</td>\n",
       "      <td>-15.199837</td>\n",
       "      <td>33.174928</td>\n",
       "      <td>-14.029706</td>\n",
       "      <td>7.369203</td>\n",
       "      <td>-3.912339</td>\n",
       "      <td>retaining</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-8.606050</td>\n",
       "      <td>-5.473516</td>\n",
       "      <td>-1.462490</td>\n",
       "      <td>-4.464921</td>\n",
       "      <td>1.025599</td>\n",
       "      <td>-3.288439</td>\n",
       "      <td>3.778120</td>\n",
       "      <td>10.125328</td>\n",
       "      <td>-3.735655</td>\n",
       "      <td>-2.181577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028277</td>\n",
       "      <td>9.084042</td>\n",
       "      <td>4.883412</td>\n",
       "      <td>2.982582</td>\n",
       "      <td>9.364935</td>\n",
       "      <td>-3.128964</td>\n",
       "      <td>-3.910031</td>\n",
       "      <td>-4.351510</td>\n",
       "      <td>focus</td>\n",
       "      <td>We focus on one such model , BERT , and aim to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-6.641381</td>\n",
       "      <td>5.396892</td>\n",
       "      <td>2.211766</td>\n",
       "      <td>-3.138479</td>\n",
       "      <td>-3.885576</td>\n",
       "      <td>-6.758107</td>\n",
       "      <td>1.300727</td>\n",
       "      <td>18.045872</td>\n",
       "      <td>-4.531725</td>\n",
       "      <td>1.488820</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.298295</td>\n",
       "      <td>20.607708</td>\n",
       "      <td>5.596874</td>\n",
       "      <td>0.512143</td>\n",
       "      <td>26.830307</td>\n",
       "      <td>-4.009975</td>\n",
       "      <td>4.508335</td>\n",
       "      <td>-3.662218</td>\n",
       "      <td>represents</td>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-6.506219</td>\n",
       "      <td>-1.258613</td>\n",
       "      <td>4.498027</td>\n",
       "      <td>-2.425730</td>\n",
       "      <td>0.442422</td>\n",
       "      <td>-12.307495</td>\n",
       "      <td>0.257704</td>\n",
       "      <td>5.079469</td>\n",
       "      <td>-10.323381</td>\n",
       "      <td>-3.873248</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.072795</td>\n",
       "      <td>4.919608</td>\n",
       "      <td>7.391818</td>\n",
       "      <td>-5.342462</td>\n",
       "      <td>19.560214</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>8.766688</td>\n",
       "      <td>-0.162169</td>\n",
       "      <td>does adjust</td>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-11.927865</td>\n",
       "      <td>0.235166</td>\n",
       "      <td>-8.899934</td>\n",
       "      <td>3.124892</td>\n",
       "      <td>13.038527</td>\n",
       "      <td>3.925919</td>\n",
       "      <td>-2.430103</td>\n",
       "      <td>14.940655</td>\n",
       "      <td>1.412362</td>\n",
       "      <td>-1.246667</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.042450</td>\n",
       "      <td>12.169796</td>\n",
       "      <td>4.650031</td>\n",
       "      <td>9.121808</td>\n",
       "      <td>19.305409</td>\n",
       "      <td>-3.089077</td>\n",
       "      <td>-0.086525</td>\n",
       "      <td>-10.832083</td>\n",
       "      <td>have had</td>\n",
       "      <td>Large pre - trained neural networks such as BE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-11.344185</td>\n",
       "      <td>1.177784</td>\n",
       "      <td>-9.253212</td>\n",
       "      <td>3.186465</td>\n",
       "      <td>15.001706</td>\n",
       "      <td>3.256440</td>\n",
       "      <td>-2.533082</td>\n",
       "      <td>15.071028</td>\n",
       "      <td>0.502282</td>\n",
       "      <td>-2.569465</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.978115</td>\n",
       "      <td>12.858985</td>\n",
       "      <td>4.650402</td>\n",
       "      <td>9.614546</td>\n",
       "      <td>20.544340</td>\n",
       "      <td>-3.845881</td>\n",
       "      <td>0.489070</td>\n",
       "      <td>-10.674088</td>\n",
       "      <td>motivating</td>\n",
       "      <td>Large pre - trained neural networks such as BE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-11.927865</td>\n",
       "      <td>0.235166</td>\n",
       "      <td>-8.899934</td>\n",
       "      <td>3.124892</td>\n",
       "      <td>13.038527</td>\n",
       "      <td>3.925919</td>\n",
       "      <td>-2.430103</td>\n",
       "      <td>14.940655</td>\n",
       "      <td>1.412362</td>\n",
       "      <td>-1.246667</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.042450</td>\n",
       "      <td>12.169796</td>\n",
       "      <td>4.650031</td>\n",
       "      <td>9.121808</td>\n",
       "      <td>19.305409</td>\n",
       "      <td>-3.089077</td>\n",
       "      <td>-0.086525</td>\n",
       "      <td>-10.832083</td>\n",
       "      <td>have had</td>\n",
       "      <td>Large pre - trained neural networks such as BE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-7.427802</td>\n",
       "      <td>-0.293290</td>\n",
       "      <td>-5.427943</td>\n",
       "      <td>-1.164961</td>\n",
       "      <td>0.556243</td>\n",
       "      <td>-0.384876</td>\n",
       "      <td>1.421317</td>\n",
       "      <td>5.890766</td>\n",
       "      <td>-3.246335</td>\n",
       "      <td>-3.968630</td>\n",
       "      <td>...</td>\n",
       "      <td>2.116340</td>\n",
       "      <td>5.264246</td>\n",
       "      <td>5.134827</td>\n",
       "      <td>6.654226</td>\n",
       "      <td>15.069799</td>\n",
       "      <td>-3.040852</td>\n",
       "      <td>1.633433</td>\n",
       "      <td>-4.254935</td>\n",
       "      <td>apply</td>\n",
       "      <td>Complementary to these works , we propose meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-9.500324</td>\n",
       "      <td>-2.124841</td>\n",
       "      <td>-2.610707</td>\n",
       "      <td>-5.096913</td>\n",
       "      <td>1.415049</td>\n",
       "      <td>-4.422856</td>\n",
       "      <td>2.508182</td>\n",
       "      <td>9.369701</td>\n",
       "      <td>-12.194330</td>\n",
       "      <td>-1.903133</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.664164</td>\n",
       "      <td>7.939153</td>\n",
       "      <td>10.828956</td>\n",
       "      <td>6.803721</td>\n",
       "      <td>14.290925</td>\n",
       "      <td>3.060732</td>\n",
       "      <td>2.591130</td>\n",
       "      <td>-2.460358</td>\n",
       "      <td>is captured</td>\n",
       "      <td>Lastly , we propose an attention - based probi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-9.569691</td>\n",
       "      <td>-4.250579</td>\n",
       "      <td>1.345933</td>\n",
       "      <td>-6.026904</td>\n",
       "      <td>2.329093</td>\n",
       "      <td>-3.669049</td>\n",
       "      <td>1.134723</td>\n",
       "      <td>7.314991</td>\n",
       "      <td>-7.288600</td>\n",
       "      <td>-6.647176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829539</td>\n",
       "      <td>5.884584</td>\n",
       "      <td>3.362310</td>\n",
       "      <td>-3.270161</td>\n",
       "      <td>5.477891</td>\n",
       "      <td>-3.683641</td>\n",
       "      <td>-1.329205</td>\n",
       "      <td>-2.490042</td>\n",
       "      <td>describe</td>\n",
       "      <td>In this paper , we describe a simple re - impl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-6.501872</td>\n",
       "      <td>-3.425783</td>\n",
       "      <td>-0.446150</td>\n",
       "      <td>-9.616967</td>\n",
       "      <td>3.290455</td>\n",
       "      <td>-9.195106</td>\n",
       "      <td>-1.383468</td>\n",
       "      <td>12.782340</td>\n",
       "      <td>-10.730900</td>\n",
       "      <td>-0.758891</td>\n",
       "      <td>...</td>\n",
       "      <td>5.959537</td>\n",
       "      <td>11.796151</td>\n",
       "      <td>8.818732</td>\n",
       "      <td>-4.588744</td>\n",
       "      <td>13.089305</td>\n",
       "      <td>-7.351201</td>\n",
       "      <td>15.065139</td>\n",
       "      <td>-3.308762</td>\n",
       "      <td>is</td>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-1.618557</td>\n",
       "      <td>13.438916</td>\n",
       "      <td>11.335039</td>\n",
       "      <td>-16.345587</td>\n",
       "      <td>9.076712</td>\n",
       "      <td>9.087056</td>\n",
       "      <td>-12.280978</td>\n",
       "      <td>35.118752</td>\n",
       "      <td>11.564481</td>\n",
       "      <td>-9.658287</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.631488</td>\n",
       "      <td>27.569690</td>\n",
       "      <td>4.866843</td>\n",
       "      <td>12.222679</td>\n",
       "      <td>45.151218</td>\n",
       "      <td>-23.893461</td>\n",
       "      <td>33.496021</td>\n",
       "      <td>0.524567</td>\n",
       "      <td>assess</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-3.756713</td>\n",
       "      <td>-3.431447</td>\n",
       "      <td>-0.776451</td>\n",
       "      <td>-1.654955</td>\n",
       "      <td>0.204977</td>\n",
       "      <td>-3.839164</td>\n",
       "      <td>2.494902</td>\n",
       "      <td>1.251581</td>\n",
       "      <td>-3.899827</td>\n",
       "      <td>-5.320324</td>\n",
       "      <td>...</td>\n",
       "      <td>1.052451</td>\n",
       "      <td>8.398514</td>\n",
       "      <td>1.754124</td>\n",
       "      <td>1.607107</td>\n",
       "      <td>5.977670</td>\n",
       "      <td>-0.172656</td>\n",
       "      <td>-2.296726</td>\n",
       "      <td>-0.300141</td>\n",
       "      <td>performs</td>\n",
       "      <td>The BERT model performs remarkably well on all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-1.678657</td>\n",
       "      <td>-1.618636</td>\n",
       "      <td>-1.056970</td>\n",
       "      <td>-6.495803</td>\n",
       "      <td>2.427297</td>\n",
       "      <td>-0.776372</td>\n",
       "      <td>1.463624</td>\n",
       "      <td>13.010853</td>\n",
       "      <td>4.336464</td>\n",
       "      <td>-7.305290</td>\n",
       "      <td>...</td>\n",
       "      <td>6.143580</td>\n",
       "      <td>13.665897</td>\n",
       "      <td>-0.847760</td>\n",
       "      <td>-3.668640</td>\n",
       "      <td>16.588429</td>\n",
       "      <td>-8.393073</td>\n",
       "      <td>0.819877</td>\n",
       "      <td>-2.598549</td>\n",
       "      <td>includes</td>\n",
       "      <td>A new release of BERT ( Devlin , 2018 ) includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-8.932182</td>\n",
       "      <td>1.804437</td>\n",
       "      <td>3.612869</td>\n",
       "      <td>-8.802635</td>\n",
       "      <td>14.310425</td>\n",
       "      <td>5.094616</td>\n",
       "      <td>1.955369</td>\n",
       "      <td>21.171011</td>\n",
       "      <td>1.801773</td>\n",
       "      <td>-3.506800</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.355513</td>\n",
       "      <td>11.139850</td>\n",
       "      <td>-6.044875</td>\n",
       "      <td>-2.024172</td>\n",
       "      <td>24.051028</td>\n",
       "      <td>-14.608838</td>\n",
       "      <td>3.623108</td>\n",
       "      <td>-11.547046</td>\n",
       "      <td>explores</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-2.359463</td>\n",
       "      <td>-4.803245</td>\n",
       "      <td>4.091218</td>\n",
       "      <td>-3.606412</td>\n",
       "      <td>5.472565</td>\n",
       "      <td>2.732024</td>\n",
       "      <td>2.485200</td>\n",
       "      <td>8.155113</td>\n",
       "      <td>-5.211341</td>\n",
       "      <td>-6.697028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154323</td>\n",
       "      <td>5.692595</td>\n",
       "      <td>8.268441</td>\n",
       "      <td>-0.012763</td>\n",
       "      <td>15.461027</td>\n",
       "      <td>-0.890030</td>\n",
       "      <td>1.216166</td>\n",
       "      <td>-1.547176</td>\n",
       "      <td>compare</td>\n",
       "      <td>We compare mBERT with the best - published met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-2.934680</td>\n",
       "      <td>-4.859911</td>\n",
       "      <td>3.720185</td>\n",
       "      <td>-3.518600</td>\n",
       "      <td>6.295261</td>\n",
       "      <td>2.396150</td>\n",
       "      <td>2.614498</td>\n",
       "      <td>8.136577</td>\n",
       "      <td>-5.204879</td>\n",
       "      <td>-6.642999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.627114</td>\n",
       "      <td>6.346710</td>\n",
       "      <td>8.189192</td>\n",
       "      <td>0.139158</td>\n",
       "      <td>15.732449</td>\n",
       "      <td>-1.588172</td>\n",
       "      <td>1.271767</td>\n",
       "      <td>-1.589851</td>\n",
       "      <td>find</td>\n",
       "      <td>We compare mBERT with the best - published met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-7.886303</td>\n",
       "      <td>12.366843</td>\n",
       "      <td>-6.381242</td>\n",
       "      <td>3.939532</td>\n",
       "      <td>2.060496</td>\n",
       "      <td>-2.566081</td>\n",
       "      <td>-0.979879</td>\n",
       "      <td>15.351861</td>\n",
       "      <td>3.247721</td>\n",
       "      <td>5.284750</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.948788</td>\n",
       "      <td>17.051996</td>\n",
       "      <td>11.458891</td>\n",
       "      <td>2.161800</td>\n",
       "      <td>20.489466</td>\n",
       "      <td>-2.117356</td>\n",
       "      <td>7.194585</td>\n",
       "      <td>3.025285</td>\n",
       "      <td>investigate</td>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-7.988239</td>\n",
       "      <td>12.108044</td>\n",
       "      <td>-7.100120</td>\n",
       "      <td>3.660490</td>\n",
       "      <td>1.394905</td>\n",
       "      <td>-2.745636</td>\n",
       "      <td>-1.350675</td>\n",
       "      <td>15.091368</td>\n",
       "      <td>2.990226</td>\n",
       "      <td>5.554305</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.461253</td>\n",
       "      <td>17.360516</td>\n",
       "      <td>12.162705</td>\n",
       "      <td>2.613528</td>\n",
       "      <td>20.586356</td>\n",
       "      <td>-2.183123</td>\n",
       "      <td>6.897504</td>\n",
       "      <td>2.153043</td>\n",
       "      <td>generalizes</td>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-6.784433</td>\n",
       "      <td>1.537288</td>\n",
       "      <td>-1.442179</td>\n",
       "      <td>-4.913676</td>\n",
       "      <td>5.795591</td>\n",
       "      <td>4.032703</td>\n",
       "      <td>1.568026</td>\n",
       "      <td>10.977729</td>\n",
       "      <td>0.083918</td>\n",
       "      <td>-6.283739</td>\n",
       "      <td>...</td>\n",
       "      <td>4.439616</td>\n",
       "      <td>10.098012</td>\n",
       "      <td>0.285778</td>\n",
       "      <td>0.682176</td>\n",
       "      <td>19.926229</td>\n",
       "      <td>-4.489634</td>\n",
       "      <td>-1.172362</td>\n",
       "      <td>-5.697744</td>\n",
       "      <td>pre</td>\n",
       "      <td>Language model pre - training , such as BERT ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-6.172527</td>\n",
       "      <td>2.890001</td>\n",
       "      <td>0.261936</td>\n",
       "      <td>-0.994849</td>\n",
       "      <td>-0.030368</td>\n",
       "      <td>-9.054016</td>\n",
       "      <td>5.608953</td>\n",
       "      <td>5.849642</td>\n",
       "      <td>-1.846521</td>\n",
       "      <td>-1.996766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940866</td>\n",
       "      <td>11.091258</td>\n",
       "      <td>5.919835</td>\n",
       "      <td>7.302510</td>\n",
       "      <td>6.999620</td>\n",
       "      <td>0.214277</td>\n",
       "      <td>-1.354657</td>\n",
       "      <td>-0.861614</td>\n",
       "      <td>can be transferred</td>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-7.609727</td>\n",
       "      <td>5.073976</td>\n",
       "      <td>0.184928</td>\n",
       "      <td>-1.595023</td>\n",
       "      <td>2.914777</td>\n",
       "      <td>-9.353750</td>\n",
       "      <td>4.346285</td>\n",
       "      <td>6.468918</td>\n",
       "      <td>-1.183586</td>\n",
       "      <td>-2.762949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.873760</td>\n",
       "      <td>11.252410</td>\n",
       "      <td>6.082944</td>\n",
       "      <td>6.883885</td>\n",
       "      <td>6.210409</td>\n",
       "      <td>0.960521</td>\n",
       "      <td>-0.780131</td>\n",
       "      <td>0.320627</td>\n",
       "      <td>encoded</td>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-3.063641</td>\n",
       "      <td>-0.135245</td>\n",
       "      <td>-0.574247</td>\n",
       "      <td>-4.809668</td>\n",
       "      <td>-1.139554</td>\n",
       "      <td>-9.228613</td>\n",
       "      <td>1.121727</td>\n",
       "      <td>19.684181</td>\n",
       "      <td>-1.025354</td>\n",
       "      <td>-15.730066</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.016086</td>\n",
       "      <td>12.004227</td>\n",
       "      <td>9.130643</td>\n",
       "      <td>-2.746708</td>\n",
       "      <td>18.251715</td>\n",
       "      <td>-1.467121</td>\n",
       "      <td>4.778063</td>\n",
       "      <td>-1.035389</td>\n",
       "      <td>introduce</td>\n",
       "      <td>Moreover , we introduce a new two - stage lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-3.063641</td>\n",
       "      <td>-0.135245</td>\n",
       "      <td>-0.574247</td>\n",
       "      <td>-4.809668</td>\n",
       "      <td>-1.139554</td>\n",
       "      <td>-9.228613</td>\n",
       "      <td>1.121727</td>\n",
       "      <td>19.684181</td>\n",
       "      <td>-1.025354</td>\n",
       "      <td>-15.730066</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.016086</td>\n",
       "      <td>12.004227</td>\n",
       "      <td>9.130643</td>\n",
       "      <td>-2.746708</td>\n",
       "      <td>18.251715</td>\n",
       "      <td>-1.467121</td>\n",
       "      <td>4.778063</td>\n",
       "      <td>-1.035389</td>\n",
       "      <td>introduce</td>\n",
       "      <td>Moreover , we introduce a new two - stage lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.589055</td>\n",
       "      <td>-1.535521</td>\n",
       "      <td>-1.934043</td>\n",
       "      <td>0.896331</td>\n",
       "      <td>-0.958272</td>\n",
       "      <td>-12.565653</td>\n",
       "      <td>-0.165108</td>\n",
       "      <td>6.061886</td>\n",
       "      <td>-4.352608</td>\n",
       "      <td>-11.093641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079090</td>\n",
       "      <td>8.144147</td>\n",
       "      <td>4.293192</td>\n",
       "      <td>-0.424058</td>\n",
       "      <td>10.494635</td>\n",
       "      <td>-0.999685</td>\n",
       "      <td>3.778612</td>\n",
       "      <td>-4.471280</td>\n",
       "      <td>can capture</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.922914</td>\n",
       "      <td>-1.683226</td>\n",
       "      <td>-2.508974</td>\n",
       "      <td>0.479189</td>\n",
       "      <td>-0.763982</td>\n",
       "      <td>-14.042357</td>\n",
       "      <td>-0.312101</td>\n",
       "      <td>6.293282</td>\n",
       "      <td>-5.783224</td>\n",
       "      <td>-11.846575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390572</td>\n",
       "      <td>9.259814</td>\n",
       "      <td>4.512382</td>\n",
       "      <td>-0.090484</td>\n",
       "      <td>10.344086</td>\n",
       "      <td>-0.319896</td>\n",
       "      <td>4.209835</td>\n",
       "      <td>-4.642836</td>\n",
       "      <td>ensures</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.589055</td>\n",
       "      <td>-1.535521</td>\n",
       "      <td>-1.934043</td>\n",
       "      <td>0.896331</td>\n",
       "      <td>-0.958272</td>\n",
       "      <td>-12.565653</td>\n",
       "      <td>-0.165108</td>\n",
       "      <td>6.061886</td>\n",
       "      <td>-4.352608</td>\n",
       "      <td>-11.093641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079090</td>\n",
       "      <td>8.144147</td>\n",
       "      <td>4.293192</td>\n",
       "      <td>-0.424058</td>\n",
       "      <td>10.494635</td>\n",
       "      <td>-0.999685</td>\n",
       "      <td>3.778612</td>\n",
       "      <td>-4.471280</td>\n",
       "      <td>can capture</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-3.946128</td>\n",
       "      <td>-1.342544</td>\n",
       "      <td>5.968694</td>\n",
       "      <td>2.504707</td>\n",
       "      <td>5.832061</td>\n",
       "      <td>-4.989057</td>\n",
       "      <td>4.295761</td>\n",
       "      <td>11.209648</td>\n",
       "      <td>-0.599167</td>\n",
       "      <td>-1.606351</td>\n",
       "      <td>...</td>\n",
       "      <td>2.480025</td>\n",
       "      <td>2.258384</td>\n",
       "      <td>6.574323</td>\n",
       "      <td>-6.102518</td>\n",
       "      <td>16.715891</td>\n",
       "      <td>-5.617371</td>\n",
       "      <td>2.532120</td>\n",
       "      <td>-0.750196</td>\n",
       "      <td>is</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4.104221</td>\n",
       "      <td>1.215291</td>\n",
       "      <td>8.802032</td>\n",
       "      <td>-5.779140</td>\n",
       "      <td>2.598017</td>\n",
       "      <td>6.221198</td>\n",
       "      <td>-4.497165</td>\n",
       "      <td>15.687531</td>\n",
       "      <td>-3.335432</td>\n",
       "      <td>-7.457595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674070</td>\n",
       "      <td>0.923321</td>\n",
       "      <td>11.339734</td>\n",
       "      <td>-3.008985</td>\n",
       "      <td>19.563908</td>\n",
       "      <td>-9.092735</td>\n",
       "      <td>7.579970</td>\n",
       "      <td>0.770921</td>\n",
       "      <td>is</td>\n",
       "      <td>TinyBERT is also significantly better than sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-7.991769</td>\n",
       "      <td>-5.940037</td>\n",
       "      <td>0.538147</td>\n",
       "      <td>-4.743810</td>\n",
       "      <td>2.453157</td>\n",
       "      <td>-0.326046</td>\n",
       "      <td>6.183125</td>\n",
       "      <td>4.377331</td>\n",
       "      <td>-1.799858</td>\n",
       "      <td>-5.183727</td>\n",
       "      <td>...</td>\n",
       "      <td>4.165982</td>\n",
       "      <td>9.869269</td>\n",
       "      <td>4.395470</td>\n",
       "      <td>-0.565412</td>\n",
       "      <td>11.098223</td>\n",
       "      <td>-4.062683</td>\n",
       "      <td>-3.216342</td>\n",
       "      <td>-2.163312</td>\n",
       "      <td>has achieved</td>\n",
       "      <td>BERT , a pre-trained Transformer model , has a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-9.396273</td>\n",
       "      <td>-3.556319</td>\n",
       "      <td>2.189933</td>\n",
       "      <td>-2.771621</td>\n",
       "      <td>1.647444</td>\n",
       "      <td>-4.849991</td>\n",
       "      <td>5.666373</td>\n",
       "      <td>4.819777</td>\n",
       "      <td>-5.078170</td>\n",
       "      <td>-5.098533</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246159</td>\n",
       "      <td>8.209491</td>\n",
       "      <td>1.363724</td>\n",
       "      <td>-0.520458</td>\n",
       "      <td>5.323790</td>\n",
       "      <td>-2.182671</td>\n",
       "      <td>-0.155358</td>\n",
       "      <td>-1.152712</td>\n",
       "      <td>describe</td>\n",
       "      <td>In this paper , we describe BERTSUM , a simple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-7.816538</td>\n",
       "      <td>-2.824656</td>\n",
       "      <td>-5.255841</td>\n",
       "      <td>-10.021337</td>\n",
       "      <td>0.287502</td>\n",
       "      <td>-5.981696</td>\n",
       "      <td>-4.701579</td>\n",
       "      <td>28.613073</td>\n",
       "      <td>-1.173010</td>\n",
       "      <td>-0.060915</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.722167</td>\n",
       "      <td>10.103693</td>\n",
       "      <td>3.007510</td>\n",
       "      <td>-8.543838</td>\n",
       "      <td>21.297284</td>\n",
       "      <td>-13.237629</td>\n",
       "      <td>5.353695</td>\n",
       "      <td>-4.915610</td>\n",
       "      <td>explore</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-7.480776</td>\n",
       "      <td>-3.281676</td>\n",
       "      <td>-4.274916</td>\n",
       "      <td>-10.083054</td>\n",
       "      <td>-0.342924</td>\n",
       "      <td>-6.227407</td>\n",
       "      <td>-4.361586</td>\n",
       "      <td>28.476350</td>\n",
       "      <td>-1.518940</td>\n",
       "      <td>-0.309611</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.968458</td>\n",
       "      <td>10.287705</td>\n",
       "      <td>3.460711</td>\n",
       "      <td>-8.365839</td>\n",
       "      <td>20.701977</td>\n",
       "      <td>-13.188794</td>\n",
       "      <td>5.126110</td>\n",
       "      <td>-4.619997</td>\n",
       "      <td>enhance</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-7.816538</td>\n",
       "      <td>-2.824656</td>\n",
       "      <td>-5.255841</td>\n",
       "      <td>-10.021337</td>\n",
       "      <td>0.287502</td>\n",
       "      <td>-5.981696</td>\n",
       "      <td>-4.701579</td>\n",
       "      <td>28.613073</td>\n",
       "      <td>-1.173010</td>\n",
       "      <td>-0.060915</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.722167</td>\n",
       "      <td>10.103693</td>\n",
       "      <td>3.007510</td>\n",
       "      <td>-8.543838</td>\n",
       "      <td>21.297284</td>\n",
       "      <td>-13.237629</td>\n",
       "      <td>5.353695</td>\n",
       "      <td>-4.915610</td>\n",
       "      <td>explore</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-7.634216</td>\n",
       "      <td>0.601353</td>\n",
       "      <td>-1.548078</td>\n",
       "      <td>-10.595251</td>\n",
       "      <td>2.242247</td>\n",
       "      <td>1.145000</td>\n",
       "      <td>1.813936</td>\n",
       "      <td>10.770768</td>\n",
       "      <td>-7.022381</td>\n",
       "      <td>-6.883127</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.472415</td>\n",
       "      <td>5.994305</td>\n",
       "      <td>6.712483</td>\n",
       "      <td>-9.512354</td>\n",
       "      <td>10.925199</td>\n",
       "      <td>-2.225764</td>\n",
       "      <td>11.702755</td>\n",
       "      <td>1.462853</td>\n",
       "      <td>To show</td>\n",
       "      <td>To show the generality of the approach , the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-7.634216</td>\n",
       "      <td>0.601353</td>\n",
       "      <td>-1.548078</td>\n",
       "      <td>-10.595251</td>\n",
       "      <td>2.242247</td>\n",
       "      <td>1.145000</td>\n",
       "      <td>1.813936</td>\n",
       "      <td>10.770768</td>\n",
       "      <td>-7.022381</td>\n",
       "      <td>-6.883127</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.472415</td>\n",
       "      <td>5.994305</td>\n",
       "      <td>6.712483</td>\n",
       "      <td>-9.512354</td>\n",
       "      <td>10.925199</td>\n",
       "      <td>-2.225764</td>\n",
       "      <td>11.702755</td>\n",
       "      <td>1.462853</td>\n",
       "      <td>To show</td>\n",
       "      <td>To show the generality of the approach , the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-4.829431</td>\n",
       "      <td>0.110716</td>\n",
       "      <td>-0.207698</td>\n",
       "      <td>-1.397317</td>\n",
       "      <td>0.347557</td>\n",
       "      <td>-4.548082</td>\n",
       "      <td>0.421450</td>\n",
       "      <td>4.435095</td>\n",
       "      <td>-0.363470</td>\n",
       "      <td>-7.103496</td>\n",
       "      <td>...</td>\n",
       "      <td>3.425266</td>\n",
       "      <td>6.508212</td>\n",
       "      <td>2.861088</td>\n",
       "      <td>0.624368</td>\n",
       "      <td>7.868575</td>\n",
       "      <td>0.435193</td>\n",
       "      <td>0.807487</td>\n",
       "      <td>-0.434667</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-7.711835</td>\n",
       "      <td>-5.841216</td>\n",
       "      <td>4.016324</td>\n",
       "      <td>-8.244751</td>\n",
       "      <td>8.520811</td>\n",
       "      <td>-1.991455</td>\n",
       "      <td>-2.504233</td>\n",
       "      <td>12.973072</td>\n",
       "      <td>-4.068198</td>\n",
       "      <td>-4.618082</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.636780</td>\n",
       "      <td>15.525980</td>\n",
       "      <td>3.832716</td>\n",
       "      <td>-1.865674</td>\n",
       "      <td>16.407986</td>\n",
       "      <td>-12.069009</td>\n",
       "      <td>2.901751</td>\n",
       "      <td>-4.904004</td>\n",
       "      <td>achieved</td>\n",
       "      <td>As a state - of - the - art language model pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-6.346138</td>\n",
       "      <td>-1.684852</td>\n",
       "      <td>-6.850927</td>\n",
       "      <td>-7.029703</td>\n",
       "      <td>3.965169</td>\n",
       "      <td>5.866407</td>\n",
       "      <td>3.319361</td>\n",
       "      <td>10.861198</td>\n",
       "      <td>-0.169591</td>\n",
       "      <td>-5.150112</td>\n",
       "      <td>...</td>\n",
       "      <td>3.481384</td>\n",
       "      <td>7.377317</td>\n",
       "      <td>3.398228</td>\n",
       "      <td>7.112475</td>\n",
       "      <td>17.290753</td>\n",
       "      <td>-3.269495</td>\n",
       "      <td>0.513007</td>\n",
       "      <td>-3.327501</td>\n",
       "      <td>to investigate</td>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-7.503979</td>\n",
       "      <td>-1.799755</td>\n",
       "      <td>-6.723387</td>\n",
       "      <td>-7.477550</td>\n",
       "      <td>3.643234</td>\n",
       "      <td>5.168287</td>\n",
       "      <td>2.803441</td>\n",
       "      <td>10.815555</td>\n",
       "      <td>-0.650478</td>\n",
       "      <td>-5.643146</td>\n",
       "      <td>...</td>\n",
       "      <td>2.702531</td>\n",
       "      <td>7.402757</td>\n",
       "      <td>3.830222</td>\n",
       "      <td>7.699432</td>\n",
       "      <td>16.961659</td>\n",
       "      <td>-3.456134</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>-3.492177</td>\n",
       "      <td>provide</td>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-7.503979</td>\n",
       "      <td>-1.799755</td>\n",
       "      <td>-6.723387</td>\n",
       "      <td>-7.477550</td>\n",
       "      <td>3.643234</td>\n",
       "      <td>5.168287</td>\n",
       "      <td>2.803441</td>\n",
       "      <td>10.815555</td>\n",
       "      <td>-0.650478</td>\n",
       "      <td>-5.643146</td>\n",
       "      <td>...</td>\n",
       "      <td>2.702531</td>\n",
       "      <td>7.402757</td>\n",
       "      <td>3.830222</td>\n",
       "      <td>7.699432</td>\n",
       "      <td>16.961659</td>\n",
       "      <td>-3.456134</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>-3.492177</td>\n",
       "      <td>provide</td>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-8.623878</td>\n",
       "      <td>-10.219848</td>\n",
       "      <td>2.513934</td>\n",
       "      <td>-4.316633</td>\n",
       "      <td>2.140724</td>\n",
       "      <td>-4.226720</td>\n",
       "      <td>2.000190</td>\n",
       "      <td>4.998448</td>\n",
       "      <td>-1.839983</td>\n",
       "      <td>4.198319</td>\n",
       "      <td>...</td>\n",
       "      <td>3.258257</td>\n",
       "      <td>10.200982</td>\n",
       "      <td>7.038782</td>\n",
       "      <td>-0.840165</td>\n",
       "      <td>8.233978</td>\n",
       "      <td>-1.729853</td>\n",
       "      <td>1.292382</td>\n",
       "      <td>-0.508297</td>\n",
       "      <td>show</td>\n",
       "      <td>We show that BERT ( Devlin et al . , 2018 ) is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-5.055044</td>\n",
       "      <td>-0.814589</td>\n",
       "      <td>3.617464</td>\n",
       "      <td>-1.412375</td>\n",
       "      <td>-1.232105</td>\n",
       "      <td>-1.011927</td>\n",
       "      <td>4.202555</td>\n",
       "      <td>3.846570</td>\n",
       "      <td>-3.197533</td>\n",
       "      <td>-7.002494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661636</td>\n",
       "      <td>6.655460</td>\n",
       "      <td>0.308756</td>\n",
       "      <td>2.906113</td>\n",
       "      <td>7.613948</td>\n",
       "      <td>1.163743</td>\n",
       "      <td>1.564938</td>\n",
       "      <td>-0.250174</td>\n",
       "      <td>gives</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-5.382120</td>\n",
       "      <td>-2.041025</td>\n",
       "      <td>1.808507</td>\n",
       "      <td>-2.660938</td>\n",
       "      <td>0.061755</td>\n",
       "      <td>0.198111</td>\n",
       "      <td>1.805010</td>\n",
       "      <td>2.358624</td>\n",
       "      <td>-1.656634</td>\n",
       "      <td>-7.536668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062768</td>\n",
       "      <td>7.857211</td>\n",
       "      <td>5.983891</td>\n",
       "      <td>3.074777</td>\n",
       "      <td>6.841979</td>\n",
       "      <td>2.747419</td>\n",
       "      <td>6.072614</td>\n",
       "      <td>-1.005269</td>\n",
       "      <td>generate</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-5.150390</td>\n",
       "      <td>-2.866864</td>\n",
       "      <td>1.845385</td>\n",
       "      <td>-2.870952</td>\n",
       "      <td>0.078189</td>\n",
       "      <td>-0.291655</td>\n",
       "      <td>2.740276</td>\n",
       "      <td>2.113062</td>\n",
       "      <td>-1.831536</td>\n",
       "      <td>-6.815415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.301712</td>\n",
       "      <td>8.539829</td>\n",
       "      <td>6.518232</td>\n",
       "      <td>2.647182</td>\n",
       "      <td>7.265409</td>\n",
       "      <td>2.858359</td>\n",
       "      <td>5.641324</td>\n",
       "      <td>-0.953440</td>\n",
       "      <td>produce</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2.120903</td>\n",
       "      <td>-1.417293</td>\n",
       "      <td>6.370067</td>\n",
       "      <td>-4.381779</td>\n",
       "      <td>3.009745</td>\n",
       "      <td>7.206445</td>\n",
       "      <td>-5.031618</td>\n",
       "      <td>1.143271</td>\n",
       "      <td>-6.043915</td>\n",
       "      <td>-1.928073</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.127294</td>\n",
       "      <td>8.737380</td>\n",
       "      <td>5.931570</td>\n",
       "      <td>0.734139</td>\n",
       "      <td>8.043051</td>\n",
       "      <td>-2.452919</td>\n",
       "      <td>3.014950</td>\n",
       "      <td>0.582443</td>\n",
       "      <td>Compared</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-8.319306</td>\n",
       "      <td>-4.505620</td>\n",
       "      <td>1.934644</td>\n",
       "      <td>-5.517525</td>\n",
       "      <td>8.769313</td>\n",
       "      <td>0.137595</td>\n",
       "      <td>7.887969</td>\n",
       "      <td>8.686139</td>\n",
       "      <td>-4.400817</td>\n",
       "      <td>-3.116157</td>\n",
       "      <td>...</td>\n",
       "      <td>1.880458</td>\n",
       "      <td>12.383476</td>\n",
       "      <td>6.361078</td>\n",
       "      <td>-3.016280</td>\n",
       "      <td>6.530642</td>\n",
       "      <td>-1.157741</td>\n",
       "      <td>-6.469904</td>\n",
       "      <td>-6.338686</td>\n",
       "      <td>has</td>\n",
       "      <td>BERT has a Mouth , and It Must Speak : BERT as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-7.802978</td>\n",
       "      <td>-3.673041</td>\n",
       "      <td>1.968938</td>\n",
       "      <td>-5.303204</td>\n",
       "      <td>9.738762</td>\n",
       "      <td>-0.043392</td>\n",
       "      <td>8.196429</td>\n",
       "      <td>8.994041</td>\n",
       "      <td>-3.286698</td>\n",
       "      <td>-2.790624</td>\n",
       "      <td>...</td>\n",
       "      <td>1.990296</td>\n",
       "      <td>12.992055</td>\n",
       "      <td>6.419940</td>\n",
       "      <td>-3.523313</td>\n",
       "      <td>6.320351</td>\n",
       "      <td>-0.957734</td>\n",
       "      <td>-5.880383</td>\n",
       "      <td>-5.284241</td>\n",
       "      <td>Must</td>\n",
       "      <td>BERT has a Mouth , and It Must Speak : BERT as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-8.061478</td>\n",
       "      <td>-1.318739</td>\n",
       "      <td>-1.891484</td>\n",
       "      <td>-3.150546</td>\n",
       "      <td>1.320672</td>\n",
       "      <td>-4.728422</td>\n",
       "      <td>4.619332</td>\n",
       "      <td>6.997402</td>\n",
       "      <td>-2.604794</td>\n",
       "      <td>-1.663895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269766</td>\n",
       "      <td>6.665802</td>\n",
       "      <td>2.353971</td>\n",
       "      <td>3.102952</td>\n",
       "      <td>3.535963</td>\n",
       "      <td>1.782552</td>\n",
       "      <td>-1.407127</td>\n",
       "      <td>-4.183691</td>\n",
       "      <td>applying</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-5.989899</td>\n",
       "      <td>-0.838542</td>\n",
       "      <td>0.531876</td>\n",
       "      <td>-0.134503</td>\n",
       "      <td>-6.617377</td>\n",
       "      <td>-5.103937</td>\n",
       "      <td>-1.647200</td>\n",
       "      <td>-2.805029</td>\n",
       "      <td>-5.915128</td>\n",
       "      <td>-0.487172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269908</td>\n",
       "      <td>4.654212</td>\n",
       "      <td>6.333701</td>\n",
       "      <td>-7.851084</td>\n",
       "      <td>11.513704</td>\n",
       "      <td>-1.176469</td>\n",
       "      <td>-2.485166</td>\n",
       "      <td>-3.204794</td>\n",
       "      <td>posed</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-5.762352</td>\n",
       "      <td>-0.522015</td>\n",
       "      <td>0.850213</td>\n",
       "      <td>0.339937</td>\n",
       "      <td>-5.478728</td>\n",
       "      <td>-3.984740</td>\n",
       "      <td>-2.295025</td>\n",
       "      <td>-3.138146</td>\n",
       "      <td>-6.889208</td>\n",
       "      <td>0.287089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.185959</td>\n",
       "      <td>4.821892</td>\n",
       "      <td>7.030187</td>\n",
       "      <td>-7.462679</td>\n",
       "      <td>11.768668</td>\n",
       "      <td>-1.821790</td>\n",
       "      <td>-2.575975</td>\n",
       "      <td>-3.196218</td>\n",
       "      <td>confronting</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-5.675892</td>\n",
       "      <td>-0.151854</td>\n",
       "      <td>4.251443</td>\n",
       "      <td>-3.820573</td>\n",
       "      <td>-0.956800</td>\n",
       "      <td>-2.478973</td>\n",
       "      <td>-0.796745</td>\n",
       "      <td>6.567544</td>\n",
       "      <td>-5.535128</td>\n",
       "      <td>-4.877003</td>\n",
       "      <td>...</td>\n",
       "      <td>3.179122</td>\n",
       "      <td>4.320226</td>\n",
       "      <td>1.401577</td>\n",
       "      <td>4.293581</td>\n",
       "      <td>13.785275</td>\n",
       "      <td>1.906998</td>\n",
       "      <td>0.569022</td>\n",
       "      <td>-2.012885</td>\n",
       "      <td>address</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-9.041723</td>\n",
       "      <td>2.769440</td>\n",
       "      <td>-2.497913</td>\n",
       "      <td>-9.308735</td>\n",
       "      <td>10.555745</td>\n",
       "      <td>0.229664</td>\n",
       "      <td>-8.823973</td>\n",
       "      <td>23.295129</td>\n",
       "      <td>-0.230362</td>\n",
       "      <td>-3.902938</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.910978</td>\n",
       "      <td>10.072254</td>\n",
       "      <td>6.071943</td>\n",
       "      <td>1.061889</td>\n",
       "      <td>16.099742</td>\n",
       "      <td>-7.476103</td>\n",
       "      <td>9.171657</td>\n",
       "      <td>-7.191566</td>\n",
       "      <td>contribute</td>\n",
       "      <td>BERT - based architectures currently give stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-8.061819</td>\n",
       "      <td>0.685658</td>\n",
       "      <td>-3.383357</td>\n",
       "      <td>-1.983651</td>\n",
       "      <td>-6.177811</td>\n",
       "      <td>-1.203286</td>\n",
       "      <td>-0.596736</td>\n",
       "      <td>6.318209</td>\n",
       "      <td>-4.157804</td>\n",
       "      <td>-4.916832</td>\n",
       "      <td>...</td>\n",
       "      <td>2.514612</td>\n",
       "      <td>6.072519</td>\n",
       "      <td>4.809406</td>\n",
       "      <td>2.667517</td>\n",
       "      <td>15.917321</td>\n",
       "      <td>-7.884004</td>\n",
       "      <td>6.265963</td>\n",
       "      <td>-4.046960</td>\n",
       "      <td>focus</td>\n",
       "      <td>In the current work , we focus on the interpre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.606710</td>\n",
       "      <td>-0.370538</td>\n",
       "      <td>-2.281677</td>\n",
       "      <td>-4.759653</td>\n",
       "      <td>-7.763980</td>\n",
       "      <td>-12.146952</td>\n",
       "      <td>4.257372</td>\n",
       "      <td>9.704921</td>\n",
       "      <td>-8.081163</td>\n",
       "      <td>-2.366418</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.868927</td>\n",
       "      <td>16.727116</td>\n",
       "      <td>4.133254</td>\n",
       "      <td>-1.935701</td>\n",
       "      <td>19.699014</td>\n",
       "      <td>-4.233355</td>\n",
       "      <td>8.245693</td>\n",
       "      <td>-5.360664</td>\n",
       "      <td>encoded</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-6.109135</td>\n",
       "      <td>-1.793426</td>\n",
       "      <td>-1.622561</td>\n",
       "      <td>-0.673257</td>\n",
       "      <td>-1.684801</td>\n",
       "      <td>-1.945940</td>\n",
       "      <td>2.420317</td>\n",
       "      <td>1.365836</td>\n",
       "      <td>-1.246891</td>\n",
       "      <td>-3.979463</td>\n",
       "      <td>...</td>\n",
       "      <td>3.150308</td>\n",
       "      <td>5.428715</td>\n",
       "      <td>6.445613</td>\n",
       "      <td>3.909162</td>\n",
       "      <td>11.398874</td>\n",
       "      <td>1.029778</td>\n",
       "      <td>-1.159045</td>\n",
       "      <td>1.053127</td>\n",
       "      <td>leads</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-2.703500</td>\n",
       "      <td>-4.501649</td>\n",
       "      <td>2.224539</td>\n",
       "      <td>-2.334781</td>\n",
       "      <td>2.088115</td>\n",
       "      <td>-0.683776</td>\n",
       "      <td>4.953191</td>\n",
       "      <td>0.851680</td>\n",
       "      <td>-5.160437</td>\n",
       "      <td>-6.058223</td>\n",
       "      <td>...</td>\n",
       "      <td>4.320101</td>\n",
       "      <td>6.814077</td>\n",
       "      <td>5.160317</td>\n",
       "      <td>1.445366</td>\n",
       "      <td>5.262548</td>\n",
       "      <td>-0.965985</td>\n",
       "      <td>-0.378083</td>\n",
       "      <td>-2.990473</td>\n",
       "      <td>Revealing</td>\n",
       "      <td>Revealing the Dark Secrets of BERT .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-3.582299</td>\n",
       "      <td>2.748349</td>\n",
       "      <td>-5.915385</td>\n",
       "      <td>-1.163482</td>\n",
       "      <td>4.243947</td>\n",
       "      <td>-5.117082</td>\n",
       "      <td>4.207947</td>\n",
       "      <td>4.947273</td>\n",
       "      <td>3.277426</td>\n",
       "      <td>-3.531257</td>\n",
       "      <td>...</td>\n",
       "      <td>1.962741</td>\n",
       "      <td>4.945492</td>\n",
       "      <td>6.805891</td>\n",
       "      <td>-1.482995</td>\n",
       "      <td>15.414236</td>\n",
       "      <td>-9.943877</td>\n",
       "      <td>-1.285170</td>\n",
       "      <td>-3.372581</td>\n",
       "      <td>has been released</td>\n",
       "      <td>Recently , an upgraded version of BERT has bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-3.487859</td>\n",
       "      <td>2.345011</td>\n",
       "      <td>-5.603522</td>\n",
       "      <td>-2.191770</td>\n",
       "      <td>5.697642</td>\n",
       "      <td>-4.810945</td>\n",
       "      <td>4.925628</td>\n",
       "      <td>7.370703</td>\n",
       "      <td>3.330924</td>\n",
       "      <td>-3.631749</td>\n",
       "      <td>...</td>\n",
       "      <td>1.633012</td>\n",
       "      <td>5.465954</td>\n",
       "      <td>6.549614</td>\n",
       "      <td>-2.008518</td>\n",
       "      <td>15.695342</td>\n",
       "      <td>-10.633431</td>\n",
       "      <td>-0.979475</td>\n",
       "      <td>-3.062044</td>\n",
       "      <td>masking</td>\n",
       "      <td>Recently , an upgraded version of BERT has bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-3.721077</td>\n",
       "      <td>-4.672690</td>\n",
       "      <td>-1.938259</td>\n",
       "      <td>-2.249724</td>\n",
       "      <td>-1.831338</td>\n",
       "      <td>-0.357096</td>\n",
       "      <td>1.884293</td>\n",
       "      <td>-0.538705</td>\n",
       "      <td>-5.946277</td>\n",
       "      <td>-8.576145</td>\n",
       "      <td>...</td>\n",
       "      <td>3.363300</td>\n",
       "      <td>9.688632</td>\n",
       "      <td>4.593385</td>\n",
       "      <td>3.049190</td>\n",
       "      <td>4.287301</td>\n",
       "      <td>-1.033919</td>\n",
       "      <td>-0.946668</td>\n",
       "      <td>-2.517152</td>\n",
       "      <td>was trained</td>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-7.191591</td>\n",
       "      <td>-2.196035</td>\n",
       "      <td>-6.222432</td>\n",
       "      <td>-3.423764</td>\n",
       "      <td>1.670846</td>\n",
       "      <td>1.164818</td>\n",
       "      <td>3.231840</td>\n",
       "      <td>6.135631</td>\n",
       "      <td>-0.087537</td>\n",
       "      <td>-4.085480</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.535226</td>\n",
       "      <td>4.751729</td>\n",
       "      <td>5.049405</td>\n",
       "      <td>-3.205985</td>\n",
       "      <td>15.024564</td>\n",
       "      <td>-7.589117</td>\n",
       "      <td>3.759112</td>\n",
       "      <td>-0.431339</td>\n",
       "      <td>to provide</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-1.094461</td>\n",
       "      <td>2.932837</td>\n",
       "      <td>-8.303083</td>\n",
       "      <td>-13.000537</td>\n",
       "      <td>3.282832</td>\n",
       "      <td>-1.228878</td>\n",
       "      <td>-23.487308</td>\n",
       "      <td>22.031591</td>\n",
       "      <td>12.221411</td>\n",
       "      <td>-1.878562</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.918839</td>\n",
       "      <td>26.535078</td>\n",
       "      <td>5.782956</td>\n",
       "      <td>-24.787497</td>\n",
       "      <td>55.313010</td>\n",
       "      <td>-13.616172</td>\n",
       "      <td>24.487531</td>\n",
       "      <td>-3.553413</td>\n",
       "      <td>is verified</td>\n",
       "      <td>The model is verified on various NLP tasks , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-11.732808</td>\n",
       "      <td>-3.867739</td>\n",
       "      <td>-2.343411</td>\n",
       "      <td>-0.444001</td>\n",
       "      <td>10.891148</td>\n",
       "      <td>-0.374534</td>\n",
       "      <td>-0.869530</td>\n",
       "      <td>9.407952</td>\n",
       "      <td>-7.741794</td>\n",
       "      <td>-1.305381</td>\n",
       "      <td>...</td>\n",
       "      <td>3.729244</td>\n",
       "      <td>11.279761</td>\n",
       "      <td>8.685210</td>\n",
       "      <td>-5.443056</td>\n",
       "      <td>15.083909</td>\n",
       "      <td>-3.052447</td>\n",
       "      <td>4.282526</td>\n",
       "      <td>-2.650203</td>\n",
       "      <td>examine</td>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-11.732808</td>\n",
       "      <td>-3.867739</td>\n",
       "      <td>-2.343411</td>\n",
       "      <td>-0.444001</td>\n",
       "      <td>10.891148</td>\n",
       "      <td>-0.374534</td>\n",
       "      <td>-0.869530</td>\n",
       "      <td>9.407952</td>\n",
       "      <td>-7.741794</td>\n",
       "      <td>-1.305381</td>\n",
       "      <td>...</td>\n",
       "      <td>3.729244</td>\n",
       "      <td>11.279761</td>\n",
       "      <td>8.685210</td>\n",
       "      <td>-5.443056</td>\n",
       "      <td>15.083909</td>\n",
       "      <td>-3.052447</td>\n",
       "      <td>4.282526</td>\n",
       "      <td>-2.650203</td>\n",
       "      <td>examine</td>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-3.161053</td>\n",
       "      <td>3.362335</td>\n",
       "      <td>-3.000309</td>\n",
       "      <td>0.331028</td>\n",
       "      <td>1.504782</td>\n",
       "      <td>0.073486</td>\n",
       "      <td>-6.518556</td>\n",
       "      <td>2.327154</td>\n",
       "      <td>0.881548</td>\n",
       "      <td>1.774245</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.003591</td>\n",
       "      <td>7.556600</td>\n",
       "      <td>0.884678</td>\n",
       "      <td>10.057627</td>\n",
       "      <td>15.990368</td>\n",
       "      <td>6.718393</td>\n",
       "      <td>0.912056</td>\n",
       "      <td>-1.860382</td>\n",
       "      <td>BERT</td>\n",
       "      <td>However , previous work trains BERT by viewing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>-9.404578</td>\n",
       "      <td>0.745903</td>\n",
       "      <td>-2.405769</td>\n",
       "      <td>-6.081145</td>\n",
       "      <td>1.754044</td>\n",
       "      <td>-3.647544</td>\n",
       "      <td>-2.439562</td>\n",
       "      <td>20.667215</td>\n",
       "      <td>-9.988467</td>\n",
       "      <td>-0.157201</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.393663</td>\n",
       "      <td>15.029621</td>\n",
       "      <td>5.853121</td>\n",
       "      <td>-0.331127</td>\n",
       "      <td>25.408912</td>\n",
       "      <td>2.289135</td>\n",
       "      <td>-1.450243</td>\n",
       "      <td>-9.260544</td>\n",
       "      <td>propose</td>\n",
       "      <td>To tackle this issue , we propose a multi - pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-3.359786</td>\n",
       "      <td>4.798028</td>\n",
       "      <td>1.632584</td>\n",
       "      <td>-1.417619</td>\n",
       "      <td>-1.918026</td>\n",
       "      <td>-3.915283</td>\n",
       "      <td>4.069832</td>\n",
       "      <td>5.790207</td>\n",
       "      <td>3.442394</td>\n",
       "      <td>-7.166829</td>\n",
       "      <td>...</td>\n",
       "      <td>5.043173</td>\n",
       "      <td>1.677227</td>\n",
       "      <td>7.590822</td>\n",
       "      <td>-0.339682</td>\n",
       "      <td>6.081468</td>\n",
       "      <td>3.514198</td>\n",
       "      <td>-5.839629</td>\n",
       "      <td>-0.771564</td>\n",
       "      <td>gains</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-9.438931</td>\n",
       "      <td>-0.227395</td>\n",
       "      <td>-2.126956</td>\n",
       "      <td>-6.278421</td>\n",
       "      <td>8.384148</td>\n",
       "      <td>-2.956160</td>\n",
       "      <td>-1.695910</td>\n",
       "      <td>8.883443</td>\n",
       "      <td>-5.046989</td>\n",
       "      <td>-4.277526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555494</td>\n",
       "      <td>8.728924</td>\n",
       "      <td>1.964609</td>\n",
       "      <td>0.293684</td>\n",
       "      <td>15.546367</td>\n",
       "      <td>-7.302793</td>\n",
       "      <td>4.867228</td>\n",
       "      <td>-4.003706</td>\n",
       "      <td>showed</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-5.049932</td>\n",
       "      <td>-0.445237</td>\n",
       "      <td>2.112530</td>\n",
       "      <td>-5.873230</td>\n",
       "      <td>6.147004</td>\n",
       "      <td>0.182442</td>\n",
       "      <td>3.296226</td>\n",
       "      <td>2.311400</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>-1.895493</td>\n",
       "      <td>...</td>\n",
       "      <td>3.249289</td>\n",
       "      <td>7.650513</td>\n",
       "      <td>0.534563</td>\n",
       "      <td>3.295841</td>\n",
       "      <td>2.980280</td>\n",
       "      <td>-1.432931</td>\n",
       "      <td>1.289183</td>\n",
       "      <td>-0.188550</td>\n",
       "      <td>exploring</td>\n",
       "      <td>However , there has not been much effort on ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>-8.081394</td>\n",
       "      <td>-3.578667</td>\n",
       "      <td>-0.990794</td>\n",
       "      <td>-3.138154</td>\n",
       "      <td>-1.156105</td>\n",
       "      <td>-3.344137</td>\n",
       "      <td>5.305189</td>\n",
       "      <td>7.424689</td>\n",
       "      <td>-2.170588</td>\n",
       "      <td>-6.392693</td>\n",
       "      <td>...</td>\n",
       "      <td>3.692980</td>\n",
       "      <td>9.747914</td>\n",
       "      <td>1.260895</td>\n",
       "      <td>-0.582983</td>\n",
       "      <td>11.546266</td>\n",
       "      <td>-4.486740</td>\n",
       "      <td>-3.020504</td>\n",
       "      <td>-1.423947</td>\n",
       "      <td>based</td>\n",
       "      <td>In this work , we propose a joint intent class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>-8.081394</td>\n",
       "      <td>-3.578667</td>\n",
       "      <td>-0.990794</td>\n",
       "      <td>-3.138154</td>\n",
       "      <td>-1.156105</td>\n",
       "      <td>-3.344137</td>\n",
       "      <td>5.305189</td>\n",
       "      <td>7.424689</td>\n",
       "      <td>-2.170588</td>\n",
       "      <td>-6.392693</td>\n",
       "      <td>...</td>\n",
       "      <td>3.692980</td>\n",
       "      <td>9.747914</td>\n",
       "      <td>1.260895</td>\n",
       "      <td>-0.582983</td>\n",
       "      <td>11.546266</td>\n",
       "      <td>-4.486740</td>\n",
       "      <td>-3.020504</td>\n",
       "      <td>-1.423947</td>\n",
       "      <td>based</td>\n",
       "      <td>In this work , we propose a joint intent class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-3.554850</td>\n",
       "      <td>10.474434</td>\n",
       "      <td>-5.939344</td>\n",
       "      <td>0.652957</td>\n",
       "      <td>6.232687</td>\n",
       "      <td>-5.346742</td>\n",
       "      <td>-6.412344</td>\n",
       "      <td>12.249691</td>\n",
       "      <td>5.603100</td>\n",
       "      <td>-10.897520</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.929304</td>\n",
       "      <td>20.165005</td>\n",
       "      <td>7.283374</td>\n",
       "      <td>-5.537001</td>\n",
       "      <td>28.937577</td>\n",
       "      <td>-11.940870</td>\n",
       "      <td>12.580129</td>\n",
       "      <td>-1.014539</td>\n",
       "      <td>achieves</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-9.286966</td>\n",
       "      <td>1.227714</td>\n",
       "      <td>-2.363088</td>\n",
       "      <td>-2.687378</td>\n",
       "      <td>2.235506</td>\n",
       "      <td>-10.526797</td>\n",
       "      <td>3.041335</td>\n",
       "      <td>10.281029</td>\n",
       "      <td>0.033257</td>\n",
       "      <td>-1.714837</td>\n",
       "      <td>...</td>\n",
       "      <td>1.976641</td>\n",
       "      <td>13.778488</td>\n",
       "      <td>0.640334</td>\n",
       "      <td>-1.357105</td>\n",
       "      <td>7.773623</td>\n",
       "      <td>-6.394884</td>\n",
       "      <td>-2.456436</td>\n",
       "      <td>-1.478438</td>\n",
       "      <td>built</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-5.298441</td>\n",
       "      <td>-1.538954</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>-1.732020</td>\n",
       "      <td>3.412107</td>\n",
       "      <td>-1.730247</td>\n",
       "      <td>5.370044</td>\n",
       "      <td>6.895778</td>\n",
       "      <td>-3.149974</td>\n",
       "      <td>-5.255588</td>\n",
       "      <td>...</td>\n",
       "      <td>1.425021</td>\n",
       "      <td>5.304224</td>\n",
       "      <td>3.821391</td>\n",
       "      <td>0.710451</td>\n",
       "      <td>2.702670</td>\n",
       "      <td>2.105006</td>\n",
       "      <td>-4.548288</td>\n",
       "      <td>-2.838934</td>\n",
       "      <td>BERT</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-5.005430</td>\n",
       "      <td>-5.019817</td>\n",
       "      <td>2.123672</td>\n",
       "      <td>-0.986476</td>\n",
       "      <td>0.034287</td>\n",
       "      <td>0.238103</td>\n",
       "      <td>0.422532</td>\n",
       "      <td>3.076760</td>\n",
       "      <td>-3.051523</td>\n",
       "      <td>-4.895040</td>\n",
       "      <td>...</td>\n",
       "      <td>2.335704</td>\n",
       "      <td>6.637191</td>\n",
       "      <td>1.971263</td>\n",
       "      <td>4.377250</td>\n",
       "      <td>10.447351</td>\n",
       "      <td>-5.581640</td>\n",
       "      <td>-0.765222</td>\n",
       "      <td>-2.706537</td>\n",
       "      <td>studies</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-6.927062</td>\n",
       "      <td>-1.716291</td>\n",
       "      <td>-0.037786</td>\n",
       "      <td>0.869291</td>\n",
       "      <td>-6.560842</td>\n",
       "      <td>-1.865368</td>\n",
       "      <td>6.229330</td>\n",
       "      <td>18.119908</td>\n",
       "      <td>-1.682998</td>\n",
       "      <td>-3.916790</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.895018</td>\n",
       "      <td>5.583397</td>\n",
       "      <td>-0.257259</td>\n",
       "      <td>-4.293489</td>\n",
       "      <td>9.379536</td>\n",
       "      <td>-2.453161</td>\n",
       "      <td>-6.630558</td>\n",
       "      <td>-4.805034</td>\n",
       "      <td>to leverage</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-6.927062</td>\n",
       "      <td>-1.716291</td>\n",
       "      <td>-0.037786</td>\n",
       "      <td>0.869291</td>\n",
       "      <td>-6.560842</td>\n",
       "      <td>-1.865368</td>\n",
       "      <td>6.229330</td>\n",
       "      <td>18.119908</td>\n",
       "      <td>-1.682998</td>\n",
       "      <td>-3.916790</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.895018</td>\n",
       "      <td>5.583397</td>\n",
       "      <td>-0.257259</td>\n",
       "      <td>-4.293489</td>\n",
       "      <td>9.379536</td>\n",
       "      <td>-2.453161</td>\n",
       "      <td>-6.630558</td>\n",
       "      <td>-4.805034</td>\n",
       "      <td>to leverage</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-11.722792</td>\n",
       "      <td>6.048747</td>\n",
       "      <td>-10.652430</td>\n",
       "      <td>-2.564454</td>\n",
       "      <td>9.279273</td>\n",
       "      <td>-8.350179</td>\n",
       "      <td>-5.691705</td>\n",
       "      <td>18.301708</td>\n",
       "      <td>-2.763800</td>\n",
       "      <td>2.316636</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.437716</td>\n",
       "      <td>17.495519</td>\n",
       "      <td>4.175534</td>\n",
       "      <td>-3.710103</td>\n",
       "      <td>15.720067</td>\n",
       "      <td>-10.224118</td>\n",
       "      <td>6.598050</td>\n",
       "      <td>-0.411762</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-11.452042</td>\n",
       "      <td>6.414863</td>\n",
       "      <td>-9.884483</td>\n",
       "      <td>-2.734204</td>\n",
       "      <td>9.550289</td>\n",
       "      <td>-8.105764</td>\n",
       "      <td>-6.140095</td>\n",
       "      <td>18.925542</td>\n",
       "      <td>-2.116054</td>\n",
       "      <td>1.279806</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.728436</td>\n",
       "      <td>18.113019</td>\n",
       "      <td>5.204794</td>\n",
       "      <td>-3.110125</td>\n",
       "      <td>16.003719</td>\n",
       "      <td>-10.214809</td>\n",
       "      <td>6.955479</td>\n",
       "      <td>-0.421273</td>\n",
       "      <td>answering</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>-3.936072</td>\n",
       "      <td>2.589637</td>\n",
       "      <td>-7.074796</td>\n",
       "      <td>-0.905881</td>\n",
       "      <td>3.649313</td>\n",
       "      <td>-6.588823</td>\n",
       "      <td>-8.123074</td>\n",
       "      <td>7.457427</td>\n",
       "      <td>0.073691</td>\n",
       "      <td>-9.273520</td>\n",
       "      <td>...</td>\n",
       "      <td>1.761048</td>\n",
       "      <td>8.690749</td>\n",
       "      <td>3.597866</td>\n",
       "      <td>3.960309</td>\n",
       "      <td>8.967296</td>\n",
       "      <td>-2.708473</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>-4.454368</td>\n",
       "      <td>pre</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-8.143539</td>\n",
       "      <td>6.442126</td>\n",
       "      <td>-8.167684</td>\n",
       "      <td>-4.431106</td>\n",
       "      <td>6.786310</td>\n",
       "      <td>-12.153201</td>\n",
       "      <td>-7.219865</td>\n",
       "      <td>9.961044</td>\n",
       "      <td>1.688841</td>\n",
       "      <td>1.841268</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.158852</td>\n",
       "      <td>15.537325</td>\n",
       "      <td>6.344458</td>\n",
       "      <td>12.138985</td>\n",
       "      <td>28.358085</td>\n",
       "      <td>-3.759067</td>\n",
       "      <td>16.144523</td>\n",
       "      <td>-2.167636</td>\n",
       "      <td>allocates</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-8.143539</td>\n",
       "      <td>6.442126</td>\n",
       "      <td>-8.167684</td>\n",
       "      <td>-4.431106</td>\n",
       "      <td>6.786310</td>\n",
       "      <td>-12.153201</td>\n",
       "      <td>-7.219865</td>\n",
       "      <td>9.961044</td>\n",
       "      <td>1.688841</td>\n",
       "      <td>1.841268</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.158852</td>\n",
       "      <td>15.537325</td>\n",
       "      <td>6.344458</td>\n",
       "      <td>12.138985</td>\n",
       "      <td>28.358085</td>\n",
       "      <td>-3.759067</td>\n",
       "      <td>16.144523</td>\n",
       "      <td>-2.167636</td>\n",
       "      <td>allocates</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-8.143539</td>\n",
       "      <td>6.442126</td>\n",
       "      <td>-8.167684</td>\n",
       "      <td>-4.431106</td>\n",
       "      <td>6.786310</td>\n",
       "      <td>-12.153201</td>\n",
       "      <td>-7.219865</td>\n",
       "      <td>9.961044</td>\n",
       "      <td>1.688841</td>\n",
       "      <td>1.841268</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.158852</td>\n",
       "      <td>15.537325</td>\n",
       "      <td>6.344458</td>\n",
       "      <td>12.138985</td>\n",
       "      <td>28.358085</td>\n",
       "      <td>-3.759067</td>\n",
       "      <td>16.144523</td>\n",
       "      <td>-2.167636</td>\n",
       "      <td>allocates</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-8.474184</td>\n",
       "      <td>6.223902</td>\n",
       "      <td>-8.268846</td>\n",
       "      <td>-4.474464</td>\n",
       "      <td>7.420579</td>\n",
       "      <td>-12.650051</td>\n",
       "      <td>-7.051151</td>\n",
       "      <td>10.404551</td>\n",
       "      <td>1.169622</td>\n",
       "      <td>2.112493</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.396002</td>\n",
       "      <td>15.518073</td>\n",
       "      <td>6.109630</td>\n",
       "      <td>11.891492</td>\n",
       "      <td>28.527454</td>\n",
       "      <td>-4.259336</td>\n",
       "      <td>16.001690</td>\n",
       "      <td>-3.082713</td>\n",
       "      <td>prefers</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-6.412910</td>\n",
       "      <td>-2.955095</td>\n",
       "      <td>1.993462</td>\n",
       "      <td>-1.719658</td>\n",
       "      <td>1.460402</td>\n",
       "      <td>0.122076</td>\n",
       "      <td>4.138150</td>\n",
       "      <td>1.347494</td>\n",
       "      <td>-4.801625</td>\n",
       "      <td>-6.144500</td>\n",
       "      <td>...</td>\n",
       "      <td>3.324086</td>\n",
       "      <td>6.055160</td>\n",
       "      <td>4.807548</td>\n",
       "      <td>3.160672</td>\n",
       "      <td>7.045570</td>\n",
       "      <td>0.044331</td>\n",
       "      <td>-2.687856</td>\n",
       "      <td>-2.395735</td>\n",
       "      <td>Understanding</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-7.665230</td>\n",
       "      <td>4.288996</td>\n",
       "      <td>-5.092207</td>\n",
       "      <td>-3.165983</td>\n",
       "      <td>5.352067</td>\n",
       "      <td>-8.033858</td>\n",
       "      <td>1.387058</td>\n",
       "      <td>6.937962</td>\n",
       "      <td>-4.919617</td>\n",
       "      <td>-7.779969</td>\n",
       "      <td>...</td>\n",
       "      <td>1.463127</td>\n",
       "      <td>17.452409</td>\n",
       "      <td>5.087848</td>\n",
       "      <td>4.862034</td>\n",
       "      <td>22.521439</td>\n",
       "      <td>-7.068395</td>\n",
       "      <td>1.661450</td>\n",
       "      <td>-2.526398</td>\n",
       "      <td>achieve</td>\n",
       "      <td>In this paper , extensive experiments on datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-7.024046</td>\n",
       "      <td>-6.249775</td>\n",
       "      <td>1.808357</td>\n",
       "      <td>-1.721041</td>\n",
       "      <td>2.785020</td>\n",
       "      <td>-0.564414</td>\n",
       "      <td>2.642710</td>\n",
       "      <td>1.975590</td>\n",
       "      <td>-4.867954</td>\n",
       "      <td>-1.388843</td>\n",
       "      <td>...</td>\n",
       "      <td>2.812707</td>\n",
       "      <td>8.342521</td>\n",
       "      <td>2.001541</td>\n",
       "      <td>4.028254</td>\n",
       "      <td>9.718325</td>\n",
       "      <td>1.901125</td>\n",
       "      <td>-0.542611</td>\n",
       "      <td>-2.394135</td>\n",
       "      <td>to apply</td>\n",
       "      <td>To our knowledge , we are the first to success...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-5.406704</td>\n",
       "      <td>-4.491922</td>\n",
       "      <td>0.864820</td>\n",
       "      <td>-1.735400</td>\n",
       "      <td>-0.822977</td>\n",
       "      <td>-0.687711</td>\n",
       "      <td>0.418488</td>\n",
       "      <td>1.613718</td>\n",
       "      <td>-5.947994</td>\n",
       "      <td>-7.612312</td>\n",
       "      <td>...</td>\n",
       "      <td>2.341839</td>\n",
       "      <td>7.727042</td>\n",
       "      <td>4.238705</td>\n",
       "      <td>5.245733</td>\n",
       "      <td>8.754437</td>\n",
       "      <td>2.671897</td>\n",
       "      <td>-1.659958</td>\n",
       "      <td>-1.247631</td>\n",
       "      <td>provide</td>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-13.333930</td>\n",
       "      <td>3.358187</td>\n",
       "      <td>-7.544338</td>\n",
       "      <td>-8.011771</td>\n",
       "      <td>0.896469</td>\n",
       "      <td>-3.283794</td>\n",
       "      <td>3.305403</td>\n",
       "      <td>22.491927</td>\n",
       "      <td>4.817725</td>\n",
       "      <td>-1.432122</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.201476</td>\n",
       "      <td>11.691244</td>\n",
       "      <td>-0.491714</td>\n",
       "      <td>-1.926360</td>\n",
       "      <td>30.130173</td>\n",
       "      <td>-9.635144</td>\n",
       "      <td>-8.225464</td>\n",
       "      <td>-5.053662</td>\n",
       "      <td>explore</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-14.278378</td>\n",
       "      <td>2.525913</td>\n",
       "      <td>-6.886722</td>\n",
       "      <td>-8.077023</td>\n",
       "      <td>0.290444</td>\n",
       "      <td>-3.151138</td>\n",
       "      <td>4.282961</td>\n",
       "      <td>22.028872</td>\n",
       "      <td>4.480502</td>\n",
       "      <td>-1.364535</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.608590</td>\n",
       "      <td>12.169183</td>\n",
       "      <td>-0.547440</td>\n",
       "      <td>-1.893743</td>\n",
       "      <td>29.230591</td>\n",
       "      <td>-9.977564</td>\n",
       "      <td>-7.740215</td>\n",
       "      <td>-4.976744</td>\n",
       "      <td>add</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-6.952142</td>\n",
       "      <td>6.253065</td>\n",
       "      <td>-4.041596</td>\n",
       "      <td>1.287190</td>\n",
       "      <td>1.568751</td>\n",
       "      <td>-9.040809</td>\n",
       "      <td>-2.007033</td>\n",
       "      <td>14.267893</td>\n",
       "      <td>1.224772</td>\n",
       "      <td>2.121941</td>\n",
       "      <td>...</td>\n",
       "      <td>1.176705</td>\n",
       "      <td>9.918837</td>\n",
       "      <td>4.641469</td>\n",
       "      <td>-1.044462</td>\n",
       "      <td>28.749455</td>\n",
       "      <td>-9.262540</td>\n",
       "      <td>-3.309269</td>\n",
       "      <td>-4.934646</td>\n",
       "      <td>using</td>\n",
       "      <td>By using PALs in parallel with BERT layers , w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>-6.749755</td>\n",
       "      <td>6.108222</td>\n",
       "      <td>-4.378015</td>\n",
       "      <td>1.513713</td>\n",
       "      <td>1.923074</td>\n",
       "      <td>-9.126699</td>\n",
       "      <td>-1.742490</td>\n",
       "      <td>13.998315</td>\n",
       "      <td>1.107352</td>\n",
       "      <td>2.630445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595252</td>\n",
       "      <td>9.602975</td>\n",
       "      <td>4.131438</td>\n",
       "      <td>-0.660476</td>\n",
       "      <td>29.046354</td>\n",
       "      <td>-9.247580</td>\n",
       "      <td>-2.836848</td>\n",
       "      <td>-5.346280</td>\n",
       "      <td>tuned</td>\n",
       "      <td>By using PALs in parallel with BERT layers , w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>-9.228063</td>\n",
       "      <td>10.961564</td>\n",
       "      <td>-11.893626</td>\n",
       "      <td>1.514488</td>\n",
       "      <td>16.766868</td>\n",
       "      <td>-5.745907</td>\n",
       "      <td>-12.086385</td>\n",
       "      <td>17.401294</td>\n",
       "      <td>-5.086328</td>\n",
       "      <td>3.783951</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.578977</td>\n",
       "      <td>21.777007</td>\n",
       "      <td>10.680769</td>\n",
       "      <td>10.953658</td>\n",
       "      <td>25.085792</td>\n",
       "      <td>0.404068</td>\n",
       "      <td>47.262965</td>\n",
       "      <td>3.839316</td>\n",
       "      <td>apply</td>\n",
       "      <td>As a case study , we apply these diagnostics t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-7.141728</td>\n",
       "      <td>9.532755</td>\n",
       "      <td>-10.964141</td>\n",
       "      <td>1.365763</td>\n",
       "      <td>16.122178</td>\n",
       "      <td>-4.192946</td>\n",
       "      <td>-12.523782</td>\n",
       "      <td>17.828103</td>\n",
       "      <td>-6.453804</td>\n",
       "      <td>4.282553</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.462033</td>\n",
       "      <td>20.270369</td>\n",
       "      <td>11.305710</td>\n",
       "      <td>10.659246</td>\n",
       "      <td>24.604149</td>\n",
       "      <td>0.651197</td>\n",
       "      <td>45.026116</td>\n",
       "      <td>3.882755</td>\n",
       "      <td>can distinguish</td>\n",
       "      <td>As a case study , we apply these diagnostics t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-8.477522</td>\n",
       "      <td>11.553133</td>\n",
       "      <td>-11.765724</td>\n",
       "      <td>1.078290</td>\n",
       "      <td>17.123946</td>\n",
       "      <td>-5.956054</td>\n",
       "      <td>-11.635807</td>\n",
       "      <td>17.779965</td>\n",
       "      <td>-6.109274</td>\n",
       "      <td>4.560082</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.217197</td>\n",
       "      <td>21.822698</td>\n",
       "      <td>11.868238</td>\n",
       "      <td>10.343766</td>\n",
       "      <td>25.470770</td>\n",
       "      <td>1.009170</td>\n",
       "      <td>46.828415</td>\n",
       "      <td>3.144294</td>\n",
       "      <td>shows</td>\n",
       "      <td>As a case study , we apply these diagnostics t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>-8.907830</td>\n",
       "      <td>11.684895</td>\n",
       "      <td>-12.117041</td>\n",
       "      <td>0.414779</td>\n",
       "      <td>17.194275</td>\n",
       "      <td>-5.579261</td>\n",
       "      <td>-12.036323</td>\n",
       "      <td>16.879939</td>\n",
       "      <td>-6.438534</td>\n",
       "      <td>3.483728</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.292415</td>\n",
       "      <td>22.044896</td>\n",
       "      <td>11.758524</td>\n",
       "      <td>10.705144</td>\n",
       "      <td>25.762278</td>\n",
       "      <td>0.363948</td>\n",
       "      <td>45.966851</td>\n",
       "      <td>3.003968</td>\n",
       "      <td>struggles</td>\n",
       "      <td>As a case study , we apply these diagnostics t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>-8.477522</td>\n",
       "      <td>11.553133</td>\n",
       "      <td>-11.765724</td>\n",
       "      <td>1.078290</td>\n",
       "      <td>17.123946</td>\n",
       "      <td>-5.956054</td>\n",
       "      <td>-11.635807</td>\n",
       "      <td>17.779965</td>\n",
       "      <td>-6.109274</td>\n",
       "      <td>4.560082</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.217197</td>\n",
       "      <td>21.822698</td>\n",
       "      <td>11.868238</td>\n",
       "      <td>10.343766</td>\n",
       "      <td>25.470770</td>\n",
       "      <td>1.009170</td>\n",
       "      <td>46.828415</td>\n",
       "      <td>3.144294</td>\n",
       "      <td>shows</td>\n",
       "      <td>As a case study , we apply these diagnostics t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>-9.220070</td>\n",
       "      <td>-4.628106</td>\n",
       "      <td>5.154650</td>\n",
       "      <td>-2.980861</td>\n",
       "      <td>6.076616</td>\n",
       "      <td>2.738985</td>\n",
       "      <td>3.187585</td>\n",
       "      <td>4.246963</td>\n",
       "      <td>-3.613858</td>\n",
       "      <td>-2.617819</td>\n",
       "      <td>...</td>\n",
       "      <td>1.386258</td>\n",
       "      <td>11.185780</td>\n",
       "      <td>6.904366</td>\n",
       "      <td>3.905912</td>\n",
       "      <td>8.260362</td>\n",
       "      <td>2.840301</td>\n",
       "      <td>-4.724039</td>\n",
       "      <td>-7.379824</td>\n",
       "      <td>Is</td>\n",
       "      <td>What BERT Is Not : Lessons from a New Suite of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>-8.673885</td>\n",
       "      <td>-3.104165</td>\n",
       "      <td>2.780159</td>\n",
       "      <td>-4.976963</td>\n",
       "      <td>5.146888</td>\n",
       "      <td>2.370353</td>\n",
       "      <td>1.300211</td>\n",
       "      <td>11.232066</td>\n",
       "      <td>-1.622563</td>\n",
       "      <td>-7.182175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413096</td>\n",
       "      <td>5.742879</td>\n",
       "      <td>2.838234</td>\n",
       "      <td>-2.468071</td>\n",
       "      <td>13.544263</td>\n",
       "      <td>-5.387967</td>\n",
       "      <td>-2.001249</td>\n",
       "      <td>-5.232017</td>\n",
       "      <td>pre</td>\n",
       "      <td>Language model pre - training , such as BERT ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-4.385092</td>\n",
       "      <td>-2.727906</td>\n",
       "      <td>1.173461</td>\n",
       "      <td>-4.494600</td>\n",
       "      <td>-2.187316</td>\n",
       "      <td>0.162762</td>\n",
       "      <td>0.891106</td>\n",
       "      <td>3.925889</td>\n",
       "      <td>-4.492952</td>\n",
       "      <td>-6.440457</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.135672</td>\n",
       "      <td>6.038065</td>\n",
       "      <td>5.769958</td>\n",
       "      <td>3.183110</td>\n",
       "      <td>17.721247</td>\n",
       "      <td>-2.200913</td>\n",
       "      <td>3.486176</td>\n",
       "      <td>-5.982644</td>\n",
       "      <td>tuning</td>\n",
       "      <td>In this paper , we propose to visualize loss l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>-8.927935</td>\n",
       "      <td>1.213526</td>\n",
       "      <td>-3.045547</td>\n",
       "      <td>-3.823425</td>\n",
       "      <td>0.095817</td>\n",
       "      <td>-6.868662</td>\n",
       "      <td>0.442578</td>\n",
       "      <td>10.440407</td>\n",
       "      <td>-0.084636</td>\n",
       "      <td>-4.435107</td>\n",
       "      <td>...</td>\n",
       "      <td>1.212849</td>\n",
       "      <td>2.450209</td>\n",
       "      <td>4.230592</td>\n",
       "      <td>2.992151</td>\n",
       "      <td>13.218968</td>\n",
       "      <td>-0.544036</td>\n",
       "      <td>3.430881</td>\n",
       "      <td>-0.937430</td>\n",
       "      <td>demonstrate</td>\n",
       "      <td>We also demonstrate that the fine - tuning pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-6.586986</td>\n",
       "      <td>10.877345</td>\n",
       "      <td>-7.687485</td>\n",
       "      <td>-4.235650</td>\n",
       "      <td>-1.375319</td>\n",
       "      <td>-4.169696</td>\n",
       "      <td>-7.817650</td>\n",
       "      <td>14.209498</td>\n",
       "      <td>-2.853696</td>\n",
       "      <td>-10.388455</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.042049</td>\n",
       "      <td>9.990411</td>\n",
       "      <td>10.245848</td>\n",
       "      <td>-0.693129</td>\n",
       "      <td>19.539787</td>\n",
       "      <td>-3.790751</td>\n",
       "      <td>11.632165</td>\n",
       "      <td>0.306327</td>\n",
       "      <td>tuning</td>\n",
       "      <td>Second , the visualization results indicate th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-5.768946</td>\n",
       "      <td>-3.837077</td>\n",
       "      <td>0.218382</td>\n",
       "      <td>-0.933357</td>\n",
       "      <td>0.930687</td>\n",
       "      <td>-0.851546</td>\n",
       "      <td>4.276881</td>\n",
       "      <td>3.015497</td>\n",
       "      <td>-3.265891</td>\n",
       "      <td>-6.607848</td>\n",
       "      <td>...</td>\n",
       "      <td>3.301120</td>\n",
       "      <td>7.775588</td>\n",
       "      <td>6.782337</td>\n",
       "      <td>-0.233345</td>\n",
       "      <td>5.895641</td>\n",
       "      <td>-0.972521</td>\n",
       "      <td>-1.677660</td>\n",
       "      <td>-2.729683</td>\n",
       "      <td>Understanding</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-7.505640</td>\n",
       "      <td>-2.201611</td>\n",
       "      <td>-0.213798</td>\n",
       "      <td>-3.581630</td>\n",
       "      <td>1.285837</td>\n",
       "      <td>-1.272398</td>\n",
       "      <td>1.404838</td>\n",
       "      <td>2.693443</td>\n",
       "      <td>0.042457</td>\n",
       "      <td>-2.873544</td>\n",
       "      <td>...</td>\n",
       "      <td>1.238098</td>\n",
       "      <td>4.090802</td>\n",
       "      <td>1.740754</td>\n",
       "      <td>-0.187513</td>\n",
       "      <td>9.913494</td>\n",
       "      <td>2.904173</td>\n",
       "      <td>-3.058375</td>\n",
       "      <td>-0.773143</td>\n",
       "      <td>propose</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-17.299811</td>\n",
       "      <td>-7.820803</td>\n",
       "      <td>-4.039708</td>\n",
       "      <td>-21.414472</td>\n",
       "      <td>-2.961928</td>\n",
       "      <td>-1.261067</td>\n",
       "      <td>-12.977974</td>\n",
       "      <td>20.862705</td>\n",
       "      <td>6.316183</td>\n",
       "      <td>2.743410</td>\n",
       "      <td>...</td>\n",
       "      <td>7.833588</td>\n",
       "      <td>12.987141</td>\n",
       "      <td>10.641031</td>\n",
       "      <td>-2.408733</td>\n",
       "      <td>21.821455</td>\n",
       "      <td>-12.087872</td>\n",
       "      <td>13.982088</td>\n",
       "      <td>-2.517636</td>\n",
       "      <td>appeared</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-5.425643</td>\n",
       "      <td>-1.286984</td>\n",
       "      <td>-2.470729</td>\n",
       "      <td>-2.227847</td>\n",
       "      <td>-0.444217</td>\n",
       "      <td>-6.069494</td>\n",
       "      <td>3.589774</td>\n",
       "      <td>4.339838</td>\n",
       "      <td>-1.064964</td>\n",
       "      <td>-3.679380</td>\n",
       "      <td>...</td>\n",
       "      <td>1.205708</td>\n",
       "      <td>7.034928</td>\n",
       "      <td>7.440086</td>\n",
       "      <td>-0.350141</td>\n",
       "      <td>9.474385</td>\n",
       "      <td>1.508683</td>\n",
       "      <td>-2.866299</td>\n",
       "      <td>-1.326611</td>\n",
       "      <td>can</td>\n",
       "      <td>The well trained conditional BERT can be appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-6.206223</td>\n",
       "      <td>6.080933</td>\n",
       "      <td>-5.470585</td>\n",
       "      <td>0.961756</td>\n",
       "      <td>3.047438</td>\n",
       "      <td>3.930315</td>\n",
       "      <td>-2.858736</td>\n",
       "      <td>3.776203</td>\n",
       "      <td>-2.160599</td>\n",
       "      <td>-13.315268</td>\n",
       "      <td>...</td>\n",
       "      <td>2.212279</td>\n",
       "      <td>10.103669</td>\n",
       "      <td>0.994473</td>\n",
       "      <td>6.101789</td>\n",
       "      <td>25.474121</td>\n",
       "      <td>1.230516</td>\n",
       "      <td>1.457471</td>\n",
       "      <td>-8.010502</td>\n",
       "      <td>show</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>-7.046846</td>\n",
       "      <td>1.922590</td>\n",
       "      <td>-2.514372</td>\n",
       "      <td>-1.769826</td>\n",
       "      <td>-1.204206</td>\n",
       "      <td>-1.508460</td>\n",
       "      <td>8.329064</td>\n",
       "      <td>15.171464</td>\n",
       "      <td>1.072007</td>\n",
       "      <td>0.634153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325760</td>\n",
       "      <td>14.522559</td>\n",
       "      <td>7.441674</td>\n",
       "      <td>-9.788158</td>\n",
       "      <td>17.406190</td>\n",
       "      <td>-4.505812</td>\n",
       "      <td>0.997122</td>\n",
       "      <td>-3.013875</td>\n",
       "      <td>Starting</td>\n",
       "      <td>Starting from a public multilingual BERT check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>-3.634989</td>\n",
       "      <td>8.272178</td>\n",
       "      <td>-6.671992</td>\n",
       "      <td>-4.226920</td>\n",
       "      <td>1.388392</td>\n",
       "      <td>-16.042635</td>\n",
       "      <td>-1.888895</td>\n",
       "      <td>7.836475</td>\n",
       "      <td>-5.981297</td>\n",
       "      <td>-2.677476</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.497188</td>\n",
       "      <td>15.018624</td>\n",
       "      <td>-0.461714</td>\n",
       "      <td>-2.765316</td>\n",
       "      <td>22.715964</td>\n",
       "      <td>-4.069490</td>\n",
       "      <td>6.038744</td>\n",
       "      <td>-5.543180</td>\n",
       "      <td>using</td>\n",
       "      <td>Recently , a simple combination of passage ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-12.545759</td>\n",
       "      <td>-12.096455</td>\n",
       "      <td>-1.068948</td>\n",
       "      <td>-2.254458</td>\n",
       "      <td>5.000356</td>\n",
       "      <td>-5.557888</td>\n",
       "      <td>-6.106970</td>\n",
       "      <td>14.061876</td>\n",
       "      <td>-4.284739</td>\n",
       "      <td>0.126025</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.226950</td>\n",
       "      <td>3.063190</td>\n",
       "      <td>-3.065869</td>\n",
       "      <td>4.256532</td>\n",
       "      <td>11.145037</td>\n",
       "      <td>-8.850985</td>\n",
       "      <td>15.672334</td>\n",
       "      <td>2.067391</td>\n",
       "      <td>argue</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>-5.185890</td>\n",
       "      <td>-0.032865</td>\n",
       "      <td>-1.920931</td>\n",
       "      <td>-1.289723</td>\n",
       "      <td>1.574704</td>\n",
       "      <td>-6.888347</td>\n",
       "      <td>-0.590250</td>\n",
       "      <td>2.798867</td>\n",
       "      <td>-5.314311</td>\n",
       "      <td>0.588136</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.473805</td>\n",
       "      <td>6.459176</td>\n",
       "      <td>8.938595</td>\n",
       "      <td>6.199151</td>\n",
       "      <td>4.663194</td>\n",
       "      <td>3.945729</td>\n",
       "      <td>7.732158</td>\n",
       "      <td>-1.865621</td>\n",
       "      <td>drops</td>\n",
       "      <td>More specifically , we show that BERT 's preci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>-9.768919</td>\n",
       "      <td>-5.329774</td>\n",
       "      <td>1.465989</td>\n",
       "      <td>-2.152859</td>\n",
       "      <td>3.555490</td>\n",
       "      <td>-1.160352</td>\n",
       "      <td>5.760525</td>\n",
       "      <td>10.444751</td>\n",
       "      <td>-4.307010</td>\n",
       "      <td>-0.275024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525275</td>\n",
       "      <td>5.437484</td>\n",
       "      <td>1.122411</td>\n",
       "      <td>-0.619819</td>\n",
       "      <td>12.104062</td>\n",
       "      <td>-5.035134</td>\n",
       "      <td>1.020755</td>\n",
       "      <td>-3.830375</td>\n",
       "      <td>propose</td>\n",
       "      <td>As a remedy , we propose E - BERT , an extensi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>-9.768919</td>\n",
       "      <td>-5.329774</td>\n",
       "      <td>1.465989</td>\n",
       "      <td>-2.152859</td>\n",
       "      <td>3.555490</td>\n",
       "      <td>-1.160352</td>\n",
       "      <td>5.760525</td>\n",
       "      <td>10.444751</td>\n",
       "      <td>-4.307010</td>\n",
       "      <td>-0.275024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525275</td>\n",
       "      <td>5.437484</td>\n",
       "      <td>1.122411</td>\n",
       "      <td>-0.619819</td>\n",
       "      <td>12.104062</td>\n",
       "      <td>-5.035134</td>\n",
       "      <td>1.020755</td>\n",
       "      <td>-3.830375</td>\n",
       "      <td>propose</td>\n",
       "      <td>As a remedy , we propose E - BERT , an extensi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>-9.351624</td>\n",
       "      <td>-9.475239</td>\n",
       "      <td>-1.477899</td>\n",
       "      <td>0.534226</td>\n",
       "      <td>9.887446</td>\n",
       "      <td>6.210289</td>\n",
       "      <td>1.186838</td>\n",
       "      <td>13.406745</td>\n",
       "      <td>-9.283910</td>\n",
       "      <td>0.918218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>6.171977</td>\n",
       "      <td>9.652146</td>\n",
       "      <td>0.092310</td>\n",
       "      <td>11.522442</td>\n",
       "      <td>-3.706923</td>\n",
       "      <td>5.188164</td>\n",
       "      <td>-0.378459</td>\n",
       "      <td>show</td>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-9.014500</td>\n",
       "      <td>-8.894805</td>\n",
       "      <td>-1.934365</td>\n",
       "      <td>0.333537</td>\n",
       "      <td>10.892067</td>\n",
       "      <td>6.752194</td>\n",
       "      <td>0.628017</td>\n",
       "      <td>13.643253</td>\n",
       "      <td>-9.387107</td>\n",
       "      <td>0.776116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730482</td>\n",
       "      <td>6.392618</td>\n",
       "      <td>8.979678</td>\n",
       "      <td>0.333410</td>\n",
       "      <td>11.445533</td>\n",
       "      <td>-4.323088</td>\n",
       "      <td>4.656177</td>\n",
       "      <td>-0.247576</td>\n",
       "      <td>take</td>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>-9.351624</td>\n",
       "      <td>-9.475239</td>\n",
       "      <td>-1.477899</td>\n",
       "      <td>0.534226</td>\n",
       "      <td>9.887446</td>\n",
       "      <td>6.210289</td>\n",
       "      <td>1.186838</td>\n",
       "      <td>13.406745</td>\n",
       "      <td>-9.283910</td>\n",
       "      <td>0.918218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>6.171977</td>\n",
       "      <td>9.652146</td>\n",
       "      <td>0.092310</td>\n",
       "      <td>11.522442</td>\n",
       "      <td>-3.706923</td>\n",
       "      <td>5.188164</td>\n",
       "      <td>-0.378459</td>\n",
       "      <td>show</td>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>-5.886172</td>\n",
       "      <td>-6.143673</td>\n",
       "      <td>2.935534</td>\n",
       "      <td>-5.238733</td>\n",
       "      <td>5.938962</td>\n",
       "      <td>-1.990211</td>\n",
       "      <td>4.902555</td>\n",
       "      <td>9.911255</td>\n",
       "      <td>-3.268961</td>\n",
       "      <td>2.361443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599493</td>\n",
       "      <td>5.156459</td>\n",
       "      <td>7.769064</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>7.859284</td>\n",
       "      <td>2.114014</td>\n",
       "      <td>-2.540214</td>\n",
       "      <td>-1.923601</td>\n",
       "      <td>is</td>\n",
       "      <td>BERT is Not a Knowledge Base ( Yet ) : Factual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>-1.182408</td>\n",
       "      <td>-1.378405</td>\n",
       "      <td>1.487383</td>\n",
       "      <td>-2.998096</td>\n",
       "      <td>3.151146</td>\n",
       "      <td>-0.042681</td>\n",
       "      <td>-1.135865</td>\n",
       "      <td>4.333825</td>\n",
       "      <td>-7.823340</td>\n",
       "      <td>-1.589594</td>\n",
       "      <td>...</td>\n",
       "      <td>1.328292</td>\n",
       "      <td>7.754937</td>\n",
       "      <td>5.557700</td>\n",
       "      <td>2.788954</td>\n",
       "      <td>3.687106</td>\n",
       "      <td>-3.074063</td>\n",
       "      <td>-0.955944</td>\n",
       "      <td>-0.552524</td>\n",
       "      <td>produced</td>\n",
       "      <td>However , just how contextual are the contextu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-0.430958</td>\n",
       "      <td>9.671779</td>\n",
       "      <td>-0.637056</td>\n",
       "      <td>-3.176493</td>\n",
       "      <td>-5.420790</td>\n",
       "      <td>-0.432747</td>\n",
       "      <td>-1.086178</td>\n",
       "      <td>22.234657</td>\n",
       "      <td>-0.588317</td>\n",
       "      <td>-10.064678</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.802404</td>\n",
       "      <td>13.057117</td>\n",
       "      <td>0.334019</td>\n",
       "      <td>1.380733</td>\n",
       "      <td>24.429209</td>\n",
       "      <td>-3.400773</td>\n",
       "      <td>3.440585</td>\n",
       "      <td>-1.792404</td>\n",
       "      <td>be explained</td>\n",
       "      <td>In all layers of ELMo , BERT , and GPT-2 , on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>-4.389965</td>\n",
       "      <td>-2.743391</td>\n",
       "      <td>1.771468</td>\n",
       "      <td>-1.251487</td>\n",
       "      <td>3.534614</td>\n",
       "      <td>-1.730209</td>\n",
       "      <td>0.929403</td>\n",
       "      <td>7.044091</td>\n",
       "      <td>-6.531026</td>\n",
       "      <td>-6.767352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861185</td>\n",
       "      <td>6.313660</td>\n",
       "      <td>3.754111</td>\n",
       "      <td>1.073486</td>\n",
       "      <td>9.881036</td>\n",
       "      <td>-1.205202</td>\n",
       "      <td>-2.344265</td>\n",
       "      <td>-2.773159</td>\n",
       "      <td>Comparing</td>\n",
       "      <td>Comparing the Geometry of BERT , ELMo , and GP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5  \\\n",
       "0    -4.859465  -3.911501   0.685157  -5.027377   1.997894   0.340094   \n",
       "1    -4.728350   1.564678   1.657149  -1.960320   2.319930  -0.572749   \n",
       "3    -5.126984  -4.240447   3.539152   0.015390   0.509718  -2.326005   \n",
       "4    -4.498888  15.824597  18.073197 -11.127961   9.263360   1.104467   \n",
       "8   -10.205494  -0.681300  -4.486080  -2.441828   0.308743 -12.273580   \n",
       "9    -8.658046  -8.063972  -2.774598  -5.325392   4.084332  -3.396239   \n",
       "10   -9.413263  -7.678121  -2.979881  -5.450416   3.172122  -3.609473   \n",
       "16   -4.707697  -1.030785  -3.864956  -4.009549  -0.104628  -1.701529   \n",
       "17  -12.279747  13.149659  -9.941004   2.849923  -2.146318 -17.102795   \n",
       "18  -12.596385  13.648245 -10.323936   3.092344  -2.611085 -17.700482   \n",
       "23   -8.606050  -5.473516  -1.462490  -4.464921   1.025599  -3.288439   \n",
       "24   -6.641381   5.396892   2.211766  -3.138479  -3.885576  -6.758107   \n",
       "25   -6.506219  -1.258613   4.498027  -2.425730   0.442422 -12.307495   \n",
       "27  -11.927865   0.235166  -8.899934   3.124892  13.038527   3.925919   \n",
       "28  -11.344185   1.177784  -9.253212   3.186465  15.001706   3.256440   \n",
       "29  -11.927865   0.235166  -8.899934   3.124892  13.038527   3.925919   \n",
       "31   -7.427802  -0.293290  -5.427943  -1.164961   0.556243  -0.384876   \n",
       "35   -9.500324  -2.124841  -2.610707  -5.096913   1.415049  -4.422856   \n",
       "39   -9.569691  -4.250579   1.345933  -6.026904   2.329093  -3.669049   \n",
       "40   -6.501872  -3.425783  -0.446150  -9.616967   3.290455  -9.195106   \n",
       "43   -1.618557  13.438916  11.335039 -16.345587   9.076712   9.087056   \n",
       "44   -3.756713  -3.431447  -0.776451  -1.654955   0.204977  -3.839164   \n",
       "47   -1.678657  -1.618636  -1.056970  -6.495803   2.427297  -0.776372   \n",
       "48   -8.932182   1.804437   3.612869  -8.802635  14.310425   5.094616   \n",
       "49   -2.359463  -4.803245   4.091218  -3.606412   5.472565   2.732024   \n",
       "50   -2.934680  -4.859911   3.720185  -3.518600   6.295261   2.396150   \n",
       "51   -7.886303  12.366843  -6.381242   3.939532   2.060496  -2.566081   \n",
       "52   -7.988239  12.108044  -7.100120   3.660490   1.394905  -2.745636   \n",
       "54   -6.784433   1.537288  -1.442179  -4.913676   5.795591   4.032703   \n",
       "57   -6.172527   2.890001   0.261936  -0.994849  -0.030368  -9.054016   \n",
       "58   -7.609727   5.073976   0.184928  -1.595023   2.914777  -9.353750   \n",
       "59   -3.063641  -0.135245  -0.574247  -4.809668  -1.139554  -9.228613   \n",
       "60   -3.063641  -0.135245  -0.574247  -4.809668  -1.139554  -9.228613   \n",
       "61   -0.589055  -1.535521  -1.934043   0.896331  -0.958272 -12.565653   \n",
       "62   -0.922914  -1.683226  -2.508974   0.479189  -0.763982 -14.042357   \n",
       "63   -0.589055  -1.535521  -1.934043   0.896331  -0.958272 -12.565653   \n",
       "64   -3.946128  -1.342544   5.968694   2.504707   5.832061  -4.989057   \n",
       "66    4.104221   1.215291   8.802032  -5.779140   2.598017   6.221198   \n",
       "68   -7.991769  -5.940037   0.538147  -4.743810   2.453157  -0.326046   \n",
       "69   -9.396273  -3.556319   2.189933  -2.771621   1.647444  -4.849991   \n",
       "77   -7.816538  -2.824656  -5.255841 -10.021337   0.287502  -5.981696   \n",
       "78   -7.480776  -3.281676  -4.274916 -10.083054  -0.342924  -6.227407   \n",
       "79   -7.816538  -2.824656  -5.255841 -10.021337   0.287502  -5.981696   \n",
       "80   -7.634216   0.601353  -1.548078 -10.595251   2.242247   1.145000   \n",
       "81   -7.634216   0.601353  -1.548078 -10.595251   2.242247   1.145000   \n",
       "82   -4.829431   0.110716  -0.207698  -1.397317   0.347557  -4.548082   \n",
       "86   -7.711835  -5.841216   4.016324  -8.244751   8.520811  -1.991455   \n",
       "87   -6.346138  -1.684852  -6.850927  -7.029703   3.965169   5.866407   \n",
       "88   -7.503979  -1.799755  -6.723387  -7.477550   3.643234   5.168287   \n",
       "89   -7.503979  -1.799755  -6.723387  -7.477550   3.643234   5.168287   \n",
       "92   -8.623878 -10.219848   2.513934  -4.316633   2.140724  -4.226720   \n",
       "93   -5.055044  -0.814589   3.617464  -1.412375  -1.232105  -1.011927   \n",
       "94   -5.382120  -2.041025   1.808507  -2.660938   0.061755   0.198111   \n",
       "95   -5.150390  -2.866864   1.845385  -2.870952   0.078189  -0.291655   \n",
       "96    2.120903  -1.417293   6.370067  -4.381779   3.009745   7.206445   \n",
       "97   -8.319306  -4.505620   1.934644  -5.517525   8.769313   0.137595   \n",
       "98   -7.802978  -3.673041   1.968938  -5.303204   9.738762  -0.043392   \n",
       "103  -8.061478  -1.318739  -1.891484  -3.150546   1.320672  -4.728422   \n",
       "104  -5.989899  -0.838542   0.531876  -0.134503  -6.617377  -5.103937   \n",
       "105  -5.762352  -0.522015   0.850213   0.339937  -5.478728  -3.984740   \n",
       "106  -5.675892  -0.151854   4.251443  -3.820573  -0.956800  -2.478973   \n",
       "110  -9.041723   2.769440  -2.497913  -9.308735  10.555745   0.229664   \n",
       "111  -8.061819   0.685658  -3.383357  -1.983651  -6.177811  -1.203286   \n",
       "112   0.606710  -0.370538  -2.281677  -4.759653  -7.763980 -12.146952   \n",
       "115  -6.109135  -1.793426  -1.622561  -0.673257  -1.684801  -1.945940   \n",
       "116  -2.703500  -4.501649   2.224539  -2.334781   2.088115  -0.683776   \n",
       "118  -3.582299   2.748349  -5.915385  -1.163482   4.243947  -5.117082   \n",
       "119  -3.487859   2.345011  -5.603522  -2.191770   5.697642  -4.810945   \n",
       "121  -3.721077  -4.672690  -1.938259  -2.249724  -1.831338  -0.357096   \n",
       "122  -7.191591  -2.196035  -6.222432  -3.423764   1.670846   1.164818   \n",
       "123  -1.094461   2.932837  -8.303083 -13.000537   3.282832  -1.228878   \n",
       "125 -11.732808  -3.867739  -2.343411  -0.444001  10.891148  -0.374534   \n",
       "126 -11.732808  -3.867739  -2.343411  -0.444001  10.891148  -0.374534   \n",
       "130  -3.161053   3.362335  -3.000309   0.331028   1.504782   0.073486   \n",
       "131  -9.404578   0.745903  -2.405769  -6.081145   1.754044  -3.647544   \n",
       "133  -3.359786   4.798028   1.632584  -1.417619  -1.918026  -3.915283   \n",
       "134  -9.438931  -0.227395  -2.126956  -6.278421   8.384148  -2.956160   \n",
       "142  -5.049932  -0.445237   2.112530  -5.873230   6.147004   0.182442   \n",
       "143  -8.081394  -3.578667  -0.990794  -3.138154  -1.156105  -3.344137   \n",
       "144  -8.081394  -3.578667  -0.990794  -3.138154  -1.156105  -3.344137   \n",
       "145  -3.554850  10.474434  -5.939344   0.652957   6.232687  -5.346742   \n",
       "151  -9.286966   1.227714  -2.363088  -2.687378   2.235506 -10.526797   \n",
       "155  -5.298441  -1.538954   0.007724  -1.732020   3.412107  -1.730247   \n",
       "156  -5.005430  -5.019817   2.123672  -0.986476   0.034287   0.238103   \n",
       "157  -6.927062  -1.716291  -0.037786   0.869291  -6.560842  -1.865368   \n",
       "158  -6.927062  -1.716291  -0.037786   0.869291  -6.560842  -1.865368   \n",
       "159 -11.722792   6.048747 -10.652430  -2.564454   9.279273  -8.350179   \n",
       "160 -11.452042   6.414863  -9.884483  -2.734204   9.550289  -8.105764   \n",
       "161  -3.936072   2.589637  -7.074796  -0.905881   3.649313  -6.588823   \n",
       "162  -8.143539   6.442126  -8.167684  -4.431106   6.786310 -12.153201   \n",
       "163  -8.143539   6.442126  -8.167684  -4.431106   6.786310 -12.153201   \n",
       "164  -8.143539   6.442126  -8.167684  -4.431106   6.786310 -12.153201   \n",
       "165  -8.474184   6.223902  -8.268846  -4.474464   7.420579 -12.650051   \n",
       "166  -6.412910  -2.955095   1.993462  -1.719658   1.460402   0.122076   \n",
       "169  -7.665230   4.288996  -5.092207  -3.165983   5.352067  -8.033858   \n",
       "170  -7.024046  -6.249775   1.808357  -1.721041   2.785020  -0.564414   \n",
       "171  -5.406704  -4.491922   0.864820  -1.735400  -0.822977  -0.687711   \n",
       "176 -13.333930   3.358187  -7.544338  -8.011771   0.896469  -3.283794   \n",
       "177 -14.278378   2.525913  -6.886722  -8.077023   0.290444  -3.151138   \n",
       "179  -6.952142   6.253065  -4.041596   1.287190   1.568751  -9.040809   \n",
       "180  -6.749755   6.108222  -4.378015   1.513713   1.923074  -9.126699   \n",
       "184  -9.228063  10.961564 -11.893626   1.514488  16.766868  -5.745907   \n",
       "185  -7.141728   9.532755 -10.964141   1.365763  16.122178  -4.192946   \n",
       "186  -8.477522  11.553133 -11.765724   1.078290  17.123946  -5.956054   \n",
       "187  -8.907830  11.684895 -12.117041   0.414779  17.194275  -5.579261   \n",
       "188  -8.477522  11.553133 -11.765724   1.078290  17.123946  -5.956054   \n",
       "189  -9.220070  -4.628106   5.154650  -2.980861   6.076616   2.738985   \n",
       "190  -8.673885  -3.104165   2.780159  -4.976963   5.146888   2.370353   \n",
       "192  -4.385092  -2.727906   1.173461  -4.494600  -2.187316   0.162762   \n",
       "194  -8.927935   1.213526  -3.045547  -3.823425   0.095817  -6.868662   \n",
       "195  -6.586986  10.877345  -7.687485  -4.235650  -1.375319  -4.169696   \n",
       "197  -5.768946  -3.837077   0.218382  -0.933357   0.930687  -0.851546   \n",
       "201  -7.505640  -2.201611  -0.213798  -3.581630   1.285837  -1.272398   \n",
       "204 -17.299811  -7.820803  -4.039708 -21.414472  -2.961928  -1.261067   \n",
       "207  -5.425643  -1.286984  -2.470729  -2.227847  -0.444217  -6.069494   \n",
       "208  -6.206223   6.080933  -5.470585   0.961756   3.047438   3.930315   \n",
       "211  -7.046846   1.922590  -2.514372  -1.769826  -1.204206  -1.508460   \n",
       "215  -3.634989   8.272178  -6.671992  -4.226920   1.388392 -16.042635   \n",
       "224 -12.545759 -12.096455  -1.068948  -2.254458   5.000356  -5.557888   \n",
       "226  -5.185890  -0.032865  -1.920931  -1.289723   1.574704  -6.888347   \n",
       "227  -9.768919  -5.329774   1.465989  -2.152859   3.555490  -1.160352   \n",
       "228  -9.768919  -5.329774   1.465989  -2.152859   3.555490  -1.160352   \n",
       "231  -9.351624  -9.475239  -1.477899   0.534226   9.887446   6.210289   \n",
       "232  -9.014500  -8.894805  -1.934365   0.333537  10.892067   6.752194   \n",
       "233  -9.351624  -9.475239  -1.477899   0.534226   9.887446   6.210289   \n",
       "234  -5.886172  -6.143673   2.935534  -5.238733   5.938962  -1.990211   \n",
       "236  -1.182408  -1.378405   1.487383  -2.998096   3.151146  -0.042681   \n",
       "241  -0.430958   9.671779  -0.637056  -3.176493  -5.420790  -0.432747   \n",
       "243  -4.389965  -2.743391   1.771468  -1.251487   3.534614  -1.730209   \n",
       "\n",
       "             6          7          8          9  ...       1016       1017  \\\n",
       "0     3.416013  10.158601  -2.106146  -8.207319  ...   0.919644  14.184185   \n",
       "1    -7.824029   9.078582  -3.022813  -9.243394  ...  -4.103853  11.668791   \n",
       "3     4.007223   3.939335  -3.068178  -5.500858  ...   2.193373   8.026586   \n",
       "4   -25.863521  26.202192  22.393852 -10.348825  ...   8.796367  20.908768   \n",
       "8    -1.296075  12.359570   3.727988   4.990989  ...  -2.120051  13.107285   \n",
       "9    -1.856154   7.292558  -0.900050  -1.248224  ...   0.993280   8.935133   \n",
       "10   -1.237080   7.007622  -0.549997  -1.044924  ...   0.638331   9.230479   \n",
       "16    7.102601  12.099053  -0.310520  -9.456530  ...   0.581415  16.120759   \n",
       "17  -13.786130   9.165139  12.048118  -3.275484  ...   0.854651  25.979131   \n",
       "18  -14.064017   9.094506  12.562291  -3.053498  ...  -0.299957  26.354039   \n",
       "23    3.778120  10.125328  -3.735655  -2.181577  ...   0.028277   9.084042   \n",
       "24    1.300727  18.045872  -4.531725   1.488820  ...  -2.298295  20.607708   \n",
       "25    0.257704   5.079469 -10.323381  -3.873248  ...  -7.072795   4.919608   \n",
       "27   -2.430103  14.940655   1.412362  -1.246667  ...  -3.042450  12.169796   \n",
       "28   -2.533082  15.071028   0.502282  -2.569465  ...  -3.978115  12.858985   \n",
       "29   -2.430103  14.940655   1.412362  -1.246667  ...  -3.042450  12.169796   \n",
       "31    1.421317   5.890766  -3.246335  -3.968630  ...   2.116340   5.264246   \n",
       "35    2.508182   9.369701 -12.194330  -1.903133  ...  -2.664164   7.939153   \n",
       "39    1.134723   7.314991  -7.288600  -6.647176  ...  -0.829539   5.884584   \n",
       "40   -1.383468  12.782340 -10.730900  -0.758891  ...   5.959537  11.796151   \n",
       "43  -12.280978  35.118752  11.564481  -9.658287  ...  -1.631488  27.569690   \n",
       "44    2.494902   1.251581  -3.899827  -5.320324  ...   1.052451   8.398514   \n",
       "47    1.463624  13.010853   4.336464  -7.305290  ...   6.143580  13.665897   \n",
       "48    1.955369  21.171011   1.801773  -3.506800  ...  -2.355513  11.139850   \n",
       "49    2.485200   8.155113  -5.211341  -6.697028  ...  -0.154323   5.692595   \n",
       "50    2.614498   8.136577  -5.204879  -6.642999  ...  -0.627114   6.346710   \n",
       "51   -0.979879  15.351861   3.247721   5.284750  ...  -1.948788  17.051996   \n",
       "52   -1.350675  15.091368   2.990226   5.554305  ...  -1.461253  17.360516   \n",
       "54    1.568026  10.977729   0.083918  -6.283739  ...   4.439616  10.098012   \n",
       "57    5.608953   5.849642  -1.846521  -1.996766  ...   0.940866  11.091258   \n",
       "58    4.346285   6.468918  -1.183586  -2.762949  ...  -0.873760  11.252410   \n",
       "59    1.121727  19.684181  -1.025354 -15.730066  ...  -2.016086  12.004227   \n",
       "60    1.121727  19.684181  -1.025354 -15.730066  ...  -2.016086  12.004227   \n",
       "61   -0.165108   6.061886  -4.352608 -11.093641  ...  -0.079090   8.144147   \n",
       "62   -0.312101   6.293282  -5.783224 -11.846575  ...   0.390572   9.259814   \n",
       "63   -0.165108   6.061886  -4.352608 -11.093641  ...  -0.079090   8.144147   \n",
       "64    4.295761  11.209648  -0.599167  -1.606351  ...   2.480025   2.258384   \n",
       "66   -4.497165  15.687531  -3.335432  -7.457595  ...  -0.674070   0.923321   \n",
       "68    6.183125   4.377331  -1.799858  -5.183727  ...   4.165982   9.869269   \n",
       "69    5.666373   4.819777  -5.078170  -5.098533  ...   1.246159   8.209491   \n",
       "77   -4.701579  28.613073  -1.173010  -0.060915  ...  -4.722167  10.103693   \n",
       "78   -4.361586  28.476350  -1.518940  -0.309611  ...  -4.968458  10.287705   \n",
       "79   -4.701579  28.613073  -1.173010  -0.060915  ...  -4.722167  10.103693   \n",
       "80    1.813936  10.770768  -7.022381  -6.883127  ...  -4.472415   5.994305   \n",
       "81    1.813936  10.770768  -7.022381  -6.883127  ...  -4.472415   5.994305   \n",
       "82    0.421450   4.435095  -0.363470  -7.103496  ...   3.425266   6.508212   \n",
       "86   -2.504233  12.973072  -4.068198  -4.618082  ...  -5.636780  15.525980   \n",
       "87    3.319361  10.861198  -0.169591  -5.150112  ...   3.481384   7.377317   \n",
       "88    2.803441  10.815555  -0.650478  -5.643146  ...   2.702531   7.402757   \n",
       "89    2.803441  10.815555  -0.650478  -5.643146  ...   2.702531   7.402757   \n",
       "92    2.000190   4.998448  -1.839983   4.198319  ...   3.258257  10.200982   \n",
       "93    4.202555   3.846570  -3.197533  -7.002494  ...   0.661636   6.655460   \n",
       "94    1.805010   2.358624  -1.656634  -7.536668  ...  -0.062768   7.857211   \n",
       "95    2.740276   2.113062  -1.831536  -6.815415  ...  -0.301712   8.539829   \n",
       "96   -5.031618   1.143271  -6.043915  -1.928073  ...  -1.127294   8.737380   \n",
       "97    7.887969   8.686139  -4.400817  -3.116157  ...   1.880458  12.383476   \n",
       "98    8.196429   8.994041  -3.286698  -2.790624  ...   1.990296  12.992055   \n",
       "103   4.619332   6.997402  -2.604794  -1.663895  ...  -0.269766   6.665802   \n",
       "104  -1.647200  -2.805029  -5.915128  -0.487172  ...  -0.269908   4.654212   \n",
       "105  -2.295025  -3.138146  -6.889208   0.287089  ...  -0.185959   4.821892   \n",
       "106  -0.796745   6.567544  -5.535128  -4.877003  ...   3.179122   4.320226   \n",
       "110  -8.823973  23.295129  -0.230362  -3.902938  ...  -2.910978  10.072254   \n",
       "111  -0.596736   6.318209  -4.157804  -4.916832  ...   2.514612   6.072519   \n",
       "112   4.257372   9.704921  -8.081163  -2.366418  ...  -3.868927  16.727116   \n",
       "115   2.420317   1.365836  -1.246891  -3.979463  ...   3.150308   5.428715   \n",
       "116   4.953191   0.851680  -5.160437  -6.058223  ...   4.320101   6.814077   \n",
       "118   4.207947   4.947273   3.277426  -3.531257  ...   1.962741   4.945492   \n",
       "119   4.925628   7.370703   3.330924  -3.631749  ...   1.633012   5.465954   \n",
       "121   1.884293  -0.538705  -5.946277  -8.576145  ...   3.363300   9.688632   \n",
       "122   3.231840   6.135631  -0.087537  -4.085480  ...  -3.535226   4.751729   \n",
       "123 -23.487308  22.031591  12.221411  -1.878562  ...  -3.918839  26.535078   \n",
       "125  -0.869530   9.407952  -7.741794  -1.305381  ...   3.729244  11.279761   \n",
       "126  -0.869530   9.407952  -7.741794  -1.305381  ...   3.729244  11.279761   \n",
       "130  -6.518556   2.327154   0.881548   1.774245  ...  -3.003591   7.556600   \n",
       "131  -2.439562  20.667215  -9.988467  -0.157201  ...  -7.393663  15.029621   \n",
       "133   4.069832   5.790207   3.442394  -7.166829  ...   5.043173   1.677227   \n",
       "134  -1.695910   8.883443  -5.046989  -4.277526  ...   0.555494   8.728924   \n",
       "142   3.296226   2.311400   0.007478  -1.895493  ...   3.249289   7.650513   \n",
       "143   5.305189   7.424689  -2.170588  -6.392693  ...   3.692980   9.747914   \n",
       "144   5.305189   7.424689  -2.170588  -6.392693  ...   3.692980   9.747914   \n",
       "145  -6.412344  12.249691   5.603100 -10.897520  ...  -7.929304  20.165005   \n",
       "151   3.041335  10.281029   0.033257  -1.714837  ...   1.976641  13.778488   \n",
       "155   5.370044   6.895778  -3.149974  -5.255588  ...   1.425021   5.304224   \n",
       "156   0.422532   3.076760  -3.051523  -4.895040  ...   2.335704   6.637191   \n",
       "157   6.229330  18.119908  -1.682998  -3.916790  ...  -3.895018   5.583397   \n",
       "158   6.229330  18.119908  -1.682998  -3.916790  ...  -3.895018   5.583397   \n",
       "159  -5.691705  18.301708  -2.763800   2.316636  ...  -3.437716  17.495519   \n",
       "160  -6.140095  18.925542  -2.116054   1.279806  ...  -3.728436  18.113019   \n",
       "161  -8.123074   7.457427   0.073691  -9.273520  ...   1.761048   8.690749   \n",
       "162  -7.219865   9.961044   1.688841   1.841268  ... -10.158852  15.537325   \n",
       "163  -7.219865   9.961044   1.688841   1.841268  ... -10.158852  15.537325   \n",
       "164  -7.219865   9.961044   1.688841   1.841268  ... -10.158852  15.537325   \n",
       "165  -7.051151  10.404551   1.169622   2.112493  ... -10.396002  15.518073   \n",
       "166   4.138150   1.347494  -4.801625  -6.144500  ...   3.324086   6.055160   \n",
       "169   1.387058   6.937962  -4.919617  -7.779969  ...   1.463127  17.452409   \n",
       "170   2.642710   1.975590  -4.867954  -1.388843  ...   2.812707   8.342521   \n",
       "171   0.418488   1.613718  -5.947994  -7.612312  ...   2.341839   7.727042   \n",
       "176   3.305403  22.491927   4.817725  -1.432122  ...  -4.201476  11.691244   \n",
       "177   4.282961  22.028872   4.480502  -1.364535  ...  -4.608590  12.169183   \n",
       "179  -2.007033  14.267893   1.224772   2.121941  ...   1.176705   9.918837   \n",
       "180  -1.742490  13.998315   1.107352   2.630445  ...   0.595252   9.602975   \n",
       "184 -12.086385  17.401294  -5.086328   3.783951  ... -12.578977  21.777007   \n",
       "185 -12.523782  17.828103  -6.453804   4.282553  ... -11.462033  20.270369   \n",
       "186 -11.635807  17.779965  -6.109274   4.560082  ... -13.217197  21.822698   \n",
       "187 -12.036323  16.879939  -6.438534   3.483728  ... -12.292415  22.044896   \n",
       "188 -11.635807  17.779965  -6.109274   4.560082  ... -13.217197  21.822698   \n",
       "189   3.187585   4.246963  -3.613858  -2.617819  ...   1.386258  11.185780   \n",
       "190   1.300211  11.232066  -1.622563  -7.182175  ...   0.413096   5.742879   \n",
       "192   0.891106   3.925889  -4.492952  -6.440457  ...  -3.135672   6.038065   \n",
       "194   0.442578  10.440407  -0.084636  -4.435107  ...   1.212849   2.450209   \n",
       "195  -7.817650  14.209498  -2.853696 -10.388455  ...  -3.042049   9.990411   \n",
       "197   4.276881   3.015497  -3.265891  -6.607848  ...   3.301120   7.775588   \n",
       "201   1.404838   2.693443   0.042457  -2.873544  ...   1.238098   4.090802   \n",
       "204 -12.977974  20.862705   6.316183   2.743410  ...   7.833588  12.987141   \n",
       "207   3.589774   4.339838  -1.064964  -3.679380  ...   1.205708   7.034928   \n",
       "208  -2.858736   3.776203  -2.160599 -13.315268  ...   2.212279  10.103669   \n",
       "211   8.329064  15.171464   1.072007   0.634153  ...   0.325760  14.522559   \n",
       "215  -1.888895   7.836475  -5.981297  -2.677476  ...  -7.497188  15.018624   \n",
       "224  -6.106970  14.061876  -4.284739   0.126025  ...  -5.226950   3.063190   \n",
       "226  -0.590250   2.798867  -5.314311   0.588136  ...  -4.473805   6.459176   \n",
       "227   5.760525  10.444751  -4.307010  -0.275024  ...   0.525275   5.437484   \n",
       "228   5.760525  10.444751  -4.307010  -0.275024  ...   0.525275   5.437484   \n",
       "231   1.186838  13.406745  -9.283910   0.918218  ...   0.020315   6.171977   \n",
       "232   0.628017  13.643253  -9.387107   0.776116  ...   0.730482   6.392618   \n",
       "233   1.186838  13.406745  -9.283910   0.918218  ...   0.020315   6.171977   \n",
       "234   4.902555   9.911255  -3.268961   2.361443  ...   0.599493   5.156459   \n",
       "236  -1.135865   4.333825  -7.823340  -1.589594  ...   1.328292   7.754937   \n",
       "241  -1.086178  22.234657  -0.588317 -10.064678  ...  -1.802404  13.057117   \n",
       "243   0.929403   7.044091  -6.531026  -6.767352  ...   1.861185   6.313660   \n",
       "\n",
       "          1018       1019       1020       1021       1022       1023  \\\n",
       "0     2.516548  -0.972502   6.017331  -2.040474  -5.436984  -2.138046   \n",
       "1    -0.148851   6.478394  22.855887  -2.133500   3.163807  -2.175767   \n",
       "3     6.021397   2.223149   3.006229   2.246306  -0.630127  -0.570627   \n",
       "4     4.457648 -34.910531  22.006920  -7.382068  17.523363  11.469820   \n",
       "8     4.218679  -1.619001  22.665985  -4.506110   3.464262  -1.422632   \n",
       "9     4.858590  -0.444038  11.929569  -4.004733  -3.337786  -0.312336   \n",
       "10    6.133819  -0.426764  13.004866  -3.556028  -3.403889  -0.108809   \n",
       "16    0.921296  -3.696960  20.956018  -8.554245  -5.580938  -3.900606   \n",
       "17    5.340115 -15.328704  32.994659 -14.922404   7.904891  -3.531432   \n",
       "18    5.540001 -15.199837  33.174928 -14.029706   7.369203  -3.912339   \n",
       "23    4.883412   2.982582   9.364935  -3.128964  -3.910031  -4.351510   \n",
       "24    5.596874   0.512143  26.830307  -4.009975   4.508335  -3.662218   \n",
       "25    7.391818  -5.342462  19.560214   0.008300   8.766688  -0.162169   \n",
       "27    4.650031   9.121808  19.305409  -3.089077  -0.086525 -10.832083   \n",
       "28    4.650402   9.614546  20.544340  -3.845881   0.489070 -10.674088   \n",
       "29    4.650031   9.121808  19.305409  -3.089077  -0.086525 -10.832083   \n",
       "31    5.134827   6.654226  15.069799  -3.040852   1.633433  -4.254935   \n",
       "35   10.828956   6.803721  14.290925   3.060732   2.591130  -2.460358   \n",
       "39    3.362310  -3.270161   5.477891  -3.683641  -1.329205  -2.490042   \n",
       "40    8.818732  -4.588744  13.089305  -7.351201  15.065139  -3.308762   \n",
       "43    4.866843  12.222679  45.151218 -23.893461  33.496021   0.524567   \n",
       "44    1.754124   1.607107   5.977670  -0.172656  -2.296726  -0.300141   \n",
       "47   -0.847760  -3.668640  16.588429  -8.393073   0.819877  -2.598549   \n",
       "48   -6.044875  -2.024172  24.051028 -14.608838   3.623108 -11.547046   \n",
       "49    8.268441  -0.012763  15.461027  -0.890030   1.216166  -1.547176   \n",
       "50    8.189192   0.139158  15.732449  -1.588172   1.271767  -1.589851   \n",
       "51   11.458891   2.161800  20.489466  -2.117356   7.194585   3.025285   \n",
       "52   12.162705   2.613528  20.586356  -2.183123   6.897504   2.153043   \n",
       "54    0.285778   0.682176  19.926229  -4.489634  -1.172362  -5.697744   \n",
       "57    5.919835   7.302510   6.999620   0.214277  -1.354657  -0.861614   \n",
       "58    6.082944   6.883885   6.210409   0.960521  -0.780131   0.320627   \n",
       "59    9.130643  -2.746708  18.251715  -1.467121   4.778063  -1.035389   \n",
       "60    9.130643  -2.746708  18.251715  -1.467121   4.778063  -1.035389   \n",
       "61    4.293192  -0.424058  10.494635  -0.999685   3.778612  -4.471280   \n",
       "62    4.512382  -0.090484  10.344086  -0.319896   4.209835  -4.642836   \n",
       "63    4.293192  -0.424058  10.494635  -0.999685   3.778612  -4.471280   \n",
       "64    6.574323  -6.102518  16.715891  -5.617371   2.532120  -0.750196   \n",
       "66   11.339734  -3.008985  19.563908  -9.092735   7.579970   0.770921   \n",
       "68    4.395470  -0.565412  11.098223  -4.062683  -3.216342  -2.163312   \n",
       "69    1.363724  -0.520458   5.323790  -2.182671  -0.155358  -1.152712   \n",
       "77    3.007510  -8.543838  21.297284 -13.237629   5.353695  -4.915610   \n",
       "78    3.460711  -8.365839  20.701977 -13.188794   5.126110  -4.619997   \n",
       "79    3.007510  -8.543838  21.297284 -13.237629   5.353695  -4.915610   \n",
       "80    6.712483  -9.512354  10.925199  -2.225764  11.702755   1.462853   \n",
       "81    6.712483  -9.512354  10.925199  -2.225764  11.702755   1.462853   \n",
       "82    2.861088   0.624368   7.868575   0.435193   0.807487  -0.434667   \n",
       "86    3.832716  -1.865674  16.407986 -12.069009   2.901751  -4.904004   \n",
       "87    3.398228   7.112475  17.290753  -3.269495   0.513007  -3.327501   \n",
       "88    3.830222   7.699432  16.961659  -3.456134   0.617805  -3.492177   \n",
       "89    3.830222   7.699432  16.961659  -3.456134   0.617805  -3.492177   \n",
       "92    7.038782  -0.840165   8.233978  -1.729853   1.292382  -0.508297   \n",
       "93    0.308756   2.906113   7.613948   1.163743   1.564938  -0.250174   \n",
       "94    5.983891   3.074777   6.841979   2.747419   6.072614  -1.005269   \n",
       "95    6.518232   2.647182   7.265409   2.858359   5.641324  -0.953440   \n",
       "96    5.931570   0.734139   8.043051  -2.452919   3.014950   0.582443   \n",
       "97    6.361078  -3.016280   6.530642  -1.157741  -6.469904  -6.338686   \n",
       "98    6.419940  -3.523313   6.320351  -0.957734  -5.880383  -5.284241   \n",
       "103   2.353971   3.102952   3.535963   1.782552  -1.407127  -4.183691   \n",
       "104   6.333701  -7.851084  11.513704  -1.176469  -2.485166  -3.204794   \n",
       "105   7.030187  -7.462679  11.768668  -1.821790  -2.575975  -3.196218   \n",
       "106   1.401577   4.293581  13.785275   1.906998   0.569022  -2.012885   \n",
       "110   6.071943   1.061889  16.099742  -7.476103   9.171657  -7.191566   \n",
       "111   4.809406   2.667517  15.917321  -7.884004   6.265963  -4.046960   \n",
       "112   4.133254  -1.935701  19.699014  -4.233355   8.245693  -5.360664   \n",
       "115   6.445613   3.909162  11.398874   1.029778  -1.159045   1.053127   \n",
       "116   5.160317   1.445366   5.262548  -0.965985  -0.378083  -2.990473   \n",
       "118   6.805891  -1.482995  15.414236  -9.943877  -1.285170  -3.372581   \n",
       "119   6.549614  -2.008518  15.695342 -10.633431  -0.979475  -3.062044   \n",
       "121   4.593385   3.049190   4.287301  -1.033919  -0.946668  -2.517152   \n",
       "122   5.049405  -3.205985  15.024564  -7.589117   3.759112  -0.431339   \n",
       "123   5.782956 -24.787497  55.313010 -13.616172  24.487531  -3.553413   \n",
       "125   8.685210  -5.443056  15.083909  -3.052447   4.282526  -2.650203   \n",
       "126   8.685210  -5.443056  15.083909  -3.052447   4.282526  -2.650203   \n",
       "130   0.884678  10.057627  15.990368   6.718393   0.912056  -1.860382   \n",
       "131   5.853121  -0.331127  25.408912   2.289135  -1.450243  -9.260544   \n",
       "133   7.590822  -0.339682   6.081468   3.514198  -5.839629  -0.771564   \n",
       "134   1.964609   0.293684  15.546367  -7.302793   4.867228  -4.003706   \n",
       "142   0.534563   3.295841   2.980280  -1.432931   1.289183  -0.188550   \n",
       "143   1.260895  -0.582983  11.546266  -4.486740  -3.020504  -1.423947   \n",
       "144   1.260895  -0.582983  11.546266  -4.486740  -3.020504  -1.423947   \n",
       "145   7.283374  -5.537001  28.937577 -11.940870  12.580129  -1.014539   \n",
       "151   0.640334  -1.357105   7.773623  -6.394884  -2.456436  -1.478438   \n",
       "155   3.821391   0.710451   2.702670   2.105006  -4.548288  -2.838934   \n",
       "156   1.971263   4.377250  10.447351  -5.581640  -0.765222  -2.706537   \n",
       "157  -0.257259  -4.293489   9.379536  -2.453161  -6.630558  -4.805034   \n",
       "158  -0.257259  -4.293489   9.379536  -2.453161  -6.630558  -4.805034   \n",
       "159   4.175534  -3.710103  15.720067 -10.224118   6.598050  -0.411762   \n",
       "160   5.204794  -3.110125  16.003719 -10.214809   6.955479  -0.421273   \n",
       "161   3.597866   3.960309   8.967296  -2.708473   0.766231  -4.454368   \n",
       "162   6.344458  12.138985  28.358085  -3.759067  16.144523  -2.167636   \n",
       "163   6.344458  12.138985  28.358085  -3.759067  16.144523  -2.167636   \n",
       "164   6.344458  12.138985  28.358085  -3.759067  16.144523  -2.167636   \n",
       "165   6.109630  11.891492  28.527454  -4.259336  16.001690  -3.082713   \n",
       "166   4.807548   3.160672   7.045570   0.044331  -2.687856  -2.395735   \n",
       "169   5.087848   4.862034  22.521439  -7.068395   1.661450  -2.526398   \n",
       "170   2.001541   4.028254   9.718325   1.901125  -0.542611  -2.394135   \n",
       "171   4.238705   5.245733   8.754437   2.671897  -1.659958  -1.247631   \n",
       "176  -0.491714  -1.926360  30.130173  -9.635144  -8.225464  -5.053662   \n",
       "177  -0.547440  -1.893743  29.230591  -9.977564  -7.740215  -4.976744   \n",
       "179   4.641469  -1.044462  28.749455  -9.262540  -3.309269  -4.934646   \n",
       "180   4.131438  -0.660476  29.046354  -9.247580  -2.836848  -5.346280   \n",
       "184  10.680769  10.953658  25.085792   0.404068  47.262965   3.839316   \n",
       "185  11.305710  10.659246  24.604149   0.651197  45.026116   3.882755   \n",
       "186  11.868238  10.343766  25.470770   1.009170  46.828415   3.144294   \n",
       "187  11.758524  10.705144  25.762278   0.363948  45.966851   3.003968   \n",
       "188  11.868238  10.343766  25.470770   1.009170  46.828415   3.144294   \n",
       "189   6.904366   3.905912   8.260362   2.840301  -4.724039  -7.379824   \n",
       "190   2.838234  -2.468071  13.544263  -5.387967  -2.001249  -5.232017   \n",
       "192   5.769958   3.183110  17.721247  -2.200913   3.486176  -5.982644   \n",
       "194   4.230592   2.992151  13.218968  -0.544036   3.430881  -0.937430   \n",
       "195  10.245848  -0.693129  19.539787  -3.790751  11.632165   0.306327   \n",
       "197   6.782337  -0.233345   5.895641  -0.972521  -1.677660  -2.729683   \n",
       "201   1.740754  -0.187513   9.913494   2.904173  -3.058375  -0.773143   \n",
       "204  10.641031  -2.408733  21.821455 -12.087872  13.982088  -2.517636   \n",
       "207   7.440086  -0.350141   9.474385   1.508683  -2.866299  -1.326611   \n",
       "208   0.994473   6.101789  25.474121   1.230516   1.457471  -8.010502   \n",
       "211   7.441674  -9.788158  17.406190  -4.505812   0.997122  -3.013875   \n",
       "215  -0.461714  -2.765316  22.715964  -4.069490   6.038744  -5.543180   \n",
       "224  -3.065869   4.256532  11.145037  -8.850985  15.672334   2.067391   \n",
       "226   8.938595   6.199151   4.663194   3.945729   7.732158  -1.865621   \n",
       "227   1.122411  -0.619819  12.104062  -5.035134   1.020755  -3.830375   \n",
       "228   1.122411  -0.619819  12.104062  -5.035134   1.020755  -3.830375   \n",
       "231   9.652146   0.092310  11.522442  -3.706923   5.188164  -0.378459   \n",
       "232   8.979678   0.333410  11.445533  -4.323088   4.656177  -0.247576   \n",
       "233   9.652146   0.092310  11.522442  -3.706923   5.188164  -0.378459   \n",
       "234   7.769064   0.002953   7.859284   2.114014  -2.540214  -1.923601   \n",
       "236   5.557700   2.788954   3.687106  -3.074063  -0.955944  -0.552524   \n",
       "241   0.334019   1.380733  24.429209  -3.400773   3.440585  -1.792404   \n",
       "243   3.754111   1.073486   9.881036  -1.205202  -2.344265  -2.773159   \n",
       "\n",
       "                   word                                           sentence  \n",
       "0                stands  We introduce a new language representation mod...  \n",
       "1           is designed  Unlike recent language representation models ,...  \n",
       "3                    is  BERT is conceptually simple and empirically po...  \n",
       "4               obtains  It obtains new state - of - the - art results ...  \n",
       "8               present  We present a replication study of BERT pretrai...  \n",
       "9                  find  We find that BERT was significantly undertrain...  \n",
       "10            published  We find that BERT was significantly undertrain...  \n",
       "16               called  In this work , we propose a method to pre-trai...  \n",
       "17            to reduce  While most prior work investigated the use of ...  \n",
       "18            retaining  While most prior work investigated the use of ...  \n",
       "23                focus  We focus on one such model , BERT , and aim to...  \n",
       "24           represents  We find that the model represents the steps of...  \n",
       "25          does adjust  Qualitative analysis reveals that the model ca...  \n",
       "27             have had  Large pre - trained neural networks such as BE...  \n",
       "28           motivating  Large pre - trained neural networks such as BE...  \n",
       "29             have had  Large pre - trained neural networks such as BE...  \n",
       "31                apply  Complementary to these works , we propose meth...  \n",
       "35          is captured  Lastly , we propose an attention - based probi...  \n",
       "39             describe  In this paper , we describe a simple re - impl...  \n",
       "40                   is  Our system is the state of the art on the TREC...  \n",
       "43               assess  I assess the extent to which the recently intr...  \n",
       "44             performs  The BERT model performs remarkably well on all...  \n",
       "47             includes  A new release of BERT ( Devlin , 2018 ) includ...  \n",
       "48             explores  This paper explores the broader cross-lingual ...  \n",
       "49              compare  We compare mBERT with the best - published met...  \n",
       "50                 find  We compare mBERT with the best - published met...  \n",
       "51          investigate  Additionally , we investigate the most effecti...  \n",
       "52          generalizes  Additionally , we investigate the most effecti...  \n",
       "54                  pre  Language model pre - training , such as BERT ,...  \n",
       "57   can be transferred  By leveraging this new KD method , the plenty ...  \n",
       "58              encoded  By leveraging this new KD method , the plenty ...  \n",
       "59            introduce  Moreover , we introduce a new two - stage lear...  \n",
       "60            introduce  Moreover , we introduce a new two - stage lear...  \n",
       "61          can capture  This framework ensures that TinyBERT can captu...  \n",
       "62              ensures  This framework ensures that TinyBERT can captu...  \n",
       "63          can capture  This framework ensures that TinyBERT can captu...  \n",
       "64                   is  TinyBERT is empirically effective and achieves...  \n",
       "66                   is  TinyBERT is also significantly better than sta...  \n",
       "68         has achieved  BERT , a pre-trained Transformer model , has a...  \n",
       "69             describe  In this paper , we describe BERTSUM , a simple...  \n",
       "77              explore  Since ReviewRC has limited training examples f...  \n",
       "78              enhance  Since ReviewRC has limited training examples f...  \n",
       "79              explore  Since ReviewRC has limited training examples f...  \n",
       "80              To show  To show the generality of the approach , the p...  \n",
       "81              To show  To show the generality of the approach , the p...  \n",
       "82          demonstrate  Experimental results demonstrate that the prop...  \n",
       "86             achieved  As a state - of - the - art language model pre...  \n",
       "87       to investigate  In this paper , we conduct exhaustive experime...  \n",
       "88              provide  In this paper , we conduct exhaustive experime...  \n",
       "89              provide  In this paper , we conduct exhaustive experime...  \n",
       "92                 show  We show that BERT ( Devlin et al . , 2018 ) is...  \n",
       "93                gives  This formulation gives way to a natural proced...  \n",
       "94             generate  We generate from BERT and find that it can pro...  \n",
       "95              produce  We generate from BERT and find that it can pro...  \n",
       "96             Compared  Compared to the generations of a traditional l...  \n",
       "97                  has  BERT has a Mouth , and It Must Speak : BERT as...  \n",
       "98                 Must  BERT has a Mouth , and It Must Speak : BERT as...  \n",
       "103            applying  Following recent successes in applying BERT to...  \n",
       "104               posed  This required confronting the challenge posed ...  \n",
       "105         confronting  This required confronting the challenge posed ...  \n",
       "106             address  We address this issue by applying inference on...  \n",
       "110          contribute  BERT - based architectures currently give stat...  \n",
       "111               focus  In the current work , we focus on the interpre...  \n",
       "112             encoded  Using a subset of GLUE tasks and a set of hand...  \n",
       "115               leads  We show that manually disabling attention in c...  \n",
       "116           Revealing               Revealing the Dark Secrets of BERT .  \n",
       "118   has been released  Recently , an upgraded version of BERT has bee...  \n",
       "119             masking  Recently , an upgraded version of BERT has bee...  \n",
       "121         was trained  The model was trained on the latest Chinese Wi...  \n",
       "122          to provide  We aim to provide easy extensibility and bette...  \n",
       "123         is verified  The model is verified on various NLP tasks , a...  \n",
       "125             examine  Moreover , we also examine the effectiveness o...  \n",
       "126             examine  Moreover , we also examine the effectiveness o...  \n",
       "130                BERT  However , previous work trains BERT by viewing...  \n",
       "131             propose  To tackle this issue , we propose a multi - pa...  \n",
       "133               gains  By leveraging a passage ranker to select high-...  \n",
       "134              showed  Experiments on four standard benchmarks showed...  \n",
       "142           exploring  However , there has not been much effort on ex...  \n",
       "143               based  In this work , we propose a joint intent class...  \n",
       "144               based  In this work , we propose a joint intent class...  \n",
       "145            achieves  Experimental results demonstrate that our prop...  \n",
       "151               built  It enables seamless integration of conversatio...  \n",
       "155                BERT  BERT with History Answer Embedding for Convers...  \n",
       "156             studies  This paper studies the performances and behavi...  \n",
       "157         to leverage  We explore several different ways to leverage ...  \n",
       "158         to leverage  We explore several different ways to leverage ...  \n",
       "159         demonstrate  Experimental results on MS MARCO demonstrate t...  \n",
       "160           answering  Experimental results on MS MARCO demonstrate t...  \n",
       "161                 pre  Experimental results on TREC show the gaps bet...  \n",
       "162           allocates  Analyses illustrate how BERT allocates its att...  \n",
       "163           allocates  Analyses illustrate how BERT allocates its att...  \n",
       "164           allocates  Analyses illustrate how BERT allocates its att...  \n",
       "165             prefers  Analyses illustrate how BERT allocates its att...  \n",
       "166       Understanding   Understanding the Behaviors of BERT in Ranking .  \n",
       "169             achieve  In this paper , extensive experiments on datas...  \n",
       "170            to apply  To our knowledge , we are the first to success...  \n",
       "171             provide  Our models provide strong baselines for future...  \n",
       "176             explore  We explore the multi-task learning setting for...  \n",
       "177                 add  We explore the multi-task learning setting for...  \n",
       "179               using  By using PALs in parallel with BERT layers , w...  \n",
       "180               tuned  By using PALs in parallel with BERT layers , w...  \n",
       "184               apply  As a case study , we apply these diagnostics t...  \n",
       "185     can distinguish  As a case study , we apply these diagnostics t...  \n",
       "186               shows  As a case study , we apply these diagnostics t...  \n",
       "187           struggles  As a case study , we apply these diagnostics t...  \n",
       "188               shows  As a case study , we apply these diagnostics t...  \n",
       "189                  Is  What BERT Is Not : Lessons from a New Suite of...  \n",
       "190                 pre  Language model pre - training , such as BERT ,...  \n",
       "192              tuning  In this paper , we propose to visualize loss l...  \n",
       "194         demonstrate  We also demonstrate that the fine - tuning pro...  \n",
       "195              tuning  Second , the visualization results indicate th...  \n",
       "197       Understanding  Visualizing and Understanding the Effectivenes...  \n",
       "201             propose  We propose a novel data augmentation method fo...  \n",
       "204            appeared  We retrofit BERT to conditional BERT by introd...  \n",
       "207                 can  The well trained conditional BERT can be appli...  \n",
       "208                show  Experiments on six various different text clas...  \n",
       "211            Starting  Starting from a public multilingual BERT check...  \n",
       "215               using  Recently , a simple combination of passage ret...  \n",
       "224               argue  We take issue with this interpretation and arg...  \n",
       "226               drops  More specifically , we show that BERT 's preci...  \n",
       "227             propose  As a remedy , we propose E - BERT , an extensi...  \n",
       "228             propose  As a remedy , we propose E - BERT , an extensi...  \n",
       "231                show  We take this as evidence that E - BERT is rich...  \n",
       "232                take  We take this as evidence that E - BERT is rich...  \n",
       "233                show  We take this as evidence that E - BERT is rich...  \n",
       "234                  is  BERT is Not a Knowledge Base ( Yet ) : Factual...  \n",
       "236            produced  However , just how contextual are the contextu...  \n",
       "241        be explained  In all layers of ELMo , BERT , and GPT-2 , on ...  \n",
       "243           Comparing  Comparing the Geometry of BERT , ELMo , and GP...  \n",
       "\n",
       "[129 rows x 1026 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def elmo_inv(row, pregenerated=None):\n",
    "    v, sent_v = get_phrase_embed_elmo(\n",
    "        elmo,\n",
    "        ' '.join(ast.literal_eval(row['split_tokens'])),\n",
    "        row['averb_span0'],\n",
    "        row['averb_span1'],\n",
    "        row['averb'], pregenerated=pregenerated\n",
    "    )\n",
    "    sent_v_inv = embed_subtract(v, sent_v, 1024)\n",
    "    return sent_v_inv\n",
    "\n",
    "x = pickle.load(open(\"temp/BERT_ELMo.pkl\", \"rb\"))\n",
    "output_elmo_inv = csv.groupby(\n",
    "    csv.index, \n",
    "    group_keys=True, \n",
    "    as_index=False,\n",
    "    sort=True\n",
    ").apply(\n",
    "    lambda group: elmo_inv(group.iloc[0], pregenerated=x)\n",
    ").reset_index(level=1, drop=True)\n",
    "\n",
    "output_elmo_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ebc9095c7955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# visualize_embeds(pd.concat([output_w2v, output_w2v_c]).reset_index())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mvisualize_embeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the vectors\n",
    "# loaned from :\n",
    "# https://github.com/cephcyn/ChatlogGrapher/blob/master/data_processing.ipynb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import altair as alt\n",
    "alt.renderers.enable('default')\n",
    "\n",
    "def visualize_embeds(data, reference, tooltip=['word']):\n",
    "    x = data.iloc[:, 0:300]\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principalDf = pd.DataFrame(\n",
    "        data=principalComponents,\n",
    "        columns=['pc1', 'pc2'])\n",
    "\n",
    "    finalDf = principalDf\n",
    "    finalDf = finalDf.set_index(data.index)\n",
    "    finalDf['word'] = data['word']\n",
    "    finalDf = finalDf.join(\n",
    "        reference, \n",
    "        how='inner',\n",
    "        lsuffix='_embed', \n",
    "        rsuffix='_ref'\n",
    "    )\n",
    "\n",
    "    return alt.Chart(finalDf).mark_circle(size=60).encode(\n",
    "        x='pc1',\n",
    "        y='pc2',\n",
    "#         color='type',\n",
    "        tooltip=tooltip\n",
    "    ).interactive()\n",
    "    return finalDf\n",
    "\n",
    "# visualize_embeds(pd.concat([output_w2v, output_w2v_c]).reset_index())\n",
    "visualize_embeds(output_w2v, csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-ccf369eb2f754e749c3e9c5749eccb6e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ccf369eb2f754e749c3e9c5749eccb6e\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ccf369eb2f754e749c3e9c5749eccb6e\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-ced3aca4d3aff93a10fe9f618b5e6c26\"}, \"mark\": {\"type\": \"circle\", \"size\": 60}, \"encoding\": {\"tooltip\": [{\"type\": \"nominal\", \"field\": \"word\"}, {\"type\": \"nominal\", \"field\": \"Text\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"pc1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"pc2\"}}, \"selection\": {\"selector004\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-ced3aca4d3aff93a10fe9f618b5e6c26\": [{\"pc1\": -5.027299579454698, \"pc2\": 1.4085076059412975, \"word\": \"stands\", \"split_0\": \"We introduce\", \"split_1\": \"a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers\", \"split_2\": \".\", \"averb\": \"stands\", \"averb_s\": \"['which', 'We introduce a new language representation model called BERT ,']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 69.0, \"averb_span1\": 76.0, \"averb_cspan0\": 56.0, \"averb_cspan1\": 63.0, \"root\": \"stands\", \"root_full\": \"stands\", \"root_s\": \"['which', 'We introduce a new language representation model called BERT ,']\", \"root_o\": \"[]\", \"root_split\": 1, \"root_span0\": 69, \"root_span1\": 76, \"root_cspan0\": 56, \"root_cspan1\": 63, \"fverb\": null, \"fword\": null, \"apos\": \"['VERB']\", \"apos_w\": \"['stands']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\", \"split_tokens\": \"['We', 'introduce', 'a', 'new', 'language', 'representation', 'model', 'called', 'BERT', ',', 'which', 'stands', 'for', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '.']\", \"split_anchor_span\": \"(2, 18)\", \"split_anchor_indices\": \"(12, 134)\", \"within_anchor_index\": 43.0}, {\"pc1\": 7.0407395622049505, \"pc2\": -1.9996913066592388, \"word\": \"is designed\", \"split_0\": \"Unlike recent language representation models ,\", \"split_1\": \"BERT\", \"split_2\": \"is designed to pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .\", \"averb\": \"is designed\", \"averb_s\": \"['recent language representation models , BERT']\", \"averb_o\": \"['pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 52.0, \"averb_span1\": 64.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"designed\", \"root_full\": \"is designed\", \"root_s\": \"['recent language representation models , BERT']\", \"root_o\": \"['pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .']\", \"root_split\": 2, \"root_span0\": 52, \"root_span1\": 64, \"root_cspan0\": 0, \"root_cspan1\": 12, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'PUNCT', 'NOUN']\", \"apos_w\": \"['BERT', ',', 'models']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\", \"split_tokens\": \"['Unlike', 'recent', 'language', 'representation', 'models', ',', 'BERT', 'is', 'designed', 'to', 'pre', '-', 'train', 'deep', 'bidirectional', 'representations', 'from', 'unlabeled', 'text', 'by', 'jointly', 'conditioning', 'on', 'both', 'left', 'and', 'right', 'context', 'in', 'all', 'layers', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(46, 50)\", \"within_anchor_index\": 0.0}, {\"pc1\": -19.481493525997916, \"pc2\": -2.361124952554243, \"word\": \"is\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"is conceptually simple and empirically powerful .\", \"averb\": \"is\", \"averb_s\": \"['BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 5.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"conceptually\", \"root_full\": \"is conceptually\", \"root_s\": \"[]\", \"root_o\": \"['simple empirically', 'powerful']\", \"root_split\": 2, \"root_span0\": 5, \"root_span1\": 21, \"root_cspan0\": 0, \"root_cspan1\": 16, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX', 'ADV']\", \"apos_w\": \"['BERT', 'is', 'conceptually']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"BERT is conceptually simple and empirically powerful.\", \"split_tokens\": \"['BERT', 'is', 'conceptually', 'simple', 'and', 'empirically', 'powerful', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": 23.643418951386405, \"pc2\": 4.099821183848874, \"word\": \"obtains\", \"split_0\": null, \"split_1\": \"It\", \"split_2\": \"obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQuAD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement )\", \"averb\": \"obtains\", \"averb_s\": \"[]\", \"averb_o\": \"['new state - - the -', 'of art', 'results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % % absolute improvement']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"point\", \"root_full\": \"point\", \"root_s\": \"['It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test 93.2 ( 1.5 and SQuAD v2.0 Test F1 to 83.1 ( 5.1']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 353, \"root_span1\": 359, \"root_cspan0\": 350, \"root_cspan1\": 356, \"fverb\": \"obtains\", \"fword\": \"obtains\", \"apos\": \"['PRON', 'VERB', 'NOUN']\", \"apos_w\": \"['It', 'obtains', 'question']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)\", \"split_tokens\": \"['It', 'obtains', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'eleven', 'natural', 'language', 'processing', 'tasks', ',', 'including', 'pushing', 'the', 'GLUE', 'score', 'to', '80.5', '%', '(', '7.7', '%', 'point', 'absolute', 'improvement', ')', ',', 'MultiNLI', 'accuracy', 'to', '86.7', '%', '(', '4.6', '%', 'absolute', 'improvement', ')', ',', 'SQuAD', 'v1.1', 'question', 'answering', 'Test', 'F1', 'to', '93.2', '(', '1.5', 'point', 'absolute', 'improvement', ')', 'and', 'SQuAD', 'v2.0', 'Test', 'F1', 'to', '83.1', '(', '5.1', 'point', 'absolute', 'improvement', ')']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 2)\", \"within_anchor_index\": -1.0}, {\"pc1\": -4.242038328720681, \"pc2\": 0.11606369200703022, \"word\": \"present\", \"split_0\": \"We present a replication study of\", \"split_1\": \"BERT\", \"split_2\": \"pretraining ( Devlin et al . , 2019 ) that carefully measures the impact of many key hyperparameters and training data size .\", \"averb\": \"present\", \"averb_s\": \"['We']\", \"averb_o\": \"['a replication study of BERT pretraining ( Devlin et al . , 2019 )']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"measures\", \"root_full\": \"measures\", \"root_s\": \"['that']\", \"root_o\": \"['the impact of many key hyperparameters and training data size .']\", \"root_split\": 2, \"root_span0\": 92, \"root_span1\": 101, \"root_cspan0\": 53, \"root_cspan1\": 62, \"fverb\": \"measures\", \"fword\": \"pretraining\", \"apos\": \"['PROPN', 'PROPN', 'ADP']\", \"apos_w\": \"['BERT', 'et', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"ID\": 1, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size.\", \"split_tokens\": \"['We', 'present', 'a', 'replication', 'study', 'of', 'BERT', 'pretraining', '(', 'Devlin', 'et', 'al', '.', ',', '2019', ')', 'that', 'carefully', 'measures', 'the', 'impact', 'of', 'many', 'key', 'hyperparameters', 'and', 'training', 'data', 'size', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(33, 37)\", \"within_anchor_index\": 0.0}, {\"pc1\": -7.569527234352897, \"pc2\": 0.20658959537505706, \"word\": \"find\", \"split_0\": \"We find that\", \"split_1\": \"BERT\", \"split_2\": \"was significantly undertrained , and can match or exceed the performance of every model published after it .\", \"averb\": \"find\", \"averb_s\": \"[]\", \"averb_o\": \"['that BERT was significantly undertrained , and can match or exceed the performance of every model published after it .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 8.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 8.0, \"root\": \"We\", \"root_full\": \"We\", \"root_s\": \"[]\", \"root_o\": \"['find that BERT was significantly undertrained , and can match or exceed the performance of every model published after it .']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"was\", \"fword\": \"was\", \"apos\": \"['PROPN', 'ADV', 'VERB']\", \"apos_w\": \"['BERT', 'significantly', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"ID\": 1, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.\", \"split_tokens\": \"['We', 'find', 'that', 'BERT', 'was', 'significantly', 'undertrained', ',', 'and', 'can', 'match', 'or', 'exceed', 'the', 'performance', 'of', 'every', 'model', 'published', 'after', 'it', '.']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(12, 16)\", \"within_anchor_index\": 0.0}, {\"pc1\": -7.554471887006396, \"pc2\": 0.15637429937935796, \"word\": \"published\", \"split_0\": \"We find that BERT was significantly undertrained , and can match or exceed the performance of every model published after\", \"split_1\": \"it\", \"split_2\": \".\", \"averb\": \"published\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 106.0, \"averb_span1\": 116.0, \"averb_cspan0\": 106.0, \"averb_cspan1\": 116.0, \"root\": \"We\", \"root_full\": \"We\", \"root_s\": \"[]\", \"root_o\": \"['find that BERT was significantly undertrained , and can match or exceed the performance of every model published after it .']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": null, \"fword\": null, \"apos\": \"['PRON', 'ADP', 'VERB']\", \"apos_w\": \"['it', 'after', 'published']\", \"URL\": \"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"ID\": 1, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.\", \"split_tokens\": \"['We', 'find', 'that', 'BERT', 'was', 'significantly', 'undertrained', ',', 'and', 'can', 'match', 'or', 'exceed', 'the', 'performance', 'of', 'every', 'model', 'published', 'after', 'it', '.']\", \"split_anchor_span\": \"(20, 21)\", \"split_anchor_indices\": \"(121, 123)\", \"within_anchor_index\": -1.0}, {\"pc1\": 10.232966705344989, \"pc2\": 2.0488353733776488, \"word\": \"called\", \"split_0\": \"In this work , we propose a method to pre-train a smaller general-purpose language representation model , called\", \"split_1\": \"DistilBERT\", \"split_2\": \", which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts .\", \"averb\": \"called\", \"averb_s\": \"[]\", \"averb_o\": \"['DistilBERT', ',']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 110.0, \"averb_span1\": 117.0, \"averb_cspan0\": 110.0, \"averb_cspan1\": 117.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['a method to pre - train a smaller general - purpose language representation model , called DistilBERT ,', 'which can then be fine - tuned with good performances on a wide range of tasks like its larger counterparts .']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 26, \"root_cspan0\": 18, \"root_cspan1\": 26, \"fverb\": \"be\", \"fword\": \"which\", \"apos\": \"['NUM', 'VERB', 'NOUN']\", \"apos_w\": \"['DistilBERT', 'called', 'model']\", \"URL\": \"https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38\", \"ID\": 2, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts.\", \"split_tokens\": \"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'method', 'to', 'pre-train', 'a', 'smaller', 'general-purpose', 'language', 'representation', 'model', ',', 'called', 'DistilBERT', ',', 'which', 'can', 'then', 'be', 'fine-tuned', 'with', 'good', 'performances', 'on', 'a', 'wide', 'range', 'of', 'tasks', 'like', 'its', 'larger', 'counterparts', '.']\", \"split_anchor_span\": \"(18, 19)\", \"split_anchor_indices\": \"(112, 122)\", \"within_anchor_index\": 6.0}, {\"pc1\": 25.843346850503192, \"pc2\": -2.978038565999207, \"word\": \"to reduce\", \"split_0\": \"While most prior work investigated the use of distillation for building task - specific models , we leverage knowledge distillation during the pre - training phase and show that it is possible to reduce the size of\", \"split_1\": \"a BERT model\", \"split_2\": \"by 40 % , while retaining 97 % of its language understanding capabilities and being 60 % faster .\", \"averb\": \"to reduce\", \"averb_s\": \"[]\", \"averb_o\": \"['we leverage knowledge distillation during phase', 'the size of a BERT model']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 193.0, \"averb_span1\": 203.0, \"averb_cspan0\": 193.0, \"averb_cspan1\": 203.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 168, \"root_span1\": 173, \"root_cspan0\": 168, \"root_cspan1\": 173, \"fverb\": \"retaining\", \"fword\": \"by\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['model', 'of', 'size']\", \"URL\": \"https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38\", \"ID\": 2, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.\", \"split_tokens\": \"['While', 'most', 'prior', 'work', 'investigated', 'the', 'use', 'of', 'distillation', 'for', 'building', 'task', '-', 'specific', 'models', ',', 'we', 'leverage', 'knowledge', 'distillation', 'during', 'the', 'pre', '-', 'training', 'phase', 'and', 'show', 'that', 'it', 'is', 'possible', 'to', 'reduce', 'the', 'size', 'of', 'a', 'BERT', 'model', 'by', '40', '%', ',', 'while', 'retaining', '97', '%', 'of', 'its', 'language', 'understanding', 'capabilities', 'and', 'being', '60', '%', 'faster', '.']\", \"split_anchor_span\": \"(37, 40)\", \"split_anchor_indices\": \"(214, 226)\", \"within_anchor_index\": 2.0}, {\"pc1\": 25.8086145559984, \"pc2\": -2.854357305221809, \"word\": \"retaining\", \"split_0\": \"While most prior work investigated the use of distillation for building task - specific models , we leverage knowledge distillation during the pre - training phase and show that it is possible to reduce the size of a BERT model by 40 % , while retaining 97 % of\", \"split_1\": \"its\", \"split_2\": \"language understanding capabilities and being 60 % faster .\", \"averb\": \"retaining\", \"averb_s\": \"[]\", \"averb_o\": \"['97 % of its language understanding capabilities and being 60 % faster .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 244.0, \"averb_span1\": 254.0, \"averb_cspan0\": 244.0, \"averb_cspan1\": 254.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 168, \"root_span1\": 173, \"root_cspan0\": 168, \"root_cspan1\": 173, \"fverb\": \"being\", \"fword\": \"language\", \"apos\": \"['DET', 'NOUN', 'ADP']\", \"apos_w\": \"['its', 'capabilities', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38\", \"ID\": 2, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.\", \"split_tokens\": \"['While', 'most', 'prior', 'work', 'investigated', 'the', 'use', 'of', 'distillation', 'for', 'building', 'task', '-', 'specific', 'models', ',', 'we', 'leverage', 'knowledge', 'distillation', 'during', 'the', 'pre', '-', 'training', 'phase', 'and', 'show', 'that', 'it', 'is', 'possible', 'to', 'reduce', 'the', 'size', 'of', 'a', 'BERT', 'model', 'by', '40', '%', ',', 'while', 'retaining', '97', '%', 'of', 'its', 'language', 'understanding', 'capabilities', 'and', 'being', '60', '%', 'faster', '.']\", \"split_anchor_span\": \"(49, 50)\", \"split_anchor_indices\": \"(261, 264)\", \"within_anchor_index\": -1.0}, {\"pc1\": -8.83585522173095, \"pc2\": -4.925019952977334, \"word\": \"focus\", \"split_0\": \"We focus on\", \"split_1\": \"one such model , BERT\", \"split_2\": \", and aim to quantify where linguistic information is captured within the network .\", \"averb\": \"focus\", \"averb_s\": \"['We']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 9.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 9.0, \"root\": \"captured\", \"root_full\": \"is captured\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 85, \"root_span1\": 97, \"root_cspan0\": 51, \"root_cspan1\": 63, \"fverb\": \"quantify\", \"fword\": \"and\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['model', 'on', 'focus']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c\", \"ID\": 3, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network.\", \"split_tokens\": \"['We', 'focus', 'on', 'one', 'such', 'model', ',', 'BERT', ',', 'and', 'aim', 'to', 'quantify', 'where', 'linguistic', 'information', 'is', 'captured', 'within', 'the', 'network', '.']\", \"split_anchor_span\": \"(3, 8)\", \"split_anchor_indices\": \"(11, 32)\", \"within_anchor_index\": 17.0}, {\"pc1\": 14.739428579309152, \"pc2\": 6.662083893315241, \"word\": \"represents\", \"split_0\": \"We find that\", \"split_1\": \"the model\", \"split_2\": \"represents the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence : POS tagging , parsing , NER , semantic roles , then coreference .\", \"averb\": \"represents\", \"averb_s\": \"['the model']\", \"averb_o\": \"['the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence :']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 23.0, \"averb_span1\": 34.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 11.0, \"root\": \"roles\", \"root_full\": \"roles\", \"root_s\": \"['POS tagging , parsing ,', 'NER', ',', 'semantic']\", \"root_o\": \"[',', 'We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence : then coreference .']\", \"root_split\": 2, \"root_span0\": 238, \"root_span1\": 244, \"root_cspan0\": 215, \"root_cspan1\": 221, \"fverb\": \"represents\", \"fword\": \"represents\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['model', 'represents', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c\", \"ID\": 3, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference.\", \"split_tokens\": \"['We', 'find', 'that', 'the', 'model', 'represents', 'the', 'steps', 'of', 'the', 'traditional', 'NLP', 'pipeline', 'in', 'an', 'interpretable', 'and', 'localizable', 'way', ',', 'and', 'that', 'the', 'regions', 'responsible', 'for', 'each', 'step', 'appear', 'in', 'the', 'expected', 'sequence', ':', 'POS', 'tagging', ',', 'parsing', ',', 'NER', ',', 'semantic', 'roles', ',', 'then', 'coreference', '.']\", \"split_anchor_span\": \"(3, 5)\", \"split_anchor_indices\": \"(12, 21)\", \"within_anchor_index\": -1.0}, {\"pc1\": 7.559583333451219, \"pc2\": -3.4867150220326257, \"word\": \"does adjust\", \"split_0\": \"Qualitative analysis reveals that\", \"split_1\": \"the model\", \"split_2\": \"can and often does adjust this pipeline dynamically , revising lower - level decisions on the basis of disambiguating information from higher - level representations .\", \"averb\": \"does adjust\", \"averb_s\": \"['the model can and often']\", \"averb_o\": \"['this pipeline dynamically , revising lower - level decisions on the basis of disambiguating information from higher - level representations .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 58.0, \"averb_span1\": 70.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 26.0, \"root\": \"analysis\", \"root_full\": \"analysis\", \"root_s\": \"[]\", \"root_o\": \"['reveals that the model can and often does adjust this pipeline dynamically , revising lower - level decisions on the basis of disambiguating information from higher - level representations .']\", \"root_split\": 0, \"root_span0\": 12, \"root_span1\": 21, \"root_cspan0\": 12, \"root_cspan1\": 21, \"fverb\": \"does\", \"fword\": \"can\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['model', 'adjust', 'reveals']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c\", \"ID\": 3, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.\", \"split_tokens\": \"['Qualitative', 'analysis', 'reveals', 'that', 'the', 'model', 'can', 'and', 'often', 'does', 'adjust', 'this', 'pipeline', 'dynamically', ',', 'revising', 'lower', '-', 'level', 'decisions', 'on', 'the', 'basis', 'of', 'disambiguating', 'information', 'from', 'higher', '-', 'level', 'representations', '.']\", \"split_anchor_span\": \"(4, 6)\", \"split_anchor_indices\": \"(33, 42)\", \"within_anchor_index\": -1.0}, {\"pc1\": 4.51492204554875, \"pc2\": 2.691472637831146, \"word\": \"have had\", \"split_0\": null, \"split_1\": \"Large pre - trained neural networks such as BERT\", \"split_2\": \"have had great recent success in NLP , motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data .\", \"averb\": \"have had\", \"averb_s\": \"['Large pre - trained neural networks such as BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 49.0, \"averb_span1\": 58.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"success\", \"root_full\": \"success\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 22, \"root_cspan1\": 30, \"fverb\": \"have\", \"fword\": \"have\", \"apos\": \"['ADJ', 'VERB', 'ADJ']\", \"apos_w\": \"['Large', 'had', 'recent']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.\", \"split_tokens\": \"['Large', 'pre', '-', 'trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']\", \"split_anchor_span\": \"(0, 9)\", \"split_anchor_indices\": \"(0, 48)\", \"within_anchor_index\": 44.0}, {\"pc1\": 4.549077426766249, \"pc2\": 2.6372065995958454, \"word\": \"motivating\", \"split_0\": \"Large pre - trained neural networks such as BERT have had great recent success in NLP , motivating a growing body of research investigating what aspects of language\", \"split_1\": \"they\", \"split_2\": \"are able to learn from unlabeled data .\", \"averb\": \"motivating\", \"averb_s\": \"[',']\", \"averb_o\": \"['a growing body of research investigating what']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 88.0, \"averb_span1\": 99.0, \"averb_cspan0\": 88.0, \"averb_cspan1\": 99.0, \"root\": \"success\", \"root_full\": \"success\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 71, \"root_cspan1\": 79, \"fverb\": \"are\", \"fword\": \"are\", \"apos\": \"['PRON', 'ADJ', 'VERB']\", \"apos_w\": \"['they', 'able', 'motivating']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.\", \"split_tokens\": \"['Large', 'pre', '-', 'trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']\", \"split_anchor_span\": \"(28, 29)\", \"split_anchor_indices\": \"(164, 168)\", \"within_anchor_index\": -1.0}, {\"pc1\": 4.51492204554875, \"pc2\": 2.691472637831146, \"word\": \"have had\", \"split_0\": \"Large pre - trained neural networks such as\", \"split_1\": \"BERT\", \"split_2\": \"have had great recent success in NLP , motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data .\", \"averb\": \"have had\", \"averb_s\": \"['Large pre - trained neural networks such as BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 49.0, \"averb_span1\": 58.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"success\", \"root_full\": \"success\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 22, \"root_cspan1\": 30, \"fverb\": \"have\", \"fword\": \"have\", \"apos\": \"['PROPN', 'SCONJ', 'NOUN']\", \"apos_w\": \"['BERT', 'as', 'networks']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.\", \"split_tokens\": \"['Large', 'pre', '-', 'trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(43, 47)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.193372354001743, \"pc2\": -0.65700003968787, \"word\": \"apply\", \"split_0\": \"Complementary to these works , we propose methods for analyzing the attention mechanisms of pre - trained models and apply them to\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"apply\", \"averb_s\": \"[]\", \"averb_o\": \"['them']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 117.0, \"averb_span1\": 123.0, \"averb_cspan0\": 117.0, \"averb_cspan1\": 123.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['Complementary to these works', ',', 'we']\", \"root_o\": \"['methods for analyzing the attention mechanisms of pre - trained models and apply them to BERT']\", \"root_split\": 0, \"root_span0\": 34, \"root_span1\": 42, \"root_cspan0\": 34, \"root_cspan1\": 42, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'to', 'apply']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT.\", \"split_tokens\": \"['Complementary', 'to', 'these', 'works', ',', 'we', 'propose', 'methods', 'for', 'analyzing', 'the', 'attention', 'mechanisms', 'of', 'pre', '-', 'trained', 'models', 'and', 'apply', 'them', 'to', 'BERT', '.']\", \"split_anchor_span\": \"(22, 23)\", \"split_anchor_indices\": \"(130, 134)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.058580038682, \"pc2\": -8.01781972016248, \"word\": \"is captured\", \"split_0\": \"Lastly , we propose an attention - based probing classifier and use it to further demonstrate that substantial syntactic information is captured in\", \"split_1\": \"BERT 's\", \"split_2\": \"attention .\", \"averb\": \"is captured\", \"averb_s\": \"['substantial syntactic information']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 133.0, \"averb_span1\": 145.0, \"averb_cspan0\": 133.0, \"averb_cspan1\": 145.0, \"root\": \"Lastly\", \"root_full\": \"Lastly\", \"root_s\": \"[]\", \"root_o\": \"[\\\"we propose an attention - based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT 's attention .\\\"]\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 7, \"root_cspan0\": 0, \"root_cspan1\": 7, \"fverb\": null, \"fword\": \"attention\", \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'in', 'captured']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.\", \"split_tokens\": \"['Lastly', ',', 'we', 'propose', 'an', 'attention', '-', 'based', 'probing', 'classifier', 'and', 'use', 'it', 'to', 'further', 'demonstrate', 'that', 'substantial', 'syntactic', 'information', 'is', 'captured', 'in', 'BERT', \\\"'s\\\", 'attention', '.']\", \"split_anchor_span\": \"(23, 25)\", \"split_anchor_indices\": \"(147, 154)\", \"within_anchor_index\": 0.0}, {\"pc1\": -13.212634403734524, \"pc2\": 0.14307492887205825, \"word\": \"describe\", \"split_0\": \"In this paper , we describe\", \"split_1\": \"a simple re - implementation of BERT for query - based passage re - ranking\", \"split_2\": \".\", \"averb\": \"describe\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"['a simple re - implementation of BERT for query - based passage re - ranking']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 28.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 28.0, \"root\": \"describe\", \"root_full\": \"describe\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['a simple re - implementation of BERT for query - based passage re - ranking']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 28, \"root_cspan0\": 19, \"root_cspan1\": 28, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['implementation', 'describe']\", \"URL\": \"https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2\", \"ID\": 5, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'describe', 'a', 'simple', 're', '-', 'implementation', 'of', 'BERT', 'for', 'query', '-', 'based', 'passage', 're', '-', 'ranking', '.']\", \"split_anchor_span\": \"(6, 21)\", \"split_anchor_indices\": \"(27, 102)\", \"within_anchor_index\": 32.0}, {\"pc1\": 3.1221147943517216, \"pc2\": 2.936183033039903, \"word\": \"is\", \"split_0\": null, \"split_1\": \"Our system\", \"split_2\": \"is the state of the art on the TREC - CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task , outperforming the previous state of the art by 27 % ( relative ) in MRR@10 .\", \"averb\": \"is\", \"averb_s\": \"['Our system']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 11.0, \"averb_span1\": 14.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"task\", \"root_full\": \"task\", \"root_s\": \"['Our system is the state of the art on the TREC - CAR dataset and the top entry in the leaderboard of', 'the MS MARCO passage retrieval']\", \"root_o\": \"[',', 'outperforming the previous state of the art by 27 % ( relative ) in MRR@10 .']\", \"root_split\": 2, \"root_span0\": 132, \"root_span1\": 137, \"root_cspan0\": 121, \"root_cspan1\": 126, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'AUX', 'NOUN']\", \"apos_w\": \"['system', 'is', 'state']\", \"URL\": \"https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2\", \"ID\": 5, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10.\", \"split_tokens\": \"['Our', 'system', 'is', 'the', 'state', 'of', 'the', 'art', 'on', 'the', 'TREC', '-', 'CAR', 'dataset', 'and', 'the', 'top', 'entry', 'in', 'the', 'leaderboard', 'of', 'the', 'MS', 'MARCO', 'passage', 'retrieval', 'task', ',', 'outperforming', 'the', 'previous', 'state', 'of', 'the', 'art', 'by', '27', '%', '(', 'relative', ')', 'in', 'MRR@10', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 10)\", \"within_anchor_index\": -1.0}, {\"pc1\": 47.12109842223541, \"pc2\": 5.80196323418323, \"word\": \"assess\", \"split_0\": \"I assess the extent to which\", \"split_1\": \"the recently introduced BERT model\", \"split_2\": \"captures English syntactic phenomena , using ( 1 ) naturally - occurring subject - verb agreement stimuli ; ( 2 ) \\\" coloreless green ideas \\\" subject - verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part - of - speech and inflection ; and ( 3 ) manually crafted stimuli for subject - verb agreement and reflexive anaphora phenomena .\", \"averb\": \"assess\", \"averb_s\": \"['I']\", \"averb_o\": \"['the extent the recently introduced BERT model captures English syntactic , using ( 1 ) naturally - occurring subject - verb agreement stimuli ( 2 ) \\\" coloreless green ideas \\\"', 'subject - verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part - of - speech and inflection ; and']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 2.0, \"averb_span1\": 9.0, \"averb_cspan0\": 2.0, \"averb_cspan1\": 9.0, \"root\": \")\", \"root_full\": \")\", \"root_s\": \"['(', '3']\", \"root_o\": \"['I assess the extent to which the recently introduced BERT model captures English syntactic phenomena , using ( 1 ) naturally - occurring subject - verb agreement stimuli ; ( 2 ) \\\" coloreless green ideas \\\" subject - verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part - of - speech and inflection ; and crafted stimuli for subject - verb agreement and reflexive anaphora phenomena .']\", \"root_split\": 2, \"root_span0\": 378, \"root_span1\": 380, \"root_cspan0\": 314, \"root_cspan1\": 316, \"fverb\": \"using\", \"fword\": \"captures\", \"apos\": \"['NOUN', 'ADJ', 'NOUN']\", \"apos_w\": \"['model', 'syntactic', 'extent']\", \"URL\": \"https://www.semanticscholar.org/paper/Assessing-BERT's-Syntactic-Abilities-Goldberg/efeab0dcdb4c1cce5e537e57745d84774be99b9a\", \"ID\": 6, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) \\\"coloreless green ideas\\\" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.\", \"split_tokens\": \"['I', 'assess', 'the', 'extent', 'to', 'which', 'the', 'recently', 'introduced', 'BERT', 'model', 'captures', 'English', 'syntactic', 'phenomena', ',', 'using', '(', '1', ')', 'naturally', '-', 'occurring', 'subject', '-', 'verb', 'agreement', 'stimuli', ';', '(', '2', ')', '\\\"', 'coloreless', 'green', 'ideas', '\\\"', 'subject', '-', 'verb', 'agreement', 'stimuli', ',', 'in', 'which', 'content', 'words', 'in', 'natural', 'sentences', 'are', 'randomly', 'replaced', 'with', 'words', 'sharing', 'the', 'same', 'part', '-', 'of', '-', 'speech', 'and', 'inflection', ';', 'and', '(', '3', ')', 'manually', 'crafted', 'stimuli', 'for', 'subject', '-', 'verb', 'agreement', 'and', 'reflexive', 'anaphora', 'phenomena', '.']\", \"split_anchor_span\": \"(6, 11)\", \"split_anchor_indices\": \"(28, 62)\", \"within_anchor_index\": 24.0}, {\"pc1\": -20.580972504800783, \"pc2\": -0.2982665474299259, \"word\": \"performs\", \"split_0\": null, \"split_1\": \"The BERT model\", \"split_2\": \"performs remarkably well on all cases .\", \"averb\": \"performs\", \"averb_s\": \"['The BERT model']\", \"averb_o\": \"['well on all cases', '.']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 15.0, \"averb_span1\": 24.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"performs\", \"root_full\": \"performs\", \"root_s\": \"['The BERT model']\", \"root_o\": \"['well on all cases', '.']\", \"root_split\": 2, \"root_span0\": 15, \"root_span1\": 24, \"root_cspan0\": 0, \"root_cspan1\": 9, \"fverb\": null, \"fword\": \"performs\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['model', 'performs']\", \"URL\": \"https://www.semanticscholar.org/paper/Assessing-BERT's-Syntactic-Abilities-Goldberg/efeab0dcdb4c1cce5e537e57745d84774be99b9a\", \"ID\": 6, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"The BERT model performs remarkably well on all cases.\", \"split_tokens\": \"['The', 'BERT', 'model', 'performs', 'remarkably', 'well', 'on', 'all', 'cases', '.']\", \"split_anchor_span\": \"(0, 3)\", \"split_anchor_indices\": \"(0, 14)\", \"within_anchor_index\": 4.0}, {\"pc1\": 2.851986042907927, \"pc2\": 2.6942621330254157, \"word\": \"includes\", \"split_0\": \"A new release of\", \"split_1\": \"BERT\", \"split_2\": \"( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task .\", \"averb\": \"includes\", \"averb_s\": \"['new release of BERT ( Devlin , 2018', ')']\", \"averb_o\": \"['a model simultaneously pretrained on 104 languages with impressive performance for zero - shot cross - lingual transfer on a natural language inference task']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 40.0, \"averb_span1\": 49.0, \"averb_cspan0\": 18.0, \"averb_cspan1\": 27.0, \"root\": \"includes\", \"root_full\": \"includes\", \"root_s\": \"['new release of BERT ( Devlin , 2018', ')']\", \"root_o\": \"['a model simultaneously pretrained on 104 languages with impressive performance for zero - shot cross - lingual transfer on a natural language inference task']\", \"root_split\": 2, \"root_span0\": 40, \"root_span1\": 49, \"root_cspan0\": 18, \"root_cspan1\": 27, \"fverb\": \"includes\", \"fword\": \"Devlin\", \"apos\": \"['PROPN', 'PROPN', 'ADP']\", \"apos_w\": \"['BERT', 'Devlin', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task.\", \"split_tokens\": \"['A', 'new', 'release', 'of', 'BERT', '(', 'Devlin', ',', '2018', ')', 'includes', 'a', 'model', 'simultaneously', 'pretrained', 'on', '104', 'languages', 'with', 'impressive', 'performance', 'for', 'zero-shot', 'cross-lingual', 'transfer', 'on', 'a', 'natural', 'language', 'inference', 'task', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(16, 20)\", \"within_anchor_index\": 0.0}, {\"pc1\": 13.699334398729025, \"pc2\": 2.744260874366976, \"word\": \"explores\", \"split_0\": \"This paper explores the broader cross-lingual potential of\", \"split_1\": \"mBERT\", \"split_2\": \"( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families : NLI , document classification , NER , POS tagging , and dependency parsing .\", \"averb\": \"explores\", \"averb_s\": \"['This', 'paper']\", \"averb_o\": \"['the broader cross - lingual potential of mBERT ( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 11.0, \"averb_span1\": 20.0, \"averb_cspan0\": 11.0, \"averb_cspan1\": 20.0, \"root\": \"NER\", \"root_full\": \"NER\", \"root_s\": \"['This paper explores the broader cross - lingual potential of mBERT ( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families : NLI , document classification , .']\", \"root_o\": \"[', POS tagging and dependency parsing', ',']\", \"root_split\": 2, \"root_span0\": 236, \"root_span1\": 240, \"root_cspan0\": 171, \"root_cspan1\": 175, \"fverb\": \"covering\", \"fword\": \"multilingual\", \"apos\": \"['ADJ', 'ADJ', 'ADP']\", \"apos_w\": \"['mBERT', 'multilingual', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing.\", \"split_tokens\": \"['This', 'paper', 'explores', 'the', 'broader', 'cross-lingual', 'potential', 'of', 'mBERT', '(', 'multilingual', ')', 'as', 'a', 'zero', 'shot', 'language', 'transfer', 'model', 'on', '5', 'NLP', 'tasks', 'covering', 'a', 'total', 'of', '39', 'languages', 'from', 'various', 'language', 'families', ':', 'NLI', ',', 'document', 'classification', ',', 'NER', ',', 'POS', 'tagging', ',', 'and', 'dependency', 'parsing', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(58, 63)\", \"within_anchor_index\": 1.0}, {\"pc1\": -7.020161917809986, \"pc2\": -0.23061168066547738, \"word\": \"compare\", \"split_0\": \"We compare\", \"split_1\": \"mBERT\", \"split_2\": \"with the best - published methods for zero - shot cross - lingual transfer and find mBERT competitive on each task .\", \"averb\": \"compare\", \"averb_s\": \"['We']\", \"averb_o\": \"['mBERT with the best - published methods for zero - shot cross']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"find\", \"root_full\": \"find\", \"root_s\": \"['- lingual transfer']\", \"root_o\": \"['We compare mBERT with the best - published methods for zero - shot cross mBERT competitive on each task .']\", \"root_split\": 2, \"root_span0\": 96, \"root_span1\": 101, \"root_cspan0\": 79, \"root_cspan1\": 84, \"fverb\": \"published\", \"fword\": \"with\", \"apos\": \"['NOUN', 'VERB', 'PUNCT']\", \"apos_w\": \"['mBERT', 'compare', '.']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task.\", \"split_tokens\": \"['We', 'compare', 'mBERT', 'with', 'the', 'best', '-', 'published', 'methods', 'for', 'zero', '-', 'shot', 'cross', '-', 'lingual', 'transfer', 'and', 'find', 'mBERT', 'competitive', 'on', 'each', 'task', '.']\", \"split_anchor_span\": \"(2, 3)\", \"split_anchor_indices\": \"(10, 15)\", \"within_anchor_index\": 1.0}, {\"pc1\": -6.972037250171475, \"pc2\": -0.08924948786252551, \"word\": \"find\", \"split_0\": \"We compare mBERT with the best - published methods for zero - shot cross - lingual transfer and find\", \"split_1\": \"mBERT\", \"split_2\": \"competitive on each task .\", \"averb\": \"find\", \"averb_s\": \"['- lingual transfer']\", \"averb_o\": \"['We compare mBERT with the best - published methods for zero - shot cross mBERT competitive on each task .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 96.0, \"averb_span1\": 101.0, \"averb_cspan0\": 96.0, \"averb_cspan1\": 101.0, \"root\": \"find\", \"root_full\": \"find\", \"root_s\": \"['- lingual transfer']\", \"root_o\": \"['We compare mBERT with the best - published methods for zero - shot cross mBERT competitive on each task .']\", \"root_split\": 0, \"root_span0\": 96, \"root_span1\": 101, \"root_cspan0\": 96, \"root_cspan1\": 101, \"fverb\": null, \"fword\": \"competitive\", \"apos\": \"['NOUN', 'ADJ', 'VERB']\", \"apos_w\": \"['mBERT', 'competitive', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task.\", \"split_tokens\": \"['We', 'compare', 'mBERT', 'with', 'the', 'best', '-', 'published', 'methods', 'for', 'zero', '-', 'shot', 'cross', '-', 'lingual', 'transfer', 'and', 'find', 'mBERT', 'competitive', 'on', 'each', 'task', '.']\", \"split_anchor_span\": \"(19, 20)\", \"split_anchor_indices\": \"(100, 105)\", \"within_anchor_index\": 1.0}, {\"pc1\": 11.082563534886313, \"pc2\": -0.8572766322932626, \"word\": \"investigate\", \"split_0\": \"Additionally , we investigate the most effective strategy for utilizing\", \"split_1\": \"mBERT\", \"split_2\": \"in this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .\", \"averb\": \"investigate\", \"averb_s\": \"['Additionally']\", \"averb_o\": \"['the most effective strategy for utilizing mBERT in', 'this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 18.0, \"averb_span1\": 30.0, \"averb_cspan0\": 18.0, \"averb_cspan1\": 30.0, \"root\": \"investigate\", \"root_full\": \"investigate\", \"root_s\": \"['Additionally']\", \"root_o\": \"['the most effective strategy for utilizing mBERT in', 'this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 30, \"root_cspan0\": 18, \"root_cspan1\": 30, \"fverb\": \"generalizes\", \"fword\": \"in\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['mBERT', 'for', 'strategy']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.\", \"split_tokens\": \"['Additionally', ',', 'we', 'investigate', 'the', 'most', 'effective', 'strategy', 'for', 'utilizing', 'mBERT', 'in', 'this', 'manner', ',', 'determine', 'to', 'what', 'extent', 'mBERT', 'generalizes', 'away', 'from', 'language', 'specific', 'features', ',', 'and', 'measure', 'factors', 'that', 'influence', 'cross', '-', 'lingual', 'transfer', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(71, 76)\", \"within_anchor_index\": 1.0}, {\"pc1\": 11.04498013755823, \"pc2\": -0.9528825221833173, \"word\": \"generalizes\", \"split_0\": \"Additionally , we investigate the most effective strategy for utilizing mBERT in this manner , determine to what extent\", \"split_1\": \"mBERT\", \"split_2\": \"generalizes away from language specific features , and measure factors that influence cross - lingual transfer .\", \"averb\": \"generalizes\", \"averb_s\": \"['what extent mBERT']\", \"averb_o\": \"['away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 126.0, \"averb_span1\": 138.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"investigate\", \"root_full\": \"investigate\", \"root_s\": \"['Additionally']\", \"root_o\": \"['the most effective strategy for utilizing mBERT in', 'this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 30, \"root_cspan0\": 18, \"root_cspan1\": 30, \"fverb\": \"generalizes\", \"fword\": \"generalizes\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['mBERT', 'generalizes', 'determine']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.\", \"split_tokens\": \"['Additionally', ',', 'we', 'investigate', 'the', 'most', 'effective', 'strategy', 'for', 'utilizing', 'mBERT', 'in', 'this', 'manner', ',', 'determine', 'to', 'what', 'extent', 'mBERT', 'generalizes', 'away', 'from', 'language', 'specific', 'features', ',', 'and', 'measure', 'factors', 'that', 'influence', 'cross', '-', 'lingual', 'transfer', '.']\", \"split_anchor_span\": \"(19, 20)\", \"split_anchor_indices\": \"(119, 124)\", \"within_anchor_index\": 1.0}, {\"pc1\": -7.953914187895203, \"pc2\": -1.3184449060424797, \"word\": \"pre\", \"split_0\": \"Language model pre - training , such as\", \"split_1\": \"BERT\", \"split_2\": \", has significantly improved the performances of many natural language processing tasks .\", \"averb\": \"pre\", \"averb_s\": \"['Language model']\", \"averb_o\": \"['-', 'training , such as BERT ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 15.0, \"averb_span1\": 19.0, \"averb_cspan0\": 15.0, \"averb_cspan1\": 19.0, \"root\": \"improved\", \"root_full\": \"has improved\", \"root_s\": \"['Language model pre - training , such as BERT ,', 'significantly']\", \"root_o\": \"['the performances of many natural language processing tasks', '.']\", \"root_split\": 2, \"root_span0\": 47, \"root_span1\": 74, \"root_cspan0\": 2, \"root_cspan1\": 29, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'SCONJ', 'ADJ']\", \"apos_w\": \"['BERT', 'as', 'training']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks.\", \"split_tokens\": \"['Language', 'model', 'pre', '-', 'training', ',', 'such', 'as', 'BERT', ',', 'has', 'significantly', 'improved', 'the', 'performances', 'of', 'many', 'natural', 'language', 'processing', 'tasks', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(39, 43)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.599255745279424, \"pc2\": 4.05367219690756, \"word\": \"can be transferred\", \"split_0\": \"By leveraging this new KD method , the plenty of knowledge encoded in a large teacher BERT can be well transferred to\", \"split_1\": \"a small student TinyBERT\", \"split_2\": \".\", \"averb\": \"can be transferred\", \"averb_s\": \"['the plenty of knowledge encoded in a large teacher BERT']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 91.0, \"averb_span1\": 115.0, \"averb_cspan0\": 91.0, \"averb_cspan1\": 115.0, \"root\": \"transferred\", \"root_full\": \"can be transferred\", \"root_s\": \"['the plenty of knowledge encoded in a large teacher BERT']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 91, \"root_span1\": 115, \"root_cspan0\": 91, \"root_cspan1\": 115, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['student', 'to', 'transferred']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT.\", \"split_tokens\": \"['By', 'leveraging', 'this', 'new', 'KD', 'method', ',', 'the', 'plenty', 'of', 'knowledge', 'encoded', 'in', 'a', 'large', 'teacher', 'BERT', 'can', 'be', 'well', 'transferred', 'to', 'a', 'small', 'student', 'TinyBERT', '.']\", \"split_anchor_span\": \"(22, 26)\", \"split_anchor_indices\": \"(117, 141)\", \"within_anchor_index\": 20.0}, {\"pc1\": -5.483065296118177, \"pc2\": 4.113875577053673, \"word\": \"encoded\", \"split_0\": \"By leveraging this new KD method , the plenty of knowledge encoded in\", \"split_1\": \"a large teacher BERT\", \"split_2\": \"can be well transferred to a small student TinyBERT .\", \"averb\": \"encoded\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 59.0, \"averb_span1\": 67.0, \"averb_cspan0\": 59.0, \"averb_cspan1\": 67.0, \"root\": \"transferred\", \"root_full\": \"can be transferred\", \"root_s\": \"['the plenty of knowledge encoded in a large teacher BERT']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 91, \"root_span1\": 115, \"root_cspan0\": 0, \"root_cspan1\": 24, \"fverb\": \"be\", \"fword\": \"can\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'in', 'encoded']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT.\", \"split_tokens\": \"['By', 'leveraging', 'this', 'new', 'KD', 'method', ',', 'the', 'plenty', 'of', 'knowledge', 'encoded', 'in', 'a', 'large', 'teacher', 'BERT', 'can', 'be', 'well', 'transferred', 'to', 'a', 'small', 'student', 'TinyBERT', '.']\", \"split_anchor_span\": \"(13, 17)\", \"split_anchor_indices\": \"(69, 89)\", \"within_anchor_index\": 16.0}, {\"pc1\": 1.0637101240115032, \"pc2\": -1.48614474548258, \"word\": \"introduce\", \"split_0\": \"Moreover , we introduce a new two - stage learning framework for\", \"split_1\": \"TinyBERT\", \"split_2\": \", which performs transformer distillation at both the pre - training and task - specific learning stages .\", \"averb\": \"introduce\", \"averb_s\": \"['we', ',']\", \"averb_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 14.0, \"averb_span1\": 24.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 24.0, \"root\": \"introduce\", \"root_full\": \"introduce\", \"root_s\": \"['we', ',']\", \"root_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"root_split\": 0, \"root_span0\": 14, \"root_span1\": 24, \"root_cspan0\": 14, \"root_cspan1\": 24, \"fverb\": \"performs\", \"fword\": \"which\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['TinyBERT', 'for', 'framework']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages.\", \"split_tokens\": \"['Moreover', ',', 'we', 'introduce', 'a', 'new', 'two', '-', 'stage', 'learning', 'framework', 'for', 'TinyBERT', ',', 'which', 'performs', 'transformer', 'distillation', 'at', 'both', 'the', 'pre', '-', 'training', 'and', 'task', '-', 'specific', 'learning', 'stages', '.']\", \"split_anchor_span\": \"(12, 13)\", \"split_anchor_indices\": \"(64, 72)\", \"within_anchor_index\": 4.0}, {\"pc1\": 1.0637101240115023, \"pc2\": -1.4861447454825814, \"word\": \"introduce\", \"split_0\": \"Moreover , we introduce\", \"split_1\": \"a new two - stage learning framework for TinyBERT , which performs transformer distillation at both the pre - training and task - specific learning stages\", \"split_2\": \".\", \"averb\": \"introduce\", \"averb_s\": \"['we', ',']\", \"averb_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"averb_relation\": 0.0, \"averb_split\": 0.0, \"averb_span0\": 14.0, \"averb_span1\": 24.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 24.0, \"root\": \"introduce\", \"root_full\": \"introduce\", \"root_s\": \"['we', ',']\", \"root_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"root_split\": 0, \"root_span0\": 14, \"root_span1\": 24, \"root_cspan0\": 14, \"root_cspan1\": 24, \"fverb\": null, \"fword\": null, \"apos\": \"['VERB']\", \"apos_w\": \"['introduce']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages.\", \"split_tokens\": \"['Moreover', ',', 'we', 'introduce', 'a', 'new', 'two', '-', 'stage', 'learning', 'framework', 'for', 'TinyBERT', ',', 'which', 'performs', 'transformer', 'distillation', 'at', 'both', 'the', 'pre', '-', 'training', 'and', 'task', '-', 'specific', 'learning', 'stages', '.']\", \"split_anchor_span\": \"(4, 30)\", \"split_anchor_indices\": \"(23, 177)\", \"within_anchor_index\": 45.0}, {\"pc1\": -8.539047417815889, \"pc2\": 1.067715453485628, \"word\": \"can capture\", \"split_0\": \"This framework ensures that\", \"split_1\": \"TinyBERT\", \"split_2\": \"can capture both the general - domain and task - specific knowledge of the teacher BERT .\", \"averb\": \"can capture\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"['both the general - domain and', 'task - specific knowledge of the teacher BERT']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 37.0, \"averb_span1\": 49.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"ensures\", \"root_full\": \"ensures\", \"root_s\": \"['This', 'framework']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 23, \"root_cspan0\": 15, \"root_cspan1\": 23, \"fverb\": \"capture\", \"fword\": \"can\", \"apos\": \"['PROPN', 'VERB', 'VERB']\", \"apos_w\": \"['TinyBERT', 'capture', 'ensures']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.\", \"split_tokens\": \"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general', '-', 'domain', 'and', 'task', '-', 'specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(27, 35)\", \"within_anchor_index\": 4.0}, {\"pc1\": -8.466322822370413, \"pc2\": 1.0113408792548597, \"word\": \"ensures\", \"split_0\": null, \"split_1\": \"This framework\", \"split_2\": \"ensures that TinyBERT can capture both the general - domain and task - specific knowledge of the teacher BERT .\", \"averb\": \"ensures\", \"averb_s\": \"['This', 'framework']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 2.0, \"averb_span0\": 15.0, \"averb_span1\": 23.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"ensures\", \"root_full\": \"ensures\", \"root_s\": \"['This', 'framework']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 15, \"root_span1\": 23, \"root_cspan0\": 0, \"root_cspan1\": 8, \"fverb\": \"ensures\", \"fword\": \"ensures\", \"apos\": \"['VERB']\", \"apos_w\": \"['ensures']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.\", \"split_tokens\": \"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general', '-', 'domain', 'and', 'task', '-', 'specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 14)\", \"within_anchor_index\": -1.0}, {\"pc1\": -8.539047417815889, \"pc2\": 1.067715453485628, \"word\": \"can capture\", \"split_0\": \"This framework ensures that TinyBERT can capture both the general - domain and task - specific knowledge of\", \"split_1\": \"the teacher BERT\", \"split_2\": \".\", \"averb\": \"can capture\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"['both the general - domain and', 'task - specific knowledge of the teacher BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 37.0, \"averb_span1\": 49.0, \"averb_cspan0\": 37.0, \"averb_cspan1\": 49.0, \"root\": \"ensures\", \"root_full\": \"ensures\", \"root_s\": \"['This', 'framework']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 23, \"root_cspan0\": 15, \"root_cspan1\": 23, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['teacher', 'of', 'knowledge']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.\", \"split_tokens\": \"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general', '-', 'domain', 'and', 'task', '-', 'specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']\", \"split_anchor_span\": \"(18, 21)\", \"split_anchor_indices\": \"(107, 123)\", \"within_anchor_index\": 12.0}, {\"pc1\": -5.635126084806319, \"pc2\": 3.49654090279163, \"word\": \"is\", \"split_0\": null, \"split_1\": \"TinyBERT\", \"split_2\": \"is empirically effective and achieves comparable results with BERT in GLUE datasets , while being 7.5x smaller and 9.4x faster on inference .\", \"averb\": \"is\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 9.0, \"averb_span1\": 12.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"empirically\", \"root_full\": \"is empirically\", \"root_s\": \"[]\", \"root_o\": \"['achieves']\", \"root_split\": 2, \"root_span0\": 9, \"root_span1\": 24, \"root_cspan0\": 0, \"root_cspan1\": 15, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX', 'ADV']\", \"apos_w\": \"['TinyBERT', 'is', 'empirically']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"TinyBERT is empirically effective and achieves comparable results with BERT in GLUE datasets, while being 7.5x smaller and 9.4x faster on inference.\", \"split_tokens\": \"['TinyBERT', 'is', 'empirically', 'effective', 'and', 'achieves', 'comparable', 'results', 'with', 'BERT', 'in', 'GLUE', 'datasets', ',', 'while', 'being', '7.5x', 'smaller', 'and', '9.4x', 'faster', 'on', 'inference', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 8)\", \"within_anchor_index\": 4.0}, {\"pc1\": -6.139181456376146, \"pc2\": -2.6672852871283372, \"word\": \"is\", \"split_0\": null, \"split_1\": \"TinyBERT\", \"split_2\": \"is also significantly better than state - of - the - art baselines , even with only about 28 % parameters and 31 % inference time of baselines\", \"averb\": \"is\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"['also significantly better than state - of - the - art baselines , even with only about 28 % parameters and 31 % inference time of baselines']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 9.0, \"averb_span1\": 12.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"is\", \"root_full\": \"is\", \"root_s\": \"['TinyBERT']\", \"root_o\": \"['also significantly better than state - of - the - art baselines , even with only about 28 % parameters and 31 % inference time of baselines']\", \"root_split\": 2, \"root_span0\": 9, \"root_span1\": 12, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['TinyBERT', 'is']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"TinyBERT is also significantly better than state-of-the-art baselines, even with only about 28% parameters and 31% inference time of baselines\", \"split_tokens\": \"['TinyBERT', 'is', 'also', 'significantly', 'better', 'than', 'state', '-', 'of', '-', 'the', '-', 'art', 'baselines', ',', 'even', 'with', 'only', 'about', '28', '%', 'parameters', 'and', '31', '%', 'inference', 'time', 'of', 'baselines']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 8)\", \"within_anchor_index\": 4.0}, {\"pc1\": -12.330835012106608, \"pc2\": 2.232630882398674, \"word\": \"has achieved\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \", a pre-trained Transformer model , has achieved ground-breaking performance on multiple NLP tasks .\", \"averb\": \"has achieved\", \"averb_s\": \"['BERT , a pre - trained Transformer model ,']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 43.0, \"averb_span1\": 56.0, \"averb_cspan0\": 38.0, \"averb_cspan1\": 51.0, \"root\": \"ground\", \"root_full\": \"ground\", \"root_s\": \"['BERT , a pre - trained Transformer model , has achieved']\", \"root_o\": \"['- breaking performance on multiple NLP tasks', '.']\", \"root_split\": 2, \"root_span0\": 56, \"root_span1\": 63, \"root_cspan0\": 51, \"root_cspan1\": 58, \"fverb\": \"has\", \"fword\": \"a\", \"apos\": \"['PROPN', 'VERB', 'NOUN']\", \"apos_w\": \"['BERT', 'achieved', 'ground']\", \"URL\": \"https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3\", \"ID\": 9, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"BERT, a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks.\", \"split_tokens\": \"['BERT', ',', 'a', 'pre-trained', 'Transformer', 'model', ',', 'has', 'achieved', 'ground-breaking', 'performance', 'on', 'multiple', 'NLP', 'tasks', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -14.952339979030318, \"pc2\": 0.24233531124852822, \"word\": \"describe\", \"split_0\": \"In this paper , we describe\", \"split_1\": \"BERTSUM\", \"split_2\": \", a simple variant of BERT , for extractive summarization .\", \"averb\": \"describe\", \"averb_s\": \"['we']\", \"averb_o\": \"['BERTSUM , a simple variant of BERT ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 28.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 28.0, \"root\": \"describe\", \"root_full\": \"describe\", \"root_s\": \"['we']\", \"root_o\": \"['BERTSUM , a simple variant of BERT ,']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 28, \"root_cspan0\": 19, \"root_cspan1\": 28, \"fverb\": null, \"fword\": \"a\", \"apos\": \"['PROPN', 'VERB']\", \"apos_w\": \"['BERTSUM', 'describe']\", \"URL\": \"https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3\", \"ID\": 9, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In this paper, we describe BERTSUM, a simple variant of BERT, for extractive summarization.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'describe', 'BERTSUM', ',', 'a', 'simple', 'variant', 'of', 'BERT', ',', 'for', 'extractive', 'summarization', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(27, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": 9.977380283468952, \"pc2\": 7.9988753220038395, \"word\": \"explore\", \"split_0\": \"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on\", \"split_1\": \"the popular language model BERT\", \"split_2\": \"to enhance the performance of fine - tuning of BERT for RRC .\", \"averb\": \"explore\", \"averb_s\": \"[]\", \"averb_o\": \"['a novel post - training approach on the popular language model BERT', 'to enhance the performance of fine - tuning of BERT for RRC']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 114.0, \"averb_span1\": 122.0, \"averb_cspan0\": 114.0, \"averb_cspan1\": 122.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['Since', 'ReviewRC']\", \"root_o\": \"['limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC']\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 19, \"root_cspan0\": 15, \"root_cspan1\": 19, \"fverb\": \"enhance\", \"fword\": \"to\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['model', 'on', 'approach']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.\", \"split_tokens\": \"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post', '-', 'training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine', '-', 'tuning', 'of', 'BERT', 'for', 'RRC', '.']\", \"split_anchor_span\": \"(29, 34)\", \"split_anchor_indices\": \"(157, 188)\", \"within_anchor_index\": 27.0}, {\"pc1\": 9.929066916015756, \"pc2\": 7.9121772072001875, \"word\": \"enhance\", \"split_0\": \"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of\", \"split_1\": \"BERT\", \"split_2\": \"for RRC .\", \"averb\": \"enhance\", \"averb_s\": \"['to']\", \"averb_o\": \"['the performance of fine - tuning of BERT for RRC']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 193.0, \"averb_span1\": 201.0, \"averb_cspan0\": 193.0, \"averb_cspan1\": 201.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['Since', 'ReviewRC']\", \"root_o\": \"['limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC']\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 19, \"root_cspan0\": 15, \"root_cspan1\": 19, \"fverb\": null, \"fword\": \"for\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'tuning']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.\", \"split_tokens\": \"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post', '-', 'training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine', '-', 'tuning', 'of', 'BERT', 'for', 'RRC', '.']\", \"split_anchor_span\": \"(43, 44)\", \"split_anchor_indices\": \"(236, 240)\", \"within_anchor_index\": 0.0}, {\"pc1\": 9.977380283468952, \"pc2\": 7.9988753220038395, \"word\": \"explore\", \"split_0\": \"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore\", \"split_1\": \"a novel post - training approach on the popular language model BERT\", \"split_2\": \"to enhance the performance of fine - tuning of BERT for RRC .\", \"averb\": \"explore\", \"averb_s\": \"[]\", \"averb_o\": \"['a novel post - training approach on the popular language model BERT', 'to enhance the performance of fine - tuning of BERT for RRC']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 114.0, \"averb_span1\": 122.0, \"averb_cspan0\": 114.0, \"averb_cspan1\": 122.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['Since', 'ReviewRC']\", \"root_o\": \"['limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC']\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 19, \"root_cspan0\": 15, \"root_cspan1\": 19, \"fverb\": \"enhance\", \"fword\": \"to\", \"apos\": \"['NOUN', 'VERB', 'ADV']\", \"apos_w\": \"['approach', 'explore', 'then']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.\", \"split_tokens\": \"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post', '-', 'training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine', '-', 'tuning', 'of', 'BERT', 'for', 'RRC', '.']\", \"split_anchor_span\": \"(22, 34)\", \"split_anchor_indices\": \"(121, 188)\", \"within_anchor_index\": 63.0}, {\"pc1\": 7.6970517854371066, \"pc2\": -4.551505303089407, \"word\": \"To show\", \"split_0\": \"To show the generality of\", \"split_1\": \"the approach\", \"split_2\": \", the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .\", \"averb\": \"To show\", \"averb_s\": \"[]\", \"averb_o\": \"['the generality of the approach , the proposed post - training']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"applied\", \"root_full\": \"applied\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 78, \"root_span1\": 86, \"root_cspan0\": 39, \"root_cspan1\": 47, \"fverb\": \"proposed\", \"fword\": \"the\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['approach', 'of', 'generality']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis.\", \"split_tokens\": \"['To', 'show', 'the', 'generality', 'of', 'the', 'approach', ',', 'the', 'proposed', 'post', '-', 'training', 'is', 'also', 'applied', 'to', 'some', 'other', 'review', '-', 'based', 'tasks', 'such', 'as', 'aspect', 'extraction', 'and', 'aspect', 'sentiment', 'classification', 'in', 'aspect', '-', 'based', 'sentiment', 'analysis', '.']\", \"split_anchor_span\": \"(5, 7)\", \"split_anchor_indices\": \"(25, 37)\", \"within_anchor_index\": -1.0}, {\"pc1\": 7.6970517854371066, \"pc2\": -4.551505303089407, \"word\": \"To show\", \"split_0\": \"To show the generality of the approach ,\", \"split_1\": \"the proposed post - training\", \"split_2\": \"is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .\", \"averb\": \"To show\", \"averb_s\": \"[]\", \"averb_o\": \"['the generality of the approach , the proposed post - training']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"applied\", \"root_full\": \"applied\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 78, \"root_span1\": 86, \"root_cspan0\": 8, \"root_cspan1\": 16, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['training', 'of', 'generality']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis.\", \"split_tokens\": \"['To', 'show', 'the', 'generality', 'of', 'the', 'approach', ',', 'the', 'proposed', 'post', '-', 'training', 'is', 'also', 'applied', 'to', 'some', 'other', 'review', '-', 'based', 'tasks', 'such', 'as', 'aspect', 'extraction', 'and', 'aspect', 'sentiment', 'classification', 'in', 'aspect', '-', 'based', 'sentiment', 'analysis', '.']\", \"split_anchor_span\": \"(8, 13)\", \"split_anchor_indices\": \"(40, 68)\", \"within_anchor_index\": -1.0}, {\"pc1\": -12.83183899925797, \"pc2\": -3.703599718951038, \"word\": \"demonstrate\", \"split_0\": \"Experimental results demonstrate that\", \"split_1\": \"the proposed post - training\", \"split_2\": \"is highly effective .\", \"averb\": \"demonstrate\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 21.0, \"averb_span1\": 33.0, \"averb_cspan0\": 21.0, \"averb_cspan1\": 33.0, \"root\": \"results\", \"root_full\": \"results\", \"root_s\": \"['Experimental']\", \"root_o\": \"['demonstrate that the proposed post - training is highly effective .']\", \"root_split\": 0, \"root_span0\": 13, \"root_span1\": 21, \"root_cspan0\": 13, \"root_cspan1\": 21, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'ADV', 'ADJ']\", \"apos_w\": \"['training', 'highly', 'effective']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"Experimental results demonstrate that the proposed post-training is highly effective.\", \"split_tokens\": \"['Experimental', 'results', 'demonstrate', 'that', 'the', 'proposed', 'post', '-', 'training', 'is', 'highly', 'effective', '.']\", \"split_anchor_span\": \"(4, 9)\", \"split_anchor_indices\": \"(37, 65)\", \"within_anchor_index\": -1.0}, {\"pc1\": 1.2718736890335411, \"pc2\": 0.31018238664080094, \"word\": \"achieved\", \"split_0\": \"As a state - of - the - art language model pre - training model ,\", \"split_1\": \"BERT ( Bidirectional Encoder Representations from Transformers )\", \"split_2\": \"has achieved amazing results in many language understanding tasks .\", \"averb\": \"achieved\", \"averb_s\": \"['BERT ( Bidirectional Encoder Representations from Transformers )', 'As', 'a state - of - the - art language model pre - training model ,', 'has']\", \"averb_o\": \"['amazing results in many language understanding tasks', '.']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 135.0, \"averb_span1\": 144.0, \"averb_cspan0\": 4.0, \"averb_cspan1\": 13.0, \"root\": \"achieved\", \"root_full\": \"achieved\", \"root_s\": \"['BERT ( Bidirectional Encoder Representations from Transformers )', 'As', 'a state - of - the - art language model pre - training model ,', 'has']\", \"root_o\": \"['amazing results in many language understanding tasks', '.']\", \"root_split\": 2, \"root_span0\": 135, \"root_span1\": 144, \"root_cspan0\": 4, \"root_cspan1\": 13, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'VERB']\", \"apos_w\": \"['Representations', 'achieved']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks.\", \"split_tokens\": \"['As', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'language', 'model', 'pre', '-', 'training', 'model', ',', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', 'has', 'achieved', 'amazing', 'results', 'in', 'many', 'language', 'understanding', 'tasks', '.']\", \"split_anchor_span\": \"(16, 24)\", \"split_anchor_indices\": \"(65, 129)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.092124068174828, \"pc2\": -1.4262535634339928, \"word\": \"to investigate\", \"split_0\": \"In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of\", \"split_1\": \"BERT\", \"split_2\": \"on text classification task and provide a general solution for BERT fine - tuning .\", \"averb\": \"to investigate\", \"averb_s\": \"[]\", \"averb_o\": \"['different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 50.0, \"averb_span1\": 65.0, \"averb_cspan0\": 50.0, \"averb_cspan1\": 65.0, \"root\": \"conduct\", \"root_full\": \"conduct\", \"root_s\": \"['we']\", \"root_o\": \"['exhaustive experiments', 'to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": \"provide\", \"fword\": \"on\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'methods']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine', '-', 'tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine', '-', 'tuning', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(99, 103)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.017020900278152, \"pc2\": -1.5286997374143059, \"word\": \"provide\", \"split_0\": \"In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for\", \"split_1\": \"BERT\", \"split_2\": \"fine - tuning .\", \"averb\": \"provide\", \"averb_s\": \"[]\", \"averb_o\": \"['a general solution for BERT fine - tuning']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 137.0, \"averb_span1\": 145.0, \"averb_cspan0\": 137.0, \"averb_cspan1\": 145.0, \"root\": \"conduct\", \"root_full\": \"conduct\", \"root_s\": \"['we']\", \"root_o\": \"['exhaustive experiments', 'to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": \"fine\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'tuning', 'for']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine', '-', 'tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine', '-', 'tuning', '.']\", \"split_anchor_span\": \"(27, 28)\", \"split_anchor_indices\": \"(167, 171)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.017020900278152, \"pc2\": -1.5286997374143059, \"word\": \"provide\", \"split_0\": \"In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of BERT on text classification task and provide\", \"split_1\": \"a general solution for BERT fine - tuning\", \"split_2\": \".\", \"averb\": \"provide\", \"averb_s\": \"[]\", \"averb_o\": \"['a general solution for BERT fine - tuning']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 137.0, \"averb_span1\": 145.0, \"averb_cspan0\": 137.0, \"averb_cspan1\": 145.0, \"root\": \"conduct\", \"root_full\": \"conduct\", \"root_s\": \"['we']\", \"root_o\": \"['exhaustive experiments', 'to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB', 'NOUN']\", \"apos_w\": \"['solution', 'provide', 'task']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine', '-', 'tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine', '-', 'tuning', '.']\", \"split_anchor_span\": \"(23, 31)\", \"split_anchor_indices\": \"(144, 185)\", \"within_anchor_index\": 23.0}, {\"pc1\": -17.99738383973211, \"pc2\": 0.37953364695219055, \"word\": \"show\", \"split_0\": \"We show that\", \"split_1\": \"BERT\", \"split_2\": \"( Devlin et al . , 2018 ) is a Markov random field language model .\", \"averb\": \"show\", \"averb_s\": \"['We']\", \"averb_o\": \"['that BERT ( Devlin et al . , 2018 )', 'is a Markov random field language model', '.']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 8.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 8.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"['We']\", \"root_o\": \"['that BERT ( Devlin et al . , 2018 )', 'is a Markov random field language model', '.']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": \"is\", \"fword\": \"Devlin\", \"apos\": \"['PROPN', 'PROPN', 'SCONJ']\", \"apos_w\": \"['BERT', 'et', 'that']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"We show that BERT (Devlin et al., 2018) is a Markov random field language model.\", \"split_tokens\": \"['We', 'show', 'that', 'BERT', '(', 'Devlin', 'et', 'al', '.', ',', '2018', ')', 'is', 'a', 'Markov', 'random', 'field', 'language', 'model', '.']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(12, 16)\", \"within_anchor_index\": 0.0}, {\"pc1\": -15.692328956286726, \"pc2\": -2.292950308054562, \"word\": \"gives\", \"split_0\": \"This formulation gives way to a natural procedure to sample sentences from\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"gives\", \"averb_s\": \"['This', 'formulation']\", \"averb_o\": \"['way', 'to a natural procedure to sample sentences from BERT', '.']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 17.0, \"averb_span1\": 23.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 23.0, \"root\": \"gives\", \"root_full\": \"gives\", \"root_s\": \"['This', 'formulation']\", \"root_o\": \"['way', 'to a natural procedure to sample sentences from BERT', '.']\", \"root_split\": 0, \"root_span0\": 17, \"root_span1\": 23, \"root_cspan0\": 17, \"root_cspan1\": 23, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'from', 'sentences']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"This formulation gives way to a natural procedure to sample sentences from BERT.\", \"split_tokens\": \"['This', 'formulation', 'gives', 'way', 'to', 'a', 'natural', 'procedure', 'to', 'sample', 'sentences', 'from', 'BERT', '.']\", \"split_anchor_span\": \"(12, 13)\", \"split_anchor_indices\": \"(74, 78)\", \"within_anchor_index\": 0.0}, {\"pc1\": -14.748599104015838, \"pc2\": -2.4817946227348333, \"word\": \"generate\", \"split_0\": \"We generate from\", \"split_1\": \"BERT\", \"split_2\": \"and find that it can produce high - quality , fluent generations .\", \"averb\": \"generate\", \"averb_s\": \"['We']\", \"averb_o\": \"['find that it can produce high - quality , fluent generations .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 12.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 12.0, \"root\": \"generate\", \"root_full\": \"generate\", \"root_s\": \"['We']\", \"root_o\": \"['find that it can produce high - quality , fluent generations .']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 12, \"root_cspan0\": 3, \"root_cspan1\": 12, \"fverb\": \"find\", \"fword\": \"and\", \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'from', 'generate']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We generate from BERT and find that it can produce high-quality, fluent generations.\", \"split_tokens\": \"['We', 'generate', 'from', 'BERT', 'and', 'find', 'that', 'it', 'can', 'produce', 'high', '-', 'quality', ',', 'fluent', 'generations', '.']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(16, 20)\", \"within_anchor_index\": 0.0}, {\"pc1\": -14.729087264150731, \"pc2\": -2.4557796909198766, \"word\": \"produce\", \"split_0\": \"We generate from BERT and find that\", \"split_1\": \"it\", \"split_2\": \"can produce high - quality , fluent generations .\", \"averb\": \"produce\", \"averb_s\": \"['it', 'can']\", \"averb_o\": \"['high - quality', ', fluent generations .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 43.0, \"averb_span1\": 51.0, \"averb_cspan0\": 4.0, \"averb_cspan1\": 12.0, \"root\": \"generate\", \"root_full\": \"generate\", \"root_s\": \"['We']\", \"root_o\": \"['find that it can produce high - quality , fluent generations .']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 12, \"root_cspan0\": 3, \"root_cspan1\": 12, \"fverb\": \"produce\", \"fword\": \"can\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['it', 'produce', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We generate from BERT and find that it can produce high-quality, fluent generations.\", \"split_tokens\": \"['We', 'generate', 'from', 'BERT', 'and', 'find', 'that', 'it', 'can', 'produce', 'high', '-', 'quality', ',', 'fluent', 'generations', '.']\", \"split_anchor_span\": \"(7, 8)\", \"split_anchor_indices\": \"(35, 37)\", \"within_anchor_index\": -1.0}, {\"pc1\": -2.586431489436918, \"pc2\": -2.066834941819068, \"word\": \"Compared\", \"split_0\": \"Compared to the generations of a traditional left - to - right language model ,\", \"split_1\": \"BERT\", \"split_2\": \"generates sentences that are more diverse but of slightly worse quality .\", \"averb\": \"Compared\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 9.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"diverse\", \"root_full\": \"are diverse\", \"root_s\": \"['that']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 110, \"root_span1\": 127, \"root_cspan0\": 25, \"root_cspan1\": 42, \"fverb\": \"generates\", \"fword\": \"generates\", \"apos\": \"['PROPN', 'NOUN', 'PUNCT']\", \"apos_w\": \"['BERT', 'sentences', ',']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.\", \"split_tokens\": \"['Compared', 'to', 'the', 'generations', 'of', 'a', 'traditional', 'left', '-', 'to', '-', 'right', 'language', 'model', ',', 'BERT', 'generates', 'sentences', 'that', 'are', 'more', 'diverse', 'but', 'of', 'slightly', 'worse', 'quality', '.']\", \"split_anchor_span\": \"(15, 16)\", \"split_anchor_indices\": \"(79, 83)\", \"within_anchor_index\": 0.0}, {\"pc1\": -18.47662240199039, \"pc2\": 2.394126641636969, \"word\": \"has\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"has a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .\", \"averb\": \"has\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 5.0, \"averb_span1\": 9.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 4.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['BERT']\", \"root_o\": \"['a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .']\", \"root_split\": 2, \"root_span0\": 5, \"root_span1\": 9, \"root_cspan0\": 0, \"root_cspan1\": 4, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['BERT', 'has']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.\", \"split_tokens\": \"['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It', 'Must', 'Speak', ':', 'BERT', 'as', 'a', 'Markov', 'Random', 'Field', 'Language', 'Model', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -18.688020092126695, \"pc2\": 2.3803404729077267, \"word\": \"Must\", \"split_0\": \"BERT has a Mouth , and\", \"split_1\": \"It\", \"split_2\": \"Must Speak : BERT as a Markov Random Field Language Model .\", \"averb\": \"Must\", \"averb_s\": \"['It']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 26.0, \"averb_span1\": 31.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 5.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['BERT']\", \"root_o\": \"['a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .']\", \"root_split\": 0, \"root_span0\": 5, \"root_span1\": 9, \"root_cspan0\": 5, \"root_cspan1\": 9, \"fverb\": null, \"fword\": \"Must\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['It', 'Must', 'Speak']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.\", \"split_tokens\": \"['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It', 'Must', 'Speak', ':', 'BERT', 'as', 'a', 'Markov', 'Random', 'Field', 'Language', 'Model', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(22, 24)\", \"within_anchor_index\": -1.0}, {\"pc1\": -6.195020761864438, \"pc2\": -0.9079559749332602, \"word\": \"applying\", \"split_0\": \"Following recent successes in applying\", \"split_1\": \"BERT\", \"split_2\": \"to question answering , we explore simple applications to ad hoc document retrieval .\", \"averb\": \"applying\", \"averb_s\": \"[]\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 30.0, \"averb_span1\": 39.0, \"averb_cspan0\": 30.0, \"averb_cspan1\": 39.0, \"root\": \"explore\", \"root_full\": \"explore\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['simple applications']\", \"root_split\": 2, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 27, \"root_cspan1\": 35, \"fverb\": \"answering\", \"fword\": \"to\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'applying', 'in']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval.\", \"split_tokens\": \"['Following', 'recent', 'successes', 'in', 'applying', 'BERT', 'to', 'question', 'answering', ',', 'we', 'explore', 'simple', 'applications', 'to', 'ad', 'hoc', 'document', 'retrieval', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(38, 42)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.143788002174661, \"pc2\": -1.5984803370381653, \"word\": \"posed\", \"split_0\": \"This required confronting the challenge posed by documents that are typically longer than the length of input\", \"split_1\": \"BERT\", \"split_2\": \"was designed to handle .\", \"averb\": \"posed\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 40.0, \"averb_span1\": 46.0, \"averb_cspan0\": 40.0, \"averb_cspan1\": 46.0, \"root\": \"required\", \"root_full\": \"required\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 5, \"root_span1\": 14, \"root_cspan0\": 5, \"root_cspan1\": 14, \"fverb\": \"was\", \"fword\": \"was\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'length']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle.\", \"split_tokens\": \"['This', 'required', 'confronting', 'the', 'challenge', 'posed', 'by', 'documents', 'that', 'are', 'typically', 'longer', 'than', 'the', 'length', 'of', 'input', 'BERT', 'was', 'designed', 'to', 'handle', '.']\", \"split_anchor_span\": \"(17, 18)\", \"split_anchor_indices\": \"(109, 113)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.07613134448825, \"pc2\": -1.5491718740832057, \"word\": \"confronting\", \"split_0\": \"This required confronting\", \"split_1\": \"the challenge posed by documents that are typically longer than the length of input BERT was designed to handle\", \"split_2\": \".\", \"averb\": \"confronting\", \"averb_s\": \"[]\", \"averb_o\": \"['the challenge posed by documents that are typically longer than the length of input BERT was designed to handle .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 14.0, \"averb_span1\": 26.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 26.0, \"root\": \"required\", \"root_full\": \"required\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 5, \"root_span1\": 14, \"root_cspan0\": 5, \"root_cspan1\": 14, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['challenge', 'confronting', 'required']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle.\", \"split_tokens\": \"['This', 'required', 'confronting', 'the', 'challenge', 'posed', 'by', 'documents', 'that', 'are', 'typically', 'longer', 'than', 'the', 'length', 'of', 'input', 'BERT', 'was', 'designed', 'to', 'handle', '.']\", \"split_anchor_span\": \"(3, 22)\", \"split_anchor_indices\": \"(25, 136)\", \"within_anchor_index\": 84.0}, {\"pc1\": -4.3726865571346405, \"pc2\": -1.2388361584238528, \"word\": \"address\", \"split_0\": \"We address\", \"split_1\": \"this issue\", \"split_2\": \"by applying inference on sentences individually , and then aggregating sentence scores to produce document scores .\", \"averb\": \"address\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"scores\", \"root_full\": \"scores\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 129, \"root_span1\": 136, \"root_cspan0\": 107, \"root_cspan1\": 114, \"fverb\": \"applying\", \"fword\": \"by\", \"apos\": \"['NOUN', 'VERB', 'ADV']\", \"apos_w\": \"['issue', 'address', 'then']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores.\", \"split_tokens\": \"['We', 'address', 'this', 'issue', 'by', 'applying', 'inference', 'on', 'sentences', 'individually', ',', 'and', 'then', 'aggregating', 'sentence', 'scores', 'to', 'produce', 'document', 'scores', '.']\", \"split_anchor_span\": \"(2, 4)\", \"split_anchor_indices\": \"(10, 20)\", \"within_anchor_index\": -1.0}, {\"pc1\": -0.6418390377745069, \"pc2\": -4.313097446402911, \"word\": \"contribute\", \"split_0\": \"BERT - based architectures currently give state - of - the - art performance on many NLP tasks , but little is known about the exact mechanisms that contribute to\", \"split_1\": \"its\", \"split_2\": \"success .\", \"averb\": \"contribute\", \"averb_s\": \"['that']\", \"averb_o\": \"['its .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 149.0, \"averb_span1\": 160.0, \"averb_cspan0\": 149.0, \"averb_cspan1\": 160.0, \"root\": \"currently\", \"root_full\": \"currently\", \"root_s\": \"['BERT - based architectures']\", \"root_o\": \"['give state - of - the - art performance on many NLP tasks , but little is known about the exact mechanisms that contribute to its success .']\", \"root_split\": 0, \"root_span0\": 27, \"root_span1\": 37, \"root_cspan0\": 27, \"root_cspan1\": 37, \"fverb\": null, \"fword\": \"success\", \"apos\": \"['DET', 'VERB', 'NOUN']\", \"apos_w\": \"['its', 'contribute', 'mechanisms']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success.\", \"split_tokens\": \"['BERT', '-', 'based', 'architectures', 'currently', 'give', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'many', 'NLP', 'tasks', ',', 'but', 'little', 'is', 'known', 'about', 'the', 'exact', 'mechanisms', 'that', 'contribute', 'to', 'its', 'success', '.']\", \"split_anchor_span\": \"(30, 31)\", \"split_anchor_indices\": \"(162, 165)\", \"within_anchor_index\": -1.0}, {\"pc1\": -5.006198236459333, \"pc2\": -3.399041083940114, \"word\": \"focus\", \"split_0\": \"In the current work , we focus on the interpretation of self - attention , which is one of the fundamental underlying components of\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"focus\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 25.0, \"averb_span1\": 31.0, \"averb_cspan0\": 25.0, \"averb_cspan1\": 31.0, \"root\": \"focus\", \"root_full\": \"focus\", \"root_s\": \"[',', 'we']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 25, \"root_span1\": 31, \"root_cspan0\": 25, \"root_cspan1\": 31, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'components']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT.\", \"split_tokens\": \"['In', 'the', 'current', 'work', ',', 'we', 'focus', 'on', 'the', 'interpretation', 'of', 'self', '-', 'attention', ',', 'which', 'is', 'one', 'of', 'the', 'fundamental', 'underlying', 'components', 'of', 'BERT', '.']\", \"split_anchor_span\": \"(24, 25)\", \"split_anchor_indices\": \"(131, 135)\", \"within_anchor_index\": 0.0}, {\"pc1\": 6.733741025818004, \"pc2\": -3.022359797472563, \"word\": \"encoded\", \"split_0\": \"Using a subset of GLUE tasks and a set of handcrafted features-of-interest , we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual\", \"split_1\": \"BERT\", \"split_2\": \"'s heads .\", \"averb\": \"encoded\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 181.0, \"averb_span1\": 189.0, \"averb_cspan0\": 181.0, \"averb_cspan1\": 189.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we', 'Using a subset of GLUE tasks and a set of handcrafted features - of - interest']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 84, \"root_span1\": 92, \"root_cspan0\": 84, \"root_cspan1\": 92, \"fverb\": null, \"fword\": \"heads\", \"apos\": \"['PROPN', 'ADJ', 'ADP']\", \"apos_w\": \"['BERT', 'individual', 'by']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT's heads.\", \"split_tokens\": \"['Using', 'a', 'subset', 'of', 'GLUE', 'tasks', 'and', 'a', 'set', 'of', 'handcrafted', 'features-of-interest', ',', 'we', 'propose', 'the', 'methodology', 'and', 'carry', 'out', 'a', 'qualitative', 'and', 'quantitative', 'analysis', 'of', 'the', 'information', 'encoded', 'by', 'the', 'individual', 'BERT', \\\"'s\\\", 'heads', '.']\", \"split_anchor_span\": \"(32, 33)\", \"split_anchor_indices\": \"(202, 206)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.102094705955891, \"pc2\": -0.5938218490314615, \"word\": \"leads\", \"split_0\": \"We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine - tuned\", \"split_1\": \"BERT\", \"split_2\": \"models\", \"averb\": \"leads\", \"averb_s\": \"['manually disabling attention in certain heads']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 59.0, \"averb_span1\": 65.0, \"averb_cspan0\": 59.0, \"averb_cspan1\": 65.0, \"root\": \"We\", \"root_full\": \"We\", \"root_s\": \"[]\", \"root_o\": \"['show that manually disabling attention in certain heads leads to a performance improvement over the regular fine - tuned BERT models']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": null, \"fword\": \"models\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'models', 'over']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models\", \"split_tokens\": \"['We', 'show', 'that', 'manually', 'disabling', 'attention', 'in', 'certain', 'heads', 'leads', 'to', 'a', 'performance', 'improvement', 'over', 'the', 'regular', 'fine', '-', 'tuned', 'BERT', 'models']\", \"split_anchor_span\": \"(20, 21)\", \"split_anchor_indices\": \"(123, 127)\", \"within_anchor_index\": 0.0}, {\"pc1\": -23.47034121371511, \"pc2\": -0.0330181127983899, \"word\": \"Revealing\", \"split_0\": \"Revealing the Dark Secrets of\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"Revealing\", \"averb_s\": \"[]\", \"averb_o\": \"['the Dark Secrets of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 10.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \"Revealing\", \"root_full\": \"Revealing\", \"root_s\": \"[]\", \"root_o\": \"['the Dark Secrets of BERT']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 10, \"root_cspan0\": 0, \"root_cspan1\": 10, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'PROPN']\", \"apos_w\": \"['BERT', 'of', 'Secrets']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"Revealing the Dark Secrets of BERT.\", \"split_tokens\": \"['Revealing', 'the', 'Dark', 'Secrets', 'of', 'BERT', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(29, 33)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.5799551122887272, \"pc2\": 4.509407329987076, \"word\": \"has been released\", \"split_0\": \"Recently , an upgraded version of\", \"split_1\": \"BERT\", \"split_2\": \"has been released with Whole Word Masking ( WWM ) , which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .\", \"averb\": \"has been released\", \"averb_s\": \"['an upgraded version of BERT', 'Recently ,']\", \"averb_o\": \"['which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 39.0, \"averb_span1\": 57.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 18.0, \"root\": \"released\", \"root_full\": \"has been released\", \"root_s\": \"['an upgraded version of BERT', 'Recently ,']\", \"root_o\": \"['which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .']\", \"root_split\": 2, \"root_span0\": 39, \"root_span1\": 57, \"root_cspan0\": 0, \"root_cspan1\": 18, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'version']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT.\", \"split_tokens\": \"['Recently', ',', 'an', 'upgraded', 'version', 'of', 'BERT', 'has', 'been', 'released', 'with', 'Whole', 'Word', 'Masking', '(', 'WWM', ')', ',', 'which', 'mitigate', 'the', 'drawbacks', 'of', 'masking', 'partial', 'WordPiece', 'tokens', 'in', 'pre', '-', 'training', 'BERT', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(33, 37)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.5726296886690277, \"pc2\": 4.470925052136331, \"word\": \"masking\", \"split_0\": \"Recently , an upgraded version of BERT has been released with Whole Word Masking ( WWM ) , which mitigate the drawbacks of masking partial WordPiece tokens in pre - training\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"masking\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 123.0, \"averb_span1\": 131.0, \"averb_cspan0\": 123.0, \"averb_cspan1\": 131.0, \"root\": \"released\", \"root_full\": \"has been released\", \"root_s\": \"['an upgraded version of BERT', 'Recently ,']\", \"root_o\": \"['which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .']\", \"root_split\": 0, \"root_span0\": 39, \"root_span1\": 57, \"root_cspan0\": 39, \"root_cspan1\": 57, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADJ', 'ADJ']\", \"apos_w\": \"['BERT', 'training', 'pre']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT.\", \"split_tokens\": \"['Recently', ',', 'an', 'upgraded', 'version', 'of', 'BERT', 'has', 'been', 'released', 'with', 'Whole', 'Word', 'Masking', '(', 'WWM', ')', ',', 'which', 'mitigate', 'the', 'drawbacks', 'of', 'masking', 'partial', 'WordPiece', 'tokens', 'in', 'pre', '-', 'training', 'BERT', '.']\", \"split_anchor_span\": \"(31, 32)\", \"split_anchor_indices\": \"(173, 177)\", \"within_anchor_index\": 0.0}, {\"pc1\": -18.770750572769355, \"pc2\": -2.4876033107205835, \"word\": \"was trained\", \"split_0\": null, \"split_1\": \"The model\", \"split_2\": \"was trained on the latest Chinese Wikipedia dump .\", \"averb\": \"was trained\", \"averb_s\": \"['The model']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 10.0, \"averb_span1\": 22.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"trained\", \"root_full\": \"was trained\", \"root_s\": \"['The model']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 10, \"root_span1\": 22, \"root_cspan0\": 0, \"root_cspan1\": 12, \"fverb\": \"was\", \"fword\": \"was\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['model', 'trained']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"The model was trained on the latest Chinese Wikipedia dump.\", \"split_tokens\": \"['The', 'model', 'was', 'trained', 'on', 'the', 'latest', 'Chinese', 'Wikipedia', 'dump', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 9)\", \"within_anchor_index\": -1.0}, {\"pc1\": -3.9890001191967013, \"pc2\": 1.3485259269501502, \"word\": \"to provide\", \"split_0\": \"We aim to provide easy extensibility and better performance for\", \"split_1\": \"Chinese BERT\", \"split_2\": \"without changing any neural architecture or even hyper - parameters .\", \"averb\": \"to provide\", \"averb_s\": \"[]\", \"averb_o\": \"['easy', 'extensibility and', 'better performance']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 7.0, \"averb_span1\": 18.0, \"averb_cspan0\": 7.0, \"averb_cspan1\": 18.0, \"root\": \"aim\", \"root_full\": \"aim\", \"root_s\": \"['We']\", \"root_o\": \"['.']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 7, \"root_cspan0\": 3, \"root_cspan1\": 7, \"fverb\": \"changing\", \"fword\": \"without\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'for', 'provide']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"We aim to provide easy extensibility and better performance for Chinese BERT without changing any neural architecture or even hyper-parameters.\", \"split_tokens\": \"['We', 'aim', 'to', 'provide', 'easy', 'extensibility', 'and', 'better', 'performance', 'for', 'Chinese', 'BERT', 'without', 'changing', 'any', 'neural', 'architecture', 'or', 'even', 'hyper', '-', 'parameters', '.']\", \"split_anchor_span\": \"(10, 12)\", \"split_anchor_indices\": \"(63, 75)\", \"within_anchor_index\": 8.0}, {\"pc1\": 22.549248607242603, \"pc2\": 16.073782284660588, \"word\": \"is verified\", \"split_0\": null, \"split_1\": \"The model\", \"split_2\": \"is verified on various NLP tasks , across sentence - level to document - level , including sentiment classification ( ChnSentiCorp , Sina Weibo ) , named entity recognition ( People Daily , MSRA - NER ) , natural language inference ( XNLI ) , sentence pair matching ( LCQMC , BQ Corpus ) , and machine reading comprehension ( CMRC 2018 , DRCD , CAIL RC ) .\", \"averb\": \"is verified\", \"averb_s\": \"['The model']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 10.0, \"averb_span1\": 22.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \",\", \"root_full\": \",\", \"root_s\": \"['The model is verified on various NLP tasks , across sentence - level to document - level , including sentiment classification ( ChnSentiCorp , Sina Weibo ) , named entity recognition ( People Daily , MSRA - NER ) , natural language inference ( XNLI ) , sentence pair matching ( LCQMC , BQ Corpus ) , and machine reading comprehension ( CMRC 2018 ,', 'DRCD']\", \"root_o\": \"['CAIL RC ) .']\", \"root_split\": 2, \"root_span0\": 353, \"root_span1\": 355, \"root_cspan0\": 343, \"root_cspan1\": 345, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'VERB', 'NOUN']\", \"apos_w\": \"['model', 'verified', 'inference']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"The model is verified on various NLP tasks, across sentence-level to document-level, including sentiment classification (ChnSentiCorp, Sina Weibo), named entity recognition (People Daily, MSRA-NER), natural language inference (XNLI), sentence pair matching (LCQMC, BQ Corpus), and machine reading comprehension (CMRC 2018, DRCD, CAIL RC).\", \"split_tokens\": \"['The', 'model', 'is', 'verified', 'on', 'various', 'NLP', 'tasks', ',', 'across', 'sentence', '-', 'level', 'to', 'document', '-', 'level', ',', 'including', 'sentiment', 'classification', '(', 'ChnSentiCorp', ',', 'Sina', 'Weibo', ')', ',', 'named', 'entity', 'recognition', '(', 'People', 'Daily', ',', 'MSRA', '-', 'NER', ')', ',', 'natural', 'language', 'inference', '(', 'XNLI', ')', ',', 'sentence', 'pair', 'matching', '(', 'LCQMC', ',', 'BQ', 'Corpus', ')', ',', 'and', 'machine', 'reading', 'comprehension', '(', 'CMRC', '2018', ',', 'DRCD', ',', 'CAIL', 'RC', ')', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 9)\", \"within_anchor_index\": -1.0}, {\"pc1\": -13.453707100094695, \"pc2\": 5.444389243463492, \"word\": \"examine\", \"split_0\": \"Moreover , we also examine the effectiveness of Chinese pre - trained models :\", \"split_1\": \"BERT\", \"split_2\": \", ERNIE , BERT - wwm .\", \"averb\": \"examine\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 27.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 27.0, \"root\": \"examine\", \"root_full\": \"examine\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": \"ERNIE\", \"apos\": \"['PROPN', 'PUNCT', 'NOUN']\", \"apos_w\": \"['BERT', ':', 'models']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"Moreover, we also examine the effectiveness of Chinese pre-trained models: BERT, ERNIE, BERT-wwm.\", \"split_tokens\": \"['Moreover', ',', 'we', 'also', 'examine', 'the', 'effectiveness', 'of', 'Chinese', 'pre', '-', 'trained', 'models', ':', 'BERT', ',', 'ERNIE', ',', 'BERT', '-', 'wwm', '.']\", \"split_anchor_span\": \"(14, 15)\", \"split_anchor_indices\": \"(78, 82)\", \"within_anchor_index\": 0.0}, {\"pc1\": -13.453707100094695, \"pc2\": 5.444389243463492, \"word\": \"examine\", \"split_0\": \"Moreover , we also examine the effectiveness of Chinese pre - trained models : BERT , ERNIE ,\", \"split_1\": \"BERT - wwm\", \"split_2\": \".\", \"averb\": \"examine\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 27.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 27.0, \"root\": \"examine\", \"root_full\": \"examine\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'PUNCT', 'PROPN']\", \"apos_w\": \"['wwm', ',', 'ERNIE']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"Moreover, we also examine the effectiveness of Chinese pre-trained models: BERT, ERNIE, BERT-wwm.\", \"split_tokens\": \"['Moreover', ',', 'we', 'also', 'examine', 'the', 'effectiveness', 'of', 'Chinese', 'pre', '-', 'trained', 'models', ':', 'BERT', ',', 'ERNIE', ',', 'BERT', '-', 'wwm', '.']\", \"split_anchor_span\": \"(18, 21)\", \"split_anchor_indices\": \"(93, 103)\", \"within_anchor_index\": 0.0}, {\"pc1\": 5.401053967326706, \"pc2\": 0.7510370530962478, \"word\": \"BERT\", \"split_0\": \"However , previous work trains\", \"split_1\": \"BERT\", \"split_2\": \"by viewing passages corresponding to the same question as independent training instances , which may cause incomparable scores for answers from different passages .\", \"averb\": \"BERT\", \"averb_s\": \"['previous work trains']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 31.0, \"averb_span1\": 36.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 5.0, \"root\": \"cause\", \"root_full\": \"may cause\", \"root_s\": \"['However', ',', 'which', 'previous work trains BERT by viewing passages corresponding to the same question as independent training instances ,']\", \"root_o\": \"['incomparable scores']\", \"root_split\": 2, \"root_span0\": 133, \"root_span1\": 143, \"root_cspan0\": 97, \"root_cspan1\": 107, \"fverb\": \"viewing\", \"fword\": \"by\", \"apos\": \"['VERB', 'VERB']\", \"apos_w\": \"['BERT', 'cause']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages.\", \"split_tokens\": \"['However', ',', 'previous', 'work', 'trains', 'BERT', 'by', 'viewing', 'passages', 'corresponding', 'to', 'the', 'same', 'question', 'as', 'independent', 'training', 'instances', ',', 'which', 'may', 'cause', 'incomparable', 'scores', 'for', 'answers', 'from', 'different', 'passages', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(30, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": 7.165397775097058, \"pc2\": -1.1423554940698537, \"word\": \"propose\", \"split_0\": \"To tackle this issue , we propose\", \"split_1\": \"a multi - passage BERT model to globally normalize answer scores across all passages of the same question\", \"split_2\": \", and this change enables our QA model find better answers by utilizing more passages .\", \"averb\": \"propose\", \"averb_s\": \"['To', 'we', 'tackle this issue']\", \"averb_o\": \"['a multi - passage BERT model to globally normalize answer scores across all passages of the same question ,', 'this change enables our QA model find better answers by utilizing more passages .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 26.0, \"averb_span1\": 34.0, \"averb_cspan0\": 26.0, \"averb_cspan1\": 34.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['To', 'we', 'tackle this issue']\", \"root_o\": \"['a multi - passage BERT model to globally normalize answer scores across all passages of the same question ,', 'this change enables our QA model find better answers by utilizing more passages .']\", \"root_split\": 0, \"root_span0\": 26, \"root_span1\": 34, \"root_cspan0\": 26, \"root_cspan1\": 34, \"fverb\": \"enables\", \"fword\": \"and\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['model', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages.\", \"split_tokens\": \"['To', 'tackle', 'this', 'issue', ',', 'we', 'propose', 'a', 'multi', '-', 'passage', 'BERT', 'model', 'to', 'globally', 'normalize', 'answer', 'scores', 'across', 'all', 'passages', 'of', 'the', 'same', 'question', ',', 'and', 'this', 'change', 'enables', 'our', 'QA', 'model', 'find', 'better', 'answers', 'by', 'utilizing', 'more', 'passages', '.']\", \"split_anchor_span\": \"(7, 25)\", \"split_anchor_indices\": \"(33, 138)\", \"within_anchor_index\": 18.0}, {\"pc1\": -13.391814761431576, \"pc2\": -2.00014657205866, \"word\": \"gains\", \"split_0\": \"By leveraging a passage ranker to select high-quality passages , multi-passage\", \"split_1\": \"BERT\", \"split_2\": \"gains additional 2 % .\", \"averb\": \"gains\", \"averb_s\": \"['BERT .']\", \"averb_o\": \"['additional 2 %']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 88.0, \"averb_span1\": 94.0, \"averb_cspan0\": 4.0, \"averb_cspan1\": 10.0, \"root\": \"By\", \"root_full\": \"By\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": null, \"fword\": \"gains\", \"apos\": \"['PROPN', 'VERB', 'ADJ']\", \"apos_w\": \"['BERT', 'gains', 'passage']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%.\", \"split_tokens\": \"['By', 'leveraging', 'a', 'passage', 'ranker', 'to', 'select', 'high-quality', 'passages', ',', 'multi-passage', 'BERT', 'gains', 'additional', '2', '%', '.']\", \"split_anchor_span\": \"(11, 12)\", \"split_anchor_indices\": \"(78, 82)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.664391016585303, \"pc2\": -1.764926646529903, \"word\": \"showed\", \"split_0\": \"Experiments on four standard benchmarks showed that\", \"split_1\": \"our multi - passage BERT\", \"split_2\": \"outperforms all state - of - the - art models on all benchmarks .\", \"averb\": \"showed\", \"averb_s\": \"['Experiments on four standard benchmarks']\", \"averb_o\": \"['that our multi - passage BERT outperforms all state - of - the - art models on all benchmarks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 40.0, \"averb_span1\": 47.0, \"averb_cspan0\": 40.0, \"averb_cspan1\": 47.0, \"root\": \"showed\", \"root_full\": \"showed\", \"root_s\": \"['Experiments on four standard benchmarks']\", \"root_o\": \"['that our multi - passage BERT outperforms all state - of - the - art models on all benchmarks']\", \"root_split\": 0, \"root_span0\": 40, \"root_span1\": 47, \"root_cspan0\": 40, \"root_cspan1\": 47, \"fverb\": \"outperforms\", \"fword\": \"outperforms\", \"apos\": \"['PROPN', 'VERB']\", \"apos_w\": \"['BERT', 'showed']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks.\", \"split_tokens\": \"['Experiments', 'on', 'four', 'standard', 'benchmarks', 'showed', 'that', 'our', 'multi', '-', 'passage', 'BERT', 'outperforms', 'all', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'on', 'all', 'benchmarks', '.']\", \"split_anchor_span\": \"(7, 12)\", \"split_anchor_indices\": \"(51, 75)\", \"within_anchor_index\": 20.0}, {\"pc1\": -13.015107468272207, \"pc2\": 0.14439097712683532, \"word\": \"exploring\", \"split_0\": \"However , there has not been much effort on exploring\", \"split_1\": \"BERT\", \"split_2\": \"for natural language understanding .\", \"averb\": \"exploring\", \"averb_s\": \"[]\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 44.0, \"averb_span1\": 54.0, \"averb_cspan0\": 44.0, \"averb_cspan1\": 54.0, \"root\": \"much\", \"root_full\": \"has not been much\", \"root_s\": \"['However', ',', 'there']\", \"root_o\": \"['effort on exploring BERT for natural language understanding']\", \"root_split\": 0, \"root_span0\": 16, \"root_span1\": 34, \"root_cspan0\": 16, \"root_cspan1\": 34, \"fverb\": null, \"fword\": \"for\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'exploring', 'on']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"However, there has not been much effort on exploring BERT for natural language understanding.\", \"split_tokens\": \"['However', ',', 'there', 'has', 'not', 'been', 'much', 'effort', 'on', 'exploring', 'BERT', 'for', 'natural', 'language', 'understanding', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(53, 57)\", \"within_anchor_index\": 0.0}, {\"pc1\": -13.09432598929986, \"pc2\": -5.1805862312635815, \"word\": \"based\", \"split_0\": \"In this work , we propose a joint intent classification and slot filling model based on\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"based\", \"averb_s\": \"['a joint intent classification and slot filling model']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 79.0, \"averb_span1\": 85.0, \"averb_cspan0\": 79.0, \"averb_cspan1\": 85.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we']\", \"root_o\": \"['a joint intent classification and slot filling model based on BERT']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 26, \"root_cspan0\": 18, \"root_cspan1\": 26, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'on', 'based']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"In this work, we propose a joint intent classification and slot filling model based on BERT.\", \"split_tokens\": \"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'joint', 'intent', 'classification', 'and', 'slot', 'filling', 'model', 'based', 'on', 'BERT', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(87, 91)\", \"within_anchor_index\": 0.0}, {\"pc1\": -13.09432598929986, \"pc2\": -5.1805862312635815, \"word\": \"based\", \"split_0\": \"In this work , we propose\", \"split_1\": \"a joint intent classification and slot filling model based on BERT\", \"split_2\": \".\", \"averb\": \"based\", \"averb_s\": \"['a joint intent classification and slot filling model']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 79.0, \"averb_span1\": 85.0, \"averb_cspan0\": 53.0, \"averb_cspan1\": 59.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we']\", \"root_o\": \"['a joint intent classification and slot filling model based on BERT']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 26, \"root_cspan0\": 18, \"root_cspan1\": 26, \"fverb\": null, \"fword\": null, \"apos\": \"['VERB', 'VERB']\", \"apos_w\": \"['based', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"In this work, we propose a joint intent classification and slot filling model based on BERT.\", \"split_tokens\": \"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'joint', 'intent', 'classification', 'and', 'slot', 'filling', 'model', 'based', 'on', 'BERT', '.']\", \"split_anchor_span\": \"(6, 17)\", \"split_anchor_indices\": \"(25, 91)\", \"within_anchor_index\": 62.0}, {\"pc1\": 25.53623293231249, \"pc2\": 1.0404785183043048, \"word\": \"achieves\", \"split_0\": \"Experimental results demonstrate that\", \"split_1\": \"our proposed model\", \"split_2\": \"achieves significant improvement on intent classification accuracy , slot filling F1 , and sentence - level semantic frame accuracy on several public benchmark datasets , compared to the attention - based recurrent neural network models and slot - gated models\", \"averb\": \"achieves\", \"averb_s\": \"['our proposed model']\", \"averb_o\": \"['significant improvement']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 57.0, \"averb_span1\": 66.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['Experimental results']\", \"root_o\": \"['and sentence - level semantic frame accuracy on several public benchmark datasets , compared to the attention - based recurrent neural network models and slot - gated models']\", \"root_split\": 0, \"root_span0\": 21, \"root_span1\": 33, \"root_cspan0\": 21, \"root_cspan1\": 33, \"fverb\": \"achieves\", \"fword\": \"achieves\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['model', 'achieves', 'demonstrate']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models\", \"split_tokens\": \"['Experimental', 'results', 'demonstrate', 'that', 'our', 'proposed', 'model', 'achieves', 'significant', 'improvement', 'on', 'intent', 'classification', 'accuracy', ',', 'slot', 'filling', 'F1', ',', 'and', 'sentence', '-', 'level', 'semantic', 'frame', 'accuracy', 'on', 'several', 'public', 'benchmark', 'datasets', ',', 'compared', 'to', 'the', 'attention', '-', 'based', 'recurrent', 'neural', 'network', 'models', 'and', 'slot', '-', 'gated', 'models']\", \"split_anchor_span\": \"(4, 7)\", \"split_anchor_indices\": \"(37, 55)\", \"within_anchor_index\": -1.0}, {\"pc1\": 2.9074826205867956, \"pc2\": -1.5844201091798562, \"word\": \"built\", \"split_0\": \"It enables seamless integration of conversation history into a conversational question answering ( ConvQA ) model built on\", \"split_1\": \"BERT\", \"split_2\": \"( Bidirectional Encoder Representations from Transformers ) .\", \"averb\": \"built\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 114.0, \"averb_span1\": 120.0, \"averb_cspan0\": 114.0, \"averb_cspan1\": 120.0, \"root\": \"history\", \"root_full\": \"history\", \"root_s\": \"['It enables', 'seamless integration of']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 48, \"root_span1\": 56, \"root_cspan0\": 48, \"root_cspan1\": 56, \"fverb\": null, \"fword\": \"Bidirectional\", \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'on', 'built']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921\", \"ID\": 19, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"It enables seamless integration of conversation history into a conversational question answering (ConvQA) model built on BERT (Bidirectional Encoder Representations from Transformers).\", \"split_tokens\": \"['It', 'enables', 'seamless', 'integration', 'of', 'conversation', 'history', 'into', 'a', 'conversational', 'question', 'answering', '(', 'ConvQA', ')', 'model', 'built', 'on', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', '.']\", \"split_anchor_span\": \"(18, 19)\", \"split_anchor_indices\": \"(122, 126)\", \"within_anchor_index\": 0.0}, {\"pc1\": -15.979555913267415, \"pc2\": -2.5028257562867378, \"word\": \"BERT\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"with History Answer Embedding for Conversational Question Answering .\", \"averb\": \"BERT\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 0.0, \"averb_span1\": 5.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 5.0, \"root\": \"BERT\", \"root_full\": \"BERT\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 1, \"root_span0\": 0, \"root_span1\": 5, \"root_cspan0\": 0, \"root_cspan1\": 5, \"fverb\": null, \"fword\": \"with\", \"apos\": \"['VERB']\", \"apos_w\": \"['BERT']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921\", \"ID\": 19, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT with History Answer Embedding for Conversational Question Answering.\", \"split_tokens\": \"['BERT', 'with', 'History', 'Answer', 'Embedding', 'for', 'Conversational', 'Question', 'Answering', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -16.46162734531587, \"pc2\": -1.4416591506363197, \"word\": \"studies\", \"split_0\": \"This paper studies the performances and behaviors of\", \"split_1\": \"BERT\", \"split_2\": \"in ranking tasks .\", \"averb\": \"studies\", \"averb_s\": \"['This']\", \"averb_o\": \"['the performances and behaviors of BERT in ranking tasks', '.']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 11.0, \"averb_span1\": 19.0, \"averb_cspan0\": 11.0, \"averb_cspan1\": 19.0, \"root\": \"studies\", \"root_full\": \"studies\", \"root_s\": \"['This']\", \"root_o\": \"['the performances and behaviors of BERT in ranking tasks', '.']\", \"root_split\": 0, \"root_span0\": 11, \"root_span1\": 19, \"root_cspan0\": 11, \"root_cspan1\": 19, \"fverb\": \"ranking\", \"fword\": \"in\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'behaviors']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"This paper studies the performances and behaviors of BERT in ranking tasks.\", \"split_tokens\": \"['This', 'paper', 'studies', 'the', 'performances', 'and', 'behaviors', 'of', 'BERT', 'in', 'ranking', 'tasks', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(52, 56)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.5464219411456297, \"pc2\": 7.313578193781883, \"word\": \"to leverage\", \"split_0\": \"We explore several different ways to leverage\", \"split_1\": \"the pre - trained BERT\", \"split_2\": \"and fine - tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document ranking .\", \"averb\": \"to leverage\", \"averb_s\": \"[]\", \"averb_o\": \"['the pre - trained BERT and', 'fine - tune it on two ranking tasks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 34.0, \"averb_span1\": 46.0, \"averb_cspan0\": 34.0, \"averb_cspan1\": 46.0, \"root\": \"ranking\", \"root_full\": \"ranking\", \"root_s\": \"['We explore several different ways to leverage the pre - trained BERT and fine - tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document']\", \"root_o\": \"['.']\", \"root_split\": 2, \"root_span0\": 173, \"root_span1\": 181, \"root_cspan0\": 104, \"root_cspan1\": 112, \"fverb\": \"ranking\", \"fword\": \"and\", \"apos\": \"['NOUN', 'VERB', 'NOUN']\", \"apos_w\": \"['BERT', 'leverage', 'ways']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking.\", \"split_tokens\": \"['We', 'explore', 'several', 'different', 'ways', 'to', 'leverage', 'the', 'pre', '-', 'trained', 'BERT', 'and', 'fine', '-', 'tune', 'it', 'on', 'two', 'ranking', 'tasks', ':', 'MS', 'MARCO', 'passage', 'reranking', 'and', 'TREC', 'Web', 'Track', 'ad', 'hoc', 'document', 'ranking', '.']\", \"split_anchor_span\": \"(7, 12)\", \"split_anchor_indices\": \"(45, 67)\", \"within_anchor_index\": 18.0}, {\"pc1\": -1.5464219411456297, \"pc2\": 7.313578193781883, \"word\": \"to leverage\", \"split_0\": \"We explore several different ways to leverage the pre - trained BERT and fine - tune\", \"split_1\": \"it\", \"split_2\": \"on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document ranking .\", \"averb\": \"to leverage\", \"averb_s\": \"[]\", \"averb_o\": \"['the pre - trained BERT and', 'fine - tune it on two ranking tasks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 34.0, \"averb_span1\": 46.0, \"averb_cspan0\": 34.0, \"averb_cspan1\": 46.0, \"root\": \"ranking\", \"root_full\": \"ranking\", \"root_s\": \"['We explore several different ways to leverage the pre - trained BERT and fine - tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document']\", \"root_o\": \"['.']\", \"root_split\": 2, \"root_span0\": 173, \"root_span1\": 181, \"root_cspan0\": 85, \"root_cspan1\": 93, \"fverb\": \"ranking\", \"fword\": \"on\", \"apos\": \"['PRON', 'NOUN', 'VERB']\", \"apos_w\": \"['it', 'tune', 'leverage']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking.\", \"split_tokens\": \"['We', 'explore', 'several', 'different', 'ways', 'to', 'leverage', 'the', 'pre', '-', 'trained', 'BERT', 'and', 'fine', '-', 'tune', 'it', 'on', 'two', 'ranking', 'tasks', ':', 'MS', 'MARCO', 'passage', 'reranking', 'and', 'TREC', 'Web', 'Track', 'ad', 'hoc', 'document', 'ranking', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(84, 86)\", \"within_anchor_index\": -1.0}, {\"pc1\": 6.134242974654292, \"pc2\": 2.0509001260563386, \"word\": \"demonstrate\", \"split_0\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of\", \"split_1\": \"BERT\", \"split_2\": \"in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .\", \"averb\": \"demonstrate\", \"averb_s\": \"['Experimental results on MS MARCO', 'the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 33.0, \"averb_span1\": 45.0, \"averb_cspan0\": 33.0, \"averb_cspan1\": 45.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['Experimental results on MS MARCO', 'the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 33, \"root_span1\": 45, \"root_cspan0\": 33, \"root_cspan1\": 45, \"fverb\": \"ranking\", \"fword\": \"in\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'effectiveness']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model.\", \"split_tokens\": \"['Experimental', 'results', 'on', 'MS', 'MARCO', 'demonstrate', 'the', 'strong', 'effectiveness', 'of', 'BERT', 'in', 'question', '-', 'answering', 'focused', 'passage', 'ranking', 'tasks', ',', 'as', 'well', 'as', 'the', 'fact', 'that', 'BERT', 'is', 'a', 'strong', 'interaction', '-', 'based', 'seq2seq', 'matching', 'model', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(72, 76)\", \"within_anchor_index\": 0.0}, {\"pc1\": 6.115006197940247, \"pc2\": 2.1654983583960954, \"word\": \"answering\", \"split_0\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that\", \"split_1\": \"BERT\", \"split_2\": \"is a strong interaction - based seq2seq matching model .\", \"averb\": \"answering\", \"averb_s\": \"[]\", \"averb_o\": \"['focused passage ranking tasks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 92.0, \"averb_span1\": 102.0, \"averb_cspan0\": 92.0, \"averb_cspan1\": 102.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['Experimental results on MS MARCO', 'the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 33, \"root_span1\": 45, \"root_cspan0\": 33, \"root_cspan1\": 45, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'NOUN', 'NOUN']\", \"apos_w\": \"['BERT', 'model', 'fact']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model.\", \"split_tokens\": \"['Experimental', 'results', 'on', 'MS', 'MARCO', 'demonstrate', 'the', 'strong', 'effectiveness', 'of', 'BERT', 'in', 'question', '-', 'answering', 'focused', 'passage', 'ranking', 'tasks', ',', 'as', 'well', 'as', 'the', 'fact', 'that', 'BERT', 'is', 'a', 'strong', 'interaction', '-', 'based', 'seq2seq', 'matching', 'model', '.']\", \"split_anchor_span\": \"(26, 27)\", \"split_anchor_indices\": \"(158, 162)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.3965002112354945, \"pc2\": 1.5638300084078178, \"word\": \"pre\", \"split_0\": \"Experimental results on TREC show the gaps between the\", \"split_1\": \"BERT\", \"split_2\": \"pre-trained on surrounding contexts and the needs of ad hoc document ranking .\", \"averb\": \"pre\", \"averb_s\": \"['the', 'BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 60.0, \"averb_span1\": 64.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 4.0, \"root\": \"results\", \"root_full\": \"results\", \"root_s\": \"['Experimental']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 13, \"root_span1\": 21, \"root_cspan0\": 13, \"root_cspan1\": 21, \"fverb\": \"surrounding\", \"fword\": \"pre-trained\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'pre', 'between']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking.\", \"split_tokens\": \"['Experimental', 'results', 'on', 'TREC', 'show', 'the', 'gaps', 'between', 'the', 'BERT', 'pre-trained', 'on', 'surrounding', 'contexts', 'and', 'the', 'needs', 'of', 'ad', 'hoc', 'document', 'ranking', '.']\", \"split_anchor_span\": \"(9, 10)\", \"split_anchor_indices\": \"(54, 58)\", \"within_anchor_index\": 0.0}, {\"pc1\": 15.809777129490076, \"pc2\": -1.7437010898075211, \"word\": \"allocates\", \"split_0\": \"Analyses illustrate how\", \"split_1\": \"BERT\", \"split_2\": \"allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"allocates\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 29.0, \"averb_span1\": 39.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"allocates\", \"fword\": \"allocates\", \"apos\": \"['PROPN', 'VERB', 'VERB']\", \"apos_w\": \"['BERT', 'allocates', 'illustrate']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(23, 27)\", \"within_anchor_index\": 0.0}, {\"pc1\": 15.809777129490076, \"pc2\": -1.7437010898075211, \"word\": \"allocates\", \"split_0\": \"Analyses illustrate how BERT allocates\", \"split_1\": \"its\", \"split_2\": \"attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"allocates\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 29.0, \"averb_span1\": 39.0, \"averb_cspan0\": 29.0, \"averb_cspan1\": 39.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"prefers\", \"fword\": \"attentions\", \"apos\": \"['DET', 'NOUN', 'VERB']\", \"apos_w\": \"['its', 'attentions', 'allocates']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(38, 41)\", \"within_anchor_index\": -1.0}, {\"pc1\": 15.809777129490076, \"pc2\": -1.7437010898075211, \"word\": \"allocates\", \"split_0\": \"Analyses illustrate how BERT allocates its attentions between query - document tokens in\", \"split_1\": \"its\", \"split_2\": \"Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"allocates\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 29.0, \"averb_span1\": 39.0, \"averb_cspan0\": 29.0, \"averb_cspan1\": 39.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"prefers\", \"fword\": \"Transformer\", \"apos\": \"['DET', 'NOUN', 'ADP']\", \"apos_w\": \"['its', 'layers', 'in']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(13, 14)\", \"split_anchor_indices\": \"(88, 91)\", \"within_anchor_index\": -1.0}, {\"pc1\": 15.871183156262347, \"pc2\": -1.7157059967740684, \"word\": \"prefers\", \"split_0\": \"Analyses illustrate how BERT allocates its attentions between query - document tokens in its Transformer layers , how\", \"split_1\": \"it\", \"split_2\": \"prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"prefers\", \"averb_s\": \"['it']\", \"averb_o\": \"['semantic', 'how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 121.0, \"averb_span1\": 129.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"prefers\", \"fword\": \"prefers\", \"apos\": \"['PRON', 'VERB', 'NOUN']\", \"apos_w\": \"['it', 'prefers', 'document']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(18, 19)\", \"split_anchor_indices\": \"(117, 119)\", \"within_anchor_index\": -1.0}, {\"pc1\": -21.29111332322512, \"pc2\": -1.799687098058203, \"word\": \"Understanding\", \"split_0\": \"Understanding the Behaviors of\", \"split_1\": \"BERT\", \"split_2\": \"in Ranking .\", \"averb\": \"Understanding\", \"averb_s\": \"[]\", \"averb_o\": \"['the Behaviors of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 14.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 14.0, \"root\": \"Understanding\", \"root_full\": \"Understanding\", \"root_s\": \"[]\", \"root_o\": \"['the Behaviors of BERT']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 14, \"root_cspan0\": 0, \"root_cspan1\": 14, \"fverb\": null, \"fword\": \"in\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'Behaviors']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"Understanding the Behaviors of BERT in Ranking.\", \"split_tokens\": \"['Understanding', 'the', 'Behaviors', 'of', 'BERT', 'in', 'Ranking', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(30, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": 1.3776110495119611, \"pc2\": -0.2992696646071981, \"word\": \"achieve\", \"split_0\": \"In this paper , extensive experiments on datasets for these two tasks show that without using any external features , a simple\", \"split_1\": \"BERT\", \"split_2\": \"-based model can achieve state-of-the-art performance .\", \"averb\": \"achieve\", \"averb_s\": \"['can']\", \"averb_o\": \"['any external features , a simple BERT -based model state of - the - art performance', '-']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 149.0, \"averb_span1\": 157.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 25.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"['these two tasks']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 70, \"root_span1\": 75, \"root_cspan0\": 70, \"root_cspan1\": 75, \"fverb\": \"-based\", \"fword\": \"based\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'model', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b\", \"ID\": 21, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'extensive', 'experiments', 'on', 'datasets', 'for', 'these', 'two', 'tasks', 'show', 'that', 'without', 'using', 'any', 'external', 'features', ',', 'a', 'simple', 'BERT', '-based', 'model', 'can', 'achieve', 'state-of-the-art', 'performance', '.']\", \"split_anchor_span\": \"(22, 23)\", \"split_anchor_indices\": \"(126, 130)\", \"within_anchor_index\": 0.0}, {\"pc1\": -16.080554495232853, \"pc2\": -2.400613745468165, \"word\": \"to apply\", \"split_0\": \"To our knowledge , we are the first to successfully apply\", \"split_1\": \"BERT\", \"split_2\": \"in this manner .\", \"averb\": \"to apply\", \"averb_s\": \"[]\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 36.0, \"averb_span1\": 58.0, \"averb_cspan0\": 36.0, \"averb_cspan1\": 58.0, \"root\": \"first\", \"root_full\": \"are first\", \"root_s\": \"['we']\", \"root_o\": \"['to successfully apply BERT in this manner', '.']\", \"root_split\": 0, \"root_span0\": 22, \"root_span1\": 36, \"root_cspan0\": 22, \"root_cspan1\": 36, \"fverb\": null, \"fword\": \"in\", \"apos\": \"['PROPN', 'VERB', 'ADJ']\", \"apos_w\": \"['BERT', 'apply', 'first']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b\", \"ID\": 21, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"To our knowledge, we are the first to successfully apply BERT in this manner.\", \"split_tokens\": \"['To', 'our', 'knowledge', ',', 'we', 'are', 'the', 'first', 'to', 'successfully', 'apply', 'BERT', 'in', 'this', 'manner', '.']\", \"split_anchor_span\": \"(11, 12)\", \"split_anchor_indices\": \"(57, 61)\", \"within_anchor_index\": 0.0}, {\"pc1\": -18.515601431204395, \"pc2\": -2.476266436077016, \"word\": \"provide\", \"split_0\": null, \"split_1\": \"Our models\", \"split_2\": \"provide strong baselines for future research\", \"averb\": \"provide\", \"averb_s\": \"['Our models']\", \"averb_o\": \"['strong baselines']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 11.0, \"averb_span1\": 19.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"provide\", \"root_full\": \"provide\", \"root_s\": \"['Our models']\", \"root_o\": \"['strong baselines']\", \"root_split\": 2, \"root_span0\": 11, \"root_span1\": 19, \"root_cspan0\": 0, \"root_cspan1\": 8, \"fverb\": \"provide\", \"fword\": \"provide\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['models', 'provide']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b\", \"ID\": 21, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Our models provide strong baselines for future research\", \"split_tokens\": \"['Our', 'models', 'provide', 'strong', 'baselines', 'for', 'future', 'research']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 10)\", \"within_anchor_index\": -1.0}, {\"pc1\": 8.065804661895944, \"pc2\": 5.424399408815661, \"word\": \"explore\", \"split_0\": \"We explore the multi-task learning setting for the recent\", \"split_1\": \"BERT\", \"split_2\": \"model on the GLUE benchmark , and how to best add task-specific parameters to a pre-trained BERT network , with a high degree of parameter sharing between tasks .\", \"averb\": \"explore\", \"averb_s\": \"['We']\", \"averb_o\": \"['the multi - task learning setting for the recent BERT model on the GLUE benchmark ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"add\", \"root_full\": \"add\", \"root_s\": \"['and how to best']\", \"root_o\": \"['We explore the multi - task learning setting for the recent BERT model on the GLUE benchmark , task - specific parameters , with a high degree of parameter sharing between tasks .']\", \"root_split\": 2, \"root_span0\": 111, \"root_span1\": 115, \"root_cspan0\": 48, \"root_cspan1\": 52, \"fverb\": \"best\", \"fword\": \"model\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'model', 'for']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark, and how to best add task-specific parameters to a pre-trained BERT network, with a high degree of parameter sharing between tasks.\", \"split_tokens\": \"['We', 'explore', 'the', 'multi-task', 'learning', 'setting', 'for', 'the', 'recent', 'BERT', 'model', 'on', 'the', 'GLUE', 'benchmark', ',', 'and', 'how', 'to', 'best', 'add', 'task-specific', 'parameters', 'to', 'a', 'pre-trained', 'BERT', 'network', ',', 'with', 'a', 'high', 'degree', 'of', 'parameter', 'sharing', 'between', 'tasks', '.']\", \"split_anchor_span\": \"(9, 10)\", \"split_anchor_indices\": \"(57, 61)\", \"within_anchor_index\": 0.0}, {\"pc1\": 7.984442461935405, \"pc2\": 5.446661630466005, \"word\": \"add\", \"split_0\": \"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark , and how to best add task-specific parameters to a pre-trained\", \"split_1\": \"BERT\", \"split_2\": \"network , with a high degree of parameter sharing between tasks .\", \"averb\": \"add\", \"averb_s\": \"['and how to best']\", \"averb_o\": \"['We explore the multi - task learning setting for the recent BERT model on the GLUE benchmark , task - specific parameters , with a high degree of parameter sharing between tasks .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 111.0, \"averb_span1\": 115.0, \"averb_cspan0\": 111.0, \"averb_cspan1\": 115.0, \"root\": \"add\", \"root_full\": \"add\", \"root_s\": \"['and how to best']\", \"root_o\": \"['We explore the multi - task learning setting for the recent BERT model on the GLUE benchmark , task - specific parameters , with a high degree of parameter sharing between tasks .']\", \"root_split\": 0, \"root_span0\": 111, \"root_span1\": 115, \"root_cspan0\": 111, \"root_cspan1\": 115, \"fverb\": \"sharing\", \"fword\": \"network\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'network', 'to']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark, and how to best add task-specific parameters to a pre-trained BERT network, with a high degree of parameter sharing between tasks.\", \"split_tokens\": \"['We', 'explore', 'the', 'multi-task', 'learning', 'setting', 'for', 'the', 'recent', 'BERT', 'model', 'on', 'the', 'GLUE', 'benchmark', ',', 'and', 'how', 'to', 'best', 'add', 'task-specific', 'parameters', 'to', 'a', 'pre-trained', 'BERT', 'network', ',', 'with', 'a', 'high', 'degree', 'of', 'parameter', 'sharing', 'between', 'tasks', '.']\", \"split_anchor_span\": \"(26, 27)\", \"split_anchor_indices\": \"(154, 158)\", \"within_anchor_index\": 0.0}, {\"pc1\": 8.499570488485128, \"pc2\": 3.1018638802836054, \"word\": \"using\", \"split_0\": \"By using PALs in parallel with\", \"split_1\": \"BERT\", \"split_2\": \"layers , we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"averb\": \"using\", \"averb_s\": \"[]\", \"averb_o\": \"['PALs']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 9.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 9.0, \"root\": \"match\", \"root_full\": \"match\", \"root_s\": \"['we', 'By using PALs in parallel with BERT layers', ',']\", \"root_o\": \"['the performance of fine - tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state - of - the - art results on the Recognizing Textual Entailment dataset']\", \"root_split\": 2, \"root_span0\": 48, \"root_span1\": 54, \"root_cspan0\": 12, \"root_cspan1\": 18, \"fverb\": \"match\", \"fword\": \"layers\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'layers', 'with']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"By using PALs in parallel with BERT layers, we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"split_tokens\": \"['By', 'using', 'PALs', 'in', 'parallel', 'with', 'BERT', 'layers', ',', 'we', 'match', 'the', 'performance', 'of', 'fine-tuned', 'BERT', 'on', 'the', 'GLUE', 'benchmark', 'with', 'roughly', '7', 'times', 'fewer', 'parameters', ',', 'and', 'obtain', 'state-of-the-art', 'results', 'on', 'the', 'Recognizing', 'Textual', 'Entailment', 'dataset']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(30, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": 8.509954907469922, \"pc2\": 3.0050773503976727, \"word\": \"tuned\", \"split_0\": \"By using PALs in parallel with BERT layers , we match the performance of fine-tuned\", \"split_1\": \"BERT\", \"split_2\": \"on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"averb\": \"tuned\", \"averb_s\": \"['-']\", \"averb_o\": \"['BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state - of - the - art results on the Recognizing Textual Entailment dataset']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 80.0, \"averb_span1\": 86.0, \"averb_cspan0\": 80.0, \"averb_cspan1\": 86.0, \"root\": \"match\", \"root_full\": \"match\", \"root_s\": \"['we', 'By using PALs in parallel with BERT layers', ',']\", \"root_o\": \"['the performance of fine - tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state - of - the - art results on the Recognizing Textual Entailment dataset']\", \"root_split\": 0, \"root_span0\": 48, \"root_span1\": 54, \"root_cspan0\": 48, \"root_cspan1\": 54, \"fverb\": \"obtain\", \"fword\": \"on\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'tuned', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"By using PALs in parallel with BERT layers, we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"split_tokens\": \"['By', 'using', 'PALs', 'in', 'parallel', 'with', 'BERT', 'layers', ',', 'we', 'match', 'the', 'performance', 'of', 'fine-tuned', 'BERT', 'on', 'the', 'GLUE', 'benchmark', 'with', 'roughly', '7', 'times', 'fewer', 'parameters', ',', 'and', 'obtain', 'state-of-the-art', 'results', 'on', 'the', 'Recognizing', 'Textual', 'Entailment', 'dataset']\", \"split_anchor_span\": \"(15, 16)\", \"split_anchor_indices\": \"(83, 87)\", \"within_anchor_index\": 0.0}, {\"pc1\": 46.76531666974059, \"pc2\": -5.62851851096166, \"word\": \"apply\", \"split_0\": \"As a case study , we apply these diagnostics to\", \"split_1\": \"the popular BERT model\", \"split_2\": \", finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"apply\", \"averb_s\": \"['we', ',', 'finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and', 'As a case study ,']\", \"averb_o\": \"['these diagnostics']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 21.0, \"averb_span1\": 27.0, \"averb_cspan0\": 21.0, \"averb_cspan1\": 27.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 318, \"root_cspan1\": 332, \"fverb\": \"finding\", \"fword\": \"finding\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['model', 'to', 'apply']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(10, 14)\", \"split_anchor_indices\": \"(47, 69)\", \"within_anchor_index\": 12.0}, {\"pc1\": 46.77526501700878, \"pc2\": -5.670308783410714, \"word\": \"can distinguish\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that\", \"split_1\": \"it\", \"split_2\": \"can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"can distinguish\", \"averb_s\": \"['it']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 89.0, \"averb_span1\": 115.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 26.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 300, \"root_cspan1\": 314, \"fverb\": \"distinguish\", \"fword\": \"can\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['it', 'distinguish', 'finding']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(17, 18)\", \"split_anchor_indices\": \"(85, 87)\", \"within_anchor_index\": -1.0}, {\"pc1\": 46.86636289215458, \"pc2\": -5.741114062082731, \"word\": \"shows\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and\", \"split_1\": \"it\", \"split_2\": \"robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"shows\", \"averb_s\": \"['it', 'As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and', 'robustly retrieves noun hypernyms it struggles with challenging inference and role - based event prediction \\u2014 and , in particular ,']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 377.0, \"averb_span1\": 383.0, \"averb_cspan0\": 141.0, \"averb_cspan1\": 147.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 153, \"root_cspan1\": 167, \"fverb\": \"retrieves\", \"fword\": \"robustly\", \"apos\": \"['PRON', 'VERB', 'ADJ']\", \"apos_w\": \"['it', 'shows', 'clear']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(40, 41)\", \"split_anchor_indices\": \"(232, 234)\", \"within_anchor_index\": -1.0}, {\"pc1\": 46.929133480362346, \"pc2\": -5.728965618175366, \"word\": \"struggles\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but\", \"split_1\": \"it\", \"split_2\": \"struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"struggles\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 279.0, \"averb_span1\": 289.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 110, \"root_cspan1\": 124, \"fverb\": \"struggles\", \"fword\": \"struggles\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['it', 'struggles', 'shows']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(47, 48)\", \"split_anchor_indices\": \"(275, 277)\", \"within_anchor_index\": -1.0}, {\"pc1\": 46.86636289215458, \"pc2\": -5.741114062082731, \"word\": \"shows\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular ,\", \"split_1\": \"it\", \"split_2\": \"shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"shows\", \"averb_s\": \"['it', 'As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and', 'robustly retrieves noun hypernyms it struggles with challenging inference and role - based event prediction \\u2014 and , in particular ,']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 377.0, \"averb_span1\": 383.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 6.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 12, \"root_cspan1\": 26, \"fverb\": \"shows\", \"fword\": \"shows\", \"apos\": \"['PRON', 'VERB', 'ADJ']\", \"apos_w\": \"['it', 'shows', 'clear']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(64, 65)\", \"split_anchor_indices\": \"(373, 375)\", \"within_anchor_index\": -1.0}, {\"pc1\": -13.377248846524092, \"pc2\": -2.1396088644545848, \"word\": \"Is\", \"split_0\": \"What\", \"split_1\": \"BERT\", \"split_2\": \"Is Not : Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .\", \"averb\": \"Is\", \"averb_s\": \"['What', 'BERT']\", \"averb_o\": \"['Not', ': Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 10.0, \"averb_span1\": 13.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"Is\", \"root_full\": \"Is\", \"root_s\": \"['What', 'BERT']\", \"root_o\": \"['Not', ': Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .']\", \"root_split\": 2, \"root_span0\": 10, \"root_span1\": 13, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"Is\", \"fword\": \"Is\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['BERT', 'Is']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.\", \"split_tokens\": \"['What', 'BERT', 'Is', 'Not', ':', 'Lessons', 'from', 'a', 'New', 'Suite', 'of', 'Psycholinguistic', 'Diagnostics', 'for', 'Language', 'Models', '.']\", \"split_anchor_span\": \"(1, 2)\", \"split_anchor_indices\": \"(4, 8)\", \"within_anchor_index\": 0.0}, {\"pc1\": -15.164167817672364, \"pc2\": 0.30270740043505306, \"word\": \"pre\", \"split_0\": \"Language model pre - training , such as\", \"split_1\": \"BERT\", \"split_2\": \", has achieved remarkable results in many NLP tasks .\", \"averb\": \"pre\", \"averb_s\": \"['model']\", \"averb_o\": \"['-', 'training , such as BERT ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 15.0, \"averb_span1\": 19.0, \"averb_cspan0\": 15.0, \"averb_cspan1\": 19.0, \"root\": \"achieved\", \"root_full\": \"has achieved\", \"root_s\": \"['Language model pre - training , such as BERT ,']\", \"root_o\": \"['remarkable results']\", \"root_split\": 2, \"root_span0\": 47, \"root_span1\": 60, \"root_cspan0\": 2, \"root_cspan1\": 15, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'SCONJ', 'ADJ']\", \"apos_w\": \"['BERT', 'as', 'training']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks.\", \"split_tokens\": \"['Language', 'model', 'pre', '-', 'training', ',', 'such', 'as', 'BERT', ',', 'has', 'achieved', 'remarkable', 'results', 'in', 'many', 'NLP', 'tasks', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(39, 43)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.97633376950277, \"pc2\": -4.1344034651895605, \"word\": \"tuning\", \"split_0\": \"In this paper , we propose to visualize loss landscapes and optimization trajectories of fine - tuning\", \"split_1\": \"BERT\", \"split_2\": \"on specific datasets .\", \"averb\": \"tuning\", \"averb_s\": \"['-']\", \"averb_o\": \"['BERT on specific datasets']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 96.0, \"averb_span1\": 103.0, \"averb_cspan0\": 96.0, \"averb_cspan1\": 103.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['and']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": \"on\", \"apos\": \"['NOUN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'tuning', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'propose', 'to', 'visualize', 'loss', 'landscapes', 'and', 'optimization', 'trajectories', 'of', 'fine', '-', 'tuning', 'BERT', 'on', 'specific', 'datasets', '.']\", \"split_anchor_span\": \"(17, 18)\", \"split_anchor_indices\": \"(102, 106)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.012870102196549, \"pc2\": -2.079347251840327, \"word\": \"demonstrate\", \"split_0\": \"We also demonstrate that the fine - tuning procedure is robust to overfitting , even though\", \"split_1\": \"BERT\", \"split_2\": \"is highly over - parameterized for downstream tasks .\", \"averb\": \"demonstrate\", \"averb_s\": \"['We']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 8.0, \"averb_span1\": 20.0, \"averb_cspan0\": 8.0, \"averb_cspan1\": 20.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['We']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 8, \"root_span1\": 20, \"root_cspan0\": 8, \"root_cspan1\": 20, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'ADV', 'ADJ']\", \"apos_w\": \"['BERT', 'highly', 'robust']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks.\", \"split_tokens\": \"['We', 'also', 'demonstrate', 'that', 'the', 'fine', '-', 'tuning', 'procedure', 'is', 'robust', 'to', 'overfitting', ',', 'even', 'though', 'BERT', 'is', 'highly', 'over', '-', 'parameterized', 'for', 'downstream', 'tasks', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(91, 95)\", \"within_anchor_index\": 0.0}, {\"pc1\": 9.686440324183168, \"pc2\": -2.270323315461095, \"word\": \"tuning\", \"split_0\": \"Second , the visualization results indicate that fine - tuning\", \"split_1\": \"BERT\", \"split_2\": \"tends to generalize better because of the flat and wide optima , and the consistency between the training loss surface and the generalization error surface .\", \"averb\": \"tuning\", \"averb_s\": \"['-']\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 56.0, \"averb_span1\": 63.0, \"averb_cspan0\": 56.0, \"averb_cspan1\": 63.0, \"root\": \"indicate\", \"root_full\": \"indicate\", \"root_s\": \"[', the visualization results']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 35, \"root_span1\": 44, \"root_cspan0\": 35, \"root_cspan1\": 44, \"fverb\": \"tends\", \"fword\": \"tends\", \"apos\": \"['PROPN', 'VERB', 'VERB']\", \"apos_w\": \"['BERT', 'tuning', 'tends']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface.\", \"split_tokens\": \"['Second', ',', 'the', 'visualization', 'results', 'indicate', 'that', 'fine', '-', 'tuning', 'BERT', 'tends', 'to', 'generalize', 'better', 'because', 'of', 'the', 'flat', 'and', 'wide', 'optima', ',', 'and', 'the', 'consistency', 'between', 'the', 'training', 'loss', 'surface', 'and', 'the', 'generalization', 'error', 'surface', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(62, 66)\", \"within_anchor_index\": 0.0}, {\"pc1\": -19.199411148090554, \"pc2\": -2.0125387231995044, \"word\": \"Understanding\", \"split_0\": \"Visualizing and Understanding the Effectiveness of\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"Understanding\", \"averb_s\": \"[]\", \"averb_o\": \"['the Effectiveness of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 16.0, \"averb_span1\": 30.0, \"averb_cspan0\": 16.0, \"averb_cspan1\": 30.0, \"root\": \"Visualizing\", \"root_full\": \"Visualizing\", \"root_s\": \"[]\", \"root_o\": \"['Understanding the Effectiveness of BERT']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 12, \"root_cspan0\": 0, \"root_cspan1\": 12, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'PROPN']\", \"apos_w\": \"['BERT', 'of', 'Effectiveness']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"Visualizing and Understanding the Effectiveness of BERT.\", \"split_tokens\": \"['Visualizing', 'and', 'Understanding', 'the', 'Effectiveness', 'of', 'BERT', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(50, 54)\", \"within_anchor_index\": 0.0}, {\"pc1\": -9.320923172234586, \"pc2\": -1.1379960966125853, \"word\": \"propose\", \"split_0\": \"We propose\", \"split_1\": \"a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation\", \"split_2\": \".\", \"averb\": \"propose\", \"averb_s\": \"['We']\", \"averb_o\": \"['a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['We']\", \"root_o\": \"['a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 11, \"root_cspan0\": 3, \"root_cspan1\": 11, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['method', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We propose a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation.\", \"split_tokens\": \"['We', 'propose', 'a', 'novel', 'data', 'augmentation', 'method', 'for', 'labeled', 'sentences', 'called', 'conditional', 'BERT', 'contextual', 'augmentation', '.']\", \"split_anchor_span\": \"(2, 15)\", \"split_anchor_indices\": \"(10, 112)\", \"within_anchor_index\": 74.0}, {\"pc1\": 14.586931620161927, \"pc2\": 2.1612672532703794, \"word\": \"appeared\", \"split_0\": \"We retrofit BERT to conditional BERT by introducing a new conditional masked language model ( The term \\u201c conditional masked language model \\u201d appeared once in original\", \"split_1\": \"BERT\", \"split_2\": \"paper , which indicates context - conditional , is equivalent to term \\u201c masked language model \\u201d .\", \"averb\": \"appeared\", \"averb_s\": \"[]\", \"averb_o\": \"['once']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 141.0, \"averb_span1\": 150.0, \"averb_cspan0\": 141.0, \"averb_cspan1\": 150.0, \"root\": \"model\", \"root_full\": \"model\", \"root_s\": \"['\\u201c', 'masked']\", \"root_o\": \"['We retrofit BERT to conditional BERT by introducing a new conditional masked language model ( The term \\u201c conditional masked language model \\u201d appeared once in original BERT paper , term \\u201d .']\", \"root_split\": 2, \"root_span0\": 260, \"root_span1\": 266, \"root_cspan0\": 88, \"root_cspan1\": 94, \"fverb\": \"indicates\", \"fword\": \"paper\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'paper', 'in']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"We retrofit BERT to conditional BERT by introducing a new conditional masked language model (The term \\u201cconditional masked language model\\u201d appeared once in original BERT paper, which indicates context-conditional, is equivalent to term \\u201cmasked language model\\u201d.\", \"split_tokens\": \"['We', 'retrofit', 'BERT', 'to', 'conditional', 'BERT', 'by', 'introducing', 'a', 'new', 'conditional', 'masked', 'language', 'model', '(', 'The', 'term', '\\u201c', 'conditional', 'masked', 'language', 'model', '\\u201d', 'appeared', 'once', 'in', 'original', 'BERT', 'paper', ',', 'which', 'indicates', 'context', '-', 'conditional', ',', 'is', 'equivalent', 'to', 'term', '\\u201c', 'masked', 'language', 'model', '\\u201d', '.']\", \"split_anchor_span\": \"(27, 28)\", \"split_anchor_indices\": \"(166, 170)\", \"within_anchor_index\": 0.0}, {\"pc1\": -14.641780711830563, \"pc2\": -1.9669880755841245, \"word\": \"can\", \"split_0\": \"The well trained conditional\", \"split_1\": \"BERT\", \"split_2\": \"can be applied to enhance contextual augmentation .\", \"averb\": \"can\", \"averb_s\": \"['The well trained conditional BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 34.0, \"averb_span1\": 38.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 4.0, \"root\": \"applied\", \"root_full\": \"can be applied\", \"root_s\": \"[]\", \"root_o\": \"['to enhance contextual augmentation']\", \"root_split\": 2, \"root_span0\": 34, \"root_span1\": 49, \"root_cspan0\": 0, \"root_cspan1\": 15, \"fverb\": \"be\", \"fword\": \"can\", \"apos\": \"['NOUN', 'ADJ', 'VERB']\", \"apos_w\": \"['BERT', 'conditional', 'can']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"The well trained conditional BERT can be applied to enhance contextual augmentation.\", \"split_tokens\": \"['The', 'well', 'trained', 'conditional', 'BERT', 'can', 'be', 'applied', 'to', 'enhance', 'contextual', 'augmentation', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(28, 32)\", \"within_anchor_index\": 0.0}, {\"pc1\": 5.508474958665745, \"pc2\": -5.451151628910967, \"word\": \"show\", \"split_0\": \"Experiments on six various different text classification tasks show that\", \"split_1\": \"our method\", \"split_2\": \"can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement\", \"averb\": \"show\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 63.0, \"averb_span1\": 68.0, \"averb_cspan0\": 63.0, \"averb_cspan1\": 68.0, \"root\": \"tasks\", \"root_full\": \"tasks\", \"root_s\": \"['various different classification']\", \"root_o\": \"['Experiments on show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement']\", \"root_split\": 0, \"root_span0\": 57, \"root_span1\": 63, \"root_cspan0\": 57, \"root_cspan1\": 63, \"fverb\": \"be\", \"fword\": \"can\", \"apos\": \"['NOUN', 'ADV', 'VERB']\", \"apos_w\": \"['method', 'easily', 'show']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 8, \"Text\": \"Experiments on six various different text classification tasks show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement\", \"split_tokens\": \"['Experiments', 'on', 'six', 'various', 'different', 'text', 'classification', 'tasks', 'show', 'that', 'our', 'method', 'can', 'be', 'easily', 'applied', 'to', 'both', 'convolutional', 'or', 'recurrent', 'neural', 'networks', 'classifier', 'to', 'obtain', 'improvement']\", \"split_anchor_span\": \"(10, 12)\", \"split_anchor_indices\": \"(72, 82)\", \"within_anchor_index\": -1.0}, {\"pc1\": -2.1791947173128166, \"pc2\": -4.041745522551116, \"word\": \"Starting\", \"split_0\": \"Starting from a public multilingual\", \"split_1\": \"BERT\", \"split_2\": \"checkpoint , our final model is 6x smaller and 27x faster , and has higher accuracy than a state-of-the-art multilingual baseline .\", \"averb\": \"Starting\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 9.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"6x\", \"root_full\": \"is 6x\", \"root_s\": \"['Starting from a public multilingual BERT checkpoint , our final model', 'and']\", \"root_o\": \"['smaller', 'has higher accuracy than a state - of - the - art multilingual baseline .']\", \"root_split\": 2, \"root_span0\": 70, \"root_span1\": 76, \"root_cspan0\": 29, \"root_cspan1\": 35, \"fverb\": \"is\", \"fword\": \"checkpoint\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'checkpoint', 'from']\", \"URL\": \"https://www.semanticscholar.org/paper/Small-and-Practical-BERT-Models-for-Sequence-Tsai-Riesa/2f9d4887d0022400fc40c774c4c78350c3bc5390\", \"ID\": 26, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Starting from a public multilingual BERT checkpoint, our final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-the-art multilingual baseline.\", \"split_tokens\": \"['Starting', 'from', 'a', 'public', 'multilingual', 'BERT', 'checkpoint', ',', 'our', 'final', 'model', 'is', '6x', 'smaller', 'and', '27x', 'faster', ',', 'and', 'has', 'higher', 'accuracy', 'than', 'a', 'state-of-the-art', 'multilingual', 'baseline', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(35, 39)\", \"within_anchor_index\": 0.0}, {\"pc1\": 18.369317630367487, \"pc2\": 3.597921891020929, \"word\": \"using\", \"split_0\": \"Recently , a simple combination of passage retrieval using off-the-shelf IR techniques and a\", \"split_1\": \"BERT\", \"split_2\": \"reader was found to be very effective for question answering directly on Wikipedia , yielding a large improvement over the previous state of the art on a standard benchmark dataset .\", \"averb\": \"using\", \"averb_s\": \"[]\", \"averb_o\": \"['off', '- the - shelf IR techniques and a BERT reader']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 53.0, \"averb_span1\": 59.0, \"averb_cspan0\": 53.0, \"averb_cspan1\": 59.0, \"root\": \"found\", \"root_full\": \"was found\", \"root_s\": \"['a simple combination of passage retrieval using off - the - shelf IR techniques and a BERT reader', 'Recently ,']\", \"root_o\": \"['to be very effective for question answering directly on Wikipedia , yielding a large improvement over the previous state of the art on a standard benchmark dataset']\", \"root_split\": 2, \"root_span0\": 109, \"root_span1\": 119, \"root_cspan0\": 11, \"root_cspan1\": 21, \"fverb\": \"was\", \"fword\": \"reader\", \"apos\": \"['PROPN', 'NOUN', 'CCONJ']\", \"apos_w\": \"['BERT', 'reader', 'and']\", \"URL\": \"https://www.semanticscholar.org/paper/Data-Augmentation-for-BERT-Fine-Tuning-in-Question-Yang-Xie/f5eaf727b80240a13e9f631211c9ecec7e3b9feb\", \"ID\": 27, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset.\", \"split_tokens\": \"['Recently', ',', 'a', 'simple', 'combination', 'of', 'passage', 'retrieval', 'using', 'off-the-shelf', 'IR', 'techniques', 'and', 'a', 'BERT', 'reader', 'was', 'found', 'to', 'be', 'very', 'effective', 'for', 'question', 'answering', 'directly', 'on', 'Wikipedia', ',', 'yielding', 'a', 'large', 'improvement', 'over', 'the', 'previous', 'state', 'of', 'the', 'art', 'on', 'a', 'standard', 'benchmark', 'dataset', '.']\", \"split_anchor_span\": \"(14, 15)\", \"split_anchor_indices\": \"(92, 96)\", \"within_anchor_index\": 0.0}, {\"pc1\": 6.542696909755476, \"pc2\": -4.812515720989858, \"word\": \"argue\", \"split_0\": \"We take issue with this interpretation and argue that the performance of\", \"split_1\": \"BERT\", \"split_2\": \"is partly due to reasoning about ( the surface form of ) entity names , e.g. , guessing that a person with an Italian - sounding name speaks Italian .\", \"averb\": \"argue\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 43.0, \"averb_span1\": 49.0, \"averb_cspan0\": 43.0, \"averb_cspan1\": 49.0, \"root\": \"issue\", \"root_full\": \"issue\", \"root_s\": \"['take']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 8, \"root_span1\": 14, \"root_cspan0\": 8, \"root_cspan1\": 14, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'performance']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.\", \"split_tokens\": \"['We', 'take', 'issue', 'with', 'this', 'interpretation', 'and', 'argue', 'that', 'the', 'performance', 'of', 'BERT', 'is', 'partly', 'due', 'to', 'reasoning', 'about', '(', 'the', 'surface', 'form', 'of', ')', 'entity', 'names', ',', 'e.g.', ',', 'guessing', 'that', 'a', 'person', 'with', 'an', 'Italian', '-', 'sounding', 'name', 'speaks', 'Italian', '.']\", \"split_anchor_span\": \"(12, 13)\", \"split_anchor_indices\": \"(72, 76)\", \"within_anchor_index\": 0.0}, {\"pc1\": -9.82103792736106, \"pc2\": -3.1577397546847803, \"word\": \"drops\", \"split_0\": \"More specifically , we show that\", \"split_1\": \"BERT 's\", \"split_2\": \"precision drops dramatically when we filter certain easy - to - guess facts .\", \"averb\": \"drops\", \"averb_s\": \"[\\\"BERT 's precision\\\"]\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 51.0, \"averb_span1\": 57.0, \"averb_cspan0\": 10.0, \"averb_cspan1\": 16.0, \"root\": \"guess\", \"root_full\": \"guess\", \"root_s\": \"['when we filter certain easy -', 'to', '-']\", \"root_o\": \"[\\\"More specifically , we show that BERT 's precision drops dramatically facts\\\", '.']\", \"root_split\": 2, \"root_span0\": 105, \"root_span1\": 111, \"root_cspan0\": 64, \"root_cspan1\": 70, \"fverb\": \"filter\", \"fword\": \"precision\", \"apos\": \"['PROPN', 'NOUN', 'VERB']\", \"apos_w\": \"['BERT', 'precision', 'drops']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"More specifically, we show that BERT's precision drops dramatically when we filter certain easy-to-guess facts.\", \"split_tokens\": \"['More', 'specifically', ',', 'we', 'show', 'that', 'BERT', \\\"'s\\\", 'precision', 'drops', 'dramatically', 'when', 'we', 'filter', 'certain', 'easy', '-', 'to', '-', 'guess', 'facts', '.']\", \"split_anchor_span\": \"(6, 8)\", \"split_anchor_indices\": \"(32, 39)\", \"within_anchor_index\": 0.0}, {\"pc1\": -9.81003413084617, \"pc2\": -0.7699027526675387, \"word\": \"propose\", \"split_0\": \"As a remedy , we propose E - BERT , an extension of\", \"split_1\": \"BERT\", \"split_2\": \"that replaces entity mentions with symbolic entity embeddings .\", \"averb\": \"propose\", \"averb_s\": \"['we', 'As a remedy']\", \"averb_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 17.0, \"averb_span1\": 25.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 25.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we', 'As a remedy']\", \"root_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"root_split\": 0, \"root_span0\": 17, \"root_span1\": 25, \"root_cspan0\": 17, \"root_cspan1\": 25, \"fverb\": \"replaces\", \"fword\": \"that\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'extension']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings.\", \"split_tokens\": \"['As', 'a', 'remedy', ',', 'we', 'propose', 'E', '-', 'BERT', ',', 'an', 'extension', 'of', 'BERT', 'that', 'replaces', 'entity', 'mentions', 'with', 'symbolic', 'entity', 'embeddings', '.']\", \"split_anchor_span\": \"(13, 14)\", \"split_anchor_indices\": \"(51, 55)\", \"within_anchor_index\": 0.0}, {\"pc1\": -9.81003413084617, \"pc2\": -0.7699027526675387, \"word\": \"propose\", \"split_0\": \"As a remedy , we propose\", \"split_1\": \"E - BERT , an extension of BERT that replaces entity mentions with symbolic entity embeddings\", \"split_2\": \".\", \"averb\": \"propose\", \"averb_s\": \"['we', 'As a remedy']\", \"averb_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 17.0, \"averb_span1\": 25.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 25.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we', 'As a remedy']\", \"root_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"root_split\": 0, \"root_span0\": 17, \"root_span1\": 25, \"root_cspan0\": 17, \"root_cspan1\": 25, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['E', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings.\", \"split_tokens\": \"['As', 'a', 'remedy', ',', 'we', 'propose', 'E', '-', 'BERT', ',', 'an', 'extension', 'of', 'BERT', 'that', 'replaces', 'entity', 'mentions', 'with', 'symbolic', 'entity', 'embeddings', '.']\", \"split_anchor_span\": \"(6, 22)\", \"split_anchor_indices\": \"(24, 117)\", \"within_anchor_index\": 4.0}, {\"pc1\": -10.275391036511065, \"pc2\": 2.5346234681979887, \"word\": \"show\", \"split_0\": \"We take this as evidence that E - BERT is richer in factual knowledge , and we show two ways of ensembling\", \"split_1\": \"BERT\", \"split_2\": \"and E - BERT\", \"averb\": \"show\", \"averb_s\": \"['we']\", \"averb_o\": \"['two ways of ensembling BERT and E - BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 79.0, \"averb_span1\": 84.0, \"averb_cspan0\": 79.0, \"averb_cspan1\": 84.0, \"root\": \"take\", \"root_full\": \"take\", \"root_s\": \"['We']\", \"root_o\": \"['this']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": null, \"fword\": \"and\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'ways']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT\", \"split_tokens\": \"['We', 'take', 'this', 'as', 'evidence', 'that', 'E', '-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E', '-', 'BERT']\", \"split_anchor_span\": \"(22, 23)\", \"split_anchor_indices\": \"(106, 110)\", \"within_anchor_index\": 0.0}, {\"pc1\": -10.264617966862058, \"pc2\": 2.582029432925815, \"word\": \"take\", \"split_0\": \"We take this as evidence that\", \"split_1\": \"E - BERT\", \"split_2\": \"is richer in factual knowledge , and we show two ways of ensembling BERT and E - BERT\", \"averb\": \"take\", \"averb_s\": \"['We']\", \"averb_o\": \"['this']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 8.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 8.0, \"root\": \"take\", \"root_full\": \"take\", \"root_s\": \"['We']\", \"root_o\": \"['this']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['ADJ', 'NOUN', 'SCONJ']\", \"apos_w\": \"['richer', 'evidence', 'as']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT\", \"split_tokens\": \"['We', 'take', 'this', 'as', 'evidence', 'that', 'E', '-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E', '-', 'BERT']\", \"split_anchor_span\": \"(6, 9)\", \"split_anchor_indices\": \"(29, 37)\", \"within_anchor_index\": 4.0}, {\"pc1\": -10.275391036511065, \"pc2\": 2.5346234681979887, \"word\": \"show\", \"split_0\": \"We take this as evidence that E - BERT is richer in factual knowledge , and we show two ways of ensembling BERT and\", \"split_1\": \"E - BERT\", \"split_2\": null, \"averb\": \"show\", \"averb_s\": \"['we']\", \"averb_o\": \"['two ways of ensembling BERT and E - BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 79.0, \"averb_span1\": 84.0, \"averb_cspan0\": 79.0, \"averb_cspan1\": 84.0, \"root\": \"take\", \"root_full\": \"take\", \"root_s\": \"['We']\", \"root_o\": \"['this']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'BERT', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT\", \"split_tokens\": \"['We', 'take', 'this', 'as', 'evidence', 'that', 'E', '-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E', '-', 'BERT']\", \"split_anchor_span\": \"(24, 27)\", \"split_anchor_indices\": \"(115, 123)\", \"within_anchor_index\": 4.0}, {\"pc1\": -14.961777703612377, \"pc2\": 3.218632581972147, \"word\": \"is\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"is Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA .\", \"averb\": \"is\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name - Based Reasoning in Unsupervised QA']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 5.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"is\", \"root_full\": \"is\", \"root_s\": \"['BERT']\", \"root_o\": \"['Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name - Based Reasoning in Unsupervised QA']\", \"root_split\": 2, \"root_span0\": 5, \"root_span1\": 8, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['BERT', 'is']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA.\", \"split_tokens\": \"['BERT', 'is', 'Not', 'a', 'Knowledge', 'Base', '(', 'Yet', ')', ':', 'Factual', 'Knowledge', 'vs.', 'Name-Based', 'Reasoning', 'in', 'Unsupervised', 'QA', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -10.029133122793302, \"pc2\": 0.315557680468066, \"word\": \"produced\", \"split_0\": \"However , just how contextual are the contextualized representations produced by models such as ELMo and\", \"split_1\": \"BERT\", \"split_2\": \"?\", \"averb\": \"produced\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 69.0, \"averb_span1\": 78.0, \"averb_cspan0\": 69.0, \"averb_cspan1\": 78.0, \"root\": \"representations\", \"root_full\": \"are representations\", \"root_s\": \"[]\", \"root_o\": \"['produced by models such as ELMo and BERT ?']\", \"root_split\": 0, \"root_span0\": 30, \"root_span1\": 69, \"root_cspan0\": 30, \"root_cspan1\": 69, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'NOUN', 'SCONJ']\", \"apos_w\": \"['BERT', 'ELMo', 'as']\", \"URL\": \"https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39\", \"ID\": 29, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"However, just how contextual are the contextualized representations produced by models such as ELMo and BERT?\", \"split_tokens\": \"['However', ',', 'just', 'how', 'contextual', 'are', 'the', 'contextualized', 'representations', 'produced', 'by', 'models', 'such', 'as', 'ELMo', 'and', 'BERT', '?']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(104, 108)\", \"within_anchor_index\": 0.0}, {\"pc1\": 12.985027240036281, \"pc2\": 1.4046219799912898, \"word\": \"be explained\", \"split_0\": \"In all layers of ELMo ,\", \"split_1\": \"BERT\", \"split_2\": \", and GPT-2 , on average , less than 5 % of the variance in a word 's contextualized representations can be explained by a static embedding for that word , providing some justification for the success of contextualized representations\", \"averb\": \"be explained\", \"averb_s\": \"['can']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 134.0, \"averb_span1\": 147.0, \"averb_cspan0\": 105.0, \"averb_cspan1\": 118.0, \"root\": \"explained\", \"root_full\": \"be explained\", \"root_s\": \"['can']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 134, \"root_span1\": 147, \"root_cspan0\": 105, \"root_cspan1\": 118, \"fverb\": \"be\", \"fword\": \"and\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'layers']\", \"URL\": \"https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39\", \"ID\": 29, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations\", \"split_tokens\": \"['In', 'all', 'layers', 'of', 'ELMo', ',', 'BERT', ',', 'and', 'GPT-2', ',', 'on', 'average', ',', 'less', 'than', '5', '%', 'of', 'the', 'variance', 'in', 'a', 'word', \\\"'s\\\", 'contextualized', 'representations', 'can', 'be', 'explained', 'by', 'a', 'static', 'embedding', 'for', 'that', 'word', ',', 'providing', 'some', 'justification', 'for', 'the', 'success', 'of', 'contextualized', 'representations']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(23, 27)\", \"within_anchor_index\": 0.0}, {\"pc1\": -20.17227156282166, \"pc2\": 1.4086123408359406, \"word\": \"Comparing\", \"split_0\": \"Comparing the Geometry of\", \"split_1\": \"BERT\", \"split_2\": \", ELMo , and GPT-2 Embeddings .\", \"averb\": \"Comparing\", \"averb_s\": \"[]\", \"averb_o\": \"['the', 'Geometry of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 10.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \",\", \"root_full\": \",\", \"root_s\": \"[]\", \"root_o\": \"['Comparing the Geometry of BERT ELMo , and GPT-2 Embeddings .']\", \"root_split\": 2, \"root_span0\": 31, \"root_span1\": 33, \"root_cspan0\": 0, \"root_cspan1\": 2, \"fverb\": null, \"fword\": \"ELMo\", \"apos\": \"['PROPN', 'ADP', 'PROPN']\", \"apos_w\": \"['BERT', 'of', 'Geometry']\", \"URL\": \"https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39\", \"ID\": 29, \"Type\": \"Title\", \"Index\": 1, \"Text\": \"Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.\", \"split_tokens\": \"['Comparing', 'the', 'Geometry', 'of', 'BERT', ',', 'ELMo', ',', 'and', 'GPT-2', 'Embeddings', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(25, 29)\", \"within_anchor_index\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_embeds(output_w2v_inv, csv, tooltip=['word', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-afebfa3fc02b4858ae9a265ebb6c4d16\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-afebfa3fc02b4858ae9a265ebb6c4d16\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-afebfa3fc02b4858ae9a265ebb6c4d16\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-7fadd7085a44f410c618303f68565459\"}, \"mark\": {\"type\": \"circle\", \"size\": 60}, \"encoding\": {\"tooltip\": [{\"type\": \"nominal\", \"field\": \"word\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"pc1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"pc2\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-7fadd7085a44f410c618303f68565459\": [{\"pc1\": 2.3088281141766673, \"pc2\": -4.752868897107626, \"word\": \"stands\", \"split_0\": \"We introduce\", \"split_1\": \"a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers\", \"split_2\": \".\", \"averb\": \"stands\", \"averb_s\": \"['which', 'We introduce a new language representation model called BERT ,']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 69.0, \"averb_span1\": 76.0, \"averb_cspan0\": 56.0, \"averb_cspan1\": 63.0, \"root\": \"stands\", \"root_full\": \"stands\", \"root_s\": \"['which', 'We introduce a new language representation model called BERT ,']\", \"root_o\": \"[]\", \"root_split\": 1, \"root_span0\": 69, \"root_span1\": 76, \"root_cspan0\": 56, \"root_cspan1\": 63, \"fverb\": null, \"fword\": null, \"apos\": \"['VERB']\", \"apos_w\": \"['stands']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\", \"split_tokens\": \"['We', 'introduce', 'a', 'new', 'language', 'representation', 'model', 'called', 'BERT', ',', 'which', 'stands', 'for', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '.']\", \"split_anchor_span\": \"(2, 18)\", \"split_anchor_indices\": \"(12, 134)\", \"within_anchor_index\": 43.0}, {\"pc1\": 10.688732285707385, \"pc2\": -7.917455201640106, \"word\": \"is designed\", \"split_0\": \"Unlike recent language representation models ,\", \"split_1\": \"BERT\", \"split_2\": \"is designed to pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .\", \"averb\": \"is designed\", \"averb_s\": \"['recent language representation models , BERT']\", \"averb_o\": \"['pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 52.0, \"averb_span1\": 64.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"designed\", \"root_full\": \"is designed\", \"root_s\": \"['recent language representation models , BERT']\", \"root_o\": \"['pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .']\", \"root_split\": 2, \"root_span0\": 52, \"root_span1\": 64, \"root_cspan0\": 0, \"root_cspan1\": 12, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'PUNCT', 'NOUN']\", \"apos_w\": \"['BERT', ',', 'models']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\", \"split_tokens\": \"['Unlike', 'recent', 'language', 'representation', 'models', ',', 'BERT', 'is', 'designed', 'to', 'pre', '-', 'train', 'deep', 'bidirectional', 'representations', 'from', 'unlabeled', 'text', 'by', 'jointly', 'conditioning', 'on', 'both', 'left', 'and', 'right', 'context', 'in', 'all', 'layers', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(46, 50)\", \"within_anchor_index\": 0.0}, {\"pc1\": 2.262778527333707, \"pc2\": -4.971942072944934, \"word\": \"is\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"is conceptually simple and empirically powerful .\", \"averb\": \"is\", \"averb_s\": \"['BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 5.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"conceptually\", \"root_full\": \"is conceptually\", \"root_s\": \"[]\", \"root_o\": \"['simple empirically', 'powerful']\", \"root_split\": 2, \"root_span0\": 5, \"root_span1\": 21, \"root_cspan0\": 0, \"root_cspan1\": 16, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX', 'ADV']\", \"apos_w\": \"['BERT', 'is', 'conceptually']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"BERT is conceptually simple and empirically powerful.\", \"split_tokens\": \"['BERT', 'is', 'conceptually', 'simple', 'and', 'empirically', 'powerful', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.7213449729321696, \"pc2\": -0.35503138355747665, \"word\": \"obtains\", \"split_0\": null, \"split_1\": \"It\", \"split_2\": \"obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQuAD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement )\", \"averb\": \"obtains\", \"averb_s\": \"[]\", \"averb_o\": \"['new state - - the -', 'of art', 'results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % % absolute improvement']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"point\", \"root_full\": \"point\", \"root_s\": \"['It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test 93.2 ( 1.5 and SQuAD v2.0 Test F1 to 83.1 ( 5.1']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 353, \"root_span1\": 359, \"root_cspan0\": 350, \"root_cspan1\": 356, \"fverb\": \"obtains\", \"fword\": \"obtains\", \"apos\": \"['PRON', 'VERB', 'NOUN']\", \"apos_w\": \"['It', 'obtains', 'question']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)\", \"split_tokens\": \"['It', 'obtains', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'eleven', 'natural', 'language', 'processing', 'tasks', ',', 'including', 'pushing', 'the', 'GLUE', 'score', 'to', '80.5', '%', '(', '7.7', '%', 'point', 'absolute', 'improvement', ')', ',', 'MultiNLI', 'accuracy', 'to', '86.7', '%', '(', '4.6', '%', 'absolute', 'improvement', ')', ',', 'SQuAD', 'v1.1', 'question', 'answering', 'Test', 'F1', 'to', '93.2', '(', '1.5', 'point', 'absolute', 'improvement', ')', 'and', 'SQuAD', 'v2.0', 'Test', 'F1', 'to', '83.1', '(', '5.1', 'point', 'absolute', 'improvement', ')']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 2)\", \"within_anchor_index\": -1.0}, {\"pc1\": -4.331394555687752, \"pc2\": 1.0289345441990454, \"word\": \"present\", \"split_0\": \"We present a replication study of\", \"split_1\": \"BERT\", \"split_2\": \"pretraining ( Devlin et al . , 2019 ) that carefully measures the impact of many key hyperparameters and training data size .\", \"averb\": \"present\", \"averb_s\": \"['We']\", \"averb_o\": \"['a replication study of BERT pretraining ( Devlin et al . , 2019 )']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"measures\", \"root_full\": \"measures\", \"root_s\": \"['that']\", \"root_o\": \"['the impact of many key hyperparameters and training data size .']\", \"root_split\": 2, \"root_span0\": 92, \"root_span1\": 101, \"root_cspan0\": 53, \"root_cspan1\": 62, \"fverb\": \"measures\", \"fword\": \"pretraining\", \"apos\": \"['PROPN', 'PROPN', 'ADP']\", \"apos_w\": \"['BERT', 'et', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"ID\": 1, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size.\", \"split_tokens\": \"['We', 'present', 'a', 'replication', 'study', 'of', 'BERT', 'pretraining', '(', 'Devlin', 'et', 'al', '.', ',', '2019', ')', 'that', 'carefully', 'measures', 'the', 'impact', 'of', 'many', 'key', 'hyperparameters', 'and', 'training', 'data', 'size', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(33, 37)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.795577907041295, \"pc2\": 1.4183128572841412, \"word\": \"find\", \"split_0\": \"We find that\", \"split_1\": \"BERT\", \"split_2\": \"was significantly undertrained , and can match or exceed the performance of every model published after it .\", \"averb\": \"find\", \"averb_s\": \"[]\", \"averb_o\": \"['that BERT was significantly undertrained , and can match or exceed the performance of every model published after it .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 8.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 8.0, \"root\": \"We\", \"root_full\": \"We\", \"root_s\": \"[]\", \"root_o\": \"['find that BERT was significantly undertrained , and can match or exceed the performance of every model published after it .']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"was\", \"fword\": \"was\", \"apos\": \"['PROPN', 'ADV', 'VERB']\", \"apos_w\": \"['BERT', 'significantly', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"ID\": 1, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.\", \"split_tokens\": \"['We', 'find', 'that', 'BERT', 'was', 'significantly', 'undertrained', ',', 'and', 'can', 'match', 'or', 'exceed', 'the', 'performance', 'of', 'every', 'model', 'published', 'after', 'it', '.']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(12, 16)\", \"within_anchor_index\": 0.0}, {\"pc1\": 2.0343162405051873, \"pc2\": -5.995096717197631, \"word\": \"published\", \"split_0\": \"We find that BERT was significantly undertrained , and can match or exceed the performance of every model published after\", \"split_1\": \"it\", \"split_2\": \".\", \"averb\": \"published\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 106.0, \"averb_span1\": 116.0, \"averb_cspan0\": 106.0, \"averb_cspan1\": 116.0, \"root\": \"We\", \"root_full\": \"We\", \"root_s\": \"[]\", \"root_o\": \"['find that BERT was significantly undertrained , and can match or exceed the performance of every model published after it .']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": null, \"fword\": null, \"apos\": \"['PRON', 'ADP', 'VERB']\", \"apos_w\": \"['it', 'after', 'published']\", \"URL\": \"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"ID\": 1, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.\", \"split_tokens\": \"['We', 'find', 'that', 'BERT', 'was', 'significantly', 'undertrained', ',', 'and', 'can', 'match', 'or', 'exceed', 'the', 'performance', 'of', 'every', 'model', 'published', 'after', 'it', '.']\", \"split_anchor_span\": \"(20, 21)\", \"split_anchor_indices\": \"(121, 123)\", \"within_anchor_index\": -1.0}, {\"pc1\": -1.1022504080951268, \"pc2\": -1.5921310758252685, \"word\": \"called\", \"split_0\": \"In this work , we propose a method to pre-train a smaller general-purpose language representation model , called\", \"split_1\": \"DistilBERT\", \"split_2\": \", which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts .\", \"averb\": \"called\", \"averb_s\": \"[]\", \"averb_o\": \"['DistilBERT', ',']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 110.0, \"averb_span1\": 117.0, \"averb_cspan0\": 110.0, \"averb_cspan1\": 117.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['a method to pre - train a smaller general - purpose language representation model , called DistilBERT ,', 'which can then be fine - tuned with good performances on a wide range of tasks like its larger counterparts .']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 26, \"root_cspan0\": 18, \"root_cspan1\": 26, \"fverb\": \"be\", \"fword\": \"which\", \"apos\": \"['NUM', 'VERB', 'NOUN']\", \"apos_w\": \"['DistilBERT', 'called', 'model']\", \"URL\": \"https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38\", \"ID\": 2, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts.\", \"split_tokens\": \"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'method', 'to', 'pre-train', 'a', 'smaller', 'general-purpose', 'language', 'representation', 'model', ',', 'called', 'DistilBERT', ',', 'which', 'can', 'then', 'be', 'fine-tuned', 'with', 'good', 'performances', 'on', 'a', 'wide', 'range', 'of', 'tasks', 'like', 'its', 'larger', 'counterparts', '.']\", \"split_anchor_span\": \"(18, 19)\", \"split_anchor_indices\": \"(112, 122)\", \"within_anchor_index\": 6.0}, {\"pc1\": 5.493282636630489, \"pc2\": 7.590940113372872, \"word\": \"to reduce\", \"split_0\": \"While most prior work investigated the use of distillation for building task - specific models , we leverage knowledge distillation during the pre - training phase and show that it is possible to reduce the size of\", \"split_1\": \"a BERT model\", \"split_2\": \"by 40 % , while retaining 97 % of its language understanding capabilities and being 60 % faster .\", \"averb\": \"to reduce\", \"averb_s\": \"[]\", \"averb_o\": \"['we leverage knowledge distillation during phase', 'the size of a BERT model']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 193.0, \"averb_span1\": 203.0, \"averb_cspan0\": 193.0, \"averb_cspan1\": 203.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 168, \"root_span1\": 173, \"root_cspan0\": 168, \"root_cspan1\": 173, \"fverb\": \"retaining\", \"fword\": \"by\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['model', 'of', 'size']\", \"URL\": \"https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38\", \"ID\": 2, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.\", \"split_tokens\": \"['While', 'most', 'prior', 'work', 'investigated', 'the', 'use', 'of', 'distillation', 'for', 'building', 'task', '-', 'specific', 'models', ',', 'we', 'leverage', 'knowledge', 'distillation', 'during', 'the', 'pre', '-', 'training', 'phase', 'and', 'show', 'that', 'it', 'is', 'possible', 'to', 'reduce', 'the', 'size', 'of', 'a', 'BERT', 'model', 'by', '40', '%', ',', 'while', 'retaining', '97', '%', 'of', 'its', 'language', 'understanding', 'capabilities', 'and', 'being', '60', '%', 'faster', '.']\", \"split_anchor_span\": \"(37, 40)\", \"split_anchor_indices\": \"(214, 226)\", \"within_anchor_index\": 2.0}, {\"pc1\": 0.1470081995418847, \"pc2\": 1.7862747431183683, \"word\": \"retaining\", \"split_0\": \"While most prior work investigated the use of distillation for building task - specific models , we leverage knowledge distillation during the pre - training phase and show that it is possible to reduce the size of a BERT model by 40 % , while retaining 97 % of\", \"split_1\": \"its\", \"split_2\": \"language understanding capabilities and being 60 % faster .\", \"averb\": \"retaining\", \"averb_s\": \"[]\", \"averb_o\": \"['97 % of its language understanding capabilities and being 60 % faster .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 244.0, \"averb_span1\": 254.0, \"averb_cspan0\": 244.0, \"averb_cspan1\": 254.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 168, \"root_span1\": 173, \"root_cspan0\": 168, \"root_cspan1\": 173, \"fverb\": \"being\", \"fword\": \"language\", \"apos\": \"['DET', 'NOUN', 'ADP']\", \"apos_w\": \"['its', 'capabilities', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38\", \"ID\": 2, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.\", \"split_tokens\": \"['While', 'most', 'prior', 'work', 'investigated', 'the', 'use', 'of', 'distillation', 'for', 'building', 'task', '-', 'specific', 'models', ',', 'we', 'leverage', 'knowledge', 'distillation', 'during', 'the', 'pre', '-', 'training', 'phase', 'and', 'show', 'that', 'it', 'is', 'possible', 'to', 'reduce', 'the', 'size', 'of', 'a', 'BERT', 'model', 'by', '40', '%', ',', 'while', 'retaining', '97', '%', 'of', 'its', 'language', 'understanding', 'capabilities', 'and', 'being', '60', '%', 'faster', '.']\", \"split_anchor_span\": \"(49, 50)\", \"split_anchor_indices\": \"(261, 264)\", \"within_anchor_index\": -1.0}, {\"pc1\": -0.2832730776503403, \"pc2\": -2.0888896850768126, \"word\": \"focus\", \"split_0\": \"We focus on\", \"split_1\": \"one such model , BERT\", \"split_2\": \", and aim to quantify where linguistic information is captured within the network .\", \"averb\": \"focus\", \"averb_s\": \"['We']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 9.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 9.0, \"root\": \"captured\", \"root_full\": \"is captured\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 85, \"root_span1\": 97, \"root_cspan0\": 51, \"root_cspan1\": 63, \"fverb\": \"quantify\", \"fword\": \"and\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['model', 'on', 'focus']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c\", \"ID\": 3, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network.\", \"split_tokens\": \"['We', 'focus', 'on', 'one', 'such', 'model', ',', 'BERT', ',', 'and', 'aim', 'to', 'quantify', 'where', 'linguistic', 'information', 'is', 'captured', 'within', 'the', 'network', '.']\", \"split_anchor_span\": \"(3, 8)\", \"split_anchor_indices\": \"(11, 32)\", \"within_anchor_index\": 17.0}, {\"pc1\": -3.4406096186156834, \"pc2\": 0.0794549180447799, \"word\": \"represents\", \"split_0\": \"We find that\", \"split_1\": \"the model\", \"split_2\": \"represents the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence : POS tagging , parsing , NER , semantic roles , then coreference .\", \"averb\": \"represents\", \"averb_s\": \"['the model']\", \"averb_o\": \"['the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence :']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 23.0, \"averb_span1\": 34.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 11.0, \"root\": \"roles\", \"root_full\": \"roles\", \"root_s\": \"['POS tagging , parsing ,', 'NER', ',', 'semantic']\", \"root_o\": \"[',', 'We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence : then coreference .']\", \"root_split\": 2, \"root_span0\": 238, \"root_span1\": 244, \"root_cspan0\": 215, \"root_cspan1\": 221, \"fverb\": \"represents\", \"fword\": \"represents\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['model', 'represents', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c\", \"ID\": 3, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference.\", \"split_tokens\": \"['We', 'find', 'that', 'the', 'model', 'represents', 'the', 'steps', 'of', 'the', 'traditional', 'NLP', 'pipeline', 'in', 'an', 'interpretable', 'and', 'localizable', 'way', ',', 'and', 'that', 'the', 'regions', 'responsible', 'for', 'each', 'step', 'appear', 'in', 'the', 'expected', 'sequence', ':', 'POS', 'tagging', ',', 'parsing', ',', 'NER', ',', 'semantic', 'roles', ',', 'then', 'coreference', '.']\", \"split_anchor_span\": \"(3, 5)\", \"split_anchor_indices\": \"(12, 21)\", \"within_anchor_index\": -1.0}, {\"pc1\": 6.803683367187818, \"pc2\": 1.284679871079163, \"word\": \"does adjust\", \"split_0\": \"Qualitative analysis reveals that\", \"split_1\": \"the model\", \"split_2\": \"can and often does adjust this pipeline dynamically , revising lower - level decisions on the basis of disambiguating information from higher - level representations .\", \"averb\": \"does adjust\", \"averb_s\": \"['the model can and often']\", \"averb_o\": \"['this pipeline dynamically , revising lower - level decisions on the basis of disambiguating information from higher - level representations .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 58.0, \"averb_span1\": 70.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 26.0, \"root\": \"analysis\", \"root_full\": \"analysis\", \"root_s\": \"[]\", \"root_o\": \"['reveals that the model can and often does adjust this pipeline dynamically , revising lower - level decisions on the basis of disambiguating information from higher - level representations .']\", \"root_split\": 0, \"root_span0\": 12, \"root_span1\": 21, \"root_cspan0\": 12, \"root_cspan1\": 21, \"fverb\": \"does\", \"fword\": \"can\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['model', 'adjust', 'reveals']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c\", \"ID\": 3, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.\", \"split_tokens\": \"['Qualitative', 'analysis', 'reveals', 'that', 'the', 'model', 'can', 'and', 'often', 'does', 'adjust', 'this', 'pipeline', 'dynamically', ',', 'revising', 'lower', '-', 'level', 'decisions', 'on', 'the', 'basis', 'of', 'disambiguating', 'information', 'from', 'higher', '-', 'level', 'representations', '.']\", \"split_anchor_span\": \"(4, 6)\", \"split_anchor_indices\": \"(33, 42)\", \"within_anchor_index\": -1.0}, {\"pc1\": 5.683740533790265, \"pc2\": -3.409931221241922, \"word\": \"have had\", \"split_0\": null, \"split_1\": \"Large pre - trained neural networks such as BERT\", \"split_2\": \"have had great recent success in NLP , motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data .\", \"averb\": \"have had\", \"averb_s\": \"['Large pre - trained neural networks such as BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 49.0, \"averb_span1\": 58.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"success\", \"root_full\": \"success\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 22, \"root_cspan1\": 30, \"fverb\": \"have\", \"fword\": \"have\", \"apos\": \"['ADJ', 'VERB', 'ADJ']\", \"apos_w\": \"['Large', 'had', 'recent']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.\", \"split_tokens\": \"['Large', 'pre', '-', 'trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']\", \"split_anchor_span\": \"(0, 9)\", \"split_anchor_indices\": \"(0, 48)\", \"within_anchor_index\": 44.0}, {\"pc1\": -2.605174087708782, \"pc2\": 0.7934940351038225, \"word\": \"motivating\", \"split_0\": \"Large pre - trained neural networks such as BERT have had great recent success in NLP , motivating a growing body of research investigating what aspects of language\", \"split_1\": \"they\", \"split_2\": \"are able to learn from unlabeled data .\", \"averb\": \"motivating\", \"averb_s\": \"[',']\", \"averb_o\": \"['a growing body of research investigating what']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 88.0, \"averb_span1\": 99.0, \"averb_cspan0\": 88.0, \"averb_cspan1\": 99.0, \"root\": \"success\", \"root_full\": \"success\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 71, \"root_cspan1\": 79, \"fverb\": \"are\", \"fword\": \"are\", \"apos\": \"['PRON', 'ADJ', 'VERB']\", \"apos_w\": \"['they', 'able', 'motivating']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.\", \"split_tokens\": \"['Large', 'pre', '-', 'trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']\", \"split_anchor_span\": \"(28, 29)\", \"split_anchor_indices\": \"(164, 168)\", \"within_anchor_index\": -1.0}, {\"pc1\": 5.683740533790267, \"pc2\": -3.4099312212419233, \"word\": \"have had\", \"split_0\": \"Large pre - trained neural networks such as\", \"split_1\": \"BERT\", \"split_2\": \"have had great recent success in NLP , motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data .\", \"averb\": \"have had\", \"averb_s\": \"['Large pre - trained neural networks such as BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 49.0, \"averb_span1\": 58.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"success\", \"root_full\": \"success\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 22, \"root_cspan1\": 30, \"fverb\": \"have\", \"fword\": \"have\", \"apos\": \"['PROPN', 'SCONJ', 'NOUN']\", \"apos_w\": \"['BERT', 'as', 'networks']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.\", \"split_tokens\": \"['Large', 'pre', '-', 'trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(43, 47)\", \"within_anchor_index\": 0.0}, {\"pc1\": 1.1238627068167024, \"pc2\": 0.20505386975970294, \"word\": \"apply\", \"split_0\": \"Complementary to these works , we propose methods for analyzing the attention mechanisms of pre - trained models and apply them to\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"apply\", \"averb_s\": \"[]\", \"averb_o\": \"['them']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 117.0, \"averb_span1\": 123.0, \"averb_cspan0\": 117.0, \"averb_cspan1\": 123.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['Complementary to these works', ',', 'we']\", \"root_o\": \"['methods for analyzing the attention mechanisms of pre - trained models and apply them to BERT']\", \"root_split\": 0, \"root_span0\": 34, \"root_span1\": 42, \"root_cspan0\": 34, \"root_cspan1\": 42, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'to', 'apply']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT.\", \"split_tokens\": \"['Complementary', 'to', 'these', 'works', ',', 'we', 'propose', 'methods', 'for', 'analyzing', 'the', 'attention', 'mechanisms', 'of', 'pre', '-', 'trained', 'models', 'and', 'apply', 'them', 'to', 'BERT', '.']\", \"split_anchor_span\": \"(22, 23)\", \"split_anchor_indices\": \"(130, 134)\", \"within_anchor_index\": 0.0}, {\"pc1\": 12.029947517137838, \"pc2\": -2.926607809945206, \"word\": \"is captured\", \"split_0\": \"Lastly , we propose an attention - based probing classifier and use it to further demonstrate that substantial syntactic information is captured in\", \"split_1\": \"BERT 's\", \"split_2\": \"attention .\", \"averb\": \"is captured\", \"averb_s\": \"['substantial syntactic information']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 133.0, \"averb_span1\": 145.0, \"averb_cspan0\": 133.0, \"averb_cspan1\": 145.0, \"root\": \"Lastly\", \"root_full\": \"Lastly\", \"root_s\": \"[]\", \"root_o\": \"[\\\"we propose an attention - based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT 's attention .\\\"]\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 7, \"root_cspan0\": 0, \"root_cspan1\": 7, \"fverb\": null, \"fword\": \"attention\", \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'in', 'captured']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.\", \"split_tokens\": \"['Lastly', ',', 'we', 'propose', 'an', 'attention', '-', 'based', 'probing', 'classifier', 'and', 'use', 'it', 'to', 'further', 'demonstrate', 'that', 'substantial', 'syntactic', 'information', 'is', 'captured', 'in', 'BERT', \\\"'s\\\", 'attention', '.']\", \"split_anchor_span\": \"(23, 25)\", \"split_anchor_indices\": \"(147, 154)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.8188269825275185, \"pc2\": 2.128231779043771, \"word\": \"describe\", \"split_0\": \"In this paper , we describe\", \"split_1\": \"a simple re - implementation of BERT for query - based passage re - ranking\", \"split_2\": \".\", \"averb\": \"describe\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"['a simple re - implementation of BERT for query - based passage re - ranking']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 28.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 28.0, \"root\": \"describe\", \"root_full\": \"describe\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['a simple re - implementation of BERT for query - based passage re - ranking']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 28, \"root_cspan0\": 19, \"root_cspan1\": 28, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['implementation', 'describe']\", \"URL\": \"https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2\", \"ID\": 5, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'describe', 'a', 'simple', 're', '-', 'implementation', 'of', 'BERT', 'for', 'query', '-', 'based', 'passage', 're', '-', 'ranking', '.']\", \"split_anchor_span\": \"(6, 21)\", \"split_anchor_indices\": \"(27, 102)\", \"within_anchor_index\": 32.0}, {\"pc1\": 1.1541033229598001, \"pc2\": -3.6914587339674094, \"word\": \"is\", \"split_0\": null, \"split_1\": \"Our system\", \"split_2\": \"is the state of the art on the TREC - CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task , outperforming the previous state of the art by 27 % ( relative ) in MRR@10 .\", \"averb\": \"is\", \"averb_s\": \"['Our system']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 11.0, \"averb_span1\": 14.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"task\", \"root_full\": \"task\", \"root_s\": \"['Our system is the state of the art on the TREC - CAR dataset and the top entry in the leaderboard of', 'the MS MARCO passage retrieval']\", \"root_o\": \"[',', 'outperforming the previous state of the art by 27 % ( relative ) in MRR@10 .']\", \"root_split\": 2, \"root_span0\": 132, \"root_span1\": 137, \"root_cspan0\": 121, \"root_cspan1\": 126, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'AUX', 'NOUN']\", \"apos_w\": \"['system', 'is', 'state']\", \"URL\": \"https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2\", \"ID\": 5, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10.\", \"split_tokens\": \"['Our', 'system', 'is', 'the', 'state', 'of', 'the', 'art', 'on', 'the', 'TREC', '-', 'CAR', 'dataset', 'and', 'the', 'top', 'entry', 'in', 'the', 'leaderboard', 'of', 'the', 'MS', 'MARCO', 'passage', 'retrieval', 'task', ',', 'outperforming', 'the', 'previous', 'state', 'of', 'the', 'art', 'by', '27', '%', '(', 'relative', ')', 'in', 'MRR@10', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 10)\", \"within_anchor_index\": -1.0}, {\"pc1\": -4.632508009759049, \"pc2\": 1.2821917326267394, \"word\": \"assess\", \"split_0\": \"I assess the extent to which\", \"split_1\": \"the recently introduced BERT model\", \"split_2\": \"captures English syntactic phenomena , using ( 1 ) naturally - occurring subject - verb agreement stimuli ; ( 2 ) \\\" coloreless green ideas \\\" subject - verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part - of - speech and inflection ; and ( 3 ) manually crafted stimuli for subject - verb agreement and reflexive anaphora phenomena .\", \"averb\": \"assess\", \"averb_s\": \"['I']\", \"averb_o\": \"['the extent the recently introduced BERT model captures English syntactic , using ( 1 ) naturally - occurring subject - verb agreement stimuli ( 2 ) \\\" coloreless green ideas \\\"', 'subject - verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part - of - speech and inflection ; and']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 2.0, \"averb_span1\": 9.0, \"averb_cspan0\": 2.0, \"averb_cspan1\": 9.0, \"root\": \")\", \"root_full\": \")\", \"root_s\": \"['(', '3']\", \"root_o\": \"['I assess the extent to which the recently introduced BERT model captures English syntactic phenomena , using ( 1 ) naturally - occurring subject - verb agreement stimuli ; ( 2 ) \\\" coloreless green ideas \\\" subject - verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part - of - speech and inflection ; and crafted stimuli for subject - verb agreement and reflexive anaphora phenomena .']\", \"root_split\": 2, \"root_span0\": 378, \"root_span1\": 380, \"root_cspan0\": 314, \"root_cspan1\": 316, \"fverb\": \"using\", \"fword\": \"captures\", \"apos\": \"['NOUN', 'ADJ', 'NOUN']\", \"apos_w\": \"['model', 'syntactic', 'extent']\", \"URL\": \"https://www.semanticscholar.org/paper/Assessing-BERT's-Syntactic-Abilities-Goldberg/efeab0dcdb4c1cce5e537e57745d84774be99b9a\", \"ID\": 6, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) \\\"coloreless green ideas\\\" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.\", \"split_tokens\": \"['I', 'assess', 'the', 'extent', 'to', 'which', 'the', 'recently', 'introduced', 'BERT', 'model', 'captures', 'English', 'syntactic', 'phenomena', ',', 'using', '(', '1', ')', 'naturally', '-', 'occurring', 'subject', '-', 'verb', 'agreement', 'stimuli', ';', '(', '2', ')', '\\\"', 'coloreless', 'green', 'ideas', '\\\"', 'subject', '-', 'verb', 'agreement', 'stimuli', ',', 'in', 'which', 'content', 'words', 'in', 'natural', 'sentences', 'are', 'randomly', 'replaced', 'with', 'words', 'sharing', 'the', 'same', 'part', '-', 'of', '-', 'speech', 'and', 'inflection', ';', 'and', '(', '3', ')', 'manually', 'crafted', 'stimuli', 'for', 'subject', '-', 'verb', 'agreement', 'and', 'reflexive', 'anaphora', 'phenomena', '.']\", \"split_anchor_span\": \"(6, 11)\", \"split_anchor_indices\": \"(28, 62)\", \"within_anchor_index\": 24.0}, {\"pc1\": 1.9420962638849248, \"pc2\": -3.1963373798207857, \"word\": \"performs\", \"split_0\": null, \"split_1\": \"The BERT model\", \"split_2\": \"performs remarkably well on all cases .\", \"averb\": \"performs\", \"averb_s\": \"['The BERT model']\", \"averb_o\": \"['well on all cases', '.']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 15.0, \"averb_span1\": 24.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"performs\", \"root_full\": \"performs\", \"root_s\": \"['The BERT model']\", \"root_o\": \"['well on all cases', '.']\", \"root_split\": 2, \"root_span0\": 15, \"root_span1\": 24, \"root_cspan0\": 0, \"root_cspan1\": 9, \"fverb\": null, \"fword\": \"performs\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['model', 'performs']\", \"URL\": \"https://www.semanticscholar.org/paper/Assessing-BERT's-Syntactic-Abilities-Goldberg/efeab0dcdb4c1cce5e537e57745d84774be99b9a\", \"ID\": 6, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"The BERT model performs remarkably well on all cases.\", \"split_tokens\": \"['The', 'BERT', 'model', 'performs', 'remarkably', 'well', 'on', 'all', 'cases', '.']\", \"split_anchor_span\": \"(0, 3)\", \"split_anchor_indices\": \"(0, 14)\", \"within_anchor_index\": 4.0}, {\"pc1\": -1.769614573828509, \"pc2\": 0.03286946977226375, \"word\": \"includes\", \"split_0\": \"A new release of\", \"split_1\": \"BERT\", \"split_2\": \"( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task .\", \"averb\": \"includes\", \"averb_s\": \"['new release of BERT ( Devlin , 2018', ')']\", \"averb_o\": \"['a model simultaneously pretrained on 104 languages with impressive performance for zero - shot cross - lingual transfer on a natural language inference task']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 40.0, \"averb_span1\": 49.0, \"averb_cspan0\": 18.0, \"averb_cspan1\": 27.0, \"root\": \"includes\", \"root_full\": \"includes\", \"root_s\": \"['new release of BERT ( Devlin , 2018', ')']\", \"root_o\": \"['a model simultaneously pretrained on 104 languages with impressive performance for zero - shot cross - lingual transfer on a natural language inference task']\", \"root_split\": 2, \"root_span0\": 40, \"root_span1\": 49, \"root_cspan0\": 18, \"root_cspan1\": 27, \"fverb\": \"includes\", \"fword\": \"Devlin\", \"apos\": \"['PROPN', 'PROPN', 'ADP']\", \"apos_w\": \"['BERT', 'Devlin', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task.\", \"split_tokens\": \"['A', 'new', 'release', 'of', 'BERT', '(', 'Devlin', ',', '2018', ')', 'includes', 'a', 'model', 'simultaneously', 'pretrained', 'on', '104', 'languages', 'with', 'impressive', 'performance', 'for', 'zero-shot', 'cross-lingual', 'transfer', 'on', 'a', 'natural', 'language', 'inference', 'task', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(16, 20)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.793837646669032, \"pc2\": 2.322668922765249, \"word\": \"explores\", \"split_0\": \"This paper explores the broader cross-lingual potential of\", \"split_1\": \"mBERT\", \"split_2\": \"( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families : NLI , document classification , NER , POS tagging , and dependency parsing .\", \"averb\": \"explores\", \"averb_s\": \"['This', 'paper']\", \"averb_o\": \"['the broader cross - lingual potential of mBERT ( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 11.0, \"averb_span1\": 20.0, \"averb_cspan0\": 11.0, \"averb_cspan1\": 20.0, \"root\": \"NER\", \"root_full\": \"NER\", \"root_s\": \"['This paper explores the broader cross - lingual potential of mBERT ( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families : NLI , document classification , .']\", \"root_o\": \"[', POS tagging and dependency parsing', ',']\", \"root_split\": 2, \"root_span0\": 236, \"root_span1\": 240, \"root_cspan0\": 171, \"root_cspan1\": 175, \"fverb\": \"covering\", \"fword\": \"multilingual\", \"apos\": \"['ADJ', 'ADJ', 'ADP']\", \"apos_w\": \"['mBERT', 'multilingual', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing.\", \"split_tokens\": \"['This', 'paper', 'explores', 'the', 'broader', 'cross-lingual', 'potential', 'of', 'mBERT', '(', 'multilingual', ')', 'as', 'a', 'zero', 'shot', 'language', 'transfer', 'model', 'on', '5', 'NLP', 'tasks', 'covering', 'a', 'total', 'of', '39', 'languages', 'from', 'various', 'language', 'families', ':', 'NLI', ',', 'document', 'classification', ',', 'NER', ',', 'POS', 'tagging', ',', 'and', 'dependency', 'parsing', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(58, 63)\", \"within_anchor_index\": 1.0}, {\"pc1\": -5.257911713314798, \"pc2\": 0.7990622019784794, \"word\": \"compare\", \"split_0\": \"We compare\", \"split_1\": \"mBERT\", \"split_2\": \"with the best - published methods for zero - shot cross - lingual transfer and find mBERT competitive on each task .\", \"averb\": \"compare\", \"averb_s\": \"['We']\", \"averb_o\": \"['mBERT with the best - published methods for zero - shot cross']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"find\", \"root_full\": \"find\", \"root_s\": \"['- lingual transfer']\", \"root_o\": \"['We compare mBERT with the best - published methods for zero - shot cross mBERT competitive on each task .']\", \"root_split\": 2, \"root_span0\": 96, \"root_span1\": 101, \"root_cspan0\": 79, \"root_cspan1\": 84, \"fverb\": \"published\", \"fword\": \"with\", \"apos\": \"['NOUN', 'VERB', 'PUNCT']\", \"apos_w\": \"['mBERT', 'compare', '.']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task.\", \"split_tokens\": \"['We', 'compare', 'mBERT', 'with', 'the', 'best', '-', 'published', 'methods', 'for', 'zero', '-', 'shot', 'cross', '-', 'lingual', 'transfer', 'and', 'find', 'mBERT', 'competitive', 'on', 'each', 'task', '.']\", \"split_anchor_span\": \"(2, 3)\", \"split_anchor_indices\": \"(10, 15)\", \"within_anchor_index\": 1.0}, {\"pc1\": -0.9928932508677223, \"pc2\": -0.9127902043731165, \"word\": \"find\", \"split_0\": \"We compare mBERT with the best - published methods for zero - shot cross - lingual transfer and find\", \"split_1\": \"mBERT\", \"split_2\": \"competitive on each task .\", \"averb\": \"find\", \"averb_s\": \"['- lingual transfer']\", \"averb_o\": \"['We compare mBERT with the best - published methods for zero - shot cross mBERT competitive on each task .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 96.0, \"averb_span1\": 101.0, \"averb_cspan0\": 96.0, \"averb_cspan1\": 101.0, \"root\": \"find\", \"root_full\": \"find\", \"root_s\": \"['- lingual transfer']\", \"root_o\": \"['We compare mBERT with the best - published methods for zero - shot cross mBERT competitive on each task .']\", \"root_split\": 0, \"root_span0\": 96, \"root_span1\": 101, \"root_cspan0\": 96, \"root_cspan1\": 101, \"fverb\": null, \"fword\": \"competitive\", \"apos\": \"['NOUN', 'ADJ', 'VERB']\", \"apos_w\": \"['mBERT', 'competitive', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task.\", \"split_tokens\": \"['We', 'compare', 'mBERT', 'with', 'the', 'best', '-', 'published', 'methods', 'for', 'zero', '-', 'shot', 'cross', '-', 'lingual', 'transfer', 'and', 'find', 'mBERT', 'competitive', 'on', 'each', 'task', '.']\", \"split_anchor_span\": \"(19, 20)\", \"split_anchor_indices\": \"(100, 105)\", \"within_anchor_index\": 1.0}, {\"pc1\": -3.7398946475291512, \"pc2\": 3.450686877591821, \"word\": \"investigate\", \"split_0\": \"Additionally , we investigate the most effective strategy for utilizing\", \"split_1\": \"mBERT\", \"split_2\": \"in this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .\", \"averb\": \"investigate\", \"averb_s\": \"['Additionally']\", \"averb_o\": \"['the most effective strategy for utilizing mBERT in', 'this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 18.0, \"averb_span1\": 30.0, \"averb_cspan0\": 18.0, \"averb_cspan1\": 30.0, \"root\": \"investigate\", \"root_full\": \"investigate\", \"root_s\": \"['Additionally']\", \"root_o\": \"['the most effective strategy for utilizing mBERT in', 'this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 30, \"root_cspan0\": 18, \"root_cspan1\": 30, \"fverb\": \"generalizes\", \"fword\": \"in\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['mBERT', 'for', 'strategy']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.\", \"split_tokens\": \"['Additionally', ',', 'we', 'investigate', 'the', 'most', 'effective', 'strategy', 'for', 'utilizing', 'mBERT', 'in', 'this', 'manner', ',', 'determine', 'to', 'what', 'extent', 'mBERT', 'generalizes', 'away', 'from', 'language', 'specific', 'features', ',', 'and', 'measure', 'factors', 'that', 'influence', 'cross', '-', 'lingual', 'transfer', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(71, 76)\", \"within_anchor_index\": 1.0}, {\"pc1\": -0.6974042150940563, \"pc2\": -0.9314931847587911, \"word\": \"generalizes\", \"split_0\": \"Additionally , we investigate the most effective strategy for utilizing mBERT in this manner , determine to what extent\", \"split_1\": \"mBERT\", \"split_2\": \"generalizes away from language specific features , and measure factors that influence cross - lingual transfer .\", \"averb\": \"generalizes\", \"averb_s\": \"['what extent mBERT']\", \"averb_o\": \"['away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 126.0, \"averb_span1\": 138.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"investigate\", \"root_full\": \"investigate\", \"root_s\": \"['Additionally']\", \"root_o\": \"['the most effective strategy for utilizing mBERT in', 'this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 30, \"root_cspan0\": 18, \"root_cspan1\": 30, \"fverb\": \"generalizes\", \"fword\": \"generalizes\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['mBERT', 'generalizes', 'determine']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.\", \"split_tokens\": \"['Additionally', ',', 'we', 'investigate', 'the', 'most', 'effective', 'strategy', 'for', 'utilizing', 'mBERT', 'in', 'this', 'manner', ',', 'determine', 'to', 'what', 'extent', 'mBERT', 'generalizes', 'away', 'from', 'language', 'specific', 'features', ',', 'and', 'measure', 'factors', 'that', 'influence', 'cross', '-', 'lingual', 'transfer', '.']\", \"split_anchor_span\": \"(19, 20)\", \"split_anchor_indices\": \"(119, 124)\", \"within_anchor_index\": 1.0}, {\"pc1\": -2.474444337405744, \"pc2\": -3.9046400443073597, \"word\": \"pre\", \"split_0\": \"Language model pre - training , such as\", \"split_1\": \"BERT\", \"split_2\": \", has significantly improved the performances of many natural language processing tasks .\", \"averb\": \"pre\", \"averb_s\": \"['Language model']\", \"averb_o\": \"['-', 'training , such as BERT ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 15.0, \"averb_span1\": 19.0, \"averb_cspan0\": 15.0, \"averb_cspan1\": 19.0, \"root\": \"improved\", \"root_full\": \"has improved\", \"root_s\": \"['Language model pre - training , such as BERT ,', 'significantly']\", \"root_o\": \"['the performances of many natural language processing tasks', '.']\", \"root_split\": 2, \"root_span0\": 47, \"root_span1\": 74, \"root_cspan0\": 2, \"root_cspan1\": 29, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'SCONJ', 'ADJ']\", \"apos_w\": \"['BERT', 'as', 'training']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks.\", \"split_tokens\": \"['Language', 'model', 'pre', '-', 'training', ',', 'such', 'as', 'BERT', ',', 'has', 'significantly', 'improved', 'the', 'performances', 'of', 'many', 'natural', 'language', 'processing', 'tasks', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(39, 43)\", \"within_anchor_index\": 0.0}, {\"pc1\": 38.57677435680989, \"pc2\": 10.14591996613481, \"word\": \"can be transferred\", \"split_0\": \"By leveraging this new KD method , the plenty of knowledge encoded in a large teacher BERT can be well transferred to\", \"split_1\": \"a small student TinyBERT\", \"split_2\": \".\", \"averb\": \"can be transferred\", \"averb_s\": \"['the plenty of knowledge encoded in a large teacher BERT']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 91.0, \"averb_span1\": 115.0, \"averb_cspan0\": 91.0, \"averb_cspan1\": 115.0, \"root\": \"transferred\", \"root_full\": \"can be transferred\", \"root_s\": \"['the plenty of knowledge encoded in a large teacher BERT']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 91, \"root_span1\": 115, \"root_cspan0\": 91, \"root_cspan1\": 115, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['student', 'to', 'transferred']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT.\", \"split_tokens\": \"['By', 'leveraging', 'this', 'new', 'KD', 'method', ',', 'the', 'plenty', 'of', 'knowledge', 'encoded', 'in', 'a', 'large', 'teacher', 'BERT', 'can', 'be', 'well', 'transferred', 'to', 'a', 'small', 'student', 'TinyBERT', '.']\", \"split_anchor_span\": \"(22, 26)\", \"split_anchor_indices\": \"(117, 141)\", \"within_anchor_index\": 20.0}, {\"pc1\": 2.6446887627041087, \"pc2\": -4.031593215167293, \"word\": \"encoded\", \"split_0\": \"By leveraging this new KD method , the plenty of knowledge encoded in\", \"split_1\": \"a large teacher BERT\", \"split_2\": \"can be well transferred to a small student TinyBERT .\", \"averb\": \"encoded\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 59.0, \"averb_span1\": 67.0, \"averb_cspan0\": 59.0, \"averb_cspan1\": 67.0, \"root\": \"transferred\", \"root_full\": \"can be transferred\", \"root_s\": \"['the plenty of knowledge encoded in a large teacher BERT']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 91, \"root_span1\": 115, \"root_cspan0\": 0, \"root_cspan1\": 24, \"fverb\": \"be\", \"fword\": \"can\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'in', 'encoded']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT.\", \"split_tokens\": \"['By', 'leveraging', 'this', 'new', 'KD', 'method', ',', 'the', 'plenty', 'of', 'knowledge', 'encoded', 'in', 'a', 'large', 'teacher', 'BERT', 'can', 'be', 'well', 'transferred', 'to', 'a', 'small', 'student', 'TinyBERT', '.']\", \"split_anchor_span\": \"(13, 17)\", \"split_anchor_indices\": \"(69, 89)\", \"within_anchor_index\": 16.0}, {\"pc1\": -3.5821938579275203, \"pc2\": 0.9510730276352136, \"word\": \"introduce\", \"split_0\": \"Moreover , we introduce a new two - stage learning framework for\", \"split_1\": \"TinyBERT\", \"split_2\": \", which performs transformer distillation at both the pre - training and task - specific learning stages .\", \"averb\": \"introduce\", \"averb_s\": \"['we', ',']\", \"averb_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 14.0, \"averb_span1\": 24.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 24.0, \"root\": \"introduce\", \"root_full\": \"introduce\", \"root_s\": \"['we', ',']\", \"root_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"root_split\": 0, \"root_span0\": 14, \"root_span1\": 24, \"root_cspan0\": 14, \"root_cspan1\": 24, \"fverb\": \"performs\", \"fword\": \"which\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['TinyBERT', 'for', 'framework']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages.\", \"split_tokens\": \"['Moreover', ',', 'we', 'introduce', 'a', 'new', 'two', '-', 'stage', 'learning', 'framework', 'for', 'TinyBERT', ',', 'which', 'performs', 'transformer', 'distillation', 'at', 'both', 'the', 'pre', '-', 'training', 'and', 'task', '-', 'specific', 'learning', 'stages', '.']\", \"split_anchor_span\": \"(12, 13)\", \"split_anchor_indices\": \"(64, 72)\", \"within_anchor_index\": 4.0}, {\"pc1\": -3.582193857927519, \"pc2\": 0.9510730276352128, \"word\": \"introduce\", \"split_0\": \"Moreover , we introduce\", \"split_1\": \"a new two - stage learning framework for TinyBERT , which performs transformer distillation at both the pre - training and task - specific learning stages\", \"split_2\": \".\", \"averb\": \"introduce\", \"averb_s\": \"['we', ',']\", \"averb_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"averb_relation\": 0.0, \"averb_split\": 0.0, \"averb_span0\": 14.0, \"averb_span1\": 24.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 24.0, \"root\": \"introduce\", \"root_full\": \"introduce\", \"root_s\": \"['we', ',']\", \"root_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"root_split\": 0, \"root_span0\": 14, \"root_span1\": 24, \"root_cspan0\": 14, \"root_cspan1\": 24, \"fverb\": null, \"fword\": null, \"apos\": \"['VERB']\", \"apos_w\": \"['introduce']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages.\", \"split_tokens\": \"['Moreover', ',', 'we', 'introduce', 'a', 'new', 'two', '-', 'stage', 'learning', 'framework', 'for', 'TinyBERT', ',', 'which', 'performs', 'transformer', 'distillation', 'at', 'both', 'the', 'pre', '-', 'training', 'and', 'task', '-', 'specific', 'learning', 'stages', '.']\", \"split_anchor_span\": \"(4, 30)\", \"split_anchor_indices\": \"(23, 177)\", \"within_anchor_index\": 45.0}, {\"pc1\": 8.434310295806283, \"pc2\": 8.04304870370813, \"word\": \"can capture\", \"split_0\": \"This framework ensures that\", \"split_1\": \"TinyBERT\", \"split_2\": \"can capture both the general - domain and task - specific knowledge of the teacher BERT .\", \"averb\": \"can capture\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"['both the general - domain and', 'task - specific knowledge of the teacher BERT']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 37.0, \"averb_span1\": 49.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"ensures\", \"root_full\": \"ensures\", \"root_s\": \"['This', 'framework']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 23, \"root_cspan0\": 15, \"root_cspan1\": 23, \"fverb\": \"capture\", \"fword\": \"can\", \"apos\": \"['PROPN', 'VERB', 'VERB']\", \"apos_w\": \"['TinyBERT', 'capture', 'ensures']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.\", \"split_tokens\": \"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general', '-', 'domain', 'and', 'task', '-', 'specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(27, 35)\", \"within_anchor_index\": 4.0}, {\"pc1\": -1.1581678887736235, \"pc2\": 1.3316810544591924, \"word\": \"ensures\", \"split_0\": null, \"split_1\": \"This framework\", \"split_2\": \"ensures that TinyBERT can capture both the general - domain and task - specific knowledge of the teacher BERT .\", \"averb\": \"ensures\", \"averb_s\": \"['This', 'framework']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 2.0, \"averb_span0\": 15.0, \"averb_span1\": 23.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"ensures\", \"root_full\": \"ensures\", \"root_s\": \"['This', 'framework']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 15, \"root_span1\": 23, \"root_cspan0\": 0, \"root_cspan1\": 8, \"fverb\": \"ensures\", \"fword\": \"ensures\", \"apos\": \"['VERB']\", \"apos_w\": \"['ensures']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.\", \"split_tokens\": \"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general', '-', 'domain', 'and', 'task', '-', 'specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 14)\", \"within_anchor_index\": -1.0}, {\"pc1\": 8.434310295806283, \"pc2\": 8.043048703708132, \"word\": \"can capture\", \"split_0\": \"This framework ensures that TinyBERT can capture both the general - domain and task - specific knowledge of\", \"split_1\": \"the teacher BERT\", \"split_2\": \".\", \"averb\": \"can capture\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"['both the general - domain and', 'task - specific knowledge of the teacher BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 37.0, \"averb_span1\": 49.0, \"averb_cspan0\": 37.0, \"averb_cspan1\": 49.0, \"root\": \"ensures\", \"root_full\": \"ensures\", \"root_s\": \"['This', 'framework']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 23, \"root_cspan0\": 15, \"root_cspan1\": 23, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['teacher', 'of', 'knowledge']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.\", \"split_tokens\": \"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general', '-', 'domain', 'and', 'task', '-', 'specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']\", \"split_anchor_span\": \"(18, 21)\", \"split_anchor_indices\": \"(107, 123)\", \"within_anchor_index\": 12.0}, {\"pc1\": 1.1923154856393385, \"pc2\": -4.384225433492708, \"word\": \"is\", \"split_0\": null, \"split_1\": \"TinyBERT\", \"split_2\": \"is empirically effective and achieves comparable results with BERT in GLUE datasets , while being 7.5x smaller and 9.4x faster on inference .\", \"averb\": \"is\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 9.0, \"averb_span1\": 12.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"empirically\", \"root_full\": \"is empirically\", \"root_s\": \"[]\", \"root_o\": \"['achieves']\", \"root_split\": 2, \"root_span0\": 9, \"root_span1\": 24, \"root_cspan0\": 0, \"root_cspan1\": 15, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX', 'ADV']\", \"apos_w\": \"['TinyBERT', 'is', 'empirically']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"TinyBERT is empirically effective and achieves comparable results with BERT in GLUE datasets, while being 7.5x smaller and 9.4x faster on inference.\", \"split_tokens\": \"['TinyBERT', 'is', 'empirically', 'effective', 'and', 'achieves', 'comparable', 'results', 'with', 'BERT', 'in', 'GLUE', 'datasets', ',', 'while', 'being', '7.5x', 'smaller', 'and', '9.4x', 'faster', 'on', 'inference', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 8)\", \"within_anchor_index\": 4.0}, {\"pc1\": 1.1923154856393385, \"pc2\": -4.384225433492708, \"word\": \"is\", \"split_0\": null, \"split_1\": \"TinyBERT\", \"split_2\": \"is also significantly better than state - of - the - art baselines , even with only about 28 % parameters and 31 % inference time of baselines\", \"averb\": \"is\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"['also significantly better than state - of - the - art baselines , even with only about 28 % parameters and 31 % inference time of baselines']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 9.0, \"averb_span1\": 12.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"is\", \"root_full\": \"is\", \"root_s\": \"['TinyBERT']\", \"root_o\": \"['also significantly better than state - of - the - art baselines , even with only about 28 % parameters and 31 % inference time of baselines']\", \"root_split\": 2, \"root_span0\": 9, \"root_span1\": 12, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['TinyBERT', 'is']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"TinyBERT is also significantly better than state-of-the-art baselines, even with only about 28% parameters and 31% inference time of baselines\", \"split_tokens\": \"['TinyBERT', 'is', 'also', 'significantly', 'better', 'than', 'state', '-', 'of', '-', 'the', '-', 'art', 'baselines', ',', 'even', 'with', 'only', 'about', '28', '%', 'parameters', 'and', '31', '%', 'inference', 'time', 'of', 'baselines']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 8)\", \"within_anchor_index\": 4.0}, {\"pc1\": 5.021315745613506, \"pc2\": -1.35530365767664, \"word\": \"has achieved\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \", a pre-trained Transformer model , has achieved ground-breaking performance on multiple NLP tasks .\", \"averb\": \"has achieved\", \"averb_s\": \"['BERT , a pre - trained Transformer model ,']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 43.0, \"averb_span1\": 56.0, \"averb_cspan0\": 38.0, \"averb_cspan1\": 51.0, \"root\": \"ground\", \"root_full\": \"ground\", \"root_s\": \"['BERT , a pre - trained Transformer model , has achieved']\", \"root_o\": \"['- breaking performance on multiple NLP tasks', '.']\", \"root_split\": 2, \"root_span0\": 56, \"root_span1\": 63, \"root_cspan0\": 51, \"root_cspan1\": 58, \"fverb\": \"has\", \"fword\": \"a\", \"apos\": \"['PROPN', 'VERB', 'NOUN']\", \"apos_w\": \"['BERT', 'achieved', 'ground']\", \"URL\": \"https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3\", \"ID\": 9, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"BERT, a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks.\", \"split_tokens\": \"['BERT', ',', 'a', 'pre-trained', 'Transformer', 'model', ',', 'has', 'achieved', 'ground-breaking', 'performance', 'on', 'multiple', 'NLP', 'tasks', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.81882698252752, \"pc2\": 2.128231779043771, \"word\": \"describe\", \"split_0\": \"In this paper , we describe\", \"split_1\": \"BERTSUM\", \"split_2\": \", a simple variant of BERT , for extractive summarization .\", \"averb\": \"describe\", \"averb_s\": \"['we']\", \"averb_o\": \"['BERTSUM , a simple variant of BERT ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 28.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 28.0, \"root\": \"describe\", \"root_full\": \"describe\", \"root_s\": \"['we']\", \"root_o\": \"['BERTSUM , a simple variant of BERT ,']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 28, \"root_cspan0\": 19, \"root_cspan1\": 28, \"fverb\": null, \"fword\": \"a\", \"apos\": \"['PROPN', 'VERB']\", \"apos_w\": \"['BERTSUM', 'describe']\", \"URL\": \"https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3\", \"ID\": 9, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In this paper, we describe BERTSUM, a simple variant of BERT, for extractive summarization.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'describe', 'BERTSUM', ',', 'a', 'simple', 'variant', 'of', 'BERT', ',', 'for', 'extractive', 'summarization', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(27, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.197499237366637, \"pc2\": 2.625941046159953, \"word\": \"explore\", \"split_0\": \"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on\", \"split_1\": \"the popular language model BERT\", \"split_2\": \"to enhance the performance of fine - tuning of BERT for RRC .\", \"averb\": \"explore\", \"averb_s\": \"[]\", \"averb_o\": \"['a novel post - training approach on the popular language model BERT', 'to enhance the performance of fine - tuning of BERT for RRC']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 114.0, \"averb_span1\": 122.0, \"averb_cspan0\": 114.0, \"averb_cspan1\": 122.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['Since', 'ReviewRC']\", \"root_o\": \"['limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC']\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 19, \"root_cspan0\": 15, \"root_cspan1\": 19, \"fverb\": \"enhance\", \"fword\": \"to\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['model', 'on', 'approach']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.\", \"split_tokens\": \"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post', '-', 'training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine', '-', 'tuning', 'of', 'BERT', 'for', 'RRC', '.']\", \"split_anchor_span\": \"(29, 34)\", \"split_anchor_indices\": \"(157, 188)\", \"within_anchor_index\": 27.0}, {\"pc1\": -0.21891771450855643, \"pc2\": 2.2363863626486373, \"word\": \"enhance\", \"split_0\": \"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of\", \"split_1\": \"BERT\", \"split_2\": \"for RRC .\", \"averb\": \"enhance\", \"averb_s\": \"['to']\", \"averb_o\": \"['the performance of fine - tuning of BERT for RRC']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 193.0, \"averb_span1\": 201.0, \"averb_cspan0\": 193.0, \"averb_cspan1\": 201.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['Since', 'ReviewRC']\", \"root_o\": \"['limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC']\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 19, \"root_cspan0\": 15, \"root_cspan1\": 19, \"fverb\": null, \"fword\": \"for\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'tuning']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.\", \"split_tokens\": \"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post', '-', 'training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine', '-', 'tuning', 'of', 'BERT', 'for', 'RRC', '.']\", \"split_anchor_span\": \"(43, 44)\", \"split_anchor_indices\": \"(236, 240)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.197499237366637, \"pc2\": 2.625941046159953, \"word\": \"explore\", \"split_0\": \"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore\", \"split_1\": \"a novel post - training approach on the popular language model BERT\", \"split_2\": \"to enhance the performance of fine - tuning of BERT for RRC .\", \"averb\": \"explore\", \"averb_s\": \"[]\", \"averb_o\": \"['a novel post - training approach on the popular language model BERT', 'to enhance the performance of fine - tuning of BERT for RRC']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 114.0, \"averb_span1\": 122.0, \"averb_cspan0\": 114.0, \"averb_cspan1\": 122.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['Since', 'ReviewRC']\", \"root_o\": \"['limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC']\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 19, \"root_cspan0\": 15, \"root_cspan1\": 19, \"fverb\": \"enhance\", \"fword\": \"to\", \"apos\": \"['NOUN', 'VERB', 'ADV']\", \"apos_w\": \"['approach', 'explore', 'then']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.\", \"split_tokens\": \"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post', '-', 'training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine', '-', 'tuning', 'of', 'BERT', 'for', 'RRC', '.']\", \"split_anchor_span\": \"(22, 34)\", \"split_anchor_indices\": \"(121, 188)\", \"within_anchor_index\": 63.0}, {\"pc1\": -5.678433973861906, \"pc2\": 9.530070796129515, \"word\": \"To show\", \"split_0\": \"To show the generality of\", \"split_1\": \"the approach\", \"split_2\": \", the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .\", \"averb\": \"To show\", \"averb_s\": \"[]\", \"averb_o\": \"['the generality of the approach , the proposed post - training']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"applied\", \"root_full\": \"applied\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 78, \"root_span1\": 86, \"root_cspan0\": 39, \"root_cspan1\": 47, \"fverb\": \"proposed\", \"fword\": \"the\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['approach', 'of', 'generality']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis.\", \"split_tokens\": \"['To', 'show', 'the', 'generality', 'of', 'the', 'approach', ',', 'the', 'proposed', 'post', '-', 'training', 'is', 'also', 'applied', 'to', 'some', 'other', 'review', '-', 'based', 'tasks', 'such', 'as', 'aspect', 'extraction', 'and', 'aspect', 'sentiment', 'classification', 'in', 'aspect', '-', 'based', 'sentiment', 'analysis', '.']\", \"split_anchor_span\": \"(5, 7)\", \"split_anchor_indices\": \"(25, 37)\", \"within_anchor_index\": -1.0}, {\"pc1\": -5.678433973861907, \"pc2\": 9.530070796129516, \"word\": \"To show\", \"split_0\": \"To show the generality of the approach ,\", \"split_1\": \"the proposed post - training\", \"split_2\": \"is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .\", \"averb\": \"To show\", \"averb_s\": \"[]\", \"averb_o\": \"['the generality of the approach , the proposed post - training']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"applied\", \"root_full\": \"applied\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 78, \"root_span1\": 86, \"root_cspan0\": 8, \"root_cspan1\": 16, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['training', 'of', 'generality']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis.\", \"split_tokens\": \"['To', 'show', 'the', 'generality', 'of', 'the', 'approach', ',', 'the', 'proposed', 'post', '-', 'training', 'is', 'also', 'applied', 'to', 'some', 'other', 'review', '-', 'based', 'tasks', 'such', 'as', 'aspect', 'extraction', 'and', 'aspect', 'sentiment', 'classification', 'in', 'aspect', '-', 'based', 'sentiment', 'analysis', '.']\", \"split_anchor_span\": \"(8, 13)\", \"split_anchor_indices\": \"(40, 68)\", \"within_anchor_index\": -1.0}, {\"pc1\": -3.277347722134376, \"pc2\": 1.9133949627801117, \"word\": \"demonstrate\", \"split_0\": \"Experimental results demonstrate that\", \"split_1\": \"the proposed post - training\", \"split_2\": \"is highly effective .\", \"averb\": \"demonstrate\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 21.0, \"averb_span1\": 33.0, \"averb_cspan0\": 21.0, \"averb_cspan1\": 33.0, \"root\": \"results\", \"root_full\": \"results\", \"root_s\": \"['Experimental']\", \"root_o\": \"['demonstrate that the proposed post - training is highly effective .']\", \"root_split\": 0, \"root_span0\": 13, \"root_span1\": 21, \"root_cspan0\": 13, \"root_cspan1\": 21, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'ADV', 'ADJ']\", \"apos_w\": \"['training', 'highly', 'effective']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"Experimental results demonstrate that the proposed post-training is highly effective.\", \"split_tokens\": \"['Experimental', 'results', 'demonstrate', 'that', 'the', 'proposed', 'post', '-', 'training', 'is', 'highly', 'effective', '.']\", \"split_anchor_span\": \"(4, 9)\", \"split_anchor_indices\": \"(37, 65)\", \"within_anchor_index\": -1.0}, {\"pc1\": 0.5426604702395909, \"pc2\": -0.49826807100463855, \"word\": \"achieved\", \"split_0\": \"As a state - of - the - art language model pre - training model ,\", \"split_1\": \"BERT ( Bidirectional Encoder Representations from Transformers )\", \"split_2\": \"has achieved amazing results in many language understanding tasks .\", \"averb\": \"achieved\", \"averb_s\": \"['BERT ( Bidirectional Encoder Representations from Transformers )', 'As', 'a state - of - the - art language model pre - training model ,', 'has']\", \"averb_o\": \"['amazing results in many language understanding tasks', '.']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 135.0, \"averb_span1\": 144.0, \"averb_cspan0\": 4.0, \"averb_cspan1\": 13.0, \"root\": \"achieved\", \"root_full\": \"achieved\", \"root_s\": \"['BERT ( Bidirectional Encoder Representations from Transformers )', 'As', 'a state - of - the - art language model pre - training model ,', 'has']\", \"root_o\": \"['amazing results in many language understanding tasks', '.']\", \"root_split\": 2, \"root_span0\": 135, \"root_span1\": 144, \"root_cspan0\": 4, \"root_cspan1\": 13, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'VERB']\", \"apos_w\": \"['Representations', 'achieved']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks.\", \"split_tokens\": \"['As', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'language', 'model', 'pre', '-', 'training', 'model', ',', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', 'has', 'achieved', 'amazing', 'results', 'in', 'many', 'language', 'understanding', 'tasks', '.']\", \"split_anchor_span\": \"(16, 24)\", \"split_anchor_indices\": \"(65, 129)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.9393509970924075, \"pc2\": 9.770194484910494, \"word\": \"to investigate\", \"split_0\": \"In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of\", \"split_1\": \"BERT\", \"split_2\": \"on text classification task and provide a general solution for BERT fine - tuning .\", \"averb\": \"to investigate\", \"averb_s\": \"[]\", \"averb_o\": \"['different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 50.0, \"averb_span1\": 65.0, \"averb_cspan0\": 50.0, \"averb_cspan1\": 65.0, \"root\": \"conduct\", \"root_full\": \"conduct\", \"root_s\": \"['we']\", \"root_o\": \"['exhaustive experiments', 'to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": \"provide\", \"fword\": \"on\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'methods']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine', '-', 'tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine', '-', 'tuning', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(99, 103)\", \"within_anchor_index\": 0.0}, {\"pc1\": 0.2640224709676682, \"pc2\": 0.6410685011180449, \"word\": \"provide\", \"split_0\": \"In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for\", \"split_1\": \"BERT\", \"split_2\": \"fine - tuning .\", \"averb\": \"provide\", \"averb_s\": \"[]\", \"averb_o\": \"['a general solution for BERT fine - tuning']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 137.0, \"averb_span1\": 145.0, \"averb_cspan0\": 137.0, \"averb_cspan1\": 145.0, \"root\": \"conduct\", \"root_full\": \"conduct\", \"root_s\": \"['we']\", \"root_o\": \"['exhaustive experiments', 'to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": \"fine\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'tuning', 'for']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine', '-', 'tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine', '-', 'tuning', '.']\", \"split_anchor_span\": \"(27, 28)\", \"split_anchor_indices\": \"(167, 171)\", \"within_anchor_index\": 0.0}, {\"pc1\": 0.26402247096766823, \"pc2\": 0.6410685011180447, \"word\": \"provide\", \"split_0\": \"In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of BERT on text classification task and provide\", \"split_1\": \"a general solution for BERT fine - tuning\", \"split_2\": \".\", \"averb\": \"provide\", \"averb_s\": \"[]\", \"averb_o\": \"['a general solution for BERT fine - tuning']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 137.0, \"averb_span1\": 145.0, \"averb_cspan0\": 137.0, \"averb_cspan1\": 145.0, \"root\": \"conduct\", \"root_full\": \"conduct\", \"root_s\": \"['we']\", \"root_o\": \"['exhaustive experiments', 'to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB', 'NOUN']\", \"apos_w\": \"['solution', 'provide', 'task']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine', '-', 'tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine', '-', 'tuning', '.']\", \"split_anchor_span\": \"(23, 31)\", \"split_anchor_indices\": \"(144, 185)\", \"within_anchor_index\": 23.0}, {\"pc1\": -5.448521921068456, \"pc2\": 1.9285243985645513, \"word\": \"show\", \"split_0\": \"We show that\", \"split_1\": \"BERT\", \"split_2\": \"( Devlin et al . , 2018 ) is a Markov random field language model .\", \"averb\": \"show\", \"averb_s\": \"['We']\", \"averb_o\": \"['that BERT ( Devlin et al . , 2018 )', 'is a Markov random field language model', '.']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 8.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 8.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"['We']\", \"root_o\": \"['that BERT ( Devlin et al . , 2018 )', 'is a Markov random field language model', '.']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": \"is\", \"fword\": \"Devlin\", \"apos\": \"['PROPN', 'PROPN', 'SCONJ']\", \"apos_w\": \"['BERT', 'et', 'that']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"We show that BERT (Devlin et al., 2018) is a Markov random field language model.\", \"split_tokens\": \"['We', 'show', 'that', 'BERT', '(', 'Devlin', 'et', 'al', '.', ',', '2018', ')', 'is', 'a', 'Markov', 'random', 'field', 'language', 'model', '.']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(12, 16)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.666714555086292, \"pc2\": 1.097037507092721, \"word\": \"gives\", \"split_0\": \"This formulation gives way to a natural procedure to sample sentences from\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"gives\", \"averb_s\": \"['This', 'formulation']\", \"averb_o\": \"['way', 'to a natural procedure to sample sentences from BERT', '.']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 17.0, \"averb_span1\": 23.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 23.0, \"root\": \"gives\", \"root_full\": \"gives\", \"root_s\": \"['This', 'formulation']\", \"root_o\": \"['way', 'to a natural procedure to sample sentences from BERT', '.']\", \"root_split\": 0, \"root_span0\": 17, \"root_span1\": 23, \"root_cspan0\": 17, \"root_cspan1\": 23, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'from', 'sentences']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"This formulation gives way to a natural procedure to sample sentences from BERT.\", \"split_tokens\": \"['This', 'formulation', 'gives', 'way', 'to', 'a', 'natural', 'procedure', 'to', 'sample', 'sentences', 'from', 'BERT', '.']\", \"split_anchor_span\": \"(12, 13)\", \"split_anchor_indices\": \"(74, 78)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.161510184055302, \"pc2\": -0.21082985338919066, \"word\": \"generate\", \"split_0\": \"We generate from\", \"split_1\": \"BERT\", \"split_2\": \"and find that it can produce high - quality , fluent generations .\", \"averb\": \"generate\", \"averb_s\": \"['We']\", \"averb_o\": \"['find that it can produce high - quality , fluent generations .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 12.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 12.0, \"root\": \"generate\", \"root_full\": \"generate\", \"root_s\": \"['We']\", \"root_o\": \"['find that it can produce high - quality , fluent generations .']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 12, \"root_cspan0\": 3, \"root_cspan1\": 12, \"fverb\": \"find\", \"fword\": \"and\", \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'from', 'generate']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We generate from BERT and find that it can produce high-quality, fluent generations.\", \"split_tokens\": \"['We', 'generate', 'from', 'BERT', 'and', 'find', 'that', 'it', 'can', 'produce', 'high', '-', 'quality', ',', 'fluent', 'generations', '.']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(16, 20)\", \"within_anchor_index\": 0.0}, {\"pc1\": 0.8566480223133832, \"pc2\": -0.43435485780475586, \"word\": \"produce\", \"split_0\": \"We generate from BERT and find that\", \"split_1\": \"it\", \"split_2\": \"can produce high - quality , fluent generations .\", \"averb\": \"produce\", \"averb_s\": \"['it', 'can']\", \"averb_o\": \"['high - quality', ', fluent generations .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 43.0, \"averb_span1\": 51.0, \"averb_cspan0\": 4.0, \"averb_cspan1\": 12.0, \"root\": \"generate\", \"root_full\": \"generate\", \"root_s\": \"['We']\", \"root_o\": \"['find that it can produce high - quality , fluent generations .']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 12, \"root_cspan0\": 3, \"root_cspan1\": 12, \"fverb\": \"produce\", \"fword\": \"can\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['it', 'produce', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We generate from BERT and find that it can produce high-quality, fluent generations.\", \"split_tokens\": \"['We', 'generate', 'from', 'BERT', 'and', 'find', 'that', 'it', 'can', 'produce', 'high', '-', 'quality', ',', 'fluent', 'generations', '.']\", \"split_anchor_span\": \"(7, 8)\", \"split_anchor_indices\": \"(35, 37)\", \"within_anchor_index\": -1.0}, {\"pc1\": -2.950840350634659, \"pc2\": -5.507694603896149, \"word\": \"Compared\", \"split_0\": \"Compared to the generations of a traditional left - to - right language model ,\", \"split_1\": \"BERT\", \"split_2\": \"generates sentences that are more diverse but of slightly worse quality .\", \"averb\": \"Compared\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 9.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"diverse\", \"root_full\": \"are diverse\", \"root_s\": \"['that']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 110, \"root_span1\": 127, \"root_cspan0\": 25, \"root_cspan1\": 42, \"fverb\": \"generates\", \"fword\": \"generates\", \"apos\": \"['PROPN', 'NOUN', 'PUNCT']\", \"apos_w\": \"['BERT', 'sentences', ',']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.\", \"split_tokens\": \"['Compared', 'to', 'the', 'generations', 'of', 'a', 'traditional', 'left', '-', 'to', '-', 'right', 'language', 'model', ',', 'BERT', 'generates', 'sentences', 'that', 'are', 'more', 'diverse', 'but', 'of', 'slightly', 'worse', 'quality', '.']\", \"split_anchor_span\": \"(15, 16)\", \"split_anchor_indices\": \"(79, 83)\", \"within_anchor_index\": 0.0}, {\"pc1\": 2.322655746657898, \"pc2\": -4.066870373935262, \"word\": \"has\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"has a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .\", \"averb\": \"has\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 5.0, \"averb_span1\": 9.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 4.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['BERT']\", \"root_o\": \"['a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .']\", \"root_split\": 2, \"root_span0\": 5, \"root_span1\": 9, \"root_cspan0\": 0, \"root_cspan1\": 4, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['BERT', 'has']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.\", \"split_tokens\": \"['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It', 'Must', 'Speak', ':', 'BERT', 'as', 'a', 'Markov', 'Random', 'Field', 'Language', 'Model', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.9716943965809497, \"pc2\": -1.9155805223017388, \"word\": \"Must\", \"split_0\": \"BERT has a Mouth , and\", \"split_1\": \"It\", \"split_2\": \"Must Speak : BERT as a Markov Random Field Language Model .\", \"averb\": \"Must\", \"averb_s\": \"['It']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 26.0, \"averb_span1\": 31.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 5.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['BERT']\", \"root_o\": \"['a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .']\", \"root_split\": 0, \"root_span0\": 5, \"root_span1\": 9, \"root_cspan0\": 5, \"root_cspan1\": 9, \"fverb\": null, \"fword\": \"Must\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['It', 'Must', 'Speak']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.\", \"split_tokens\": \"['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It', 'Must', 'Speak', ':', 'BERT', 'as', 'a', 'Markov', 'Random', 'Field', 'Language', 'Model', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(22, 24)\", \"within_anchor_index\": -1.0}, {\"pc1\": -2.3985574007550805, \"pc2\": 2.2536897360987913, \"word\": \"applying\", \"split_0\": \"Following recent successes in applying\", \"split_1\": \"BERT\", \"split_2\": \"to question answering , we explore simple applications to ad hoc document retrieval .\", \"averb\": \"applying\", \"averb_s\": \"[]\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 30.0, \"averb_span1\": 39.0, \"averb_cspan0\": 30.0, \"averb_cspan1\": 39.0, \"root\": \"explore\", \"root_full\": \"explore\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['simple applications']\", \"root_split\": 2, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 27, \"root_cspan1\": 35, \"fverb\": \"answering\", \"fword\": \"to\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'applying', 'in']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval.\", \"split_tokens\": \"['Following', 'recent', 'successes', 'in', 'applying', 'BERT', 'to', 'question', 'answering', ',', 'we', 'explore', 'simple', 'applications', 'to', 'ad', 'hoc', 'document', 'retrieval', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(38, 42)\", \"within_anchor_index\": 0.0}, {\"pc1\": 1.003350710743048, \"pc2\": -6.9717167930674995, \"word\": \"posed\", \"split_0\": \"This required confronting the challenge posed by documents that are typically longer than the length of input\", \"split_1\": \"BERT\", \"split_2\": \"was designed to handle .\", \"averb\": \"posed\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 40.0, \"averb_span1\": 46.0, \"averb_cspan0\": 40.0, \"averb_cspan1\": 46.0, \"root\": \"required\", \"root_full\": \"required\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 5, \"root_span1\": 14, \"root_cspan0\": 5, \"root_cspan1\": 14, \"fverb\": \"was\", \"fword\": \"was\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'length']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle.\", \"split_tokens\": \"['This', 'required', 'confronting', 'the', 'challenge', 'posed', 'by', 'documents', 'that', 'are', 'typically', 'longer', 'than', 'the', 'length', 'of', 'input', 'BERT', 'was', 'designed', 'to', 'handle', '.']\", \"split_anchor_span\": \"(17, 18)\", \"split_anchor_indices\": \"(109, 113)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.4974033633861215, \"pc2\": -0.8696229446586667, \"word\": \"confronting\", \"split_0\": \"This required confronting\", \"split_1\": \"the challenge posed by documents that are typically longer than the length of input BERT was designed to handle\", \"split_2\": \".\", \"averb\": \"confronting\", \"averb_s\": \"[]\", \"averb_o\": \"['the challenge posed by documents that are typically longer than the length of input BERT was designed to handle .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 14.0, \"averb_span1\": 26.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 26.0, \"root\": \"required\", \"root_full\": \"required\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 5, \"root_span1\": 14, \"root_cspan0\": 5, \"root_cspan1\": 14, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['challenge', 'confronting', 'required']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle.\", \"split_tokens\": \"['This', 'required', 'confronting', 'the', 'challenge', 'posed', 'by', 'documents', 'that', 'are', 'typically', 'longer', 'than', 'the', 'length', 'of', 'input', 'BERT', 'was', 'designed', 'to', 'handle', '.']\", \"split_anchor_span\": \"(3, 22)\", \"split_anchor_indices\": \"(25, 136)\", \"within_anchor_index\": 84.0}, {\"pc1\": -5.327448028617495, \"pc2\": 1.512109612163717, \"word\": \"address\", \"split_0\": \"We address\", \"split_1\": \"this issue\", \"split_2\": \"by applying inference on sentences individually , and then aggregating sentence scores to produce document scores .\", \"averb\": \"address\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"scores\", \"root_full\": \"scores\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 129, \"root_span1\": 136, \"root_cspan0\": 107, \"root_cspan1\": 114, \"fverb\": \"applying\", \"fword\": \"by\", \"apos\": \"['NOUN', 'VERB', 'ADV']\", \"apos_w\": \"['issue', 'address', 'then']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores.\", \"split_tokens\": \"['We', 'address', 'this', 'issue', 'by', 'applying', 'inference', 'on', 'sentences', 'individually', ',', 'and', 'then', 'aggregating', 'sentence', 'scores', 'to', 'produce', 'document', 'scores', '.']\", \"split_anchor_span\": \"(2, 4)\", \"split_anchor_indices\": \"(10, 20)\", \"within_anchor_index\": -1.0}, {\"pc1\": 1.102635752041828, \"pc2\": -0.5454894719591105, \"word\": \"contribute\", \"split_0\": \"BERT - based architectures currently give state - of - the - art performance on many NLP tasks , but little is known about the exact mechanisms that contribute to\", \"split_1\": \"its\", \"split_2\": \"success .\", \"averb\": \"contribute\", \"averb_s\": \"['that']\", \"averb_o\": \"['its .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 149.0, \"averb_span1\": 160.0, \"averb_cspan0\": 149.0, \"averb_cspan1\": 160.0, \"root\": \"currently\", \"root_full\": \"currently\", \"root_s\": \"['BERT - based architectures']\", \"root_o\": \"['give state - of - the - art performance on many NLP tasks , but little is known about the exact mechanisms that contribute to its success .']\", \"root_split\": 0, \"root_span0\": 27, \"root_span1\": 37, \"root_cspan0\": 27, \"root_cspan1\": 37, \"fverb\": null, \"fword\": \"success\", \"apos\": \"['DET', 'VERB', 'NOUN']\", \"apos_w\": \"['its', 'contribute', 'mechanisms']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success.\", \"split_tokens\": \"['BERT', '-', 'based', 'architectures', 'currently', 'give', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'many', 'NLP', 'tasks', ',', 'but', 'little', 'is', 'known', 'about', 'the', 'exact', 'mechanisms', 'that', 'contribute', 'to', 'its', 'success', '.']\", \"split_anchor_span\": \"(30, 31)\", \"split_anchor_indices\": \"(162, 165)\", \"within_anchor_index\": -1.0}, {\"pc1\": 0.3317774108080842, \"pc2\": -1.5831003509720825, \"word\": \"focus\", \"split_0\": \"In the current work , we focus on the interpretation of self - attention , which is one of the fundamental underlying components of\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"focus\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 25.0, \"averb_span1\": 31.0, \"averb_cspan0\": 25.0, \"averb_cspan1\": 31.0, \"root\": \"focus\", \"root_full\": \"focus\", \"root_s\": \"[',', 'we']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 25, \"root_span1\": 31, \"root_cspan0\": 25, \"root_cspan1\": 31, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'components']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT.\", \"split_tokens\": \"['In', 'the', 'current', 'work', ',', 'we', 'focus', 'on', 'the', 'interpretation', 'of', 'self', '-', 'attention', ',', 'which', 'is', 'one', 'of', 'the', 'fundamental', 'underlying', 'components', 'of', 'BERT', '.']\", \"split_anchor_span\": \"(24, 25)\", \"split_anchor_indices\": \"(131, 135)\", \"within_anchor_index\": 0.0}, {\"pc1\": 5.2150234331648795, \"pc2\": -1.5536422454764978, \"word\": \"encoded\", \"split_0\": \"Using a subset of GLUE tasks and a set of handcrafted features-of-interest , we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual\", \"split_1\": \"BERT\", \"split_2\": \"'s heads .\", \"averb\": \"encoded\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 181.0, \"averb_span1\": 189.0, \"averb_cspan0\": 181.0, \"averb_cspan1\": 189.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we', 'Using a subset of GLUE tasks and a set of handcrafted features - of - interest']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 84, \"root_span1\": 92, \"root_cspan0\": 84, \"root_cspan1\": 92, \"fverb\": null, \"fword\": \"heads\", \"apos\": \"['PROPN', 'ADJ', 'ADP']\", \"apos_w\": \"['BERT', 'individual', 'by']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT's heads.\", \"split_tokens\": \"['Using', 'a', 'subset', 'of', 'GLUE', 'tasks', 'and', 'a', 'set', 'of', 'handcrafted', 'features-of-interest', ',', 'we', 'propose', 'the', 'methodology', 'and', 'carry', 'out', 'a', 'qualitative', 'and', 'quantitative', 'analysis', 'of', 'the', 'information', 'encoded', 'by', 'the', 'individual', 'BERT', \\\"'s\\\", 'heads', '.']\", \"split_anchor_span\": \"(32, 33)\", \"split_anchor_indices\": \"(202, 206)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.7092087246391189, \"pc2\": -0.4845134684638324, \"word\": \"leads\", \"split_0\": \"We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine - tuned\", \"split_1\": \"BERT\", \"split_2\": \"models\", \"averb\": \"leads\", \"averb_s\": \"['manually disabling attention in certain heads']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 59.0, \"averb_span1\": 65.0, \"averb_cspan0\": 59.0, \"averb_cspan1\": 65.0, \"root\": \"We\", \"root_full\": \"We\", \"root_s\": \"[]\", \"root_o\": \"['show that manually disabling attention in certain heads leads to a performance improvement over the regular fine - tuned BERT models']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": null, \"fword\": \"models\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'models', 'over']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models\", \"split_tokens\": \"['We', 'show', 'that', 'manually', 'disabling', 'attention', 'in', 'certain', 'heads', 'leads', 'to', 'a', 'performance', 'improvement', 'over', 'the', 'regular', 'fine', '-', 'tuned', 'BERT', 'models']\", \"split_anchor_span\": \"(20, 21)\", \"split_anchor_indices\": \"(123, 127)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.311961400142922, \"pc2\": 0.26687240307773774, \"word\": \"Revealing\", \"split_0\": \"Revealing the Dark Secrets of\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"Revealing\", \"averb_s\": \"[]\", \"averb_o\": \"['the Dark Secrets of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 10.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \"Revealing\", \"root_full\": \"Revealing\", \"root_s\": \"[]\", \"root_o\": \"['the Dark Secrets of BERT']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 10, \"root_cspan0\": 0, \"root_cspan1\": 10, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'PROPN']\", \"apos_w\": \"['BERT', 'of', 'Secrets']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"Revealing the Dark Secrets of BERT.\", \"split_tokens\": \"['Revealing', 'the', 'Dark', 'Secrets', 'of', 'BERT', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(29, 33)\", \"within_anchor_index\": 0.0}, {\"pc1\": 17.18788668229863, \"pc2\": -7.697451996256276, \"word\": \"has been released\", \"split_0\": \"Recently , an upgraded version of\", \"split_1\": \"BERT\", \"split_2\": \"has been released with Whole Word Masking ( WWM ) , which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .\", \"averb\": \"has been released\", \"averb_s\": \"['an upgraded version of BERT', 'Recently ,']\", \"averb_o\": \"['which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 39.0, \"averb_span1\": 57.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 18.0, \"root\": \"released\", \"root_full\": \"has been released\", \"root_s\": \"['an upgraded version of BERT', 'Recently ,']\", \"root_o\": \"['which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .']\", \"root_split\": 2, \"root_span0\": 39, \"root_span1\": 57, \"root_cspan0\": 0, \"root_cspan1\": 18, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'version']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT.\", \"split_tokens\": \"['Recently', ',', 'an', 'upgraded', 'version', 'of', 'BERT', 'has', 'been', 'released', 'with', 'Whole', 'Word', 'Masking', '(', 'WWM', ')', ',', 'which', 'mitigate', 'the', 'drawbacks', 'of', 'masking', 'partial', 'WordPiece', 'tokens', 'in', 'pre', '-', 'training', 'BERT', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(33, 37)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.9294133109325476, \"pc2\": -0.02305916404156912, \"word\": \"masking\", \"split_0\": \"Recently , an upgraded version of BERT has been released with Whole Word Masking ( WWM ) , which mitigate the drawbacks of masking partial WordPiece tokens in pre - training\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"masking\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 123.0, \"averb_span1\": 131.0, \"averb_cspan0\": 123.0, \"averb_cspan1\": 131.0, \"root\": \"released\", \"root_full\": \"has been released\", \"root_s\": \"['an upgraded version of BERT', 'Recently ,']\", \"root_o\": \"['which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .']\", \"root_split\": 0, \"root_span0\": 39, \"root_span1\": 57, \"root_cspan0\": 39, \"root_cspan1\": 57, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADJ', 'ADJ']\", \"apos_w\": \"['BERT', 'training', 'pre']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT.\", \"split_tokens\": \"['Recently', ',', 'an', 'upgraded', 'version', 'of', 'BERT', 'has', 'been', 'released', 'with', 'Whole', 'Word', 'Masking', '(', 'WWM', ')', ',', 'which', 'mitigate', 'the', 'drawbacks', 'of', 'masking', 'partial', 'WordPiece', 'tokens', 'in', 'pre', '-', 'training', 'BERT', '.']\", \"split_anchor_span\": \"(31, 32)\", \"split_anchor_indices\": \"(173, 177)\", \"within_anchor_index\": 0.0}, {\"pc1\": 6.2763689749817075, \"pc2\": -8.355548175768503, \"word\": \"was trained\", \"split_0\": null, \"split_1\": \"The model\", \"split_2\": \"was trained on the latest Chinese Wikipedia dump .\", \"averb\": \"was trained\", \"averb_s\": \"['The model']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 10.0, \"averb_span1\": 22.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"trained\", \"root_full\": \"was trained\", \"root_s\": \"['The model']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 10, \"root_span1\": 22, \"root_cspan0\": 0, \"root_cspan1\": 12, \"fverb\": \"was\", \"fword\": \"was\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['model', 'trained']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"The model was trained on the latest Chinese Wikipedia dump.\", \"split_tokens\": \"['The', 'model', 'was', 'trained', 'on', 'the', 'latest', 'Chinese', 'Wikipedia', 'dump', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 9)\", \"within_anchor_index\": -1.0}, {\"pc1\": 0.09116071740005917, \"pc2\": 6.244977664253517, \"word\": \"to provide\", \"split_0\": \"We aim to provide easy extensibility and better performance for\", \"split_1\": \"Chinese BERT\", \"split_2\": \"without changing any neural architecture or even hyper - parameters .\", \"averb\": \"to provide\", \"averb_s\": \"[]\", \"averb_o\": \"['easy', 'extensibility and', 'better performance']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 7.0, \"averb_span1\": 18.0, \"averb_cspan0\": 7.0, \"averb_cspan1\": 18.0, \"root\": \"aim\", \"root_full\": \"aim\", \"root_s\": \"['We']\", \"root_o\": \"['.']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 7, \"root_cspan0\": 3, \"root_cspan1\": 7, \"fverb\": \"changing\", \"fword\": \"without\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'for', 'provide']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"We aim to provide easy extensibility and better performance for Chinese BERT without changing any neural architecture or even hyper-parameters.\", \"split_tokens\": \"['We', 'aim', 'to', 'provide', 'easy', 'extensibility', 'and', 'better', 'performance', 'for', 'Chinese', 'BERT', 'without', 'changing', 'any', 'neural', 'architecture', 'or', 'even', 'hyper', '-', 'parameters', '.']\", \"split_anchor_span\": \"(10, 12)\", \"split_anchor_indices\": \"(63, 75)\", \"within_anchor_index\": 8.0}, {\"pc1\": 6.517507933236633, \"pc2\": -5.56471337516931, \"word\": \"is verified\", \"split_0\": null, \"split_1\": \"The model\", \"split_2\": \"is verified on various NLP tasks , across sentence - level to document - level , including sentiment classification ( ChnSentiCorp , Sina Weibo ) , named entity recognition ( People Daily , MSRA - NER ) , natural language inference ( XNLI ) , sentence pair matching ( LCQMC , BQ Corpus ) , and machine reading comprehension ( CMRC 2018 , DRCD , CAIL RC ) .\", \"averb\": \"is verified\", \"averb_s\": \"['The model']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 10.0, \"averb_span1\": 22.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \",\", \"root_full\": \",\", \"root_s\": \"['The model is verified on various NLP tasks , across sentence - level to document - level , including sentiment classification ( ChnSentiCorp , Sina Weibo ) , named entity recognition ( People Daily , MSRA - NER ) , natural language inference ( XNLI ) , sentence pair matching ( LCQMC , BQ Corpus ) , and machine reading comprehension ( CMRC 2018 ,', 'DRCD']\", \"root_o\": \"['CAIL RC ) .']\", \"root_split\": 2, \"root_span0\": 353, \"root_span1\": 355, \"root_cspan0\": 343, \"root_cspan1\": 345, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'VERB', 'NOUN']\", \"apos_w\": \"['model', 'verified', 'inference']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"The model is verified on various NLP tasks, across sentence-level to document-level, including sentiment classification (ChnSentiCorp, Sina Weibo), named entity recognition (People Daily, MSRA-NER), natural language inference (XNLI), sentence pair matching (LCQMC, BQ Corpus), and machine reading comprehension (CMRC 2018, DRCD, CAIL RC).\", \"split_tokens\": \"['The', 'model', 'is', 'verified', 'on', 'various', 'NLP', 'tasks', ',', 'across', 'sentence', '-', 'level', 'to', 'document', '-', 'level', ',', 'including', 'sentiment', 'classification', '(', 'ChnSentiCorp', ',', 'Sina', 'Weibo', ')', ',', 'named', 'entity', 'recognition', '(', 'People', 'Daily', ',', 'MSRA', '-', 'NER', ')', ',', 'natural', 'language', 'inference', '(', 'XNLI', ')', ',', 'sentence', 'pair', 'matching', '(', 'LCQMC', ',', 'BQ', 'Corpus', ')', ',', 'and', 'machine', 'reading', 'comprehension', '(', 'CMRC', '2018', ',', 'DRCD', ',', 'CAIL', 'RC', ')', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 9)\", \"within_anchor_index\": -1.0}, {\"pc1\": -4.394343408946245, \"pc2\": 3.672375785167686, \"word\": \"examine\", \"split_0\": \"Moreover , we also examine the effectiveness of Chinese pre - trained models :\", \"split_1\": \"BERT\", \"split_2\": \", ERNIE , BERT - wwm .\", \"averb\": \"examine\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 27.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 27.0, \"root\": \"examine\", \"root_full\": \"examine\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": \"ERNIE\", \"apos\": \"['PROPN', 'PUNCT', 'NOUN']\", \"apos_w\": \"['BERT', ':', 'models']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"Moreover, we also examine the effectiveness of Chinese pre-trained models: BERT, ERNIE, BERT-wwm.\", \"split_tokens\": \"['Moreover', ',', 'we', 'also', 'examine', 'the', 'effectiveness', 'of', 'Chinese', 'pre', '-', 'trained', 'models', ':', 'BERT', ',', 'ERNIE', ',', 'BERT', '-', 'wwm', '.']\", \"split_anchor_span\": \"(14, 15)\", \"split_anchor_indices\": \"(78, 82)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.394343408946245, \"pc2\": 3.672375785167687, \"word\": \"examine\", \"split_0\": \"Moreover , we also examine the effectiveness of Chinese pre - trained models : BERT , ERNIE ,\", \"split_1\": \"BERT - wwm\", \"split_2\": \".\", \"averb\": \"examine\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 27.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 27.0, \"root\": \"examine\", \"root_full\": \"examine\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'PUNCT', 'PROPN']\", \"apos_w\": \"['wwm', ',', 'ERNIE']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"Moreover, we also examine the effectiveness of Chinese pre-trained models: BERT, ERNIE, BERT-wwm.\", \"split_tokens\": \"['Moreover', ',', 'we', 'also', 'examine', 'the', 'effectiveness', 'of', 'Chinese', 'pre', '-', 'trained', 'models', ':', 'BERT', ',', 'ERNIE', ',', 'BERT', '-', 'wwm', '.']\", \"split_anchor_span\": \"(18, 21)\", \"split_anchor_indices\": \"(93, 103)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.3969726372334539, \"pc2\": -5.249518665834527, \"word\": \"BERT\", \"split_0\": \"However , previous work trains\", \"split_1\": \"BERT\", \"split_2\": \"by viewing passages corresponding to the same question as independent training instances , which may cause incomparable scores for answers from different passages .\", \"averb\": \"BERT\", \"averb_s\": \"['previous work trains']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 31.0, \"averb_span1\": 36.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 5.0, \"root\": \"cause\", \"root_full\": \"may cause\", \"root_s\": \"['However', ',', 'which', 'previous work trains BERT by viewing passages corresponding to the same question as independent training instances ,']\", \"root_o\": \"['incomparable scores']\", \"root_split\": 2, \"root_span0\": 133, \"root_span1\": 143, \"root_cspan0\": 97, \"root_cspan1\": 107, \"fverb\": \"viewing\", \"fword\": \"by\", \"apos\": \"['VERB', 'VERB']\", \"apos_w\": \"['BERT', 'cause']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages.\", \"split_tokens\": \"['However', ',', 'previous', 'work', 'trains', 'BERT', 'by', 'viewing', 'passages', 'corresponding', 'to', 'the', 'same', 'question', 'as', 'independent', 'training', 'instances', ',', 'which', 'may', 'cause', 'incomparable', 'scores', 'for', 'answers', 'from', 'different', 'passages', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(30, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.014457263550266, \"pc2\": -0.6897497300331202, \"word\": \"propose\", \"split_0\": \"To tackle this issue , we propose\", \"split_1\": \"a multi - passage BERT model to globally normalize answer scores across all passages of the same question\", \"split_2\": \", and this change enables our QA model find better answers by utilizing more passages .\", \"averb\": \"propose\", \"averb_s\": \"['To', 'we', 'tackle this issue']\", \"averb_o\": \"['a multi - passage BERT model to globally normalize answer scores across all passages of the same question ,', 'this change enables our QA model find better answers by utilizing more passages .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 26.0, \"averb_span1\": 34.0, \"averb_cspan0\": 26.0, \"averb_cspan1\": 34.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['To', 'we', 'tackle this issue']\", \"root_o\": \"['a multi - passage BERT model to globally normalize answer scores across all passages of the same question ,', 'this change enables our QA model find better answers by utilizing more passages .']\", \"root_split\": 0, \"root_span0\": 26, \"root_span1\": 34, \"root_cspan0\": 26, \"root_cspan1\": 34, \"fverb\": \"enables\", \"fword\": \"and\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['model', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages.\", \"split_tokens\": \"['To', 'tackle', 'this', 'issue', ',', 'we', 'propose', 'a', 'multi', '-', 'passage', 'BERT', 'model', 'to', 'globally', 'normalize', 'answer', 'scores', 'across', 'all', 'passages', 'of', 'the', 'same', 'question', ',', 'and', 'this', 'change', 'enables', 'our', 'QA', 'model', 'find', 'better', 'answers', 'by', 'utilizing', 'more', 'passages', '.']\", \"split_anchor_span\": \"(7, 25)\", \"split_anchor_indices\": \"(33, 138)\", \"within_anchor_index\": 18.0}, {\"pc1\": 2.4675586140916406, \"pc2\": -0.43982512227840503, \"word\": \"gains\", \"split_0\": \"By leveraging a passage ranker to select high-quality passages , multi-passage\", \"split_1\": \"BERT\", \"split_2\": \"gains additional 2 % .\", \"averb\": \"gains\", \"averb_s\": \"['BERT .']\", \"averb_o\": \"['additional 2 %']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 88.0, \"averb_span1\": 94.0, \"averb_cspan0\": 4.0, \"averb_cspan1\": 10.0, \"root\": \"By\", \"root_full\": \"By\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": null, \"fword\": \"gains\", \"apos\": \"['PROPN', 'VERB', 'ADJ']\", \"apos_w\": \"['BERT', 'gains', 'passage']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%.\", \"split_tokens\": \"['By', 'leveraging', 'a', 'passage', 'ranker', 'to', 'select', 'high-quality', 'passages', ',', 'multi-passage', 'BERT', 'gains', 'additional', '2', '%', '.']\", \"split_anchor_span\": \"(11, 12)\", \"split_anchor_indices\": \"(78, 82)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.183065247226218, \"pc2\": 1.0684182937006523, \"word\": \"showed\", \"split_0\": \"Experiments on four standard benchmarks showed that\", \"split_1\": \"our multi - passage BERT\", \"split_2\": \"outperforms all state - of - the - art models on all benchmarks .\", \"averb\": \"showed\", \"averb_s\": \"['Experiments on four standard benchmarks']\", \"averb_o\": \"['that our multi - passage BERT outperforms all state - of - the - art models on all benchmarks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 40.0, \"averb_span1\": 47.0, \"averb_cspan0\": 40.0, \"averb_cspan1\": 47.0, \"root\": \"showed\", \"root_full\": \"showed\", \"root_s\": \"['Experiments on four standard benchmarks']\", \"root_o\": \"['that our multi - passage BERT outperforms all state - of - the - art models on all benchmarks']\", \"root_split\": 0, \"root_span0\": 40, \"root_span1\": 47, \"root_cspan0\": 40, \"root_cspan1\": 47, \"fverb\": \"outperforms\", \"fword\": \"outperforms\", \"apos\": \"['PROPN', 'VERB']\", \"apos_w\": \"['BERT', 'showed']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks.\", \"split_tokens\": \"['Experiments', 'on', 'four', 'standard', 'benchmarks', 'showed', 'that', 'our', 'multi', '-', 'passage', 'BERT', 'outperforms', 'all', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'on', 'all', 'benchmarks', '.']\", \"split_anchor_span\": \"(7, 12)\", \"split_anchor_indices\": \"(51, 75)\", \"within_anchor_index\": 20.0}, {\"pc1\": -6.085317616831325, \"pc2\": 2.6818772324754105, \"word\": \"exploring\", \"split_0\": \"However , there has not been much effort on exploring\", \"split_1\": \"BERT\", \"split_2\": \"for natural language understanding .\", \"averb\": \"exploring\", \"averb_s\": \"[]\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 44.0, \"averb_span1\": 54.0, \"averb_cspan0\": 44.0, \"averb_cspan1\": 54.0, \"root\": \"much\", \"root_full\": \"has not been much\", \"root_s\": \"['However', ',', 'there']\", \"root_o\": \"['effort on exploring BERT for natural language understanding']\", \"root_split\": 0, \"root_span0\": 16, \"root_span1\": 34, \"root_cspan0\": 16, \"root_cspan1\": 34, \"fverb\": null, \"fword\": \"for\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'exploring', 'on']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"However, there has not been much effort on exploring BERT for natural language understanding.\", \"split_tokens\": \"['However', ',', 'there', 'has', 'not', 'been', 'much', 'effort', 'on', 'exploring', 'BERT', 'for', 'natural', 'language', 'understanding', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(53, 57)\", \"within_anchor_index\": 0.0}, {\"pc1\": 2.7212884727015116, \"pc2\": -7.1784821375710335, \"word\": \"based\", \"split_0\": \"In this work , we propose a joint intent classification and slot filling model based on\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"based\", \"averb_s\": \"['a joint intent classification and slot filling model']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 79.0, \"averb_span1\": 85.0, \"averb_cspan0\": 79.0, \"averb_cspan1\": 85.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we']\", \"root_o\": \"['a joint intent classification and slot filling model based on BERT']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 26, \"root_cspan0\": 18, \"root_cspan1\": 26, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'on', 'based']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"In this work, we propose a joint intent classification and slot filling model based on BERT.\", \"split_tokens\": \"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'joint', 'intent', 'classification', 'and', 'slot', 'filling', 'model', 'based', 'on', 'BERT', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(87, 91)\", \"within_anchor_index\": 0.0}, {\"pc1\": 2.7212884727015108, \"pc2\": -7.178482137571034, \"word\": \"based\", \"split_0\": \"In this work , we propose\", \"split_1\": \"a joint intent classification and slot filling model based on BERT\", \"split_2\": \".\", \"averb\": \"based\", \"averb_s\": \"['a joint intent classification and slot filling model']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 79.0, \"averb_span1\": 85.0, \"averb_cspan0\": 53.0, \"averb_cspan1\": 59.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we']\", \"root_o\": \"['a joint intent classification and slot filling model based on BERT']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 26, \"root_cspan0\": 18, \"root_cspan1\": 26, \"fverb\": null, \"fword\": null, \"apos\": \"['VERB', 'VERB']\", \"apos_w\": \"['based', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"In this work, we propose a joint intent classification and slot filling model based on BERT.\", \"split_tokens\": \"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'joint', 'intent', 'classification', 'and', 'slot', 'filling', 'model', 'based', 'on', 'BERT', '.']\", \"split_anchor_span\": \"(6, 17)\", \"split_anchor_indices\": \"(25, 91)\", \"within_anchor_index\": 62.0}, {\"pc1\": 0.9821618435856201, \"pc2\": 0.8640276864481025, \"word\": \"achieves\", \"split_0\": \"Experimental results demonstrate that\", \"split_1\": \"our proposed model\", \"split_2\": \"achieves significant improvement on intent classification accuracy , slot filling F1 , and sentence - level semantic frame accuracy on several public benchmark datasets , compared to the attention - based recurrent neural network models and slot - gated models\", \"averb\": \"achieves\", \"averb_s\": \"['our proposed model']\", \"averb_o\": \"['significant improvement']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 57.0, \"averb_span1\": 66.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['Experimental results']\", \"root_o\": \"['and sentence - level semantic frame accuracy on several public benchmark datasets , compared to the attention - based recurrent neural network models and slot - gated models']\", \"root_split\": 0, \"root_span0\": 21, \"root_span1\": 33, \"root_cspan0\": 21, \"root_cspan1\": 33, \"fverb\": \"achieves\", \"fword\": \"achieves\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['model', 'achieves', 'demonstrate']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models\", \"split_tokens\": \"['Experimental', 'results', 'demonstrate', 'that', 'our', 'proposed', 'model', 'achieves', 'significant', 'improvement', 'on', 'intent', 'classification', 'accuracy', ',', 'slot', 'filling', 'F1', ',', 'and', 'sentence', '-', 'level', 'semantic', 'frame', 'accuracy', 'on', 'several', 'public', 'benchmark', 'datasets', ',', 'compared', 'to', 'the', 'attention', '-', 'based', 'recurrent', 'neural', 'network', 'models', 'and', 'slot', '-', 'gated', 'models']\", \"split_anchor_span\": \"(4, 7)\", \"split_anchor_indices\": \"(37, 55)\", \"within_anchor_index\": -1.0}, {\"pc1\": 3.8895133238439095, \"pc2\": -6.99287123319143, \"word\": \"built\", \"split_0\": \"It enables seamless integration of conversation history into a conversational question answering ( ConvQA ) model built on\", \"split_1\": \"BERT\", \"split_2\": \"( Bidirectional Encoder Representations from Transformers ) .\", \"averb\": \"built\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 114.0, \"averb_span1\": 120.0, \"averb_cspan0\": 114.0, \"averb_cspan1\": 120.0, \"root\": \"history\", \"root_full\": \"history\", \"root_s\": \"['It enables', 'seamless integration of']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 48, \"root_span1\": 56, \"root_cspan0\": 48, \"root_cspan1\": 56, \"fverb\": null, \"fword\": \"Bidirectional\", \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'on', 'built']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921\", \"ID\": 19, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"It enables seamless integration of conversation history into a conversational question answering (ConvQA) model built on BERT (Bidirectional Encoder Representations from Transformers).\", \"split_tokens\": \"['It', 'enables', 'seamless', 'integration', 'of', 'conversation', 'history', 'into', 'a', 'conversational', 'question', 'answering', '(', 'ConvQA', ')', 'model', 'built', 'on', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', '.']\", \"split_anchor_span\": \"(18, 19)\", \"split_anchor_indices\": \"(122, 126)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.2652754722783815, \"pc2\": -7.844900434691354, \"word\": \"BERT\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"with History Answer Embedding for Conversational Question Answering .\", \"averb\": \"BERT\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 0.0, \"averb_span1\": 5.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 5.0, \"root\": \"BERT\", \"root_full\": \"BERT\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 1, \"root_span0\": 0, \"root_span1\": 5, \"root_cspan0\": 0, \"root_cspan1\": 5, \"fverb\": null, \"fword\": \"with\", \"apos\": \"['VERB']\", \"apos_w\": \"['BERT']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921\", \"ID\": 19, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT with History Answer Embedding for Conversational Question Answering.\", \"split_tokens\": \"['BERT', 'with', 'History', 'Answer', 'Embedding', 'for', 'Conversational', 'Question', 'Answering', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.9684412747470463, \"pc2\": 0.9934175922724955, \"word\": \"studies\", \"split_0\": \"This paper studies the performances and behaviors of\", \"split_1\": \"BERT\", \"split_2\": \"in ranking tasks .\", \"averb\": \"studies\", \"averb_s\": \"['This']\", \"averb_o\": \"['the performances and behaviors of BERT in ranking tasks', '.']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 11.0, \"averb_span1\": 19.0, \"averb_cspan0\": 11.0, \"averb_cspan1\": 19.0, \"root\": \"studies\", \"root_full\": \"studies\", \"root_s\": \"['This']\", \"root_o\": \"['the performances and behaviors of BERT in ranking tasks', '.']\", \"root_split\": 0, \"root_span0\": 11, \"root_span1\": 19, \"root_cspan0\": 11, \"root_cspan1\": 19, \"fverb\": \"ranking\", \"fword\": \"in\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'behaviors']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"This paper studies the performances and behaviors of BERT in ranking tasks.\", \"split_tokens\": \"['This', 'paper', 'studies', 'the', 'performances', 'and', 'behaviors', 'of', 'BERT', 'in', 'ranking', 'tasks', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(52, 56)\", \"within_anchor_index\": 0.0}, {\"pc1\": 0.2981990225319187, \"pc2\": 10.071734056172932, \"word\": \"to leverage\", \"split_0\": \"We explore several different ways to leverage\", \"split_1\": \"the pre - trained BERT\", \"split_2\": \"and fine - tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document ranking .\", \"averb\": \"to leverage\", \"averb_s\": \"[]\", \"averb_o\": \"['the pre - trained BERT and', 'fine - tune it on two ranking tasks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 34.0, \"averb_span1\": 46.0, \"averb_cspan0\": 34.0, \"averb_cspan1\": 46.0, \"root\": \"ranking\", \"root_full\": \"ranking\", \"root_s\": \"['We explore several different ways to leverage the pre - trained BERT and fine - tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document']\", \"root_o\": \"['.']\", \"root_split\": 2, \"root_span0\": 173, \"root_span1\": 181, \"root_cspan0\": 104, \"root_cspan1\": 112, \"fverb\": \"ranking\", \"fword\": \"and\", \"apos\": \"['NOUN', 'VERB', 'NOUN']\", \"apos_w\": \"['BERT', 'leverage', 'ways']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking.\", \"split_tokens\": \"['We', 'explore', 'several', 'different', 'ways', 'to', 'leverage', 'the', 'pre', '-', 'trained', 'BERT', 'and', 'fine', '-', 'tune', 'it', 'on', 'two', 'ranking', 'tasks', ':', 'MS', 'MARCO', 'passage', 'reranking', 'and', 'TREC', 'Web', 'Track', 'ad', 'hoc', 'document', 'ranking', '.']\", \"split_anchor_span\": \"(7, 12)\", \"split_anchor_indices\": \"(45, 67)\", \"within_anchor_index\": 18.0}, {\"pc1\": 0.2981990225319181, \"pc2\": 10.071734056172936, \"word\": \"to leverage\", \"split_0\": \"We explore several different ways to leverage the pre - trained BERT and fine - tune\", \"split_1\": \"it\", \"split_2\": \"on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document ranking .\", \"averb\": \"to leverage\", \"averb_s\": \"[]\", \"averb_o\": \"['the pre - trained BERT and', 'fine - tune it on two ranking tasks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 34.0, \"averb_span1\": 46.0, \"averb_cspan0\": 34.0, \"averb_cspan1\": 46.0, \"root\": \"ranking\", \"root_full\": \"ranking\", \"root_s\": \"['We explore several different ways to leverage the pre - trained BERT and fine - tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document']\", \"root_o\": \"['.']\", \"root_split\": 2, \"root_span0\": 173, \"root_span1\": 181, \"root_cspan0\": 85, \"root_cspan1\": 93, \"fverb\": \"ranking\", \"fword\": \"on\", \"apos\": \"['PRON', 'NOUN', 'VERB']\", \"apos_w\": \"['it', 'tune', 'leverage']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking.\", \"split_tokens\": \"['We', 'explore', 'several', 'different', 'ways', 'to', 'leverage', 'the', 'pre', '-', 'trained', 'BERT', 'and', 'fine', '-', 'tune', 'it', 'on', 'two', 'ranking', 'tasks', ':', 'MS', 'MARCO', 'passage', 'reranking', 'and', 'TREC', 'Web', 'Track', 'ad', 'hoc', 'document', 'ranking', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(84, 86)\", \"within_anchor_index\": -1.0}, {\"pc1\": -2.2447872884775233, \"pc2\": 1.6041030730617478, \"word\": \"demonstrate\", \"split_0\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of\", \"split_1\": \"BERT\", \"split_2\": \"in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .\", \"averb\": \"demonstrate\", \"averb_s\": \"['Experimental results on MS MARCO', 'the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 33.0, \"averb_span1\": 45.0, \"averb_cspan0\": 33.0, \"averb_cspan1\": 45.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['Experimental results on MS MARCO', 'the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 33, \"root_span1\": 45, \"root_cspan0\": 33, \"root_cspan1\": 45, \"fverb\": \"ranking\", \"fword\": \"in\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'effectiveness']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model.\", \"split_tokens\": \"['Experimental', 'results', 'on', 'MS', 'MARCO', 'demonstrate', 'the', 'strong', 'effectiveness', 'of', 'BERT', 'in', 'question', '-', 'answering', 'focused', 'passage', 'ranking', 'tasks', ',', 'as', 'well', 'as', 'the', 'fact', 'that', 'BERT', 'is', 'a', 'strong', 'interaction', '-', 'based', 'seq2seq', 'matching', 'model', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(72, 76)\", \"within_anchor_index\": 0.0}, {\"pc1\": -2.1554531774452004, \"pc2\": 0.6448014034272673, \"word\": \"answering\", \"split_0\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that\", \"split_1\": \"BERT\", \"split_2\": \"is a strong interaction - based seq2seq matching model .\", \"averb\": \"answering\", \"averb_s\": \"[]\", \"averb_o\": \"['focused passage ranking tasks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 92.0, \"averb_span1\": 102.0, \"averb_cspan0\": 92.0, \"averb_cspan1\": 102.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['Experimental results on MS MARCO', 'the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 33, \"root_span1\": 45, \"root_cspan0\": 33, \"root_cspan1\": 45, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'NOUN', 'NOUN']\", \"apos_w\": \"['BERT', 'model', 'fact']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model.\", \"split_tokens\": \"['Experimental', 'results', 'on', 'MS', 'MARCO', 'demonstrate', 'the', 'strong', 'effectiveness', 'of', 'BERT', 'in', 'question', '-', 'answering', 'focused', 'passage', 'ranking', 'tasks', ',', 'as', 'well', 'as', 'the', 'fact', 'that', 'BERT', 'is', 'a', 'strong', 'interaction', '-', 'based', 'seq2seq', 'matching', 'model', '.']\", \"split_anchor_span\": \"(26, 27)\", \"split_anchor_indices\": \"(158, 162)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.7498939440158483, \"pc2\": -3.050204634580302, \"word\": \"pre\", \"split_0\": \"Experimental results on TREC show the gaps between the\", \"split_1\": \"BERT\", \"split_2\": \"pre-trained on surrounding contexts and the needs of ad hoc document ranking .\", \"averb\": \"pre\", \"averb_s\": \"['the', 'BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 60.0, \"averb_span1\": 64.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 4.0, \"root\": \"results\", \"root_full\": \"results\", \"root_s\": \"['Experimental']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 13, \"root_span1\": 21, \"root_cspan0\": 13, \"root_cspan1\": 21, \"fverb\": \"surrounding\", \"fword\": \"pre-trained\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'pre', 'between']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking.\", \"split_tokens\": \"['Experimental', 'results', 'on', 'TREC', 'show', 'the', 'gaps', 'between', 'the', 'BERT', 'pre-trained', 'on', 'surrounding', 'contexts', 'and', 'the', 'needs', 'of', 'ad', 'hoc', 'document', 'ranking', '.']\", \"split_anchor_span\": \"(9, 10)\", \"split_anchor_indices\": \"(54, 58)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.5882735646082966, \"pc2\": 0.6854031192503438, \"word\": \"allocates\", \"split_0\": \"Analyses illustrate how\", \"split_1\": \"BERT\", \"split_2\": \"allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"allocates\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 29.0, \"averb_span1\": 39.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"allocates\", \"fword\": \"allocates\", \"apos\": \"['PROPN', 'VERB', 'VERB']\", \"apos_w\": \"['BERT', 'allocates', 'illustrate']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(23, 27)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.588273564608297, \"pc2\": 0.6854031192503438, \"word\": \"allocates\", \"split_0\": \"Analyses illustrate how BERT allocates\", \"split_1\": \"its\", \"split_2\": \"attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"allocates\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 29.0, \"averb_span1\": 39.0, \"averb_cspan0\": 29.0, \"averb_cspan1\": 39.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"prefers\", \"fword\": \"attentions\", \"apos\": \"['DET', 'NOUN', 'VERB']\", \"apos_w\": \"['its', 'attentions', 'allocates']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(38, 41)\", \"within_anchor_index\": -1.0}, {\"pc1\": -1.588273564608297, \"pc2\": 0.6854031192503434, \"word\": \"allocates\", \"split_0\": \"Analyses illustrate how BERT allocates its attentions between query - document tokens in\", \"split_1\": \"its\", \"split_2\": \"Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"allocates\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 29.0, \"averb_span1\": 39.0, \"averb_cspan0\": 29.0, \"averb_cspan1\": 39.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"prefers\", \"fword\": \"Transformer\", \"apos\": \"['DET', 'NOUN', 'ADP']\", \"apos_w\": \"['its', 'layers', 'in']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(13, 14)\", \"split_anchor_indices\": \"(88, 91)\", \"within_anchor_index\": -1.0}, {\"pc1\": -0.16516238936221436, \"pc2\": -1.3635579002082578, \"word\": \"prefers\", \"split_0\": \"Analyses illustrate how BERT allocates its attentions between query - document tokens in its Transformer layers , how\", \"split_1\": \"it\", \"split_2\": \"prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"prefers\", \"averb_s\": \"['it']\", \"averb_o\": \"['semantic', 'how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 121.0, \"averb_span1\": 129.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"prefers\", \"fword\": \"prefers\", \"apos\": \"['PRON', 'VERB', 'NOUN']\", \"apos_w\": \"['it', 'prefers', 'document']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(18, 19)\", \"split_anchor_indices\": \"(117, 119)\", \"within_anchor_index\": -1.0}, {\"pc1\": -3.5888348972663238, \"pc2\": 1.7092698965640818, \"word\": \"Understanding\", \"split_0\": \"Understanding the Behaviors of\", \"split_1\": \"BERT\", \"split_2\": \"in Ranking .\", \"averb\": \"Understanding\", \"averb_s\": \"[]\", \"averb_o\": \"['the Behaviors of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 14.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 14.0, \"root\": \"Understanding\", \"root_full\": \"Understanding\", \"root_s\": \"[]\", \"root_o\": \"['the Behaviors of BERT']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 14, \"root_cspan0\": 0, \"root_cspan1\": 14, \"fverb\": null, \"fword\": \"in\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'Behaviors']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"Understanding the Behaviors of BERT in Ranking.\", \"split_tokens\": \"['Understanding', 'the', 'Behaviors', 'of', 'BERT', 'in', 'Ranking', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(30, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": 1.6632772926727881, \"pc2\": 0.8732141213723017, \"word\": \"achieve\", \"split_0\": \"In this paper , extensive experiments on datasets for these two tasks show that without using any external features , a simple\", \"split_1\": \"BERT\", \"split_2\": \"-based model can achieve state-of-the-art performance .\", \"averb\": \"achieve\", \"averb_s\": \"['can']\", \"averb_o\": \"['any external features , a simple BERT -based model state of - the - art performance', '-']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 149.0, \"averb_span1\": 157.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 25.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"['these two tasks']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 70, \"root_span1\": 75, \"root_cspan0\": 70, \"root_cspan1\": 75, \"fverb\": \"-based\", \"fword\": \"based\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'model', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b\", \"ID\": 21, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'extensive', 'experiments', 'on', 'datasets', 'for', 'these', 'two', 'tasks', 'show', 'that', 'without', 'using', 'any', 'external', 'features', ',', 'a', 'simple', 'BERT', '-based', 'model', 'can', 'achieve', 'state-of-the-art', 'performance', '.']\", \"split_anchor_span\": \"(22, 23)\", \"split_anchor_indices\": \"(126, 130)\", \"within_anchor_index\": 0.0}, {\"pc1\": 6.199517562426956, \"pc2\": 12.541770394268053, \"word\": \"to apply\", \"split_0\": \"To our knowledge , we are the first to successfully apply\", \"split_1\": \"BERT\", \"split_2\": \"in this manner .\", \"averb\": \"to apply\", \"averb_s\": \"[]\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 36.0, \"averb_span1\": 58.0, \"averb_cspan0\": 36.0, \"averb_cspan1\": 58.0, \"root\": \"first\", \"root_full\": \"are first\", \"root_s\": \"['we']\", \"root_o\": \"['to successfully apply BERT in this manner', '.']\", \"root_split\": 0, \"root_span0\": 22, \"root_span1\": 36, \"root_cspan0\": 22, \"root_cspan1\": 36, \"fverb\": null, \"fword\": \"in\", \"apos\": \"['PROPN', 'VERB', 'ADJ']\", \"apos_w\": \"['BERT', 'apply', 'first']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b\", \"ID\": 21, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"To our knowledge, we are the first to successfully apply BERT in this manner.\", \"split_tokens\": \"['To', 'our', 'knowledge', ',', 'we', 'are', 'the', 'first', 'to', 'successfully', 'apply', 'BERT', 'in', 'this', 'manner', '.']\", \"split_anchor_span\": \"(11, 12)\", \"split_anchor_indices\": \"(57, 61)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.663894866480351, \"pc2\": 0.528160702644243, \"word\": \"provide\", \"split_0\": null, \"split_1\": \"Our models\", \"split_2\": \"provide strong baselines for future research\", \"averb\": \"provide\", \"averb_s\": \"['Our models']\", \"averb_o\": \"['strong baselines']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 11.0, \"averb_span1\": 19.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"provide\", \"root_full\": \"provide\", \"root_s\": \"['Our models']\", \"root_o\": \"['strong baselines']\", \"root_split\": 2, \"root_span0\": 11, \"root_span1\": 19, \"root_cspan0\": 0, \"root_cspan1\": 8, \"fverb\": \"provide\", \"fword\": \"provide\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['models', 'provide']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b\", \"ID\": 21, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Our models provide strong baselines for future research\", \"split_tokens\": \"['Our', 'models', 'provide', 'strong', 'baselines', 'for', 'future', 'research']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 10)\", \"within_anchor_index\": -1.0}, {\"pc1\": -5.299293592942834, \"pc2\": 2.105665791789082, \"word\": \"explore\", \"split_0\": \"We explore the multi-task learning setting for the recent\", \"split_1\": \"BERT\", \"split_2\": \"model on the GLUE benchmark , and how to best add task-specific parameters to a pre-trained BERT network , with a high degree of parameter sharing between tasks .\", \"averb\": \"explore\", \"averb_s\": \"['We']\", \"averb_o\": \"['the multi - task learning setting for the recent BERT model on the GLUE benchmark ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"add\", \"root_full\": \"add\", \"root_s\": \"['and how to best']\", \"root_o\": \"['We explore the multi - task learning setting for the recent BERT model on the GLUE benchmark , task - specific parameters , with a high degree of parameter sharing between tasks .']\", \"root_split\": 2, \"root_span0\": 111, \"root_span1\": 115, \"root_cspan0\": 48, \"root_cspan1\": 52, \"fverb\": \"best\", \"fword\": \"model\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'model', 'for']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark, and how to best add task-specific parameters to a pre-trained BERT network, with a high degree of parameter sharing between tasks.\", \"split_tokens\": \"['We', 'explore', 'the', 'multi-task', 'learning', 'setting', 'for', 'the', 'recent', 'BERT', 'model', 'on', 'the', 'GLUE', 'benchmark', ',', 'and', 'how', 'to', 'best', 'add', 'task-specific', 'parameters', 'to', 'a', 'pre-trained', 'BERT', 'network', ',', 'with', 'a', 'high', 'degree', 'of', 'parameter', 'sharing', 'between', 'tasks', '.']\", \"split_anchor_span\": \"(9, 10)\", \"split_anchor_indices\": \"(57, 61)\", \"within_anchor_index\": 0.0}, {\"pc1\": 0.13252686816544795, \"pc2\": 1.635236881526552, \"word\": \"add\", \"split_0\": \"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark , and how to best add task-specific parameters to a pre-trained\", \"split_1\": \"BERT\", \"split_2\": \"network , with a high degree of parameter sharing between tasks .\", \"averb\": \"add\", \"averb_s\": \"['and how to best']\", \"averb_o\": \"['We explore the multi - task learning setting for the recent BERT model on the GLUE benchmark , task - specific parameters , with a high degree of parameter sharing between tasks .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 111.0, \"averb_span1\": 115.0, \"averb_cspan0\": 111.0, \"averb_cspan1\": 115.0, \"root\": \"add\", \"root_full\": \"add\", \"root_s\": \"['and how to best']\", \"root_o\": \"['We explore the multi - task learning setting for the recent BERT model on the GLUE benchmark , task - specific parameters , with a high degree of parameter sharing between tasks .']\", \"root_split\": 0, \"root_span0\": 111, \"root_span1\": 115, \"root_cspan0\": 111, \"root_cspan1\": 115, \"fverb\": \"sharing\", \"fword\": \"network\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'network', 'to']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark, and how to best add task-specific parameters to a pre-trained BERT network, with a high degree of parameter sharing between tasks.\", \"split_tokens\": \"['We', 'explore', 'the', 'multi-task', 'learning', 'setting', 'for', 'the', 'recent', 'BERT', 'model', 'on', 'the', 'GLUE', 'benchmark', ',', 'and', 'how', 'to', 'best', 'add', 'task-specific', 'parameters', 'to', 'a', 'pre-trained', 'BERT', 'network', ',', 'with', 'a', 'high', 'degree', 'of', 'parameter', 'sharing', 'between', 'tasks', '.']\", \"split_anchor_span\": \"(26, 27)\", \"split_anchor_indices\": \"(154, 158)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.0969462020705714, \"pc2\": 1.8150072277923808, \"word\": \"using\", \"split_0\": \"By using PALs in parallel with\", \"split_1\": \"BERT\", \"split_2\": \"layers , we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"averb\": \"using\", \"averb_s\": \"[]\", \"averb_o\": \"['PALs']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 9.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 9.0, \"root\": \"match\", \"root_full\": \"match\", \"root_s\": \"['we', 'By using PALs in parallel with BERT layers', ',']\", \"root_o\": \"['the performance of fine - tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state - of - the - art results on the Recognizing Textual Entailment dataset']\", \"root_split\": 2, \"root_span0\": 48, \"root_span1\": 54, \"root_cspan0\": 12, \"root_cspan1\": 18, \"fverb\": \"match\", \"fword\": \"layers\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'layers', 'with']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"By using PALs in parallel with BERT layers, we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"split_tokens\": \"['By', 'using', 'PALs', 'in', 'parallel', 'with', 'BERT', 'layers', ',', 'we', 'match', 'the', 'performance', 'of', 'fine-tuned', 'BERT', 'on', 'the', 'GLUE', 'benchmark', 'with', 'roughly', '7', 'times', 'fewer', 'parameters', ',', 'and', 'obtain', 'state-of-the-art', 'results', 'on', 'the', 'Recognizing', 'Textual', 'Entailment', 'dataset']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(30, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.8795086490250223, \"pc2\": -0.4082886293576107, \"word\": \"tuned\", \"split_0\": \"By using PALs in parallel with BERT layers , we match the performance of fine-tuned\", \"split_1\": \"BERT\", \"split_2\": \"on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"averb\": \"tuned\", \"averb_s\": \"['-']\", \"averb_o\": \"['BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state - of - the - art results on the Recognizing Textual Entailment dataset']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 80.0, \"averb_span1\": 86.0, \"averb_cspan0\": 80.0, \"averb_cspan1\": 86.0, \"root\": \"match\", \"root_full\": \"match\", \"root_s\": \"['we', 'By using PALs in parallel with BERT layers', ',']\", \"root_o\": \"['the performance of fine - tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state - of - the - art results on the Recognizing Textual Entailment dataset']\", \"root_split\": 0, \"root_span0\": 48, \"root_span1\": 54, \"root_cspan0\": 48, \"root_cspan1\": 54, \"fverb\": \"obtain\", \"fword\": \"on\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'tuned', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"By using PALs in parallel with BERT layers, we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"split_tokens\": \"['By', 'using', 'PALs', 'in', 'parallel', 'with', 'BERT', 'layers', ',', 'we', 'match', 'the', 'performance', 'of', 'fine-tuned', 'BERT', 'on', 'the', 'GLUE', 'benchmark', 'with', 'roughly', '7', 'times', 'fewer', 'parameters', ',', 'and', 'obtain', 'state-of-the-art', 'results', 'on', 'the', 'Recognizing', 'Textual', 'Entailment', 'dataset']\", \"split_anchor_span\": \"(15, 16)\", \"split_anchor_indices\": \"(83, 87)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.7490010971485849, \"pc2\": 1.4169356382052387, \"word\": \"apply\", \"split_0\": \"As a case study , we apply these diagnostics to\", \"split_1\": \"the popular BERT model\", \"split_2\": \", finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"apply\", \"averb_s\": \"['we', ',', 'finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and', 'As a case study ,']\", \"averb_o\": \"['these diagnostics']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 21.0, \"averb_span1\": 27.0, \"averb_cspan0\": 21.0, \"averb_cspan1\": 27.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 318, \"root_cspan1\": 332, \"fverb\": \"finding\", \"fword\": \"finding\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['model', 'to', 'apply']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(10, 14)\", \"split_anchor_indices\": \"(47, 69)\", \"within_anchor_index\": 12.0}, {\"pc1\": 15.748132877618433, \"pc2\": 14.387232264212424, \"word\": \"can distinguish\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that\", \"split_1\": \"it\", \"split_2\": \"can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"can distinguish\", \"averb_s\": \"['it']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 89.0, \"averb_span1\": 115.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 26.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 300, \"root_cspan1\": 314, \"fverb\": \"distinguish\", \"fword\": \"can\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['it', 'distinguish', 'finding']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(17, 18)\", \"split_anchor_indices\": \"(85, 87)\", \"within_anchor_index\": -1.0}, {\"pc1\": -2.803999226816611, \"pc2\": 1.08289063831589, \"word\": \"shows\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and\", \"split_1\": \"it\", \"split_2\": \"robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"shows\", \"averb_s\": \"['it', 'As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and', 'robustly retrieves noun hypernyms it struggles with challenging inference and role - based event prediction \\u2014 and , in particular ,']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 377.0, \"averb_span1\": 383.0, \"averb_cspan0\": 141.0, \"averb_cspan1\": 147.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 153, \"root_cspan1\": 167, \"fverb\": \"retrieves\", \"fword\": \"robustly\", \"apos\": \"['PRON', 'VERB', 'ADJ']\", \"apos_w\": \"['it', 'shows', 'clear']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(40, 41)\", \"split_anchor_indices\": \"(232, 234)\", \"within_anchor_index\": -1.0}, {\"pc1\": 1.6586286176718483, \"pc2\": -5.218622165085159, \"word\": \"struggles\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but\", \"split_1\": \"it\", \"split_2\": \"struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"struggles\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 279.0, \"averb_span1\": 289.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 110, \"root_cspan1\": 124, \"fverb\": \"struggles\", \"fword\": \"struggles\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['it', 'struggles', 'shows']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(47, 48)\", \"split_anchor_indices\": \"(275, 277)\", \"within_anchor_index\": -1.0}, {\"pc1\": -2.8039992268166096, \"pc2\": 1.0828906383158898, \"word\": \"shows\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular ,\", \"split_1\": \"it\", \"split_2\": \"shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"shows\", \"averb_s\": \"['it', 'As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and', 'robustly retrieves noun hypernyms it struggles with challenging inference and role - based event prediction \\u2014 and , in particular ,']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 377.0, \"averb_span1\": 383.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 6.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 12, \"root_cspan1\": 26, \"fverb\": \"shows\", \"fword\": \"shows\", \"apos\": \"['PRON', 'VERB', 'ADJ']\", \"apos_w\": \"['it', 'shows', 'clear']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(64, 65)\", \"split_anchor_indices\": \"(373, 375)\", \"within_anchor_index\": -1.0}, {\"pc1\": -1.6962708931092474, \"pc2\": -3.8496547239545076, \"word\": \"Is\", \"split_0\": \"What\", \"split_1\": \"BERT\", \"split_2\": \"Is Not : Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .\", \"averb\": \"Is\", \"averb_s\": \"['What', 'BERT']\", \"averb_o\": \"['Not', ': Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 10.0, \"averb_span1\": 13.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"Is\", \"root_full\": \"Is\", \"root_s\": \"['What', 'BERT']\", \"root_o\": \"['Not', ': Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .']\", \"root_split\": 2, \"root_span0\": 10, \"root_span1\": 13, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"Is\", \"fword\": \"Is\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['BERT', 'Is']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.\", \"split_tokens\": \"['What', 'BERT', 'Is', 'Not', ':', 'Lessons', 'from', 'a', 'New', 'Suite', 'of', 'Psycholinguistic', 'Diagnostics', 'for', 'Language', 'Models', '.']\", \"split_anchor_span\": \"(1, 2)\", \"split_anchor_indices\": \"(4, 8)\", \"within_anchor_index\": 0.0}, {\"pc1\": -2.4744443374057448, \"pc2\": -3.9046400443073623, \"word\": \"pre\", \"split_0\": \"Language model pre - training , such as\", \"split_1\": \"BERT\", \"split_2\": \", has achieved remarkable results in many NLP tasks .\", \"averb\": \"pre\", \"averb_s\": \"['model']\", \"averb_o\": \"['-', 'training , such as BERT ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 15.0, \"averb_span1\": 19.0, \"averb_cspan0\": 15.0, \"averb_cspan1\": 19.0, \"root\": \"achieved\", \"root_full\": \"has achieved\", \"root_s\": \"['Language model pre - training , such as BERT ,']\", \"root_o\": \"['remarkable results']\", \"root_split\": 2, \"root_span0\": 47, \"root_span1\": 60, \"root_cspan0\": 2, \"root_cspan1\": 15, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'SCONJ', 'ADJ']\", \"apos_w\": \"['BERT', 'as', 'training']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks.\", \"split_tokens\": \"['Language', 'model', 'pre', '-', 'training', ',', 'such', 'as', 'BERT', ',', 'has', 'achieved', 'remarkable', 'results', 'in', 'many', 'NLP', 'tasks', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(39, 43)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.8580385796715574, \"pc2\": -1.6037540861261192, \"word\": \"tuning\", \"split_0\": \"In this paper , we propose to visualize loss landscapes and optimization trajectories of fine - tuning\", \"split_1\": \"BERT\", \"split_2\": \"on specific datasets .\", \"averb\": \"tuning\", \"averb_s\": \"['-']\", \"averb_o\": \"['BERT on specific datasets']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 96.0, \"averb_span1\": 103.0, \"averb_cspan0\": 96.0, \"averb_cspan1\": 103.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['and']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": \"on\", \"apos\": \"['NOUN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'tuning', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'propose', 'to', 'visualize', 'loss', 'landscapes', 'and', 'optimization', 'trajectories', 'of', 'fine', '-', 'tuning', 'BERT', 'on', 'specific', 'datasets', '.']\", \"split_anchor_span\": \"(17, 18)\", \"split_anchor_indices\": \"(102, 106)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.7442310639978986, \"pc2\": 2.1584856315537824, \"word\": \"demonstrate\", \"split_0\": \"We also demonstrate that the fine - tuning procedure is robust to overfitting , even though\", \"split_1\": \"BERT\", \"split_2\": \"is highly over - parameterized for downstream tasks .\", \"averb\": \"demonstrate\", \"averb_s\": \"['We']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 8.0, \"averb_span1\": 20.0, \"averb_cspan0\": 8.0, \"averb_cspan1\": 20.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['We']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 8, \"root_span1\": 20, \"root_cspan0\": 8, \"root_cspan1\": 20, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'ADV', 'ADJ']\", \"apos_w\": \"['BERT', 'highly', 'robust']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks.\", \"split_tokens\": \"['We', 'also', 'demonstrate', 'that', 'the', 'fine', '-', 'tuning', 'procedure', 'is', 'robust', 'to', 'overfitting', ',', 'even', 'though', 'BERT', 'is', 'highly', 'over', '-', 'parameterized', 'for', 'downstream', 'tasks', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(91, 95)\", \"within_anchor_index\": 0.0}, {\"pc1\": -0.630227444827043, \"pc2\": -2.2477096550494196, \"word\": \"tuning\", \"split_0\": \"Second , the visualization results indicate that fine - tuning\", \"split_1\": \"BERT\", \"split_2\": \"tends to generalize better because of the flat and wide optima , and the consistency between the training loss surface and the generalization error surface .\", \"averb\": \"tuning\", \"averb_s\": \"['-']\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 56.0, \"averb_span1\": 63.0, \"averb_cspan0\": 56.0, \"averb_cspan1\": 63.0, \"root\": \"indicate\", \"root_full\": \"indicate\", \"root_s\": \"[', the visualization results']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 35, \"root_span1\": 44, \"root_cspan0\": 35, \"root_cspan1\": 44, \"fverb\": \"tends\", \"fword\": \"tends\", \"apos\": \"['PROPN', 'VERB', 'VERB']\", \"apos_w\": \"['BERT', 'tuning', 'tends']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface.\", \"split_tokens\": \"['Second', ',', 'the', 'visualization', 'results', 'indicate', 'that', 'fine', '-', 'tuning', 'BERT', 'tends', 'to', 'generalize', 'better', 'because', 'of', 'the', 'flat', 'and', 'wide', 'optima', ',', 'and', 'the', 'consistency', 'between', 'the', 'training', 'loss', 'surface', 'and', 'the', 'generalization', 'error', 'surface', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(62, 66)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.509780239017319, \"pc2\": 0.2527855510928305, \"word\": \"Understanding\", \"split_0\": \"Visualizing and Understanding the Effectiveness of\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"Understanding\", \"averb_s\": \"[]\", \"averb_o\": \"['the Effectiveness of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 16.0, \"averb_span1\": 30.0, \"averb_cspan0\": 16.0, \"averb_cspan1\": 30.0, \"root\": \"Visualizing\", \"root_full\": \"Visualizing\", \"root_s\": \"[]\", \"root_o\": \"['Understanding the Effectiveness of BERT']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 12, \"root_cspan0\": 0, \"root_cspan1\": 12, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'PROPN']\", \"apos_w\": \"['BERT', 'of', 'Effectiveness']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"Visualizing and Understanding the Effectiveness of BERT.\", \"split_tokens\": \"['Visualizing', 'and', 'Understanding', 'the', 'Effectiveness', 'of', 'BERT', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(50, 54)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.063962534593409, \"pc2\": -0.3585574329503214, \"word\": \"propose\", \"split_0\": \"We propose\", \"split_1\": \"a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation\", \"split_2\": \".\", \"averb\": \"propose\", \"averb_s\": \"['We']\", \"averb_o\": \"['a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['We']\", \"root_o\": \"['a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 11, \"root_cspan0\": 3, \"root_cspan1\": 11, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['method', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We propose a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation.\", \"split_tokens\": \"['We', 'propose', 'a', 'novel', 'data', 'augmentation', 'method', 'for', 'labeled', 'sentences', 'called', 'conditional', 'BERT', 'contextual', 'augmentation', '.']\", \"split_anchor_span\": \"(2, 15)\", \"split_anchor_indices\": \"(10, 112)\", \"within_anchor_index\": 74.0}, {\"pc1\": 2.632626693199032, \"pc2\": -7.675741760662864, \"word\": \"appeared\", \"split_0\": \"We retrofit BERT to conditional BERT by introducing a new conditional masked language model ( The term \\u201c conditional masked language model \\u201d appeared once in original\", \"split_1\": \"BERT\", \"split_2\": \"paper , which indicates context - conditional , is equivalent to term \\u201c masked language model \\u201d .\", \"averb\": \"appeared\", \"averb_s\": \"[]\", \"averb_o\": \"['once']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 141.0, \"averb_span1\": 150.0, \"averb_cspan0\": 141.0, \"averb_cspan1\": 150.0, \"root\": \"model\", \"root_full\": \"model\", \"root_s\": \"['\\u201c', 'masked']\", \"root_o\": \"['We retrofit BERT to conditional BERT by introducing a new conditional masked language model ( The term \\u201c conditional masked language model \\u201d appeared once in original BERT paper , term \\u201d .']\", \"root_split\": 2, \"root_span0\": 260, \"root_span1\": 266, \"root_cspan0\": 88, \"root_cspan1\": 94, \"fverb\": \"indicates\", \"fword\": \"paper\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'paper', 'in']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"We retrofit BERT to conditional BERT by introducing a new conditional masked language model (The term \\u201cconditional masked language model\\u201d appeared once in original BERT paper, which indicates context-conditional, is equivalent to term \\u201cmasked language model\\u201d.\", \"split_tokens\": \"['We', 'retrofit', 'BERT', 'to', 'conditional', 'BERT', 'by', 'introducing', 'a', 'new', 'conditional', 'masked', 'language', 'model', '(', 'The', 'term', '\\u201c', 'conditional', 'masked', 'language', 'model', '\\u201d', 'appeared', 'once', 'in', 'original', 'BERT', 'paper', ',', 'which', 'indicates', 'context', '-', 'conditional', ',', 'is', 'equivalent', 'to', 'term', '\\u201c', 'masked', 'language', 'model', '\\u201d', '.']\", \"split_anchor_span\": \"(27, 28)\", \"split_anchor_indices\": \"(166, 170)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.6595252901004933, \"pc2\": 1.2554691751614915, \"word\": \"can\", \"split_0\": \"The well trained conditional\", \"split_1\": \"BERT\", \"split_2\": \"can be applied to enhance contextual augmentation .\", \"averb\": \"can\", \"averb_s\": \"['The well trained conditional BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 34.0, \"averb_span1\": 38.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 4.0, \"root\": \"applied\", \"root_full\": \"can be applied\", \"root_s\": \"[]\", \"root_o\": \"['to enhance contextual augmentation']\", \"root_split\": 2, \"root_span0\": 34, \"root_span1\": 49, \"root_cspan0\": 0, \"root_cspan1\": 15, \"fverb\": \"be\", \"fword\": \"can\", \"apos\": \"['NOUN', 'ADJ', 'VERB']\", \"apos_w\": \"['BERT', 'conditional', 'can']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"The well trained conditional BERT can be applied to enhance contextual augmentation.\", \"split_tokens\": \"['The', 'well', 'trained', 'conditional', 'BERT', 'can', 'be', 'applied', 'to', 'enhance', 'contextual', 'augmentation', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(28, 32)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.702891334252849, \"pc2\": 1.7440597584246842, \"word\": \"show\", \"split_0\": \"Experiments on six various different text classification tasks show that\", \"split_1\": \"our method\", \"split_2\": \"can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement\", \"averb\": \"show\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 63.0, \"averb_span1\": 68.0, \"averb_cspan0\": 63.0, \"averb_cspan1\": 68.0, \"root\": \"tasks\", \"root_full\": \"tasks\", \"root_s\": \"['various different classification']\", \"root_o\": \"['Experiments on show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement']\", \"root_split\": 0, \"root_span0\": 57, \"root_span1\": 63, \"root_cspan0\": 57, \"root_cspan1\": 63, \"fverb\": \"be\", \"fword\": \"can\", \"apos\": \"['NOUN', 'ADV', 'VERB']\", \"apos_w\": \"['method', 'easily', 'show']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 8, \"Text\": \"Experiments on six various different text classification tasks show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement\", \"split_tokens\": \"['Experiments', 'on', 'six', 'various', 'different', 'text', 'classification', 'tasks', 'show', 'that', 'our', 'method', 'can', 'be', 'easily', 'applied', 'to', 'both', 'convolutional', 'or', 'recurrent', 'neural', 'networks', 'classifier', 'to', 'obtain', 'improvement']\", \"split_anchor_span\": \"(10, 12)\", \"split_anchor_indices\": \"(72, 82)\", \"within_anchor_index\": -1.0}, {\"pc1\": -2.9277679920553625, \"pc2\": -4.829801871515771, \"word\": \"Starting\", \"split_0\": \"Starting from a public multilingual\", \"split_1\": \"BERT\", \"split_2\": \"checkpoint , our final model is 6x smaller and 27x faster , and has higher accuracy than a state-of-the-art multilingual baseline .\", \"averb\": \"Starting\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 9.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"6x\", \"root_full\": \"is 6x\", \"root_s\": \"['Starting from a public multilingual BERT checkpoint , our final model', 'and']\", \"root_o\": \"['smaller', 'has higher accuracy than a state - of - the - art multilingual baseline .']\", \"root_split\": 2, \"root_span0\": 70, \"root_span1\": 76, \"root_cspan0\": 29, \"root_cspan1\": 35, \"fverb\": \"is\", \"fword\": \"checkpoint\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'checkpoint', 'from']\", \"URL\": \"https://www.semanticscholar.org/paper/Small-and-Practical-BERT-Models-for-Sequence-Tsai-Riesa/2f9d4887d0022400fc40c774c4c78350c3bc5390\", \"ID\": 26, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Starting from a public multilingual BERT checkpoint, our final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-the-art multilingual baseline.\", \"split_tokens\": \"['Starting', 'from', 'a', 'public', 'multilingual', 'BERT', 'checkpoint', ',', 'our', 'final', 'model', 'is', '6x', 'smaller', 'and', '27x', 'faster', ',', 'and', 'has', 'higher', 'accuracy', 'than', 'a', 'state-of-the-art', 'multilingual', 'baseline', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(35, 39)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.5407462381026673, \"pc2\": 0.21180975805562885, \"word\": \"using\", \"split_0\": \"Recently , a simple combination of passage retrieval using off-the-shelf IR techniques and a\", \"split_1\": \"BERT\", \"split_2\": \"reader was found to be very effective for question answering directly on Wikipedia , yielding a large improvement over the previous state of the art on a standard benchmark dataset .\", \"averb\": \"using\", \"averb_s\": \"[]\", \"averb_o\": \"['off', '- the - shelf IR techniques and a BERT reader']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 53.0, \"averb_span1\": 59.0, \"averb_cspan0\": 53.0, \"averb_cspan1\": 59.0, \"root\": \"found\", \"root_full\": \"was found\", \"root_s\": \"['a simple combination of passage retrieval using off - the - shelf IR techniques and a BERT reader', 'Recently ,']\", \"root_o\": \"['to be very effective for question answering directly on Wikipedia , yielding a large improvement over the previous state of the art on a standard benchmark dataset']\", \"root_split\": 2, \"root_span0\": 109, \"root_span1\": 119, \"root_cspan0\": 11, \"root_cspan1\": 21, \"fverb\": \"was\", \"fword\": \"reader\", \"apos\": \"['PROPN', 'NOUN', 'CCONJ']\", \"apos_w\": \"['BERT', 'reader', 'and']\", \"URL\": \"https://www.semanticscholar.org/paper/Data-Augmentation-for-BERT-Fine-Tuning-in-Question-Yang-Xie/f5eaf727b80240a13e9f631211c9ecec7e3b9feb\", \"ID\": 27, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset.\", \"split_tokens\": \"['Recently', ',', 'a', 'simple', 'combination', 'of', 'passage', 'retrieval', 'using', 'off-the-shelf', 'IR', 'techniques', 'and', 'a', 'BERT', 'reader', 'was', 'found', 'to', 'be', 'very', 'effective', 'for', 'question', 'answering', 'directly', 'on', 'Wikipedia', ',', 'yielding', 'a', 'large', 'improvement', 'over', 'the', 'previous', 'state', 'of', 'the', 'art', 'on', 'a', 'standard', 'benchmark', 'dataset', '.']\", \"split_anchor_span\": \"(14, 15)\", \"split_anchor_indices\": \"(92, 96)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.1962757101852155, \"pc2\": 0.05577969647185372, \"word\": \"argue\", \"split_0\": \"We take issue with this interpretation and argue that the performance of\", \"split_1\": \"BERT\", \"split_2\": \"is partly due to reasoning about ( the surface form of ) entity names , e.g. , guessing that a person with an Italian - sounding name speaks Italian .\", \"averb\": \"argue\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 43.0, \"averb_span1\": 49.0, \"averb_cspan0\": 43.0, \"averb_cspan1\": 49.0, \"root\": \"issue\", \"root_full\": \"issue\", \"root_s\": \"['take']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 8, \"root_span1\": 14, \"root_cspan0\": 8, \"root_cspan1\": 14, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'performance']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.\", \"split_tokens\": \"['We', 'take', 'issue', 'with', 'this', 'interpretation', 'and', 'argue', 'that', 'the', 'performance', 'of', 'BERT', 'is', 'partly', 'due', 'to', 'reasoning', 'about', '(', 'the', 'surface', 'form', 'of', ')', 'entity', 'names', ',', 'e.g.', ',', 'guessing', 'that', 'a', 'person', 'with', 'an', 'Italian', '-', 'sounding', 'name', 'speaks', 'Italian', '.']\", \"split_anchor_span\": \"(12, 13)\", \"split_anchor_indices\": \"(72, 76)\", \"within_anchor_index\": 0.0}, {\"pc1\": 1.828760300175955, \"pc2\": -1.0087779123074394, \"word\": \"drops\", \"split_0\": \"More specifically , we show that\", \"split_1\": \"BERT 's\", \"split_2\": \"precision drops dramatically when we filter certain easy - to - guess facts .\", \"averb\": \"drops\", \"averb_s\": \"[\\\"BERT 's precision\\\"]\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 51.0, \"averb_span1\": 57.0, \"averb_cspan0\": 10.0, \"averb_cspan1\": 16.0, \"root\": \"guess\", \"root_full\": \"guess\", \"root_s\": \"['when we filter certain easy -', 'to', '-']\", \"root_o\": \"[\\\"More specifically , we show that BERT 's precision drops dramatically facts\\\", '.']\", \"root_split\": 2, \"root_span0\": 105, \"root_span1\": 111, \"root_cspan0\": 64, \"root_cspan1\": 70, \"fverb\": \"filter\", \"fword\": \"precision\", \"apos\": \"['PROPN', 'NOUN', 'VERB']\", \"apos_w\": \"['BERT', 'precision', 'drops']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"More specifically, we show that BERT's precision drops dramatically when we filter certain easy-to-guess facts.\", \"split_tokens\": \"['More', 'specifically', ',', 'we', 'show', 'that', 'BERT', \\\"'s\\\", 'precision', 'drops', 'dramatically', 'when', 'we', 'filter', 'certain', 'easy', '-', 'to', '-', 'guess', 'facts', '.']\", \"split_anchor_span\": \"(6, 8)\", \"split_anchor_indices\": \"(32, 39)\", \"within_anchor_index\": 0.0}, {\"pc1\": -2.760479209228633, \"pc2\": -0.4950684747213763, \"word\": \"propose\", \"split_0\": \"As a remedy , we propose E - BERT , an extension of\", \"split_1\": \"BERT\", \"split_2\": \"that replaces entity mentions with symbolic entity embeddings .\", \"averb\": \"propose\", \"averb_s\": \"['we', 'As a remedy']\", \"averb_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 17.0, \"averb_span1\": 25.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 25.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we', 'As a remedy']\", \"root_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"root_split\": 0, \"root_span0\": 17, \"root_span1\": 25, \"root_cspan0\": 17, \"root_cspan1\": 25, \"fverb\": \"replaces\", \"fword\": \"that\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'extension']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings.\", \"split_tokens\": \"['As', 'a', 'remedy', ',', 'we', 'propose', 'E', '-', 'BERT', ',', 'an', 'extension', 'of', 'BERT', 'that', 'replaces', 'entity', 'mentions', 'with', 'symbolic', 'entity', 'embeddings', '.']\", \"split_anchor_span\": \"(13, 14)\", \"split_anchor_indices\": \"(51, 55)\", \"within_anchor_index\": 0.0}, {\"pc1\": -2.760479209228633, \"pc2\": -0.49506847472137644, \"word\": \"propose\", \"split_0\": \"As a remedy , we propose\", \"split_1\": \"E - BERT , an extension of BERT that replaces entity mentions with symbolic entity embeddings\", \"split_2\": \".\", \"averb\": \"propose\", \"averb_s\": \"['we', 'As a remedy']\", \"averb_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 17.0, \"averb_span1\": 25.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 25.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we', 'As a remedy']\", \"root_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"root_split\": 0, \"root_span0\": 17, \"root_span1\": 25, \"root_cspan0\": 17, \"root_cspan1\": 25, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['E', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings.\", \"split_tokens\": \"['As', 'a', 'remedy', ',', 'we', 'propose', 'E', '-', 'BERT', ',', 'an', 'extension', 'of', 'BERT', 'that', 'replaces', 'entity', 'mentions', 'with', 'symbolic', 'entity', 'embeddings', '.']\", \"split_anchor_span\": \"(6, 22)\", \"split_anchor_indices\": \"(24, 117)\", \"within_anchor_index\": 4.0}, {\"pc1\": -3.3852686053806744, \"pc2\": 1.4856855912375249, \"word\": \"show\", \"split_0\": \"We take this as evidence that E - BERT is richer in factual knowledge , and we show two ways of ensembling\", \"split_1\": \"BERT\", \"split_2\": \"and E - BERT\", \"averb\": \"show\", \"averb_s\": \"['we']\", \"averb_o\": \"['two ways of ensembling BERT and E - BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 79.0, \"averb_span1\": 84.0, \"averb_cspan0\": 79.0, \"averb_cspan1\": 84.0, \"root\": \"take\", \"root_full\": \"take\", \"root_s\": \"['We']\", \"root_o\": \"['this']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": null, \"fword\": \"and\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'ways']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT\", \"split_tokens\": \"['We', 'take', 'this', 'as', 'evidence', 'that', 'E', '-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E', '-', 'BERT']\", \"split_anchor_span\": \"(22, 23)\", \"split_anchor_indices\": \"(106, 110)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.334012525673737, \"pc2\": 0.21780642478593729, \"word\": \"take\", \"split_0\": \"We take this as evidence that\", \"split_1\": \"E - BERT\", \"split_2\": \"is richer in factual knowledge , and we show two ways of ensembling BERT and E - BERT\", \"averb\": \"take\", \"averb_s\": \"['We']\", \"averb_o\": \"['this']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 8.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 8.0, \"root\": \"take\", \"root_full\": \"take\", \"root_s\": \"['We']\", \"root_o\": \"['this']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['ADJ', 'NOUN', 'SCONJ']\", \"apos_w\": \"['richer', 'evidence', 'as']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT\", \"split_tokens\": \"['We', 'take', 'this', 'as', 'evidence', 'that', 'E', '-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E', '-', 'BERT']\", \"split_anchor_span\": \"(6, 9)\", \"split_anchor_indices\": \"(29, 37)\", \"within_anchor_index\": 4.0}, {\"pc1\": -3.3852686053806744, \"pc2\": 1.4856855912375255, \"word\": \"show\", \"split_0\": \"We take this as evidence that E - BERT is richer in factual knowledge , and we show two ways of ensembling BERT and\", \"split_1\": \"E - BERT\", \"split_2\": null, \"averb\": \"show\", \"averb_s\": \"['we']\", \"averb_o\": \"['two ways of ensembling BERT and E - BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 79.0, \"averb_span1\": 84.0, \"averb_cspan0\": 79.0, \"averb_cspan1\": 84.0, \"root\": \"take\", \"root_full\": \"take\", \"root_s\": \"['We']\", \"root_o\": \"['this']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'BERT', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT\", \"split_tokens\": \"['We', 'take', 'this', 'as', 'evidence', 'that', 'E', '-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E', '-', 'BERT']\", \"split_anchor_span\": \"(24, 27)\", \"split_anchor_indices\": \"(115, 123)\", \"within_anchor_index\": 4.0}, {\"pc1\": 2.2627785273336944, \"pc2\": -4.97194207294492, \"word\": \"is\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"is Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA .\", \"averb\": \"is\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name - Based Reasoning in Unsupervised QA']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 5.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"is\", \"root_full\": \"is\", \"root_s\": \"['BERT']\", \"root_o\": \"['Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name - Based Reasoning in Unsupervised QA']\", \"root_split\": 2, \"root_span0\": 5, \"root_span1\": 8, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['BERT', 'is']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA.\", \"split_tokens\": \"['BERT', 'is', 'Not', 'a', 'Knowledge', 'Base', '(', 'Yet', ')', ':', 'Factual', 'Knowledge', 'vs.', 'Name-Based', 'Reasoning', 'in', 'Unsupervised', 'QA', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": 1.4668292845517805, \"pc2\": -6.356685661209306, \"word\": \"produced\", \"split_0\": \"However , just how contextual are the contextualized representations produced by models such as ELMo and\", \"split_1\": \"BERT\", \"split_2\": \"?\", \"averb\": \"produced\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 69.0, \"averb_span1\": 78.0, \"averb_cspan0\": 69.0, \"averb_cspan1\": 78.0, \"root\": \"representations\", \"root_full\": \"are representations\", \"root_s\": \"[]\", \"root_o\": \"['produced by models such as ELMo and BERT ?']\", \"root_split\": 0, \"root_span0\": 30, \"root_span1\": 69, \"root_cspan0\": 30, \"root_cspan1\": 69, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'NOUN', 'SCONJ']\", \"apos_w\": \"['BERT', 'ELMo', 'as']\", \"URL\": \"https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39\", \"ID\": 29, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"However, just how contextual are the contextualized representations produced by models such as ELMo and BERT?\", \"split_tokens\": \"['However', ',', 'just', 'how', 'contextual', 'are', 'the', 'contextualized', 'representations', 'produced', 'by', 'models', 'such', 'as', 'ELMo', 'and', 'BERT', '?']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(104, 108)\", \"within_anchor_index\": 0.0}, {\"pc1\": 13.171205757064229, \"pc2\": -0.887217704215734, \"word\": \"be explained\", \"split_0\": \"In all layers of ELMo ,\", \"split_1\": \"BERT\", \"split_2\": \", and GPT-2 , on average , less than 5 % of the variance in a word 's contextualized representations can be explained by a static embedding for that word , providing some justification for the success of contextualized representations\", \"averb\": \"be explained\", \"averb_s\": \"['can']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 134.0, \"averb_span1\": 147.0, \"averb_cspan0\": 105.0, \"averb_cspan1\": 118.0, \"root\": \"explained\", \"root_full\": \"be explained\", \"root_s\": \"['can']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 134, \"root_span1\": 147, \"root_cspan0\": 105, \"root_cspan1\": 118, \"fverb\": \"be\", \"fword\": \"and\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'layers']\", \"URL\": \"https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39\", \"ID\": 29, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations\", \"split_tokens\": \"['In', 'all', 'layers', 'of', 'ELMo', ',', 'BERT', ',', 'and', 'GPT-2', ',', 'on', 'average', ',', 'less', 'than', '5', '%', 'of', 'the', 'variance', 'in', 'a', 'word', \\\"'s\\\", 'contextualized', 'representations', 'can', 'be', 'explained', 'by', 'a', 'static', 'embedding', 'for', 'that', 'word', ',', 'providing', 'some', 'justification', 'for', 'the', 'success', 'of', 'contextualized', 'representations']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(23, 27)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.832834128734779, \"pc2\": 0.7323357947741822, \"word\": \"Comparing\", \"split_0\": \"Comparing the Geometry of\", \"split_1\": \"BERT\", \"split_2\": \", ELMo , and GPT-2 Embeddings .\", \"averb\": \"Comparing\", \"averb_s\": \"[]\", \"averb_o\": \"['the', 'Geometry of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 10.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \",\", \"root_full\": \",\", \"root_s\": \"[]\", \"root_o\": \"['Comparing the Geometry of BERT ELMo , and GPT-2 Embeddings .']\", \"root_split\": 2, \"root_span0\": 31, \"root_span1\": 33, \"root_cspan0\": 0, \"root_cspan1\": 2, \"fverb\": null, \"fword\": \"ELMo\", \"apos\": \"['PROPN', 'ADP', 'PROPN']\", \"apos_w\": \"['BERT', 'of', 'Geometry']\", \"URL\": \"https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39\", \"ID\": 29, \"Type\": \"Title\", \"Index\": 1, \"Text\": \"Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.\", \"split_tokens\": \"['Comparing', 'the', 'Geometry', 'of', 'BERT', ',', 'ELMo', ',', 'and', 'GPT-2', 'Embeddings', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(25, 29)\", \"within_anchor_index\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_embeds(output_elmo, csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-1e6c847130f74ce88e994c203848b07c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1e6c847130f74ce88e994c203848b07c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1e6c847130f74ce88e994c203848b07c\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-dfe5566f35b0f9e535abf5fd3784468d\"}, \"mark\": {\"type\": \"circle\", \"size\": 60}, \"encoding\": {\"tooltip\": [{\"type\": \"nominal\", \"field\": \"word\"}, {\"type\": \"nominal\", \"field\": \"Text\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"pc1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"pc2\"}}, \"selection\": {\"selector002\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-dfe5566f35b0f9e535abf5fd3784468d\": [{\"pc1\": -5.299738388031692, \"pc2\": 2.620377729967869, \"word\": \"stands\", \"split_0\": \"We introduce\", \"split_1\": \"a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers\", \"split_2\": \".\", \"averb\": \"stands\", \"averb_s\": \"['which', 'We introduce a new language representation model called BERT ,']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 69.0, \"averb_span1\": 76.0, \"averb_cspan0\": 56.0, \"averb_cspan1\": 63.0, \"root\": \"stands\", \"root_full\": \"stands\", \"root_s\": \"['which', 'We introduce a new language representation model called BERT ,']\", \"root_o\": \"[]\", \"root_split\": 1, \"root_span0\": 69, \"root_span1\": 76, \"root_cspan0\": 56, \"root_cspan1\": 63, \"fverb\": null, \"fword\": null, \"apos\": \"['VERB']\", \"apos_w\": \"['stands']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\", \"split_tokens\": \"['We', 'introduce', 'a', 'new', 'language', 'representation', 'model', 'called', 'BERT', ',', 'which', 'stands', 'for', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', '.']\", \"split_anchor_span\": \"(2, 18)\", \"split_anchor_indices\": \"(12, 134)\", \"within_anchor_index\": 43.0}, {\"pc1\": 0.22489484142766722, \"pc2\": -0.9359176648066022, \"word\": \"is designed\", \"split_0\": \"Unlike recent language representation models ,\", \"split_1\": \"BERT\", \"split_2\": \"is designed to pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .\", \"averb\": \"is designed\", \"averb_s\": \"['recent language representation models , BERT']\", \"averb_o\": \"['pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 52.0, \"averb_span1\": 64.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"designed\", \"root_full\": \"is designed\", \"root_s\": \"['recent language representation models , BERT']\", \"root_o\": \"['pre - train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .']\", \"root_split\": 2, \"root_span0\": 52, \"root_span1\": 64, \"root_cspan0\": 0, \"root_cspan1\": 12, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'PUNCT', 'NOUN']\", \"apos_w\": \"['BERT', ',', 'models']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\", \"split_tokens\": \"['Unlike', 'recent', 'language', 'representation', 'models', ',', 'BERT', 'is', 'designed', 'to', 'pre', '-', 'train', 'deep', 'bidirectional', 'representations', 'from', 'unlabeled', 'text', 'by', 'jointly', 'conditioning', 'on', 'both', 'left', 'and', 'right', 'context', 'in', 'all', 'layers', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(46, 50)\", \"within_anchor_index\": 0.0}, {\"pc1\": -9.853169984050693, \"pc2\": -0.16860390115907894, \"word\": \"is\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"is conceptually simple and empirically powerful .\", \"averb\": \"is\", \"averb_s\": \"['BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 5.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"conceptually\", \"root_full\": \"is conceptually\", \"root_s\": \"[]\", \"root_o\": \"['simple empirically', 'powerful']\", \"root_split\": 2, \"root_span0\": 5, \"root_span1\": 21, \"root_cspan0\": 0, \"root_cspan1\": 16, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX', 'ADV']\", \"apos_w\": \"['BERT', 'is', 'conceptually']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"BERT is conceptually simple and empirically powerful.\", \"split_tokens\": \"['BERT', 'is', 'conceptually', 'simple', 'and', 'empirically', 'powerful', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": 26.922318810975437, \"pc2\": 29.13026669555135, \"word\": \"obtains\", \"split_0\": null, \"split_1\": \"It\", \"split_2\": \"obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQuAD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement )\", \"averb\": \"obtains\", \"averb_s\": \"[]\", \"averb_o\": \"['new state - - the -', 'of art', 'results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % % absolute improvement']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"point\", \"root_full\": \"point\", \"root_s\": \"['It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test 93.2 ( 1.5 and SQuAD v2.0 Test F1 to 83.1 ( 5.1']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 353, \"root_span1\": 359, \"root_cspan0\": 350, \"root_cspan1\": 356, \"fverb\": \"obtains\", \"fword\": \"obtains\", \"apos\": \"['PRON', 'VERB', 'NOUN']\", \"apos_w\": \"['It', 'obtains', 'question']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"ID\": 0, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)\", \"split_tokens\": \"['It', 'obtains', 'new', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'eleven', 'natural', 'language', 'processing', 'tasks', ',', 'including', 'pushing', 'the', 'GLUE', 'score', 'to', '80.5', '%', '(', '7.7', '%', 'point', 'absolute', 'improvement', ')', ',', 'MultiNLI', 'accuracy', 'to', '86.7', '%', '(', '4.6', '%', 'absolute', 'improvement', ')', ',', 'SQuAD', 'v1.1', 'question', 'answering', 'Test', 'F1', 'to', '93.2', '(', '1.5', 'point', 'absolute', 'improvement', ')', 'and', 'SQuAD', 'v2.0', 'Test', 'F1', 'to', '83.1', '(', '5.1', 'point', 'absolute', 'improvement', ')']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 2)\", \"within_anchor_index\": -1.0}, {\"pc1\": 2.0836584268338822, \"pc2\": 0.8795780674811463, \"word\": \"present\", \"split_0\": \"We present a replication study of\", \"split_1\": \"BERT\", \"split_2\": \"pretraining ( Devlin et al . , 2019 ) that carefully measures the impact of many key hyperparameters and training data size .\", \"averb\": \"present\", \"averb_s\": \"['We']\", \"averb_o\": \"['a replication study of BERT pretraining ( Devlin et al . , 2019 )']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"measures\", \"root_full\": \"measures\", \"root_s\": \"['that']\", \"root_o\": \"['the impact of many key hyperparameters and training data size .']\", \"root_split\": 2, \"root_span0\": 92, \"root_span1\": 101, \"root_cspan0\": 53, \"root_cspan1\": 62, \"fverb\": \"measures\", \"fword\": \"pretraining\", \"apos\": \"['PROPN', 'PROPN', 'ADP']\", \"apos_w\": \"['BERT', 'et', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"ID\": 1, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size.\", \"split_tokens\": \"['We', 'present', 'a', 'replication', 'study', 'of', 'BERT', 'pretraining', '(', 'Devlin', 'et', 'al', '.', ',', '2019', ')', 'that', 'carefully', 'measures', 'the', 'impact', 'of', 'many', 'key', 'hyperparameters', 'and', 'training', 'data', 'size', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(33, 37)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.950216792404015, \"pc2\": -2.5397264240083572, \"word\": \"find\", \"split_0\": \"We find that\", \"split_1\": \"BERT\", \"split_2\": \"was significantly undertrained , and can match or exceed the performance of every model published after it .\", \"averb\": \"find\", \"averb_s\": \"[]\", \"averb_o\": \"['that BERT was significantly undertrained , and can match or exceed the performance of every model published after it .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 8.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 8.0, \"root\": \"We\", \"root_full\": \"We\", \"root_s\": \"[]\", \"root_o\": \"['find that BERT was significantly undertrained , and can match or exceed the performance of every model published after it .']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"was\", \"fword\": \"was\", \"apos\": \"['PROPN', 'ADV', 'VERB']\", \"apos_w\": \"['BERT', 'significantly', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"ID\": 1, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.\", \"split_tokens\": \"['We', 'find', 'that', 'BERT', 'was', 'significantly', 'undertrained', ',', 'and', 'can', 'match', 'or', 'exceed', 'the', 'performance', 'of', 'every', 'model', 'published', 'after', 'it', '.']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(12, 16)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.129195580317763, \"pc2\": -2.804293524627986, \"word\": \"published\", \"split_0\": \"We find that BERT was significantly undertrained , and can match or exceed the performance of every model published after\", \"split_1\": \"it\", \"split_2\": \".\", \"averb\": \"published\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 106.0, \"averb_span1\": 116.0, \"averb_cspan0\": 106.0, \"averb_cspan1\": 116.0, \"root\": \"We\", \"root_full\": \"We\", \"root_s\": \"[]\", \"root_o\": \"['find that BERT was significantly undertrained , and can match or exceed the performance of every model published after it .']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": null, \"fword\": null, \"apos\": \"['PRON', 'ADP', 'VERB']\", \"apos_w\": \"['it', 'after', 'published']\", \"URL\": \"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"ID\": 1, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.\", \"split_tokens\": \"['We', 'find', 'that', 'BERT', 'was', 'significantly', 'undertrained', ',', 'and', 'can', 'match', 'or', 'exceed', 'the', 'performance', 'of', 'every', 'model', 'published', 'after', 'it', '.']\", \"split_anchor_span\": \"(20, 21)\", \"split_anchor_indices\": \"(121, 123)\", \"within_anchor_index\": -1.0}, {\"pc1\": 1.7730091490158746, \"pc2\": -0.47849771230210575, \"word\": \"called\", \"split_0\": \"In this work , we propose a method to pre-train a smaller general-purpose language representation model , called\", \"split_1\": \"DistilBERT\", \"split_2\": \", which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts .\", \"averb\": \"called\", \"averb_s\": \"[]\", \"averb_o\": \"['DistilBERT', ',']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 110.0, \"averb_span1\": 117.0, \"averb_cspan0\": 110.0, \"averb_cspan1\": 117.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['a method to pre - train a smaller general - purpose language representation model , called DistilBERT ,', 'which can then be fine - tuned with good performances on a wide range of tasks like its larger counterparts .']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 26, \"root_cspan0\": 18, \"root_cspan1\": 26, \"fverb\": \"be\", \"fword\": \"which\", \"apos\": \"['NUM', 'VERB', 'NOUN']\", \"apos_w\": \"['DistilBERT', 'called', 'model']\", \"URL\": \"https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38\", \"ID\": 2, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts.\", \"split_tokens\": \"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'method', 'to', 'pre-train', 'a', 'smaller', 'general-purpose', 'language', 'representation', 'model', ',', 'called', 'DistilBERT', ',', 'which', 'can', 'then', 'be', 'fine-tuned', 'with', 'good', 'performances', 'on', 'a', 'wide', 'range', 'of', 'tasks', 'like', 'its', 'larger', 'counterparts', '.']\", \"split_anchor_span\": \"(18, 19)\", \"split_anchor_indices\": \"(112, 122)\", \"within_anchor_index\": 6.0}, {\"pc1\": 14.255524263651132, \"pc2\": -3.920595694971085, \"word\": \"to reduce\", \"split_0\": \"While most prior work investigated the use of distillation for building task - specific models , we leverage knowledge distillation during the pre - training phase and show that it is possible to reduce the size of\", \"split_1\": \"a BERT model\", \"split_2\": \"by 40 % , while retaining 97 % of its language understanding capabilities and being 60 % faster .\", \"averb\": \"to reduce\", \"averb_s\": \"[]\", \"averb_o\": \"['we leverage knowledge distillation during phase', 'the size of a BERT model']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 193.0, \"averb_span1\": 203.0, \"averb_cspan0\": 193.0, \"averb_cspan1\": 203.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 168, \"root_span1\": 173, \"root_cspan0\": 168, \"root_cspan1\": 173, \"fverb\": \"retaining\", \"fword\": \"by\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['model', 'of', 'size']\", \"URL\": \"https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38\", \"ID\": 2, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.\", \"split_tokens\": \"['While', 'most', 'prior', 'work', 'investigated', 'the', 'use', 'of', 'distillation', 'for', 'building', 'task', '-', 'specific', 'models', ',', 'we', 'leverage', 'knowledge', 'distillation', 'during', 'the', 'pre', '-', 'training', 'phase', 'and', 'show', 'that', 'it', 'is', 'possible', 'to', 'reduce', 'the', 'size', 'of', 'a', 'BERT', 'model', 'by', '40', '%', ',', 'while', 'retaining', '97', '%', 'of', 'its', 'language', 'understanding', 'capabilities', 'and', 'being', '60', '%', 'faster', '.']\", \"split_anchor_span\": \"(37, 40)\", \"split_anchor_indices\": \"(214, 226)\", \"within_anchor_index\": 2.0}, {\"pc1\": 14.610120261296077, \"pc2\": -4.107909415618609, \"word\": \"retaining\", \"split_0\": \"While most prior work investigated the use of distillation for building task - specific models , we leverage knowledge distillation during the pre - training phase and show that it is possible to reduce the size of a BERT model by 40 % , while retaining 97 % of\", \"split_1\": \"its\", \"split_2\": \"language understanding capabilities and being 60 % faster .\", \"averb\": \"retaining\", \"averb_s\": \"[]\", \"averb_o\": \"['97 % of its language understanding capabilities and being 60 % faster .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 244.0, \"averb_span1\": 254.0, \"averb_cspan0\": 244.0, \"averb_cspan1\": 254.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 168, \"root_span1\": 173, \"root_cspan0\": 168, \"root_cspan1\": 173, \"fverb\": \"being\", \"fword\": \"language\", \"apos\": \"['DET', 'NOUN', 'ADP']\", \"apos_w\": \"['its', 'capabilities', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/DistilBERT%2C-a-distilled-version-of-BERT%3A-smaller%2C-Sanh-Debut/9df8a296b4f414dfea1997d6662b812c9d626c38\", \"ID\": 2, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.\", \"split_tokens\": \"['While', 'most', 'prior', 'work', 'investigated', 'the', 'use', 'of', 'distillation', 'for', 'building', 'task', '-', 'specific', 'models', ',', 'we', 'leverage', 'knowledge', 'distillation', 'during', 'the', 'pre', '-', 'training', 'phase', 'and', 'show', 'that', 'it', 'is', 'possible', 'to', 'reduce', 'the', 'size', 'of', 'a', 'BERT', 'model', 'by', '40', '%', ',', 'while', 'retaining', '97', '%', 'of', 'its', 'language', 'understanding', 'capabilities', 'and', 'being', '60', '%', 'faster', '.']\", \"split_anchor_span\": \"(49, 50)\", \"split_anchor_indices\": \"(261, 264)\", \"within_anchor_index\": -1.0}, {\"pc1\": -5.027150955422625, \"pc2\": -0.33887001436370306, \"word\": \"focus\", \"split_0\": \"We focus on\", \"split_1\": \"one such model , BERT\", \"split_2\": \", and aim to quantify where linguistic information is captured within the network .\", \"averb\": \"focus\", \"averb_s\": \"['We']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 9.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 9.0, \"root\": \"captured\", \"root_full\": \"is captured\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 85, \"root_span1\": 97, \"root_cspan0\": 51, \"root_cspan1\": 63, \"fverb\": \"quantify\", \"fword\": \"and\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['model', 'on', 'focus']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c\", \"ID\": 3, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network.\", \"split_tokens\": \"['We', 'focus', 'on', 'one', 'such', 'model', ',', 'BERT', ',', 'and', 'aim', 'to', 'quantify', 'where', 'linguistic', 'information', 'is', 'captured', 'within', 'the', 'network', '.']\", \"split_anchor_span\": \"(3, 8)\", \"split_anchor_indices\": \"(11, 32)\", \"within_anchor_index\": 17.0}, {\"pc1\": 4.506713121087028, \"pc2\": -1.0311560760743077, \"word\": \"represents\", \"split_0\": \"We find that\", \"split_1\": \"the model\", \"split_2\": \"represents the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence : POS tagging , parsing , NER , semantic roles , then coreference .\", \"averb\": \"represents\", \"averb_s\": \"['the model']\", \"averb_o\": \"['the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence :']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 23.0, \"averb_span1\": 34.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 11.0, \"root\": \"roles\", \"root_full\": \"roles\", \"root_s\": \"['POS tagging , parsing ,', 'NER', ',', 'semantic']\", \"root_o\": \"[',', 'We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way , and that the regions responsible for each step appear in the expected sequence : then coreference .']\", \"root_split\": 2, \"root_span0\": 238, \"root_span1\": 244, \"root_cspan0\": 215, \"root_cspan1\": 221, \"fverb\": \"represents\", \"fword\": \"represents\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['model', 'represents', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c\", \"ID\": 3, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference.\", \"split_tokens\": \"['We', 'find', 'that', 'the', 'model', 'represents', 'the', 'steps', 'of', 'the', 'traditional', 'NLP', 'pipeline', 'in', 'an', 'interpretable', 'and', 'localizable', 'way', ',', 'and', 'that', 'the', 'regions', 'responsible', 'for', 'each', 'step', 'appear', 'in', 'the', 'expected', 'sequence', ':', 'POS', 'tagging', ',', 'parsing', ',', 'NER', ',', 'semantic', 'roles', ',', 'then', 'coreference', '.']\", \"split_anchor_span\": \"(3, 5)\", \"split_anchor_indices\": \"(12, 21)\", \"within_anchor_index\": -1.0}, {\"pc1\": 0.7762914604874503, \"pc2\": -2.960310234130194, \"word\": \"does adjust\", \"split_0\": \"Qualitative analysis reveals that\", \"split_1\": \"the model\", \"split_2\": \"can and often does adjust this pipeline dynamically , revising lower - level decisions on the basis of disambiguating information from higher - level representations .\", \"averb\": \"does adjust\", \"averb_s\": \"['the model can and often']\", \"averb_o\": \"['this pipeline dynamically , revising lower - level decisions on the basis of disambiguating information from higher - level representations .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 58.0, \"averb_span1\": 70.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 26.0, \"root\": \"analysis\", \"root_full\": \"analysis\", \"root_s\": \"[]\", \"root_o\": \"['reveals that the model can and often does adjust this pipeline dynamically , revising lower - level decisions on the basis of disambiguating information from higher - level representations .']\", \"root_split\": 0, \"root_span0\": 12, \"root_span1\": 21, \"root_cspan0\": 12, \"root_cspan1\": 21, \"fverb\": \"does\", \"fword\": \"can\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['model', 'adjust', 'reveals']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Rediscovers-the-Classical-NLP-Pipeline-Tenney-Das/97906df07855b029b7aae7c2a1c6c5e8df1d531c\", \"ID\": 3, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.\", \"split_tokens\": \"['Qualitative', 'analysis', 'reveals', 'that', 'the', 'model', 'can', 'and', 'often', 'does', 'adjust', 'this', 'pipeline', 'dynamically', ',', 'revising', 'lower', '-', 'level', 'decisions', 'on', 'the', 'basis', 'of', 'disambiguating', 'information', 'from', 'higher', '-', 'level', 'representations', '.']\", \"split_anchor_span\": \"(4, 6)\", \"split_anchor_indices\": \"(33, 42)\", \"within_anchor_index\": -1.0}, {\"pc1\": 0.7061734383454388, \"pc2\": -2.8815318677195263, \"word\": \"have had\", \"split_0\": null, \"split_1\": \"Large pre - trained neural networks such as BERT\", \"split_2\": \"have had great recent success in NLP , motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data .\", \"averb\": \"have had\", \"averb_s\": \"['Large pre - trained neural networks such as BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 49.0, \"averb_span1\": 58.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"success\", \"root_full\": \"success\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 22, \"root_cspan1\": 30, \"fverb\": \"have\", \"fword\": \"have\", \"apos\": \"['ADJ', 'VERB', 'ADJ']\", \"apos_w\": \"['Large', 'had', 'recent']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.\", \"split_tokens\": \"['Large', 'pre', '-', 'trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']\", \"split_anchor_span\": \"(0, 9)\", \"split_anchor_indices\": \"(0, 48)\", \"within_anchor_index\": 44.0}, {\"pc1\": 1.0746085223076907, \"pc2\": -2.9018151977256674, \"word\": \"motivating\", \"split_0\": \"Large pre - trained neural networks such as BERT have had great recent success in NLP , motivating a growing body of research investigating what aspects of language\", \"split_1\": \"they\", \"split_2\": \"are able to learn from unlabeled data .\", \"averb\": \"motivating\", \"averb_s\": \"[',']\", \"averb_o\": \"['a growing body of research investigating what']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 88.0, \"averb_span1\": 99.0, \"averb_cspan0\": 88.0, \"averb_cspan1\": 99.0, \"root\": \"success\", \"root_full\": \"success\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 71, \"root_cspan1\": 79, \"fverb\": \"are\", \"fword\": \"are\", \"apos\": \"['PRON', 'ADJ', 'VERB']\", \"apos_w\": \"['they', 'able', 'motivating']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.\", \"split_tokens\": \"['Large', 'pre', '-', 'trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']\", \"split_anchor_span\": \"(28, 29)\", \"split_anchor_indices\": \"(164, 168)\", \"within_anchor_index\": -1.0}, {\"pc1\": 0.7061734383454384, \"pc2\": -2.881531867719527, \"word\": \"have had\", \"split_0\": \"Large pre - trained neural networks such as\", \"split_1\": \"BERT\", \"split_2\": \"have had great recent success in NLP , motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data .\", \"averb\": \"have had\", \"averb_s\": \"['Large pre - trained neural networks such as BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 49.0, \"averb_span1\": 58.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"success\", \"root_full\": \"success\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 22, \"root_cspan1\": 30, \"fverb\": \"have\", \"fword\": \"have\", \"apos\": \"['PROPN', 'SCONJ', 'NOUN']\", \"apos_w\": \"['BERT', 'as', 'networks']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data.\", \"split_tokens\": \"['Large', 'pre', '-', 'trained', 'neural', 'networks', 'such', 'as', 'BERT', 'have', 'had', 'great', 'recent', 'success', 'in', 'NLP', ',', 'motivating', 'a', 'growing', 'body', 'of', 'research', 'investigating', 'what', 'aspects', 'of', 'language', 'they', 'are', 'able', 'to', 'learn', 'from', 'unlabeled', 'data', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(43, 47)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.562536220130245, \"pc2\": -2.9797846473384237, \"word\": \"apply\", \"split_0\": \"Complementary to these works , we propose methods for analyzing the attention mechanisms of pre - trained models and apply them to\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"apply\", \"averb_s\": \"[]\", \"averb_o\": \"['them']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 117.0, \"averb_span1\": 123.0, \"averb_cspan0\": 117.0, \"averb_cspan1\": 123.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['Complementary to these works', ',', 'we']\", \"root_o\": \"['methods for analyzing the attention mechanisms of pre - trained models and apply them to BERT']\", \"root_split\": 0, \"root_span0\": 34, \"root_span1\": 42, \"root_cspan0\": 34, \"root_cspan1\": 42, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'to', 'apply']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT.\", \"split_tokens\": \"['Complementary', 'to', 'these', 'works', ',', 'we', 'propose', 'methods', 'for', 'analyzing', 'the', 'attention', 'mechanisms', 'of', 'pre', '-', 'trained', 'models', 'and', 'apply', 'them', 'to', 'BERT', '.']\", \"split_anchor_span\": \"(22, 23)\", \"split_anchor_indices\": \"(130, 134)\", \"within_anchor_index\": 0.0}, {\"pc1\": -2.6749815671521864, \"pc2\": -1.9966049109411594, \"word\": \"is captured\", \"split_0\": \"Lastly , we propose an attention - based probing classifier and use it to further demonstrate that substantial syntactic information is captured in\", \"split_1\": \"BERT 's\", \"split_2\": \"attention .\", \"averb\": \"is captured\", \"averb_s\": \"['substantial syntactic information']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 133.0, \"averb_span1\": 145.0, \"averb_cspan0\": 133.0, \"averb_cspan1\": 145.0, \"root\": \"Lastly\", \"root_full\": \"Lastly\", \"root_s\": \"[]\", \"root_o\": \"[\\\"we propose an attention - based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT 's attention .\\\"]\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 7, \"root_cspan0\": 0, \"root_cspan1\": 7, \"fverb\": null, \"fword\": \"attention\", \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'in', 'captured']\", \"URL\": \"https://www.semanticscholar.org/paper/What-Does-BERT-Look-At-An-Analysis-of-BERT's-Clark-Khandelwal/95a251513853c6032bdecebd4b74e15795662986\", \"ID\": 4, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.\", \"split_tokens\": \"['Lastly', ',', 'we', 'propose', 'an', 'attention', '-', 'based', 'probing', 'classifier', 'and', 'use', 'it', 'to', 'further', 'demonstrate', 'that', 'substantial', 'syntactic', 'information', 'is', 'captured', 'in', 'BERT', \\\"'s\\\", 'attention', '.']\", \"split_anchor_span\": \"(23, 25)\", \"split_anchor_indices\": \"(147, 154)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.870612590095666, \"pc2\": 1.810963496959746, \"word\": \"describe\", \"split_0\": \"In this paper , we describe\", \"split_1\": \"a simple re - implementation of BERT for query - based passage re - ranking\", \"split_2\": \".\", \"averb\": \"describe\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"['a simple re - implementation of BERT for query - based passage re - ranking']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 28.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 28.0, \"root\": \"describe\", \"root_full\": \"describe\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['a simple re - implementation of BERT for query - based passage re - ranking']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 28, \"root_cspan0\": 19, \"root_cspan1\": 28, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['implementation', 'describe']\", \"URL\": \"https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2\", \"ID\": 5, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'describe', 'a', 'simple', 're', '-', 'implementation', 'of', 'BERT', 'for', 'query', '-', 'based', 'passage', 're', '-', 'ranking', '.']\", \"split_anchor_span\": \"(6, 21)\", \"split_anchor_indices\": \"(27, 102)\", \"within_anchor_index\": 32.0}, {\"pc1\": 5.412322286171481, \"pc2\": 6.837224369729253, \"word\": \"is\", \"split_0\": null, \"split_1\": \"Our system\", \"split_2\": \"is the state of the art on the TREC - CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task , outperforming the previous state of the art by 27 % ( relative ) in MRR@10 .\", \"averb\": \"is\", \"averb_s\": \"['Our system']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 11.0, \"averb_span1\": 14.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"task\", \"root_full\": \"task\", \"root_s\": \"['Our system is the state of the art on the TREC - CAR dataset and the top entry in the leaderboard of', 'the MS MARCO passage retrieval']\", \"root_o\": \"[',', 'outperforming the previous state of the art by 27 % ( relative ) in MRR@10 .']\", \"root_split\": 2, \"root_span0\": 132, \"root_span1\": 137, \"root_cspan0\": 121, \"root_cspan1\": 126, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'AUX', 'NOUN']\", \"apos_w\": \"['system', 'is', 'state']\", \"URL\": \"https://www.semanticscholar.org/paper/Passage-Re-ranking-with-BERT-Nogueira-Cho/85e07116316e686bf787114ba10ca60f4ea7c5b2\", \"ID\": 5, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10.\", \"split_tokens\": \"['Our', 'system', 'is', 'the', 'state', 'of', 'the', 'art', 'on', 'the', 'TREC', '-', 'CAR', 'dataset', 'and', 'the', 'top', 'entry', 'in', 'the', 'leaderboard', 'of', 'the', 'MS', 'MARCO', 'passage', 'retrieval', 'task', ',', 'outperforming', 'the', 'previous', 'state', 'of', 'the', 'art', 'by', '27', '%', '(', 'relative', ')', 'in', 'MRR@10', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 10)\", \"within_anchor_index\": -1.0}, {\"pc1\": 32.48655357538487, \"pc2\": 6.2722840614130595, \"word\": \"assess\", \"split_0\": \"I assess the extent to which\", \"split_1\": \"the recently introduced BERT model\", \"split_2\": \"captures English syntactic phenomena , using ( 1 ) naturally - occurring subject - verb agreement stimuli ; ( 2 ) \\\" coloreless green ideas \\\" subject - verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part - of - speech and inflection ; and ( 3 ) manually crafted stimuli for subject - verb agreement and reflexive anaphora phenomena .\", \"averb\": \"assess\", \"averb_s\": \"['I']\", \"averb_o\": \"['the extent the recently introduced BERT model captures English syntactic , using ( 1 ) naturally - occurring subject - verb agreement stimuli ( 2 ) \\\" coloreless green ideas \\\"', 'subject - verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part - of - speech and inflection ; and']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 2.0, \"averb_span1\": 9.0, \"averb_cspan0\": 2.0, \"averb_cspan1\": 9.0, \"root\": \")\", \"root_full\": \")\", \"root_s\": \"['(', '3']\", \"root_o\": \"['I assess the extent to which the recently introduced BERT model captures English syntactic phenomena , using ( 1 ) naturally - occurring subject - verb agreement stimuli ; ( 2 ) \\\" coloreless green ideas \\\" subject - verb agreement stimuli , in which content words in natural sentences are randomly replaced with words sharing the same part - of - speech and inflection ; and crafted stimuli for subject - verb agreement and reflexive anaphora phenomena .']\", \"root_split\": 2, \"root_span0\": 378, \"root_span1\": 380, \"root_cspan0\": 314, \"root_cspan1\": 316, \"fverb\": \"using\", \"fword\": \"captures\", \"apos\": \"['NOUN', 'ADJ', 'NOUN']\", \"apos_w\": \"['model', 'syntactic', 'extent']\", \"URL\": \"https://www.semanticscholar.org/paper/Assessing-BERT's-Syntactic-Abilities-Goldberg/efeab0dcdb4c1cce5e537e57745d84774be99b9a\", \"ID\": 6, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) \\\"coloreless green ideas\\\" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.\", \"split_tokens\": \"['I', 'assess', 'the', 'extent', 'to', 'which', 'the', 'recently', 'introduced', 'BERT', 'model', 'captures', 'English', 'syntactic', 'phenomena', ',', 'using', '(', '1', ')', 'naturally', '-', 'occurring', 'subject', '-', 'verb', 'agreement', 'stimuli', ';', '(', '2', ')', '\\\"', 'coloreless', 'green', 'ideas', '\\\"', 'subject', '-', 'verb', 'agreement', 'stimuli', ',', 'in', 'which', 'content', 'words', 'in', 'natural', 'sentences', 'are', 'randomly', 'replaced', 'with', 'words', 'sharing', 'the', 'same', 'part', '-', 'of', '-', 'speech', 'and', 'inflection', ';', 'and', '(', '3', ')', 'manually', 'crafted', 'stimuli', 'for', 'subject', '-', 'verb', 'agreement', 'and', 'reflexive', 'anaphora', 'phenomena', '.']\", \"split_anchor_span\": \"(6, 11)\", \"split_anchor_indices\": \"(28, 62)\", \"within_anchor_index\": 24.0}, {\"pc1\": -9.12450484585591, \"pc2\": -0.08241547818106665, \"word\": \"performs\", \"split_0\": null, \"split_1\": \"The BERT model\", \"split_2\": \"performs remarkably well on all cases .\", \"averb\": \"performs\", \"averb_s\": \"['The BERT model']\", \"averb_o\": \"['well on all cases', '.']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 15.0, \"averb_span1\": 24.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"performs\", \"root_full\": \"performs\", \"root_s\": \"['The BERT model']\", \"root_o\": \"['well on all cases', '.']\", \"root_split\": 2, \"root_span0\": 15, \"root_span1\": 24, \"root_cspan0\": 0, \"root_cspan1\": 9, \"fverb\": null, \"fword\": \"performs\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['model', 'performs']\", \"URL\": \"https://www.semanticscholar.org/paper/Assessing-BERT's-Syntactic-Abilities-Goldberg/efeab0dcdb4c1cce5e537e57745d84774be99b9a\", \"ID\": 6, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"The BERT model performs remarkably well on all cases.\", \"split_tokens\": \"['The', 'BERT', 'model', 'performs', 'remarkably', 'well', 'on', 'all', 'cases', '.']\", \"split_anchor_span\": \"(0, 3)\", \"split_anchor_indices\": \"(0, 14)\", \"within_anchor_index\": 4.0}, {\"pc1\": 0.1257893871085734, \"pc2\": 4.908226181408446, \"word\": \"includes\", \"split_0\": \"A new release of\", \"split_1\": \"BERT\", \"split_2\": \"( Devlin , 2018 ) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task .\", \"averb\": \"includes\", \"averb_s\": \"['new release of BERT ( Devlin , 2018', ')']\", \"averb_o\": \"['a model simultaneously pretrained on 104 languages with impressive performance for zero - shot cross - lingual transfer on a natural language inference task']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 40.0, \"averb_span1\": 49.0, \"averb_cspan0\": 18.0, \"averb_cspan1\": 27.0, \"root\": \"includes\", \"root_full\": \"includes\", \"root_s\": \"['new release of BERT ( Devlin , 2018', ')']\", \"root_o\": \"['a model simultaneously pretrained on 104 languages with impressive performance for zero - shot cross - lingual transfer on a natural language inference task']\", \"root_split\": 2, \"root_span0\": 40, \"root_span1\": 49, \"root_cspan0\": 18, \"root_cspan1\": 27, \"fverb\": \"includes\", \"fword\": \"Devlin\", \"apos\": \"['PROPN', 'PROPN', 'ADP']\", \"apos_w\": \"['BERT', 'Devlin', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task.\", \"split_tokens\": \"['A', 'new', 'release', 'of', 'BERT', '(', 'Devlin', ',', '2018', ')', 'includes', 'a', 'model', 'simultaneously', 'pretrained', 'on', '104', 'languages', 'with', 'impressive', 'performance', 'for', 'zero-shot', 'cross-lingual', 'transfer', 'on', 'a', 'natural', 'language', 'inference', 'task', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(16, 20)\", \"within_anchor_index\": 0.0}, {\"pc1\": 9.36072114560013, \"pc2\": 7.325671262057566, \"word\": \"explores\", \"split_0\": \"This paper explores the broader cross-lingual potential of\", \"split_1\": \"mBERT\", \"split_2\": \"( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families : NLI , document classification , NER , POS tagging , and dependency parsing .\", \"averb\": \"explores\", \"averb_s\": \"['This', 'paper']\", \"averb_o\": \"['the broader cross - lingual potential of mBERT ( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 11.0, \"averb_span1\": 20.0, \"averb_cspan0\": 11.0, \"averb_cspan1\": 20.0, \"root\": \"NER\", \"root_full\": \"NER\", \"root_s\": \"['This paper explores the broader cross - lingual potential of mBERT ( multilingual ) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families : NLI , document classification , .']\", \"root_o\": \"[', POS tagging and dependency parsing', ',']\", \"root_split\": 2, \"root_span0\": 236, \"root_span1\": 240, \"root_cspan0\": 171, \"root_cspan1\": 175, \"fverb\": \"covering\", \"fword\": \"multilingual\", \"apos\": \"['ADJ', 'ADJ', 'ADP']\", \"apos_w\": \"['mBERT', 'multilingual', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing.\", \"split_tokens\": \"['This', 'paper', 'explores', 'the', 'broader', 'cross-lingual', 'potential', 'of', 'mBERT', '(', 'multilingual', ')', 'as', 'a', 'zero', 'shot', 'language', 'transfer', 'model', 'on', '5', 'NLP', 'tasks', 'covering', 'a', 'total', 'of', '39', 'languages', 'from', 'various', 'language', 'families', ':', 'NLI', ',', 'document', 'classification', ',', 'NER', ',', 'POS', 'tagging', ',', 'and', 'dependency', 'parsing', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(58, 63)\", \"within_anchor_index\": 1.0}, {\"pc1\": -2.68953466639527, \"pc2\": 0.9407059851491609, \"word\": \"compare\", \"split_0\": \"We compare\", \"split_1\": \"mBERT\", \"split_2\": \"with the best - published methods for zero - shot cross - lingual transfer and find mBERT competitive on each task .\", \"averb\": \"compare\", \"averb_s\": \"['We']\", \"averb_o\": \"['mBERT with the best - published methods for zero - shot cross']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"find\", \"root_full\": \"find\", \"root_s\": \"['- lingual transfer']\", \"root_o\": \"['We compare mBERT with the best - published methods for zero - shot cross mBERT competitive on each task .']\", \"root_split\": 2, \"root_span0\": 96, \"root_span1\": 101, \"root_cspan0\": 79, \"root_cspan1\": 84, \"fverb\": \"published\", \"fword\": \"with\", \"apos\": \"['NOUN', 'VERB', 'PUNCT']\", \"apos_w\": \"['mBERT', 'compare', '.']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task.\", \"split_tokens\": \"['We', 'compare', 'mBERT', 'with', 'the', 'best', '-', 'published', 'methods', 'for', 'zero', '-', 'shot', 'cross', '-', 'lingual', 'transfer', 'and', 'find', 'mBERT', 'competitive', 'on', 'each', 'task', '.']\", \"split_anchor_span\": \"(2, 3)\", \"split_anchor_indices\": \"(10, 15)\", \"within_anchor_index\": 1.0}, {\"pc1\": -2.7651869797807183, \"pc2\": 0.5985117541872622, \"word\": \"find\", \"split_0\": \"We compare mBERT with the best - published methods for zero - shot cross - lingual transfer and find\", \"split_1\": \"mBERT\", \"split_2\": \"competitive on each task .\", \"averb\": \"find\", \"averb_s\": \"['- lingual transfer']\", \"averb_o\": \"['We compare mBERT with the best - published methods for zero - shot cross mBERT competitive on each task .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 96.0, \"averb_span1\": 101.0, \"averb_cspan0\": 96.0, \"averb_cspan1\": 101.0, \"root\": \"find\", \"root_full\": \"find\", \"root_s\": \"['- lingual transfer']\", \"root_o\": \"['We compare mBERT with the best - published methods for zero - shot cross mBERT competitive on each task .']\", \"root_split\": 0, \"root_span0\": 96, \"root_span1\": 101, \"root_cspan0\": 96, \"root_cspan1\": 101, \"fverb\": null, \"fword\": \"competitive\", \"apos\": \"['NOUN', 'ADJ', 'VERB']\", \"apos_w\": \"['mBERT', 'competitive', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task.\", \"split_tokens\": \"['We', 'compare', 'mBERT', 'with', 'the', 'best', '-', 'published', 'methods', 'for', 'zero', '-', 'shot', 'cross', '-', 'lingual', 'transfer', 'and', 'find', 'mBERT', 'competitive', 'on', 'each', 'task', '.']\", \"split_anchor_span\": \"(19, 20)\", \"split_anchor_indices\": \"(100, 105)\", \"within_anchor_index\": 1.0}, {\"pc1\": 6.232693522967477, \"pc2\": -4.7145580094482, \"word\": \"investigate\", \"split_0\": \"Additionally , we investigate the most effective strategy for utilizing\", \"split_1\": \"mBERT\", \"split_2\": \"in this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .\", \"averb\": \"investigate\", \"averb_s\": \"['Additionally']\", \"averb_o\": \"['the most effective strategy for utilizing mBERT in', 'this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 18.0, \"averb_span1\": 30.0, \"averb_cspan0\": 18.0, \"averb_cspan1\": 30.0, \"root\": \"investigate\", \"root_full\": \"investigate\", \"root_s\": \"['Additionally']\", \"root_o\": \"['the most effective strategy for utilizing mBERT in', 'this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 30, \"root_cspan0\": 18, \"root_cspan1\": 30, \"fverb\": \"generalizes\", \"fword\": \"in\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['mBERT', 'for', 'strategy']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.\", \"split_tokens\": \"['Additionally', ',', 'we', 'investigate', 'the', 'most', 'effective', 'strategy', 'for', 'utilizing', 'mBERT', 'in', 'this', 'manner', ',', 'determine', 'to', 'what', 'extent', 'mBERT', 'generalizes', 'away', 'from', 'language', 'specific', 'features', ',', 'and', 'measure', 'factors', 'that', 'influence', 'cross', '-', 'lingual', 'transfer', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(71, 76)\", \"within_anchor_index\": 1.0}, {\"pc1\": 6.189331884152508, \"pc2\": -4.739040550414178, \"word\": \"generalizes\", \"split_0\": \"Additionally , we investigate the most effective strategy for utilizing mBERT in this manner , determine to what extent\", \"split_1\": \"mBERT\", \"split_2\": \"generalizes away from language specific features , and measure factors that influence cross - lingual transfer .\", \"averb\": \"generalizes\", \"averb_s\": \"['what extent mBERT']\", \"averb_o\": \"['away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 126.0, \"averb_span1\": 138.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"investigate\", \"root_full\": \"investigate\", \"root_s\": \"['Additionally']\", \"root_o\": \"['the most effective strategy for utilizing mBERT in', 'this manner , determine to what extent mBERT generalizes away from language specific features , and measure factors that influence cross - lingual transfer .']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 30, \"root_cspan0\": 18, \"root_cspan1\": 30, \"fverb\": \"generalizes\", \"fword\": \"generalizes\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['mBERT', 'generalizes', 'determine']\", \"URL\": \"https://www.semanticscholar.org/paper/Beto%2C-Bentz%2C-Becas%3A-The-Surprising-Cross-Lingual-of-Wu-Dredze/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3\", \"ID\": 7, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.\", \"split_tokens\": \"['Additionally', ',', 'we', 'investigate', 'the', 'most', 'effective', 'strategy', 'for', 'utilizing', 'mBERT', 'in', 'this', 'manner', ',', 'determine', 'to', 'what', 'extent', 'mBERT', 'generalizes', 'away', 'from', 'language', 'specific', 'features', ',', 'and', 'measure', 'factors', 'that', 'influence', 'cross', '-', 'lingual', 'transfer', '.']\", \"split_anchor_span\": \"(19, 20)\", \"split_anchor_indices\": \"(119, 124)\", \"within_anchor_index\": 1.0}, {\"pc1\": -3.668426843033288, \"pc2\": 1.199840642717561, \"word\": \"pre\", \"split_0\": \"Language model pre - training , such as\", \"split_1\": \"BERT\", \"split_2\": \", has significantly improved the performances of many natural language processing tasks .\", \"averb\": \"pre\", \"averb_s\": \"['Language model']\", \"averb_o\": \"['-', 'training , such as BERT ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 15.0, \"averb_span1\": 19.0, \"averb_cspan0\": 15.0, \"averb_cspan1\": 19.0, \"root\": \"improved\", \"root_full\": \"has improved\", \"root_s\": \"['Language model pre - training , such as BERT ,', 'significantly']\", \"root_o\": \"['the performances of many natural language processing tasks', '.']\", \"root_split\": 2, \"root_span0\": 47, \"root_span1\": 74, \"root_cspan0\": 2, \"root_cspan1\": 29, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'SCONJ', 'ADJ']\", \"apos_w\": \"['BERT', 'as', 'training']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks.\", \"split_tokens\": \"['Language', 'model', 'pre', '-', 'training', ',', 'such', 'as', 'BERT', ',', 'has', 'significantly', 'improved', 'the', 'performances', 'of', 'many', 'natural', 'language', 'processing', 'tasks', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(39, 43)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.301884746390193, \"pc2\": 0.7681562860870793, \"word\": \"can be transferred\", \"split_0\": \"By leveraging this new KD method , the plenty of knowledge encoded in a large teacher BERT can be well transferred to\", \"split_1\": \"a small student TinyBERT\", \"split_2\": \".\", \"averb\": \"can be transferred\", \"averb_s\": \"['the plenty of knowledge encoded in a large teacher BERT']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 91.0, \"averb_span1\": 115.0, \"averb_cspan0\": 91.0, \"averb_cspan1\": 115.0, \"root\": \"transferred\", \"root_full\": \"can be transferred\", \"root_s\": \"['the plenty of knowledge encoded in a large teacher BERT']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 91, \"root_span1\": 115, \"root_cspan0\": 91, \"root_cspan1\": 115, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['student', 'to', 'transferred']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT.\", \"split_tokens\": \"['By', 'leveraging', 'this', 'new', 'KD', 'method', ',', 'the', 'plenty', 'of', 'knowledge', 'encoded', 'in', 'a', 'large', 'teacher', 'BERT', 'can', 'be', 'well', 'transferred', 'to', 'a', 'small', 'student', 'TinyBERT', '.']\", \"split_anchor_span\": \"(22, 26)\", \"split_anchor_indices\": \"(117, 141)\", \"within_anchor_index\": 20.0}, {\"pc1\": -4.249081773167452, \"pc2\": 0.5048334901388729, \"word\": \"encoded\", \"split_0\": \"By leveraging this new KD method , the plenty of knowledge encoded in\", \"split_1\": \"a large teacher BERT\", \"split_2\": \"can be well transferred to a small student TinyBERT .\", \"averb\": \"encoded\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 59.0, \"averb_span1\": 67.0, \"averb_cspan0\": 59.0, \"averb_cspan1\": 67.0, \"root\": \"transferred\", \"root_full\": \"can be transferred\", \"root_s\": \"['the plenty of knowledge encoded in a large teacher BERT']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 91, \"root_span1\": 115, \"root_cspan0\": 0, \"root_cspan1\": 24, \"fverb\": \"be\", \"fword\": \"can\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'in', 'encoded']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT.\", \"split_tokens\": \"['By', 'leveraging', 'this', 'new', 'KD', 'method', ',', 'the', 'plenty', 'of', 'knowledge', 'encoded', 'in', 'a', 'large', 'teacher', 'BERT', 'can', 'be', 'well', 'transferred', 'to', 'a', 'small', 'student', 'TinyBERT', '.']\", \"split_anchor_span\": \"(13, 17)\", \"split_anchor_indices\": \"(69, 89)\", \"within_anchor_index\": 16.0}, {\"pc1\": 0.9928787915526867, \"pc2\": 3.6712834209506906, \"word\": \"introduce\", \"split_0\": \"Moreover , we introduce a new two - stage learning framework for\", \"split_1\": \"TinyBERT\", \"split_2\": \", which performs transformer distillation at both the pre - training and task - specific learning stages .\", \"averb\": \"introduce\", \"averb_s\": \"['we', ',']\", \"averb_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 14.0, \"averb_span1\": 24.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 24.0, \"root\": \"introduce\", \"root_full\": \"introduce\", \"root_s\": \"['we', ',']\", \"root_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"root_split\": 0, \"root_span0\": 14, \"root_span1\": 24, \"root_cspan0\": 14, \"root_cspan1\": 24, \"fverb\": \"performs\", \"fword\": \"which\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['TinyBERT', 'for', 'framework']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages.\", \"split_tokens\": \"['Moreover', ',', 'we', 'introduce', 'a', 'new', 'two', '-', 'stage', 'learning', 'framework', 'for', 'TinyBERT', ',', 'which', 'performs', 'transformer', 'distillation', 'at', 'both', 'the', 'pre', '-', 'training', 'and', 'task', '-', 'specific', 'learning', 'stages', '.']\", \"split_anchor_span\": \"(12, 13)\", \"split_anchor_indices\": \"(64, 72)\", \"within_anchor_index\": 4.0}, {\"pc1\": 0.9928787915526877, \"pc2\": 3.671283420950692, \"word\": \"introduce\", \"split_0\": \"Moreover , we introduce\", \"split_1\": \"a new two - stage learning framework for TinyBERT , which performs transformer distillation at both the pre - training and task - specific learning stages\", \"split_2\": \".\", \"averb\": \"introduce\", \"averb_s\": \"['we', ',']\", \"averb_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"averb_relation\": 0.0, \"averb_split\": 0.0, \"averb_span0\": 14.0, \"averb_span1\": 24.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 24.0, \"root\": \"introduce\", \"root_full\": \"introduce\", \"root_s\": \"['we', ',']\", \"root_o\": \"['a new two - stage learning framework for TinyBERT', 'which performs transformer distillation at both the pre - training and task - specific learning stages .']\", \"root_split\": 0, \"root_span0\": 14, \"root_span1\": 24, \"root_cspan0\": 14, \"root_cspan1\": 24, \"fverb\": null, \"fword\": null, \"apos\": \"['VERB']\", \"apos_w\": \"['introduce']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages.\", \"split_tokens\": \"['Moreover', ',', 'we', 'introduce', 'a', 'new', 'two', '-', 'stage', 'learning', 'framework', 'for', 'TinyBERT', ',', 'which', 'performs', 'transformer', 'distillation', 'at', 'both', 'the', 'pre', '-', 'training', 'and', 'task', '-', 'specific', 'learning', 'stages', '.']\", \"split_anchor_span\": \"(4, 30)\", \"split_anchor_indices\": \"(23, 177)\", \"within_anchor_index\": 45.0}, {\"pc1\": -5.001089749848006, \"pc2\": 0.7876860490433116, \"word\": \"can capture\", \"split_0\": \"This framework ensures that\", \"split_1\": \"TinyBERT\", \"split_2\": \"can capture both the general - domain and task - specific knowledge of the teacher BERT .\", \"averb\": \"can capture\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"['both the general - domain and', 'task - specific knowledge of the teacher BERT']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 37.0, \"averb_span1\": 49.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"ensures\", \"root_full\": \"ensures\", \"root_s\": \"['This', 'framework']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 23, \"root_cspan0\": 15, \"root_cspan1\": 23, \"fverb\": \"capture\", \"fword\": \"can\", \"apos\": \"['PROPN', 'VERB', 'VERB']\", \"apos_w\": \"['TinyBERT', 'capture', 'ensures']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.\", \"split_tokens\": \"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general', '-', 'domain', 'and', 'task', '-', 'specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(27, 35)\", \"within_anchor_index\": 4.0}, {\"pc1\": -4.650268309683027, \"pc2\": 0.8925962939072665, \"word\": \"ensures\", \"split_0\": null, \"split_1\": \"This framework\", \"split_2\": \"ensures that TinyBERT can capture both the general - domain and task - specific knowledge of the teacher BERT .\", \"averb\": \"ensures\", \"averb_s\": \"['This', 'framework']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 2.0, \"averb_span0\": 15.0, \"averb_span1\": 23.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"ensures\", \"root_full\": \"ensures\", \"root_s\": \"['This', 'framework']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 15, \"root_span1\": 23, \"root_cspan0\": 0, \"root_cspan1\": 8, \"fverb\": \"ensures\", \"fword\": \"ensures\", \"apos\": \"['VERB']\", \"apos_w\": \"['ensures']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.\", \"split_tokens\": \"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general', '-', 'domain', 'and', 'task', '-', 'specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 14)\", \"within_anchor_index\": -1.0}, {\"pc1\": -5.001089749848006, \"pc2\": 0.7876860490433116, \"word\": \"can capture\", \"split_0\": \"This framework ensures that TinyBERT can capture both the general - domain and task - specific knowledge of\", \"split_1\": \"the teacher BERT\", \"split_2\": \".\", \"averb\": \"can capture\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"['both the general - domain and', 'task - specific knowledge of the teacher BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 37.0, \"averb_span1\": 49.0, \"averb_cspan0\": 37.0, \"averb_cspan1\": 49.0, \"root\": \"ensures\", \"root_full\": \"ensures\", \"root_s\": \"['This', 'framework']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 23, \"root_cspan0\": 15, \"root_cspan1\": 23, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['teacher', 'of', 'knowledge']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.\", \"split_tokens\": \"['This', 'framework', 'ensures', 'that', 'TinyBERT', 'can', 'capture', 'both', 'the', 'general', '-', 'domain', 'and', 'task', '-', 'specific', 'knowledge', 'of', 'the', 'teacher', 'BERT', '.']\", \"split_anchor_span\": \"(18, 21)\", \"split_anchor_indices\": \"(107, 123)\", \"within_anchor_index\": 12.0}, {\"pc1\": -1.9868425986972182, \"pc2\": -0.026545747112363845, \"word\": \"is\", \"split_0\": null, \"split_1\": \"TinyBERT\", \"split_2\": \"is empirically effective and achieves comparable results with BERT in GLUE datasets , while being 7.5x smaller and 9.4x faster on inference .\", \"averb\": \"is\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 9.0, \"averb_span1\": 12.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"empirically\", \"root_full\": \"is empirically\", \"root_s\": \"[]\", \"root_o\": \"['achieves']\", \"root_split\": 2, \"root_span0\": 9, \"root_span1\": 24, \"root_cspan0\": 0, \"root_cspan1\": 15, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX', 'ADV']\", \"apos_w\": \"['TinyBERT', 'is', 'empirically']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"TinyBERT is empirically effective and achieves comparable results with BERT in GLUE datasets, while being 7.5x smaller and 9.4x faster on inference.\", \"split_tokens\": \"['TinyBERT', 'is', 'empirically', 'effective', 'and', 'achieves', 'comparable', 'results', 'with', 'BERT', 'in', 'GLUE', 'datasets', ',', 'while', 'being', '7.5x', 'smaller', 'and', '9.4x', 'faster', 'on', 'inference', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 8)\", \"within_anchor_index\": 4.0}, {\"pc1\": -1.5200454484087211, \"pc2\": 1.696451541203765, \"word\": \"is\", \"split_0\": null, \"split_1\": \"TinyBERT\", \"split_2\": \"is also significantly better than state - of - the - art baselines , even with only about 28 % parameters and 31 % inference time of baselines\", \"averb\": \"is\", \"averb_s\": \"['TinyBERT']\", \"averb_o\": \"['also significantly better than state - of - the - art baselines , even with only about 28 % parameters and 31 % inference time of baselines']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 9.0, \"averb_span1\": 12.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"is\", \"root_full\": \"is\", \"root_s\": \"['TinyBERT']\", \"root_o\": \"['also significantly better than state - of - the - art baselines , even with only about 28 % parameters and 31 % inference time of baselines']\", \"root_split\": 2, \"root_span0\": 9, \"root_span1\": 12, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['TinyBERT', 'is']\", \"URL\": \"https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7\", \"ID\": 8, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"TinyBERT is also significantly better than state-of-the-art baselines, even with only about 28% parameters and 31% inference time of baselines\", \"split_tokens\": \"['TinyBERT', 'is', 'also', 'significantly', 'better', 'than', 'state', '-', 'of', '-', 'the', '-', 'art', 'baselines', ',', 'even', 'with', 'only', 'about', '28', '%', 'parameters', 'and', '31', '%', 'inference', 'time', 'of', 'baselines']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 8)\", \"within_anchor_index\": 4.0}, {\"pc1\": -8.148236894912534, \"pc2\": 1.1238040572594354, \"word\": \"has achieved\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \", a pre-trained Transformer model , has achieved ground-breaking performance on multiple NLP tasks .\", \"averb\": \"has achieved\", \"averb_s\": \"['BERT , a pre - trained Transformer model ,']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 43.0, \"averb_span1\": 56.0, \"averb_cspan0\": 38.0, \"averb_cspan1\": 51.0, \"root\": \"ground\", \"root_full\": \"ground\", \"root_s\": \"['BERT , a pre - trained Transformer model , has achieved']\", \"root_o\": \"['- breaking performance on multiple NLP tasks', '.']\", \"root_split\": 2, \"root_span0\": 56, \"root_span1\": 63, \"root_cspan0\": 51, \"root_cspan1\": 58, \"fverb\": \"has\", \"fword\": \"a\", \"apos\": \"['PROPN', 'VERB', 'NOUN']\", \"apos_w\": \"['BERT', 'achieved', 'ground']\", \"URL\": \"https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3\", \"ID\": 9, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"BERT, a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks.\", \"split_tokens\": \"['BERT', ',', 'a', 'pre-trained', 'Transformer', 'model', ',', 'has', 'achieved', 'ground-breaking', 'performance', 'on', 'multiple', 'NLP', 'tasks', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.823116322557562, \"pc2\": 0.0883154732067757, \"word\": \"describe\", \"split_0\": \"In this paper , we describe\", \"split_1\": \"BERTSUM\", \"split_2\": \", a simple variant of BERT , for extractive summarization .\", \"averb\": \"describe\", \"averb_s\": \"['we']\", \"averb_o\": \"['BERTSUM , a simple variant of BERT ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 28.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 28.0, \"root\": \"describe\", \"root_full\": \"describe\", \"root_s\": \"['we']\", \"root_o\": \"['BERTSUM , a simple variant of BERT ,']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 28, \"root_cspan0\": 19, \"root_cspan1\": 28, \"fverb\": null, \"fword\": \"a\", \"apos\": \"['PROPN', 'VERB']\", \"apos_w\": \"['BERTSUM', 'describe']\", \"URL\": \"https://www.semanticscholar.org/paper/Fine-tune-BERT-for-Extractive-Summarization-Liu/a10a6495723ea8c928680ecdd61714f5750586c3\", \"ID\": 9, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In this paper, we describe BERTSUM, a simple variant of BERT, for extractive summarization.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'describe', 'BERTSUM', ',', 'a', 'simple', 'variant', 'of', 'BERT', ',', 'for', 'extractive', 'summarization', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(27, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": 7.951726817187553, \"pc2\": 3.3449844534376627, \"word\": \"explore\", \"split_0\": \"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on\", \"split_1\": \"the popular language model BERT\", \"split_2\": \"to enhance the performance of fine - tuning of BERT for RRC .\", \"averb\": \"explore\", \"averb_s\": \"[]\", \"averb_o\": \"['a novel post - training approach on the popular language model BERT', 'to enhance the performance of fine - tuning of BERT for RRC']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 114.0, \"averb_span1\": 122.0, \"averb_cspan0\": 114.0, \"averb_cspan1\": 122.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['Since', 'ReviewRC']\", \"root_o\": \"['limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC']\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 19, \"root_cspan0\": 15, \"root_cspan1\": 19, \"fverb\": \"enhance\", \"fword\": \"to\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['model', 'on', 'approach']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.\", \"split_tokens\": \"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post', '-', 'training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine', '-', 'tuning', 'of', 'BERT', 'for', 'RRC', '.']\", \"split_anchor_span\": \"(29, 34)\", \"split_anchor_indices\": \"(157, 188)\", \"within_anchor_index\": 27.0}, {\"pc1\": 7.821918567578604, \"pc2\": 3.367874155313452, \"word\": \"enhance\", \"split_0\": \"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of\", \"split_1\": \"BERT\", \"split_2\": \"for RRC .\", \"averb\": \"enhance\", \"averb_s\": \"['to']\", \"averb_o\": \"['the performance of fine - tuning of BERT for RRC']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 193.0, \"averb_span1\": 201.0, \"averb_cspan0\": 193.0, \"averb_cspan1\": 201.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['Since', 'ReviewRC']\", \"root_o\": \"['limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC']\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 19, \"root_cspan0\": 15, \"root_cspan1\": 19, \"fverb\": null, \"fword\": \"for\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'tuning']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.\", \"split_tokens\": \"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post', '-', 'training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine', '-', 'tuning', 'of', 'BERT', 'for', 'RRC', '.']\", \"split_anchor_span\": \"(43, 44)\", \"split_anchor_indices\": \"(236, 240)\", \"within_anchor_index\": 0.0}, {\"pc1\": 7.951726817187552, \"pc2\": 3.344984453437665, \"word\": \"explore\", \"split_0\": \"Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore\", \"split_1\": \"a novel post - training approach on the popular language model BERT\", \"split_2\": \"to enhance the performance of fine - tuning of BERT for RRC .\", \"averb\": \"explore\", \"averb_s\": \"[]\", \"averb_o\": \"['a novel post - training approach on the popular language model BERT', 'to enhance the performance of fine - tuning of BERT for RRC']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 114.0, \"averb_span1\": 122.0, \"averb_cspan0\": 114.0, \"averb_cspan1\": 122.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['Since', 'ReviewRC']\", \"root_o\": \"['limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC']\", \"root_split\": 0, \"root_span0\": 15, \"root_span1\": 19, \"root_cspan0\": 15, \"root_cspan1\": 19, \"fverb\": \"enhance\", \"fword\": \"to\", \"apos\": \"['NOUN', 'VERB', 'ADV']\", \"apos_w\": \"['approach', 'explore', 'then']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC.\", \"split_tokens\": \"['Since', 'ReviewRC', 'has', 'limited', 'training', 'examples', 'for', 'RRC', '(', 'and', 'also', 'for', 'aspect', '-', 'based', 'sentiment', 'analysis', ')', ',', 'we', 'then', 'explore', 'a', 'novel', 'post', '-', 'training', 'approach', 'on', 'the', 'popular', 'language', 'model', 'BERT', 'to', 'enhance', 'the', 'performance', 'of', 'fine', '-', 'tuning', 'of', 'BERT', 'for', 'RRC', '.']\", \"split_anchor_span\": \"(22, 34)\", \"split_anchor_indices\": \"(121, 188)\", \"within_anchor_index\": 63.0}, {\"pc1\": 1.0271083500845883, \"pc2\": 0.6111810906790504, \"word\": \"To show\", \"split_0\": \"To show the generality of\", \"split_1\": \"the approach\", \"split_2\": \", the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .\", \"averb\": \"To show\", \"averb_s\": \"[]\", \"averb_o\": \"['the generality of the approach , the proposed post - training']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"applied\", \"root_full\": \"applied\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 78, \"root_span1\": 86, \"root_cspan0\": 39, \"root_cspan1\": 47, \"fverb\": \"proposed\", \"fword\": \"the\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['approach', 'of', 'generality']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis.\", \"split_tokens\": \"['To', 'show', 'the', 'generality', 'of', 'the', 'approach', ',', 'the', 'proposed', 'post', '-', 'training', 'is', 'also', 'applied', 'to', 'some', 'other', 'review', '-', 'based', 'tasks', 'such', 'as', 'aspect', 'extraction', 'and', 'aspect', 'sentiment', 'classification', 'in', 'aspect', '-', 'based', 'sentiment', 'analysis', '.']\", \"split_anchor_span\": \"(5, 7)\", \"split_anchor_indices\": \"(25, 37)\", \"within_anchor_index\": -1.0}, {\"pc1\": 1.0271083500845883, \"pc2\": 0.6111810906790501, \"word\": \"To show\", \"split_0\": \"To show the generality of the approach ,\", \"split_1\": \"the proposed post - training\", \"split_2\": \"is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .\", \"averb\": \"To show\", \"averb_s\": \"[]\", \"averb_o\": \"['the generality of the approach , the proposed post - training']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"applied\", \"root_full\": \"applied\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 78, \"root_span1\": 86, \"root_cspan0\": 8, \"root_cspan1\": 16, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['training', 'of', 'generality']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis.\", \"split_tokens\": \"['To', 'show', 'the', 'generality', 'of', 'the', 'approach', ',', 'the', 'proposed', 'post', '-', 'training', 'is', 'also', 'applied', 'to', 'some', 'other', 'review', '-', 'based', 'tasks', 'such', 'as', 'aspect', 'extraction', 'and', 'aspect', 'sentiment', 'classification', 'in', 'aspect', '-', 'based', 'sentiment', 'analysis', '.']\", \"split_anchor_span\": \"(8, 13)\", \"split_anchor_indices\": \"(40, 68)\", \"within_anchor_index\": -1.0}, {\"pc1\": -8.293207236919287, \"pc2\": -0.9342434288487522, \"word\": \"demonstrate\", \"split_0\": \"Experimental results demonstrate that\", \"split_1\": \"the proposed post - training\", \"split_2\": \"is highly effective .\", \"averb\": \"demonstrate\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 21.0, \"averb_span1\": 33.0, \"averb_cspan0\": 21.0, \"averb_cspan1\": 33.0, \"root\": \"results\", \"root_full\": \"results\", \"root_s\": \"['Experimental']\", \"root_o\": \"['demonstrate that the proposed post - training is highly effective .']\", \"root_split\": 0, \"root_span0\": 13, \"root_span1\": 21, \"root_cspan0\": 13, \"root_cspan1\": 21, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'ADV', 'ADJ']\", \"apos_w\": \"['training', 'highly', 'effective']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-Post-Training-for-Review-Reading-Comprehension-Xu-Liu/a4bc4b98a917174ac2ab14bd5e66d64306079ab5\", \"ID\": 10, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"Experimental results demonstrate that the proposed post-training is highly effective.\", \"split_tokens\": \"['Experimental', 'results', 'demonstrate', 'that', 'the', 'proposed', 'post', '-', 'training', 'is', 'highly', 'effective', '.']\", \"split_anchor_span\": \"(4, 9)\", \"split_anchor_indices\": \"(37, 65)\", \"within_anchor_index\": -1.0}, {\"pc1\": -1.060567235796411, \"pc2\": 4.346258855754947, \"word\": \"achieved\", \"split_0\": \"As a state - of - the - art language model pre - training model ,\", \"split_1\": \"BERT ( Bidirectional Encoder Representations from Transformers )\", \"split_2\": \"has achieved amazing results in many language understanding tasks .\", \"averb\": \"achieved\", \"averb_s\": \"['BERT ( Bidirectional Encoder Representations from Transformers )', 'As', 'a state - of - the - art language model pre - training model ,', 'has']\", \"averb_o\": \"['amazing results in many language understanding tasks', '.']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 135.0, \"averb_span1\": 144.0, \"averb_cspan0\": 4.0, \"averb_cspan1\": 13.0, \"root\": \"achieved\", \"root_full\": \"achieved\", \"root_s\": \"['BERT ( Bidirectional Encoder Representations from Transformers )', 'As', 'a state - of - the - art language model pre - training model ,', 'has']\", \"root_o\": \"['amazing results in many language understanding tasks', '.']\", \"root_split\": 2, \"root_span0\": 135, \"root_span1\": 144, \"root_cspan0\": 4, \"root_cspan1\": 13, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'VERB']\", \"apos_w\": \"['Representations', 'achieved']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks.\", \"split_tokens\": \"['As', 'a', 'state', '-', 'of', '-', 'the', '-', 'art', 'language', 'model', 'pre', '-', 'training', 'model', ',', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', 'has', 'achieved', 'amazing', 'results', 'in', 'many', 'language', 'understanding', 'tasks', '.']\", \"split_anchor_span\": \"(16, 24)\", \"split_anchor_indices\": \"(65, 129)\", \"within_anchor_index\": 0.0}, {\"pc1\": 1.23149890182283, \"pc2\": -0.21305049319591993, \"word\": \"to investigate\", \"split_0\": \"In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of\", \"split_1\": \"BERT\", \"split_2\": \"on text classification task and provide a general solution for BERT fine - tuning .\", \"averb\": \"to investigate\", \"averb_s\": \"[]\", \"averb_o\": \"['different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 50.0, \"averb_span1\": 65.0, \"averb_cspan0\": 50.0, \"averb_cspan1\": 65.0, \"root\": \"conduct\", \"root_full\": \"conduct\", \"root_s\": \"['we']\", \"root_o\": \"['exhaustive experiments', 'to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": \"provide\", \"fword\": \"on\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'methods']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine', '-', 'tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine', '-', 'tuning', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(99, 103)\", \"within_anchor_index\": 0.0}, {\"pc1\": 1.3659961281167419, \"pc2\": -0.3698182471758863, \"word\": \"provide\", \"split_0\": \"In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for\", \"split_1\": \"BERT\", \"split_2\": \"fine - tuning .\", \"averb\": \"provide\", \"averb_s\": \"[]\", \"averb_o\": \"['a general solution for BERT fine - tuning']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 137.0, \"averb_span1\": 145.0, \"averb_cspan0\": 137.0, \"averb_cspan1\": 145.0, \"root\": \"conduct\", \"root_full\": \"conduct\", \"root_s\": \"['we']\", \"root_o\": \"['exhaustive experiments', 'to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": \"fine\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'tuning', 'for']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine', '-', 'tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine', '-', 'tuning', '.']\", \"split_anchor_span\": \"(27, 28)\", \"split_anchor_indices\": \"(167, 171)\", \"within_anchor_index\": 0.0}, {\"pc1\": 1.3659961281167419, \"pc2\": -0.3698182471758863, \"word\": \"provide\", \"split_0\": \"In this paper , we conduct exhaustive experiments to investigate different fine - tuning methods of BERT on text classification task and provide\", \"split_1\": \"a general solution for BERT fine - tuning\", \"split_2\": \".\", \"averb\": \"provide\", \"averb_s\": \"[]\", \"averb_o\": \"['a general solution for BERT fine - tuning']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 137.0, \"averb_span1\": 145.0, \"averb_cspan0\": 137.0, \"averb_cspan1\": 145.0, \"root\": \"conduct\", \"root_full\": \"conduct\", \"root_s\": \"['we']\", \"root_o\": \"['exhaustive experiments', 'to investigate different fine - tuning methods of BERT on text classification task and provide a general solution for BERT fine - tuning']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB', 'NOUN']\", \"apos_w\": \"['solution', 'provide', 'task']\", \"URL\": \"https://www.semanticscholar.org/paper/How-to-Fine-Tune-BERT-for-Text-Classification-Sun-Qiu/a022bda79947d1f656a1164003c1b3ae9a843df9\", \"ID\": 11, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'conduct', 'exhaustive', 'experiments', 'to', 'investigate', 'different', 'fine', '-', 'tuning', 'methods', 'of', 'BERT', 'on', 'text', 'classification', 'task', 'and', 'provide', 'a', 'general', 'solution', 'for', 'BERT', 'fine', '-', 'tuning', '.']\", \"split_anchor_span\": \"(23, 31)\", \"split_anchor_indices\": \"(144, 185)\", \"within_anchor_index\": 23.0}, {\"pc1\": -7.4160544051883726, \"pc2\": 2.9597759583804013, \"word\": \"show\", \"split_0\": \"We show that\", \"split_1\": \"BERT\", \"split_2\": \"( Devlin et al . , 2018 ) is a Markov random field language model .\", \"averb\": \"show\", \"averb_s\": \"['We']\", \"averb_o\": \"['that BERT ( Devlin et al . , 2018 )', 'is a Markov random field language model', '.']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 8.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 8.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"['We']\", \"root_o\": \"['that BERT ( Devlin et al . , 2018 )', 'is a Markov random field language model', '.']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": \"is\", \"fword\": \"Devlin\", \"apos\": \"['PROPN', 'PROPN', 'SCONJ']\", \"apos_w\": \"['BERT', 'et', 'that']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"We show that BERT (Devlin et al., 2018) is a Markov random field language model.\", \"split_tokens\": \"['We', 'show', 'that', 'BERT', '(', 'Devlin', 'et', 'al', '.', ',', '2018', ')', 'is', 'a', 'Markov', 'random', 'field', 'language', 'model', '.']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(12, 16)\", \"within_anchor_index\": 0.0}, {\"pc1\": -7.90275230171895, \"pc2\": -1.3263086326959428, \"word\": \"gives\", \"split_0\": \"This formulation gives way to a natural procedure to sample sentences from\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"gives\", \"averb_s\": \"['This', 'formulation']\", \"averb_o\": \"['way', 'to a natural procedure to sample sentences from BERT', '.']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 17.0, \"averb_span1\": 23.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 23.0, \"root\": \"gives\", \"root_full\": \"gives\", \"root_s\": \"['This', 'formulation']\", \"root_o\": \"['way', 'to a natural procedure to sample sentences from BERT', '.']\", \"root_split\": 0, \"root_span0\": 17, \"root_span1\": 23, \"root_cspan0\": 17, \"root_cspan1\": 23, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'from', 'sentences']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"This formulation gives way to a natural procedure to sample sentences from BERT.\", \"split_tokens\": \"['This', 'formulation', 'gives', 'way', 'to', 'a', 'natural', 'procedure', 'to', 'sample', 'sentences', 'from', 'BERT', '.']\", \"split_anchor_span\": \"(12, 13)\", \"split_anchor_indices\": \"(74, 78)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.799561271421916, \"pc2\": -0.6835682297890017, \"word\": \"generate\", \"split_0\": \"We generate from\", \"split_1\": \"BERT\", \"split_2\": \"and find that it can produce high - quality , fluent generations .\", \"averb\": \"generate\", \"averb_s\": \"['We']\", \"averb_o\": \"['find that it can produce high - quality , fluent generations .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 12.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 12.0, \"root\": \"generate\", \"root_full\": \"generate\", \"root_s\": \"['We']\", \"root_o\": \"['find that it can produce high - quality , fluent generations .']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 12, \"root_cspan0\": 3, \"root_cspan1\": 12, \"fverb\": \"find\", \"fword\": \"and\", \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'from', 'generate']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We generate from BERT and find that it can produce high-quality, fluent generations.\", \"split_tokens\": \"['We', 'generate', 'from', 'BERT', 'and', 'find', 'that', 'it', 'can', 'produce', 'high', '-', 'quality', ',', 'fluent', 'generations', '.']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(16, 20)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.866892908863132, \"pc2\": -0.730727677633518, \"word\": \"produce\", \"split_0\": \"We generate from BERT and find that\", \"split_1\": \"it\", \"split_2\": \"can produce high - quality , fluent generations .\", \"averb\": \"produce\", \"averb_s\": \"['it', 'can']\", \"averb_o\": \"['high - quality', ', fluent generations .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 43.0, \"averb_span1\": 51.0, \"averb_cspan0\": 4.0, \"averb_cspan1\": 12.0, \"root\": \"generate\", \"root_full\": \"generate\", \"root_s\": \"['We']\", \"root_o\": \"['find that it can produce high - quality , fluent generations .']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 12, \"root_cspan0\": 3, \"root_cspan1\": 12, \"fverb\": \"produce\", \"fword\": \"can\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['it', 'produce', 'find']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We generate from BERT and find that it can produce high-quality, fluent generations.\", \"split_tokens\": \"['We', 'generate', 'from', 'BERT', 'and', 'find', 'that', 'it', 'can', 'produce', 'high', '-', 'quality', ',', 'fluent', 'generations', '.']\", \"split_anchor_span\": \"(7, 8)\", \"split_anchor_indices\": \"(35, 37)\", \"within_anchor_index\": -1.0}, {\"pc1\": -5.2191582987711795, \"pc2\": -2.753606410935084, \"word\": \"Compared\", \"split_0\": \"Compared to the generations of a traditional left - to - right language model ,\", \"split_1\": \"BERT\", \"split_2\": \"generates sentences that are more diverse but of slightly worse quality .\", \"averb\": \"Compared\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 9.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"diverse\", \"root_full\": \"are diverse\", \"root_s\": \"['that']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 110, \"root_span1\": 127, \"root_cspan0\": 25, \"root_cspan1\": 42, \"fverb\": \"generates\", \"fword\": \"generates\", \"apos\": \"['PROPN', 'NOUN', 'PUNCT']\", \"apos_w\": \"['BERT', 'sentences', ',']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.\", \"split_tokens\": \"['Compared', 'to', 'the', 'generations', 'of', 'a', 'traditional', 'left', '-', 'to', '-', 'right', 'language', 'model', ',', 'BERT', 'generates', 'sentences', 'that', 'are', 'more', 'diverse', 'but', 'of', 'slightly', 'worse', 'quality', '.']\", \"split_anchor_span\": \"(15, 16)\", \"split_anchor_indices\": \"(79, 83)\", \"within_anchor_index\": 0.0}, {\"pc1\": -7.569148302193097, \"pc2\": 2.8638384640113794, \"word\": \"has\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"has a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .\", \"averb\": \"has\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 5.0, \"averb_span1\": 9.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 4.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['BERT']\", \"root_o\": \"['a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .']\", \"root_split\": 2, \"root_span0\": 5, \"root_span1\": 9, \"root_cspan0\": 0, \"root_cspan1\": 4, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['BERT', 'has']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.\", \"split_tokens\": \"['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It', 'Must', 'Speak', ':', 'BERT', 'as', 'a', 'Markov', 'Random', 'Field', 'Language', 'Model', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -7.333778081981883, \"pc2\": 2.8149889041917198, \"word\": \"Must\", \"split_0\": \"BERT has a Mouth , and\", \"split_1\": \"It\", \"split_2\": \"Must Speak : BERT as a Markov Random Field Language Model .\", \"averb\": \"Must\", \"averb_s\": \"['It']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 26.0, \"averb_span1\": 31.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 5.0, \"root\": \"has\", \"root_full\": \"has\", \"root_s\": \"['BERT']\", \"root_o\": \"['a Mouth , and It Must Speak : BERT as a Markov Random Field Language Model .']\", \"root_split\": 0, \"root_span0\": 5, \"root_span1\": 9, \"root_cspan0\": 5, \"root_cspan1\": 9, \"fverb\": null, \"fword\": \"Must\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['It', 'Must', 'Speak']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-has-a-Mouth%2C-and-It-Must-Speak%3A-BERT-as-a-Wang-Cho/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\", \"ID\": 12, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.\", \"split_tokens\": \"['BERT', 'has', 'a', 'Mouth', ',', 'and', 'It', 'Must', 'Speak', ':', 'BERT', 'as', 'a', 'Markov', 'Random', 'Field', 'Language', 'Model', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(22, 24)\", \"within_anchor_index\": -1.0}, {\"pc1\": -5.075319637258141, \"pc2\": 0.3037518164857217, \"word\": \"applying\", \"split_0\": \"Following recent successes in applying\", \"split_1\": \"BERT\", \"split_2\": \"to question answering , we explore simple applications to ad hoc document retrieval .\", \"averb\": \"applying\", \"averb_s\": \"[]\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 30.0, \"averb_span1\": 39.0, \"averb_cspan0\": 30.0, \"averb_cspan1\": 39.0, \"root\": \"explore\", \"root_full\": \"explore\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['simple applications']\", \"root_split\": 2, \"root_span0\": 71, \"root_span1\": 79, \"root_cspan0\": 27, \"root_cspan1\": 35, \"fverb\": \"answering\", \"fword\": \"to\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'applying', 'in']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval.\", \"split_tokens\": \"['Following', 'recent', 'successes', 'in', 'applying', 'BERT', 'to', 'question', 'answering', ',', 'we', 'explore', 'simple', 'applications', 'to', 'ad', 'hoc', 'document', 'retrieval', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(38, 42)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.8151083339278165, \"pc2\": -1.7062528676893853, \"word\": \"posed\", \"split_0\": \"This required confronting the challenge posed by documents that are typically longer than the length of input\", \"split_1\": \"BERT\", \"split_2\": \"was designed to handle .\", \"averb\": \"posed\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 40.0, \"averb_span1\": 46.0, \"averb_cspan0\": 40.0, \"averb_cspan1\": 46.0, \"root\": \"required\", \"root_full\": \"required\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 5, \"root_span1\": 14, \"root_cspan0\": 5, \"root_cspan1\": 14, \"fverb\": \"was\", \"fword\": \"was\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'length']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle.\", \"split_tokens\": \"['This', 'required', 'confronting', 'the', 'challenge', 'posed', 'by', 'documents', 'that', 'are', 'typically', 'longer', 'than', 'the', 'length', 'of', 'input', 'BERT', 'was', 'designed', 'to', 'handle', '.']\", \"split_anchor_span\": \"(17, 18)\", \"split_anchor_indices\": \"(109, 113)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.70526335499006, \"pc2\": -1.7109649849805044, \"word\": \"confronting\", \"split_0\": \"This required confronting\", \"split_1\": \"the challenge posed by documents that are typically longer than the length of input BERT was designed to handle\", \"split_2\": \".\", \"averb\": \"confronting\", \"averb_s\": \"[]\", \"averb_o\": \"['the challenge posed by documents that are typically longer than the length of input BERT was designed to handle .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 14.0, \"averb_span1\": 26.0, \"averb_cspan0\": 14.0, \"averb_cspan1\": 26.0, \"root\": \"required\", \"root_full\": \"required\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 5, \"root_span1\": 14, \"root_cspan0\": 5, \"root_cspan1\": 14, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['challenge', 'confronting', 'required']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle.\", \"split_tokens\": \"['This', 'required', 'confronting', 'the', 'challenge', 'posed', 'by', 'documents', 'that', 'are', 'typically', 'longer', 'than', 'the', 'length', 'of', 'input', 'BERT', 'was', 'designed', 'to', 'handle', '.']\", \"split_anchor_span\": \"(3, 22)\", \"split_anchor_indices\": \"(25, 136)\", \"within_anchor_index\": 84.0}, {\"pc1\": -2.7413623816243757, \"pc2\": -0.9236829216105785, \"word\": \"address\", \"split_0\": \"We address\", \"split_1\": \"this issue\", \"split_2\": \"by applying inference on sentences individually , and then aggregating sentence scores to produce document scores .\", \"averb\": \"address\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"scores\", \"root_full\": \"scores\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 129, \"root_span1\": 136, \"root_cspan0\": 107, \"root_cspan1\": 114, \"fverb\": \"applying\", \"fword\": \"by\", \"apos\": \"['NOUN', 'VERB', 'ADV']\", \"apos_w\": \"['issue', 'address', 'then']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-Applications-of-BERT-for-Ad-Hoc-Document-Yang-Zhang/ea57734824426a427f8b9139da1ae574cc929543\", \"ID\": 14, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores.\", \"split_tokens\": \"['We', 'address', 'this', 'issue', 'by', 'applying', 'inference', 'on', 'sentences', 'individually', ',', 'and', 'then', 'aggregating', 'sentence', 'scores', 'to', 'produce', 'document', 'scores', '.']\", \"split_anchor_span\": \"(2, 4)\", \"split_anchor_indices\": \"(10, 20)\", \"within_anchor_index\": -1.0}, {\"pc1\": 0.950334029956379, \"pc2\": 4.427548220971341, \"word\": \"contribute\", \"split_0\": \"BERT - based architectures currently give state - of - the - art performance on many NLP tasks , but little is known about the exact mechanisms that contribute to\", \"split_1\": \"its\", \"split_2\": \"success .\", \"averb\": \"contribute\", \"averb_s\": \"['that']\", \"averb_o\": \"['its .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 149.0, \"averb_span1\": 160.0, \"averb_cspan0\": 149.0, \"averb_cspan1\": 160.0, \"root\": \"currently\", \"root_full\": \"currently\", \"root_s\": \"['BERT - based architectures']\", \"root_o\": \"['give state - of - the - art performance on many NLP tasks , but little is known about the exact mechanisms that contribute to its success .']\", \"root_split\": 0, \"root_span0\": 27, \"root_span1\": 37, \"root_cspan0\": 27, \"root_cspan1\": 37, \"fverb\": null, \"fword\": \"success\", \"apos\": \"['DET', 'VERB', 'NOUN']\", \"apos_w\": \"['its', 'contribute', 'mechanisms']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success.\", \"split_tokens\": \"['BERT', '-', 'based', 'architectures', 'currently', 'give', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'on', 'many', 'NLP', 'tasks', ',', 'but', 'little', 'is', 'known', 'about', 'the', 'exact', 'mechanisms', 'that', 'contribute', 'to', 'its', 'success', '.']\", \"split_anchor_span\": \"(30, 31)\", \"split_anchor_indices\": \"(162, 165)\", \"within_anchor_index\": -1.0}, {\"pc1\": -2.8303950072506736, \"pc2\": -3.6567200142324636, \"word\": \"focus\", \"split_0\": \"In the current work , we focus on the interpretation of self - attention , which is one of the fundamental underlying components of\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"focus\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 25.0, \"averb_span1\": 31.0, \"averb_cspan0\": 25.0, \"averb_cspan1\": 31.0, \"root\": \"focus\", \"root_full\": \"focus\", \"root_s\": \"[',', 'we']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 25, \"root_span1\": 31, \"root_cspan0\": 25, \"root_cspan1\": 31, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'components']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT.\", \"split_tokens\": \"['In', 'the', 'current', 'work', ',', 'we', 'focus', 'on', 'the', 'interpretation', 'of', 'self', '-', 'attention', ',', 'which', 'is', 'one', 'of', 'the', 'fundamental', 'underlying', 'components', 'of', 'BERT', '.']\", \"split_anchor_span\": \"(24, 25)\", \"split_anchor_indices\": \"(131, 135)\", \"within_anchor_index\": 0.0}, {\"pc1\": 2.9610844553715774, \"pc2\": 0.07344112815357859, \"word\": \"encoded\", \"split_0\": \"Using a subset of GLUE tasks and a set of handcrafted features-of-interest , we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual\", \"split_1\": \"BERT\", \"split_2\": \"'s heads .\", \"averb\": \"encoded\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 181.0, \"averb_span1\": 189.0, \"averb_cspan0\": 181.0, \"averb_cspan1\": 189.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we', 'Using a subset of GLUE tasks and a set of handcrafted features - of - interest']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 84, \"root_span1\": 92, \"root_cspan0\": 84, \"root_cspan1\": 92, \"fverb\": null, \"fword\": \"heads\", \"apos\": \"['PROPN', 'ADJ', 'ADP']\", \"apos_w\": \"['BERT', 'individual', 'by']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT's heads.\", \"split_tokens\": \"['Using', 'a', 'subset', 'of', 'GLUE', 'tasks', 'and', 'a', 'set', 'of', 'handcrafted', 'features-of-interest', ',', 'we', 'propose', 'the', 'methodology', 'and', 'carry', 'out', 'a', 'qualitative', 'and', 'quantitative', 'analysis', 'of', 'the', 'information', 'encoded', 'by', 'the', 'individual', 'BERT', \\\"'s\\\", 'heads', '.']\", \"split_anchor_span\": \"(32, 33)\", \"split_anchor_indices\": \"(202, 206)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.541484118848485, \"pc2\": -2.188847309138652, \"word\": \"leads\", \"split_0\": \"We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine - tuned\", \"split_1\": \"BERT\", \"split_2\": \"models\", \"averb\": \"leads\", \"averb_s\": \"['manually disabling attention in certain heads']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 59.0, \"averb_span1\": 65.0, \"averb_cspan0\": 59.0, \"averb_cspan1\": 65.0, \"root\": \"We\", \"root_full\": \"We\", \"root_s\": \"[]\", \"root_o\": \"['show that manually disabling attention in certain heads leads to a performance improvement over the regular fine - tuned BERT models']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": null, \"fword\": \"models\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'models', 'over']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models\", \"split_tokens\": \"['We', 'show', 'that', 'manually', 'disabling', 'attention', 'in', 'certain', 'heads', 'leads', 'to', 'a', 'performance', 'improvement', 'over', 'the', 'regular', 'fine', '-', 'tuned', 'BERT', 'models']\", \"split_anchor_span\": \"(20, 21)\", \"split_anchor_indices\": \"(123, 127)\", \"within_anchor_index\": 0.0}, {\"pc1\": -12.003936361787058, \"pc2\": 0.2774477807543168, \"word\": \"Revealing\", \"split_0\": \"Revealing the Dark Secrets of\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"Revealing\", \"averb_s\": \"[]\", \"averb_o\": \"['the Dark Secrets of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 10.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \"Revealing\", \"root_full\": \"Revealing\", \"root_s\": \"[]\", \"root_o\": \"['the Dark Secrets of BERT']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 10, \"root_cspan0\": 0, \"root_cspan1\": 10, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'PROPN']\", \"apos_w\": \"['BERT', 'of', 'Secrets']\", \"URL\": \"https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103\", \"ID\": 15, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"Revealing the Dark Secrets of BERT.\", \"split_tokens\": \"['Revealing', 'the', 'Dark', 'Secrets', 'of', 'BERT', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(29, 33)\", \"within_anchor_index\": 0.0}, {\"pc1\": -2.080963159939867, \"pc2\": 1.333504745160361, \"word\": \"has been released\", \"split_0\": \"Recently , an upgraded version of\", \"split_1\": \"BERT\", \"split_2\": \"has been released with Whole Word Masking ( WWM ) , which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .\", \"averb\": \"has been released\", \"averb_s\": \"['an upgraded version of BERT', 'Recently ,']\", \"averb_o\": \"['which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 39.0, \"averb_span1\": 57.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 18.0, \"root\": \"released\", \"root_full\": \"has been released\", \"root_s\": \"['an upgraded version of BERT', 'Recently ,']\", \"root_o\": \"['which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .']\", \"root_split\": 2, \"root_span0\": 39, \"root_span1\": 57, \"root_cspan0\": 0, \"root_cspan1\": 18, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'version']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT.\", \"split_tokens\": \"['Recently', ',', 'an', 'upgraded', 'version', 'of', 'BERT', 'has', 'been', 'released', 'with', 'Whole', 'Word', 'Masking', '(', 'WWM', ')', ',', 'which', 'mitigate', 'the', 'drawbacks', 'of', 'masking', 'partial', 'WordPiece', 'tokens', 'in', 'pre', '-', 'training', 'BERT', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(33, 37)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.587610324403281, \"pc2\": 1.655756946611221, \"word\": \"masking\", \"split_0\": \"Recently , an upgraded version of BERT has been released with Whole Word Masking ( WWM ) , which mitigate the drawbacks of masking partial WordPiece tokens in pre - training\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"masking\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 123.0, \"averb_span1\": 131.0, \"averb_cspan0\": 123.0, \"averb_cspan1\": 131.0, \"root\": \"released\", \"root_full\": \"has been released\", \"root_s\": \"['an upgraded version of BERT', 'Recently ,']\", \"root_o\": \"['which mitigate the drawbacks of masking partial WordPiece tokens in pre - training BERT .']\", \"root_split\": 0, \"root_span0\": 39, \"root_span1\": 57, \"root_cspan0\": 39, \"root_cspan1\": 57, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADJ', 'ADJ']\", \"apos_w\": \"['BERT', 'training', 'pre']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT.\", \"split_tokens\": \"['Recently', ',', 'an', 'upgraded', 'version', 'of', 'BERT', 'has', 'been', 'released', 'with', 'Whole', 'Word', 'Masking', '(', 'WWM', ')', ',', 'which', 'mitigate', 'the', 'drawbacks', 'of', 'masking', 'partial', 'WordPiece', 'tokens', 'in', 'pre', '-', 'training', 'BERT', '.']\", \"split_anchor_span\": \"(31, 32)\", \"split_anchor_indices\": \"(173, 177)\", \"within_anchor_index\": 0.0}, {\"pc1\": -10.8345354574274, \"pc2\": -0.036500792856450574, \"word\": \"was trained\", \"split_0\": null, \"split_1\": \"The model\", \"split_2\": \"was trained on the latest Chinese Wikipedia dump .\", \"averb\": \"was trained\", \"averb_s\": \"['The model']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 10.0, \"averb_span1\": 22.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \"trained\", \"root_full\": \"was trained\", \"root_s\": \"['The model']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 10, \"root_span1\": 22, \"root_cspan0\": 0, \"root_cspan1\": 12, \"fverb\": \"was\", \"fword\": \"was\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['model', 'trained']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"The model was trained on the latest Chinese Wikipedia dump.\", \"split_tokens\": \"['The', 'model', 'was', 'trained', 'on', 'the', 'latest', 'Chinese', 'Wikipedia', 'dump', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 9)\", \"within_anchor_index\": -1.0}, {\"pc1\": -3.6864381267678685, \"pc2\": -1.0704100399961036, \"word\": \"to provide\", \"split_0\": \"We aim to provide easy extensibility and better performance for\", \"split_1\": \"Chinese BERT\", \"split_2\": \"without changing any neural architecture or even hyper - parameters .\", \"averb\": \"to provide\", \"averb_s\": \"[]\", \"averb_o\": \"['easy', 'extensibility and', 'better performance']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 7.0, \"averb_span1\": 18.0, \"averb_cspan0\": 7.0, \"averb_cspan1\": 18.0, \"root\": \"aim\", \"root_full\": \"aim\", \"root_s\": \"['We']\", \"root_o\": \"['.']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 7, \"root_cspan0\": 3, \"root_cspan1\": 7, \"fverb\": \"changing\", \"fword\": \"without\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'for', 'provide']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"We aim to provide easy extensibility and better performance for Chinese BERT without changing any neural architecture or even hyper-parameters.\", \"split_tokens\": \"['We', 'aim', 'to', 'provide', 'easy', 'extensibility', 'and', 'better', 'performance', 'for', 'Chinese', 'BERT', 'without', 'changing', 'any', 'neural', 'architecture', 'or', 'even', 'hyper', '-', 'parameters', '.']\", \"split_anchor_span\": \"(10, 12)\", \"split_anchor_indices\": \"(63, 75)\", \"within_anchor_index\": 8.0}, {\"pc1\": 30.126886454382813, \"pc2\": 25.09596729111159, \"word\": \"is verified\", \"split_0\": null, \"split_1\": \"The model\", \"split_2\": \"is verified on various NLP tasks , across sentence - level to document - level , including sentiment classification ( ChnSentiCorp , Sina Weibo ) , named entity recognition ( People Daily , MSRA - NER ) , natural language inference ( XNLI ) , sentence pair matching ( LCQMC , BQ Corpus ) , and machine reading comprehension ( CMRC 2018 , DRCD , CAIL RC ) .\", \"averb\": \"is verified\", \"averb_s\": \"['The model']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 10.0, \"averb_span1\": 22.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 12.0, \"root\": \",\", \"root_full\": \",\", \"root_s\": \"['The model is verified on various NLP tasks , across sentence - level to document - level , including sentiment classification ( ChnSentiCorp , Sina Weibo ) , named entity recognition ( People Daily , MSRA - NER ) , natural language inference ( XNLI ) , sentence pair matching ( LCQMC , BQ Corpus ) , and machine reading comprehension ( CMRC 2018 ,', 'DRCD']\", \"root_o\": \"['CAIL RC ) .']\", \"root_split\": 2, \"root_span0\": 353, \"root_span1\": 355, \"root_cspan0\": 343, \"root_cspan1\": 345, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['NOUN', 'VERB', 'NOUN']\", \"apos_w\": \"['model', 'verified', 'inference']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"The model is verified on various NLP tasks, across sentence-level to document-level, including sentiment classification (ChnSentiCorp, Sina Weibo), named entity recognition (People Daily, MSRA-NER), natural language inference (XNLI), sentence pair matching (LCQMC, BQ Corpus), and machine reading comprehension (CMRC 2018, DRCD, CAIL RC).\", \"split_tokens\": \"['The', 'model', 'is', 'verified', 'on', 'various', 'NLP', 'tasks', ',', 'across', 'sentence', '-', 'level', 'to', 'document', '-', 'level', ',', 'including', 'sentiment', 'classification', '(', 'ChnSentiCorp', ',', 'Sina', 'Weibo', ')', ',', 'named', 'entity', 'recognition', '(', 'People', 'Daily', ',', 'MSRA', '-', 'NER', ')', ',', 'natural', 'language', 'inference', '(', 'XNLI', ')', ',', 'sentence', 'pair', 'matching', '(', 'LCQMC', ',', 'BQ', 'Corpus', ')', ',', 'and', 'machine', 'reading', 'comprehension', '(', 'CMRC', '2018', ',', 'DRCD', ',', 'CAIL', 'RC', ')', '.']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 9)\", \"within_anchor_index\": -1.0}, {\"pc1\": -4.939802293585376, \"pc2\": 0.44718686064531693, \"word\": \"examine\", \"split_0\": \"Moreover , we also examine the effectiveness of Chinese pre - trained models :\", \"split_1\": \"BERT\", \"split_2\": \", ERNIE , BERT - wwm .\", \"averb\": \"examine\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 27.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 27.0, \"root\": \"examine\", \"root_full\": \"examine\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": \"ERNIE\", \"apos\": \"['PROPN', 'PUNCT', 'NOUN']\", \"apos_w\": \"['BERT', ':', 'models']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"Moreover, we also examine the effectiveness of Chinese pre-trained models: BERT, ERNIE, BERT-wwm.\", \"split_tokens\": \"['Moreover', ',', 'we', 'also', 'examine', 'the', 'effectiveness', 'of', 'Chinese', 'pre', '-', 'trained', 'models', ':', 'BERT', ',', 'ERNIE', ',', 'BERT', '-', 'wwm', '.']\", \"split_anchor_span\": \"(14, 15)\", \"split_anchor_indices\": \"(78, 82)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.939802293585376, \"pc2\": 0.44718686064531693, \"word\": \"examine\", \"split_0\": \"Moreover , we also examine the effectiveness of Chinese pre - trained models : BERT , ERNIE ,\", \"split_1\": \"BERT - wwm\", \"split_2\": \".\", \"averb\": \"examine\", \"averb_s\": \"[',', 'we']\", \"averb_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 19.0, \"averb_span1\": 27.0, \"averb_cspan0\": 19.0, \"averb_cspan1\": 27.0, \"root\": \"examine\", \"root_full\": \"examine\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['the effectiveness of Chinese pre - trained models : BERT , ERNIE , BERT - wwm .']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'PUNCT', 'PROPN']\", \"apos_w\": \"['wwm', ',', 'ERNIE']\", \"URL\": \"https://www.semanticscholar.org/paper/Pre-Training-with-Whole-Word-Masking-for-Chinese-Cui-Che/2ff41a463a374b138bb5a012e5a32bc4beefec20\", \"ID\": 16, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"Moreover, we also examine the effectiveness of Chinese pre-trained models: BERT, ERNIE, BERT-wwm.\", \"split_tokens\": \"['Moreover', ',', 'we', 'also', 'examine', 'the', 'effectiveness', 'of', 'Chinese', 'pre', '-', 'trained', 'models', ':', 'BERT', ',', 'ERNIE', ',', 'BERT', '-', 'wwm', '.']\", \"split_anchor_span\": \"(18, 21)\", \"split_anchor_indices\": \"(93, 103)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.427530779076181, \"pc2\": -4.3785054721896195, \"word\": \"BERT\", \"split_0\": \"However , previous work trains\", \"split_1\": \"BERT\", \"split_2\": \"by viewing passages corresponding to the same question as independent training instances , which may cause incomparable scores for answers from different passages .\", \"averb\": \"BERT\", \"averb_s\": \"['previous work trains']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 31.0, \"averb_span1\": 36.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 5.0, \"root\": \"cause\", \"root_full\": \"may cause\", \"root_s\": \"['However', ',', 'which', 'previous work trains BERT by viewing passages corresponding to the same question as independent training instances ,']\", \"root_o\": \"['incomparable scores']\", \"root_split\": 2, \"root_span0\": 133, \"root_span1\": 143, \"root_cspan0\": 97, \"root_cspan1\": 107, \"fverb\": \"viewing\", \"fword\": \"by\", \"apos\": \"['VERB', 'VERB']\", \"apos_w\": \"['BERT', 'cause']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages.\", \"split_tokens\": \"['However', ',', 'previous', 'work', 'trains', 'BERT', 'by', 'viewing', 'passages', 'corresponding', 'to', 'the', 'same', 'question', 'as', 'independent', 'training', 'instances', ',', 'which', 'may', 'cause', 'incomparable', 'scores', 'for', 'answers', 'from', 'different', 'passages', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(30, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.5954998838839765, \"pc2\": 2.030175467352342, \"word\": \"propose\", \"split_0\": \"To tackle this issue , we propose\", \"split_1\": \"a multi - passage BERT model to globally normalize answer scores across all passages of the same question\", \"split_2\": \", and this change enables our QA model find better answers by utilizing more passages .\", \"averb\": \"propose\", \"averb_s\": \"['To', 'we', 'tackle this issue']\", \"averb_o\": \"['a multi - passage BERT model to globally normalize answer scores across all passages of the same question ,', 'this change enables our QA model find better answers by utilizing more passages .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 26.0, \"averb_span1\": 34.0, \"averb_cspan0\": 26.0, \"averb_cspan1\": 34.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['To', 'we', 'tackle this issue']\", \"root_o\": \"['a multi - passage BERT model to globally normalize answer scores across all passages of the same question ,', 'this change enables our QA model find better answers by utilizing more passages .']\", \"root_split\": 0, \"root_span0\": 26, \"root_span1\": 34, \"root_cspan0\": 26, \"root_cspan1\": 34, \"fverb\": \"enables\", \"fword\": \"and\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['model', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages.\", \"split_tokens\": \"['To', 'tackle', 'this', 'issue', ',', 'we', 'propose', 'a', 'multi', '-', 'passage', 'BERT', 'model', 'to', 'globally', 'normalize', 'answer', 'scores', 'across', 'all', 'passages', 'of', 'the', 'same', 'question', ',', 'and', 'this', 'change', 'enables', 'our', 'QA', 'model', 'find', 'better', 'answers', 'by', 'utilizing', 'more', 'passages', '.']\", \"split_anchor_span\": \"(7, 25)\", \"split_anchor_indices\": \"(33, 138)\", \"within_anchor_index\": 18.0}, {\"pc1\": -6.617648089480857, \"pc2\": 0.7988297507750112, \"word\": \"gains\", \"split_0\": \"By leveraging a passage ranker to select high-quality passages , multi-passage\", \"split_1\": \"BERT\", \"split_2\": \"gains additional 2 % .\", \"averb\": \"gains\", \"averb_s\": \"['BERT .']\", \"averb_o\": \"['additional 2 %']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 88.0, \"averb_span1\": 94.0, \"averb_cspan0\": 4.0, \"averb_cspan1\": 10.0, \"root\": \"By\", \"root_full\": \"By\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 3, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": null, \"fword\": \"gains\", \"apos\": \"['PROPN', 'VERB', 'ADJ']\", \"apos_w\": \"['BERT', 'gains', 'passage']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%.\", \"split_tokens\": \"['By', 'leveraging', 'a', 'passage', 'ranker', 'to', 'select', 'high-quality', 'passages', ',', 'multi-passage', 'BERT', 'gains', 'additional', '2', '%', '.']\", \"split_anchor_span\": \"(11, 12)\", \"split_anchor_indices\": \"(78, 82)\", \"within_anchor_index\": 0.0}, {\"pc1\": -2.962434540044031, \"pc2\": -0.403881893904544, \"word\": \"showed\", \"split_0\": \"Experiments on four standard benchmarks showed that\", \"split_1\": \"our multi - passage BERT\", \"split_2\": \"outperforms all state - of - the - art models on all benchmarks .\", \"averb\": \"showed\", \"averb_s\": \"['Experiments on four standard benchmarks']\", \"averb_o\": \"['that our multi - passage BERT outperforms all state - of - the - art models on all benchmarks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 40.0, \"averb_span1\": 47.0, \"averb_cspan0\": 40.0, \"averb_cspan1\": 47.0, \"root\": \"showed\", \"root_full\": \"showed\", \"root_s\": \"['Experiments on four standard benchmarks']\", \"root_o\": \"['that our multi - passage BERT outperforms all state - of - the - art models on all benchmarks']\", \"root_split\": 0, \"root_span0\": 40, \"root_span1\": 47, \"root_cspan0\": 40, \"root_cspan1\": 47, \"fverb\": \"outperforms\", \"fword\": \"outperforms\", \"apos\": \"['PROPN', 'VERB']\", \"apos_w\": \"['BERT', 'showed']\", \"URL\": \"https://www.semanticscholar.org/paper/Multi-passage-BERT%3A-A-Globally-Normalized-BERT-for-Wang-Ng/0cf535110808d33fdf4db3ffa1621dea16e29c0d\", \"ID\": 17, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks.\", \"split_tokens\": \"['Experiments', 'on', 'four', 'standard', 'benchmarks', 'showed', 'that', 'our', 'multi', '-', 'passage', 'BERT', 'outperforms', 'all', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'on', 'all', 'benchmarks', '.']\", \"split_anchor_span\": \"(7, 12)\", \"split_anchor_indices\": \"(51, 75)\", \"within_anchor_index\": 20.0}, {\"pc1\": -8.46077007030605, \"pc2\": -2.4951080955875455, \"word\": \"exploring\", \"split_0\": \"However , there has not been much effort on exploring\", \"split_1\": \"BERT\", \"split_2\": \"for natural language understanding .\", \"averb\": \"exploring\", \"averb_s\": \"[]\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 44.0, \"averb_span1\": 54.0, \"averb_cspan0\": 44.0, \"averb_cspan1\": 54.0, \"root\": \"much\", \"root_full\": \"has not been much\", \"root_s\": \"['However', ',', 'there']\", \"root_o\": \"['effort on exploring BERT for natural language understanding']\", \"root_split\": 0, \"root_span0\": 16, \"root_span1\": 34, \"root_cspan0\": 16, \"root_cspan1\": 34, \"fverb\": null, \"fword\": \"for\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'exploring', 'on']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"However, there has not been much effort on exploring BERT for natural language understanding.\", \"split_tokens\": \"['However', ',', 'there', 'has', 'not', 'been', 'much', 'effort', 'on', 'exploring', 'BERT', 'for', 'natural', 'language', 'understanding', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(53, 57)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.763883663249279, \"pc2\": 0.06276435322332387, \"word\": \"based\", \"split_0\": \"In this work , we propose a joint intent classification and slot filling model based on\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"based\", \"averb_s\": \"['a joint intent classification and slot filling model']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 79.0, \"averb_span1\": 85.0, \"averb_cspan0\": 79.0, \"averb_cspan1\": 85.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we']\", \"root_o\": \"['a joint intent classification and slot filling model based on BERT']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 26, \"root_cspan0\": 18, \"root_cspan1\": 26, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'on', 'based']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"In this work, we propose a joint intent classification and slot filling model based on BERT.\", \"split_tokens\": \"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'joint', 'intent', 'classification', 'and', 'slot', 'filling', 'model', 'based', 'on', 'BERT', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(87, 91)\", \"within_anchor_index\": 0.0}, {\"pc1\": -6.763883663249279, \"pc2\": 0.06276435322332388, \"word\": \"based\", \"split_0\": \"In this work , we propose\", \"split_1\": \"a joint intent classification and slot filling model based on BERT\", \"split_2\": \".\", \"averb\": \"based\", \"averb_s\": \"['a joint intent classification and slot filling model']\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 79.0, \"averb_span1\": 85.0, \"averb_cspan0\": 53.0, \"averb_cspan1\": 59.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we']\", \"root_o\": \"['a joint intent classification and slot filling model based on BERT']\", \"root_split\": 0, \"root_span0\": 18, \"root_span1\": 26, \"root_cspan0\": 18, \"root_cspan1\": 26, \"fverb\": null, \"fword\": null, \"apos\": \"['VERB', 'VERB']\", \"apos_w\": \"['based', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"In this work, we propose a joint intent classification and slot filling model based on BERT.\", \"split_tokens\": \"['In', 'this', 'work', ',', 'we', 'propose', 'a', 'joint', 'intent', 'classification', 'and', 'slot', 'filling', 'model', 'based', 'on', 'BERT', '.']\", \"split_anchor_span\": \"(6, 17)\", \"split_anchor_indices\": \"(25, 91)\", \"within_anchor_index\": 62.0}, {\"pc1\": 12.902333796934148, \"pc2\": -0.04329447778448207, \"word\": \"achieves\", \"split_0\": \"Experimental results demonstrate that\", \"split_1\": \"our proposed model\", \"split_2\": \"achieves significant improvement on intent classification accuracy , slot filling F1 , and sentence - level semantic frame accuracy on several public benchmark datasets , compared to the attention - based recurrent neural network models and slot - gated models\", \"averb\": \"achieves\", \"averb_s\": \"['our proposed model']\", \"averb_o\": \"['significant improvement']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 57.0, \"averb_span1\": 66.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['Experimental results']\", \"root_o\": \"['and sentence - level semantic frame accuracy on several public benchmark datasets , compared to the attention - based recurrent neural network models and slot - gated models']\", \"root_split\": 0, \"root_span0\": 21, \"root_span1\": 33, \"root_cspan0\": 21, \"root_cspan1\": 33, \"fverb\": \"achieves\", \"fword\": \"achieves\", \"apos\": \"['NOUN', 'VERB', 'VERB']\", \"apos_w\": \"['model', 'achieves', 'demonstrate']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-for-Joint-Intent-Classification-and-Slot-Chen-Zhuo/476029ac9be26bf7f121a388f5c1e45d204efe52\", \"ID\": 18, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models\", \"split_tokens\": \"['Experimental', 'results', 'demonstrate', 'that', 'our', 'proposed', 'model', 'achieves', 'significant', 'improvement', 'on', 'intent', 'classification', 'accuracy', ',', 'slot', 'filling', 'F1', ',', 'and', 'sentence', '-', 'level', 'semantic', 'frame', 'accuracy', 'on', 'several', 'public', 'benchmark', 'datasets', ',', 'compared', 'to', 'the', 'attention', '-', 'based', 'recurrent', 'neural', 'network', 'models', 'and', 'slot', '-', 'gated', 'models']\", \"split_anchor_span\": \"(4, 7)\", \"split_anchor_indices\": \"(37, 55)\", \"within_anchor_index\": -1.0}, {\"pc1\": 0.4883453303504102, \"pc2\": 4.089905466709921, \"word\": \"built\", \"split_0\": \"It enables seamless integration of conversation history into a conversational question answering ( ConvQA ) model built on\", \"split_1\": \"BERT\", \"split_2\": \"( Bidirectional Encoder Representations from Transformers ) .\", \"averb\": \"built\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 114.0, \"averb_span1\": 120.0, \"averb_cspan0\": 114.0, \"averb_cspan1\": 120.0, \"root\": \"history\", \"root_full\": \"history\", \"root_s\": \"['It enables', 'seamless integration of']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 48, \"root_span1\": 56, \"root_cspan0\": 48, \"root_cspan1\": 56, \"fverb\": null, \"fword\": \"Bidirectional\", \"apos\": \"['PROPN', 'ADP', 'VERB']\", \"apos_w\": \"['BERT', 'on', 'built']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921\", \"ID\": 19, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"It enables seamless integration of conversation history into a conversational question answering (ConvQA) model built on BERT (Bidirectional Encoder Representations from Transformers).\", \"split_tokens\": \"['It', 'enables', 'seamless', 'integration', 'of', 'conversation', 'history', 'into', 'a', 'conversational', 'question', 'answering', '(', 'ConvQA', ')', 'model', 'built', 'on', 'BERT', '(', 'Bidirectional', 'Encoder', 'Representations', 'from', 'Transformers', ')', '.']\", \"split_anchor_span\": \"(18, 19)\", \"split_anchor_indices\": \"(122, 126)\", \"within_anchor_index\": 0.0}, {\"pc1\": -9.167935595781444, \"pc2\": 1.7434287275061866, \"word\": \"BERT\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"with History Answer Embedding for Conversational Question Answering .\", \"averb\": \"BERT\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": 0.0, \"averb_split\": 1.0, \"averb_span0\": 0.0, \"averb_span1\": 5.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 5.0, \"root\": \"BERT\", \"root_full\": \"BERT\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 1, \"root_span0\": 0, \"root_span1\": 5, \"root_cspan0\": 0, \"root_cspan1\": 5, \"fverb\": null, \"fword\": \"with\", \"apos\": \"['VERB']\", \"apos_w\": \"['BERT']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-with-History-Answer-Embedding-for-Question-Qu-Yang/8495c1722e1f5107733c842839c2d298b9116921\", \"ID\": 19, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT with History Answer Embedding for Conversational Question Answering.\", \"split_tokens\": \"['BERT', 'with', 'History', 'Answer', 'Embedding', 'for', 'Conversational', 'Question', 'Answering', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -8.338105052426505, \"pc2\": -1.153648468290139, \"word\": \"studies\", \"split_0\": \"This paper studies the performances and behaviors of\", \"split_1\": \"BERT\", \"split_2\": \"in ranking tasks .\", \"averb\": \"studies\", \"averb_s\": \"['This']\", \"averb_o\": \"['the performances and behaviors of BERT in ranking tasks', '.']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 11.0, \"averb_span1\": 19.0, \"averb_cspan0\": 11.0, \"averb_cspan1\": 19.0, \"root\": \"studies\", \"root_full\": \"studies\", \"root_s\": \"['This']\", \"root_o\": \"['the performances and behaviors of BERT in ranking tasks', '.']\", \"root_split\": 0, \"root_span0\": 11, \"root_span1\": 19, \"root_cspan0\": 11, \"root_cspan1\": 19, \"fverb\": \"ranking\", \"fword\": \"in\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'behaviors']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"This paper studies the performances and behaviors of BERT in ranking tasks.\", \"split_tokens\": \"['This', 'paper', 'studies', 'the', 'performances', 'and', 'behaviors', 'of', 'BERT', 'in', 'ranking', 'tasks', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(52, 56)\", \"within_anchor_index\": 0.0}, {\"pc1\": 0.24849999262515704, \"pc2\": 5.322164713180493, \"word\": \"to leverage\", \"split_0\": \"We explore several different ways to leverage\", \"split_1\": \"the pre - trained BERT\", \"split_2\": \"and fine - tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document ranking .\", \"averb\": \"to leverage\", \"averb_s\": \"[]\", \"averb_o\": \"['the pre - trained BERT and', 'fine - tune it on two ranking tasks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 34.0, \"averb_span1\": 46.0, \"averb_cspan0\": 34.0, \"averb_cspan1\": 46.0, \"root\": \"ranking\", \"root_full\": \"ranking\", \"root_s\": \"['We explore several different ways to leverage the pre - trained BERT and fine - tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document']\", \"root_o\": \"['.']\", \"root_split\": 2, \"root_span0\": 173, \"root_span1\": 181, \"root_cspan0\": 104, \"root_cspan1\": 112, \"fverb\": \"ranking\", \"fword\": \"and\", \"apos\": \"['NOUN', 'VERB', 'NOUN']\", \"apos_w\": \"['BERT', 'leverage', 'ways']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking.\", \"split_tokens\": \"['We', 'explore', 'several', 'different', 'ways', 'to', 'leverage', 'the', 'pre', '-', 'trained', 'BERT', 'and', 'fine', '-', 'tune', 'it', 'on', 'two', 'ranking', 'tasks', ':', 'MS', 'MARCO', 'passage', 'reranking', 'and', 'TREC', 'Web', 'Track', 'ad', 'hoc', 'document', 'ranking', '.']\", \"split_anchor_span\": \"(7, 12)\", \"split_anchor_indices\": \"(45, 67)\", \"within_anchor_index\": 18.0}, {\"pc1\": 0.24849999262515737, \"pc2\": 5.322164713180493, \"word\": \"to leverage\", \"split_0\": \"We explore several different ways to leverage the pre - trained BERT and fine - tune\", \"split_1\": \"it\", \"split_2\": \"on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document ranking .\", \"averb\": \"to leverage\", \"averb_s\": \"[]\", \"averb_o\": \"['the pre - trained BERT and', 'fine - tune it on two ranking tasks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 34.0, \"averb_span1\": 46.0, \"averb_cspan0\": 34.0, \"averb_cspan1\": 46.0, \"root\": \"ranking\", \"root_full\": \"ranking\", \"root_s\": \"['We explore several different ways to leverage the pre - trained BERT and fine - tune it on two ranking tasks : MS MARCO passage reranking and TREC Web Track ad hoc document']\", \"root_o\": \"['.']\", \"root_split\": 2, \"root_span0\": 173, \"root_span1\": 181, \"root_cspan0\": 85, \"root_cspan1\": 93, \"fverb\": \"ranking\", \"fword\": \"on\", \"apos\": \"['PRON', 'NOUN', 'VERB']\", \"apos_w\": \"['it', 'tune', 'leverage']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking.\", \"split_tokens\": \"['We', 'explore', 'several', 'different', 'ways', 'to', 'leverage', 'the', 'pre', '-', 'trained', 'BERT', 'and', 'fine', '-', 'tune', 'it', 'on', 'two', 'ranking', 'tasks', ':', 'MS', 'MARCO', 'passage', 'reranking', 'and', 'TREC', 'Web', 'Track', 'ad', 'hoc', 'document', 'ranking', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(84, 86)\", \"within_anchor_index\": -1.0}, {\"pc1\": 5.231438102494101, \"pc2\": -2.1735320521616974, \"word\": \"demonstrate\", \"split_0\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of\", \"split_1\": \"BERT\", \"split_2\": \"in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .\", \"averb\": \"demonstrate\", \"averb_s\": \"['Experimental results on MS MARCO', 'the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 33.0, \"averb_span1\": 45.0, \"averb_cspan0\": 33.0, \"averb_cspan1\": 45.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['Experimental results on MS MARCO', 'the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 33, \"root_span1\": 45, \"root_cspan0\": 33, \"root_cspan1\": 45, \"fverb\": \"ranking\", \"fword\": \"in\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'effectiveness']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model.\", \"split_tokens\": \"['Experimental', 'results', 'on', 'MS', 'MARCO', 'demonstrate', 'the', 'strong', 'effectiveness', 'of', 'BERT', 'in', 'question', '-', 'answering', 'focused', 'passage', 'ranking', 'tasks', ',', 'as', 'well', 'as', 'the', 'fact', 'that', 'BERT', 'is', 'a', 'strong', 'interaction', '-', 'based', 'seq2seq', 'matching', 'model', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(72, 76)\", \"within_anchor_index\": 0.0}, {\"pc1\": 5.283198185174737, \"pc2\": -2.143684290718796, \"word\": \"answering\", \"split_0\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that\", \"split_1\": \"BERT\", \"split_2\": \"is a strong interaction - based seq2seq matching model .\", \"averb\": \"answering\", \"averb_s\": \"[]\", \"averb_o\": \"['focused passage ranking tasks']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 92.0, \"averb_span1\": 102.0, \"averb_cspan0\": 92.0, \"averb_cspan1\": 102.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['Experimental results on MS MARCO', 'the strong effectiveness of BERT in question - answering focused passage ranking tasks , as well as the fact that BERT is a strong interaction - based seq2seq matching model .']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 33, \"root_span1\": 45, \"root_cspan0\": 33, \"root_cspan1\": 45, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'NOUN', 'NOUN']\", \"apos_w\": \"['BERT', 'model', 'fact']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model.\", \"split_tokens\": \"['Experimental', 'results', 'on', 'MS', 'MARCO', 'demonstrate', 'the', 'strong', 'effectiveness', 'of', 'BERT', 'in', 'question', '-', 'answering', 'focused', 'passage', 'ranking', 'tasks', ',', 'as', 'well', 'as', 'the', 'fact', 'that', 'BERT', 'is', 'a', 'strong', 'interaction', '-', 'based', 'seq2seq', 'matching', 'model', '.']\", \"split_anchor_span\": \"(26, 27)\", \"split_anchor_indices\": \"(158, 162)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.4704386179186093, \"pc2\": -1.6843425366057252, \"word\": \"pre\", \"split_0\": \"Experimental results on TREC show the gaps between the\", \"split_1\": \"BERT\", \"split_2\": \"pre-trained on surrounding contexts and the needs of ad hoc document ranking .\", \"averb\": \"pre\", \"averb_s\": \"['the', 'BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 60.0, \"averb_span1\": 64.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 4.0, \"root\": \"results\", \"root_full\": \"results\", \"root_s\": \"['Experimental']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 13, \"root_span1\": 21, \"root_cspan0\": 13, \"root_cspan1\": 21, \"fverb\": \"surrounding\", \"fword\": \"pre-trained\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'pre', 'between']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking.\", \"split_tokens\": \"['Experimental', 'results', 'on', 'TREC', 'show', 'the', 'gaps', 'between', 'the', 'BERT', 'pre-trained', 'on', 'surrounding', 'contexts', 'and', 'the', 'needs', 'of', 'ad', 'hoc', 'document', 'ranking', '.']\", \"split_anchor_span\": \"(9, 10)\", \"split_anchor_indices\": \"(54, 58)\", \"within_anchor_index\": 0.0}, {\"pc1\": 7.32306508731113, \"pc2\": 0.05053167935878067, \"word\": \"allocates\", \"split_0\": \"Analyses illustrate how\", \"split_1\": \"BERT\", \"split_2\": \"allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"allocates\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 29.0, \"averb_span1\": 39.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"allocates\", \"fword\": \"allocates\", \"apos\": \"['PROPN', 'VERB', 'VERB']\", \"apos_w\": \"['BERT', 'allocates', 'illustrate']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(3, 4)\", \"split_anchor_indices\": \"(23, 27)\", \"within_anchor_index\": 0.0}, {\"pc1\": 7.32306508731113, \"pc2\": 0.05053167935878067, \"word\": \"allocates\", \"split_0\": \"Analyses illustrate how BERT allocates\", \"split_1\": \"its\", \"split_2\": \"attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"allocates\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 29.0, \"averb_span1\": 39.0, \"averb_cspan0\": 29.0, \"averb_cspan1\": 39.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"prefers\", \"fword\": \"attentions\", \"apos\": \"['DET', 'NOUN', 'VERB']\", \"apos_w\": \"['its', 'attentions', 'allocates']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(38, 41)\", \"within_anchor_index\": -1.0}, {\"pc1\": 7.323065087311128, \"pc2\": 0.05053167935878114, \"word\": \"allocates\", \"split_0\": \"Analyses illustrate how BERT allocates its attentions between query - document tokens in\", \"split_1\": \"its\", \"split_2\": \"Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"allocates\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 29.0, \"averb_span1\": 39.0, \"averb_cspan0\": 29.0, \"averb_cspan1\": 39.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"prefers\", \"fword\": \"Transformer\", \"apos\": \"['DET', 'NOUN', 'ADP']\", \"apos_w\": \"['its', 'layers', 'in']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(13, 14)\", \"split_anchor_indices\": \"(88, 91)\", \"within_anchor_index\": -1.0}, {\"pc1\": 7.265708559735524, \"pc2\": -0.04350704157069497, \"word\": \"prefers\", \"split_0\": \"Analyses illustrate how BERT allocates its attentions between query - document tokens in its Transformer layers , how\", \"split_1\": \"it\", \"split_2\": \"prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker\", \"averb\": \"prefers\", \"averb_s\": \"['it']\", \"averb_o\": \"['semantic', 'how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 121.0, \"averb_span1\": 129.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"illustrate\", \"root_full\": \"illustrate\", \"root_s\": \"['Analyses']\", \"root_o\": \"['BERT allocates its attentions between query - document tokens in its Transformer layers , how it prefers semantic matches between paraphrase tokens , and how that differs with the soft match patterns learned by a click - trained neural ranker']\", \"root_split\": 0, \"root_span0\": 9, \"root_span1\": 20, \"root_cspan0\": 9, \"root_cspan1\": 20, \"fverb\": \"prefers\", \"fword\": \"prefers\", \"apos\": \"['PRON', 'VERB', 'NOUN']\", \"apos_w\": \"['it', 'prefers', 'document']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker\", \"split_tokens\": \"['Analyses', 'illustrate', 'how', 'BERT', 'allocates', 'its', 'attentions', 'between', 'query', '-', 'document', 'tokens', 'in', 'its', 'Transformer', 'layers', ',', 'how', 'it', 'prefers', 'semantic', 'matches', 'between', 'paraphrase', 'tokens', ',', 'and', 'how', 'that', 'differs', 'with', 'the', 'soft', 'match', 'patterns', 'learned', 'by', 'a', 'click', '-', 'trained', 'neural', 'ranker']\", \"split_anchor_span\": \"(18, 19)\", \"split_anchor_indices\": \"(117, 119)\", \"within_anchor_index\": -1.0}, {\"pc1\": -10.582087680767938, \"pc2\": -0.08917344394066132, \"word\": \"Understanding\", \"split_0\": \"Understanding the Behaviors of\", \"split_1\": \"BERT\", \"split_2\": \"in Ranking .\", \"averb\": \"Understanding\", \"averb_s\": \"[]\", \"averb_o\": \"['the Behaviors of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 14.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 14.0, \"root\": \"Understanding\", \"root_full\": \"Understanding\", \"root_s\": \"[]\", \"root_o\": \"['the Behaviors of BERT']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 14, \"root_cspan0\": 0, \"root_cspan1\": 14, \"fverb\": null, \"fword\": \"in\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'Behaviors']\", \"URL\": \"https://www.semanticscholar.org/paper/Understanding-the-Behaviors-of-BERT-in-Ranking-Qiao-Xiong/ff580d712ba7e2ad7b60be0f1b1f67d2ab8333da\", \"ID\": 20, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"Understanding the Behaviors of BERT in Ranking.\", \"split_tokens\": \"['Understanding', 'the', 'Behaviors', 'of', 'BERT', 'in', 'Ranking', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(30, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.8260630044741915, \"pc2\": -2.3000621667402705, \"word\": \"achieve\", \"split_0\": \"In this paper , extensive experiments on datasets for these two tasks show that without using any external features , a simple\", \"split_1\": \"BERT\", \"split_2\": \"-based model can achieve state-of-the-art performance .\", \"averb\": \"achieve\", \"averb_s\": \"['can']\", \"averb_o\": \"['any external features , a simple BERT -based model state of - the - art performance', '-']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 149.0, \"averb_span1\": 157.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 25.0, \"root\": \"show\", \"root_full\": \"show\", \"root_s\": \"['these two tasks']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 70, \"root_span1\": 75, \"root_cspan0\": 70, \"root_cspan1\": 75, \"fverb\": \"-based\", \"fword\": \"based\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'model', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b\", \"ID\": 21, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'extensive', 'experiments', 'on', 'datasets', 'for', 'these', 'two', 'tasks', 'show', 'that', 'without', 'using', 'any', 'external', 'features', ',', 'a', 'simple', 'BERT', '-based', 'model', 'can', 'achieve', 'state-of-the-art', 'performance', '.']\", \"split_anchor_span\": \"(22, 23)\", \"split_anchor_indices\": \"(126, 130)\", \"within_anchor_index\": 0.0}, {\"pc1\": -9.266181393328676, \"pc2\": -1.0293503018983647, \"word\": \"to apply\", \"split_0\": \"To our knowledge , we are the first to successfully apply\", \"split_1\": \"BERT\", \"split_2\": \"in this manner .\", \"averb\": \"to apply\", \"averb_s\": \"[]\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 36.0, \"averb_span1\": 58.0, \"averb_cspan0\": 36.0, \"averb_cspan1\": 58.0, \"root\": \"first\", \"root_full\": \"are first\", \"root_s\": \"['we']\", \"root_o\": \"['to successfully apply BERT in this manner', '.']\", \"root_split\": 0, \"root_span0\": 22, \"root_span1\": 36, \"root_cspan0\": 22, \"root_cspan1\": 36, \"fverb\": null, \"fword\": \"in\", \"apos\": \"['PROPN', 'VERB', 'ADJ']\", \"apos_w\": \"['BERT', 'apply', 'first']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b\", \"ID\": 21, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"To our knowledge, we are the first to successfully apply BERT in this manner.\", \"split_tokens\": \"['To', 'our', 'knowledge', ',', 'we', 'are', 'the', 'first', 'to', 'successfully', 'apply', 'BERT', 'in', 'this', 'manner', '.']\", \"split_anchor_span\": \"(11, 12)\", \"split_anchor_indices\": \"(57, 61)\", \"within_anchor_index\": 0.0}, {\"pc1\": -9.63198013517815, \"pc2\": -0.9040185438857444, \"word\": \"provide\", \"split_0\": null, \"split_1\": \"Our models\", \"split_2\": \"provide strong baselines for future research\", \"averb\": \"provide\", \"averb_s\": \"['Our models']\", \"averb_o\": \"['strong baselines']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 11.0, \"averb_span1\": 19.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 8.0, \"root\": \"provide\", \"root_full\": \"provide\", \"root_s\": \"['Our models']\", \"root_o\": \"['strong baselines']\", \"root_split\": 2, \"root_span0\": 11, \"root_span1\": 19, \"root_cspan0\": 0, \"root_cspan1\": 8, \"fverb\": \"provide\", \"fword\": \"provide\", \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['models', 'provide']\", \"URL\": \"https://www.semanticscholar.org/paper/Simple-BERT-Models-for-Relation-Extraction-and-Role-Shi-Lin/fddbcabe0fc9be0684855ae3dd059fb525a69e5b\", \"ID\": 21, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"Our models provide strong baselines for future research\", \"split_tokens\": \"['Our', 'models', 'provide', 'strong', 'baselines', 'for', 'future', 'research']\", \"split_anchor_span\": \"(0, 2)\", \"split_anchor_indices\": \"(0, 10)\", \"within_anchor_index\": -1.0}, {\"pc1\": 3.94585935228102, \"pc2\": 2.6069624177910575, \"word\": \"explore\", \"split_0\": \"We explore the multi-task learning setting for the recent\", \"split_1\": \"BERT\", \"split_2\": \"model on the GLUE benchmark , and how to best add task-specific parameters to a pre-trained BERT network , with a high degree of parameter sharing between tasks .\", \"averb\": \"explore\", \"averb_s\": \"['We']\", \"averb_o\": \"['the multi - task learning setting for the recent BERT model on the GLUE benchmark ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"add\", \"root_full\": \"add\", \"root_s\": \"['and how to best']\", \"root_o\": \"['We explore the multi - task learning setting for the recent BERT model on the GLUE benchmark , task - specific parameters , with a high degree of parameter sharing between tasks .']\", \"root_split\": 2, \"root_span0\": 111, \"root_span1\": 115, \"root_cspan0\": 48, \"root_cspan1\": 52, \"fverb\": \"best\", \"fword\": \"model\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'model', 'for']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark, and how to best add task-specific parameters to a pre-trained BERT network, with a high degree of parameter sharing between tasks.\", \"split_tokens\": \"['We', 'explore', 'the', 'multi-task', 'learning', 'setting', 'for', 'the', 'recent', 'BERT', 'model', 'on', 'the', 'GLUE', 'benchmark', ',', 'and', 'how', 'to', 'best', 'add', 'task-specific', 'parameters', 'to', 'a', 'pre-trained', 'BERT', 'network', ',', 'with', 'a', 'high', 'degree', 'of', 'parameter', 'sharing', 'between', 'tasks', '.']\", \"split_anchor_span\": \"(9, 10)\", \"split_anchor_indices\": \"(57, 61)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.706632047738901, \"pc2\": 2.5265749561070088, \"word\": \"add\", \"split_0\": \"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark , and how to best add task-specific parameters to a pre-trained\", \"split_1\": \"BERT\", \"split_2\": \"network , with a high degree of parameter sharing between tasks .\", \"averb\": \"add\", \"averb_s\": \"['and how to best']\", \"averb_o\": \"['We explore the multi - task learning setting for the recent BERT model on the GLUE benchmark , task - specific parameters , with a high degree of parameter sharing between tasks .']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 111.0, \"averb_span1\": 115.0, \"averb_cspan0\": 111.0, \"averb_cspan1\": 115.0, \"root\": \"add\", \"root_full\": \"add\", \"root_s\": \"['and how to best']\", \"root_o\": \"['We explore the multi - task learning setting for the recent BERT model on the GLUE benchmark , task - specific parameters , with a high degree of parameter sharing between tasks .']\", \"root_split\": 0, \"root_span0\": 111, \"root_span1\": 115, \"root_cspan0\": 111, \"root_cspan1\": 115, \"fverb\": \"sharing\", \"fword\": \"network\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'network', 'to']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We explore the multi-task learning setting for the recent BERT model on the GLUE benchmark, and how to best add task-specific parameters to a pre-trained BERT network, with a high degree of parameter sharing between tasks.\", \"split_tokens\": \"['We', 'explore', 'the', 'multi-task', 'learning', 'setting', 'for', 'the', 'recent', 'BERT', 'model', 'on', 'the', 'GLUE', 'benchmark', ',', 'and', 'how', 'to', 'best', 'add', 'task-specific', 'parameters', 'to', 'a', 'pre-trained', 'BERT', 'network', ',', 'with', 'a', 'high', 'degree', 'of', 'parameter', 'sharing', 'between', 'tasks', '.']\", \"split_anchor_span\": \"(26, 27)\", \"split_anchor_indices\": \"(154, 158)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.985559145226712, \"pc2\": 1.5462915051394066, \"word\": \"using\", \"split_0\": \"By using PALs in parallel with\", \"split_1\": \"BERT\", \"split_2\": \"layers , we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"averb\": \"using\", \"averb_s\": \"[]\", \"averb_o\": \"['PALs']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 9.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 9.0, \"root\": \"match\", \"root_full\": \"match\", \"root_s\": \"['we', 'By using PALs in parallel with BERT layers', ',']\", \"root_o\": \"['the performance of fine - tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state - of - the - art results on the Recognizing Textual Entailment dataset']\", \"root_split\": 2, \"root_span0\": 48, \"root_span1\": 54, \"root_cspan0\": 12, \"root_cspan1\": 18, \"fverb\": \"match\", \"fword\": \"layers\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'layers', 'with']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"By using PALs in parallel with BERT layers, we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"split_tokens\": \"['By', 'using', 'PALs', 'in', 'parallel', 'with', 'BERT', 'layers', ',', 'we', 'match', 'the', 'performance', 'of', 'fine-tuned', 'BERT', 'on', 'the', 'GLUE', 'benchmark', 'with', 'roughly', '7', 'times', 'fewer', 'parameters', ',', 'and', 'obtain', 'state-of-the-art', 'results', 'on', 'the', 'Recognizing', 'Textual', 'Entailment', 'dataset']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(30, 34)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.7963570375676894, \"pc2\": 1.3556566147631097, \"word\": \"tuned\", \"split_0\": \"By using PALs in parallel with BERT layers , we match the performance of fine-tuned\", \"split_1\": \"BERT\", \"split_2\": \"on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"averb\": \"tuned\", \"averb_s\": \"['-']\", \"averb_o\": \"['BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state - of - the - art results on the Recognizing Textual Entailment dataset']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 80.0, \"averb_span1\": 86.0, \"averb_cspan0\": 80.0, \"averb_cspan1\": 86.0, \"root\": \"match\", \"root_full\": \"match\", \"root_s\": \"['we', 'By using PALs in parallel with BERT layers', ',']\", \"root_o\": \"['the performance of fine - tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters , and obtain state - of - the - art results on the Recognizing Textual Entailment dataset']\", \"root_split\": 0, \"root_span0\": 48, \"root_span1\": 54, \"root_cspan0\": 48, \"root_cspan1\": 54, \"fverb\": \"obtain\", \"fword\": \"on\", \"apos\": \"['PROPN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'tuned', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-and-PALs%3A-Projected-Attention-Layers-for-in-Stickland-Murray/660d3472d9c3733dedcf911187b234f2b65561b5\", \"ID\": 22, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"By using PALs in parallel with BERT layers, we match the performance of fine-tuned BERT on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset\", \"split_tokens\": \"['By', 'using', 'PALs', 'in', 'parallel', 'with', 'BERT', 'layers', ',', 'we', 'match', 'the', 'performance', 'of', 'fine-tuned', 'BERT', 'on', 'the', 'GLUE', 'benchmark', 'with', 'roughly', '7', 'times', 'fewer', 'parameters', ',', 'and', 'obtain', 'state-of-the-art', 'results', 'on', 'the', 'Recognizing', 'Textual', 'Entailment', 'dataset']\", \"split_anchor_span\": \"(15, 16)\", \"split_anchor_indices\": \"(83, 87)\", \"within_anchor_index\": 0.0}, {\"pc1\": 23.55610193159275, \"pc2\": -16.53336483601816, \"word\": \"apply\", \"split_0\": \"As a case study , we apply these diagnostics to\", \"split_1\": \"the popular BERT model\", \"split_2\": \", finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"apply\", \"averb_s\": \"['we', ',', 'finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and', 'As a case study ,']\", \"averb_o\": \"['these diagnostics']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 21.0, \"averb_span1\": 27.0, \"averb_cspan0\": 21.0, \"averb_cspan1\": 27.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 318, \"root_cspan1\": 332, \"fverb\": \"finding\", \"fword\": \"finding\", \"apos\": \"['NOUN', 'ADP', 'VERB']\", \"apos_w\": \"['model', 'to', 'apply']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(10, 14)\", \"split_anchor_indices\": \"(47, 69)\", \"within_anchor_index\": 12.0}, {\"pc1\": 22.888564086800148, \"pc2\": -15.972016162436939, \"word\": \"can distinguish\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that\", \"split_1\": \"it\", \"split_2\": \"can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"can distinguish\", \"averb_s\": \"['it']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 89.0, \"averb_span1\": 115.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 26.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 300, \"root_cspan1\": 314, \"fverb\": \"distinguish\", \"fword\": \"can\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['it', 'distinguish', 'finding']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(17, 18)\", \"split_anchor_indices\": \"(85, 87)\", \"within_anchor_index\": -1.0}, {\"pc1\": 23.696620330914737, \"pc2\": -16.542360263785646, \"word\": \"shows\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and\", \"split_1\": \"it\", \"split_2\": \"robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"shows\", \"averb_s\": \"['it', 'As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and', 'robustly retrieves noun hypernyms it struggles with challenging inference and role - based event prediction \\u2014 and , in particular ,']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 377.0, \"averb_span1\": 383.0, \"averb_cspan0\": 141.0, \"averb_cspan1\": 147.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 153, \"root_cspan1\": 167, \"fverb\": \"retrieves\", \"fword\": \"robustly\", \"apos\": \"['PRON', 'VERB', 'ADJ']\", \"apos_w\": \"['it', 'shows', 'clear']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(40, 41)\", \"split_anchor_indices\": \"(232, 234)\", \"within_anchor_index\": -1.0}, {\"pc1\": 23.774398593435475, \"pc2\": -16.47265295796356, \"word\": \"struggles\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but\", \"split_1\": \"it\", \"split_2\": \"struggles with challenging inference and role - based event prediction \\u2014 and , in particular , it shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"struggles\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 279.0, \"averb_span1\": 289.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 110, \"root_cspan1\": 124, \"fverb\": \"struggles\", \"fword\": \"struggles\", \"apos\": \"['PRON', 'VERB', 'VERB']\", \"apos_w\": \"['it', 'struggles', 'shows']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(47, 48)\", \"split_anchor_indices\": \"(275, 277)\", \"within_anchor_index\": -1.0}, {\"pc1\": 23.696620330914737, \"pc2\": -16.542360263785646, \"word\": \"shows\", \"split_0\": \"As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and it robustly retrieves noun hypernyms , but it struggles with challenging inference and role - based event prediction \\u2014 and , in particular ,\", \"split_1\": \"it\", \"split_2\": \"shows clear insensitivity to the contextual impacts of negation\", \"averb\": \"shows\", \"averb_s\": \"['it', 'As a case study , we apply these diagnostics to the popular BERT model , finding that it can generally distinguish good from bad completions involving shared category or role reversal , albeit with less sensitivity than humans , and', 'robustly retrieves noun hypernyms it struggles with challenging inference and role - based event prediction \\u2014 and , in particular ,']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 377.0, \"averb_span1\": 383.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 6.0, \"root\": \"insensitivity\", \"root_full\": \"insensitivity\", \"root_s\": \"[]\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 389, \"root_span1\": 403, \"root_cspan0\": 12, \"root_cspan1\": 26, \"fverb\": \"shows\", \"fword\": \"shows\", \"apos\": \"['PRON', 'VERB', 'ADJ']\", \"apos_w\": \"['it', 'shows', 'clear']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction\\u2014 and, in particular, it shows clear insensitivity to the contextual impacts of negation\", \"split_tokens\": \"['As', 'a', 'case', 'study', ',', 'we', 'apply', 'these', 'diagnostics', 'to', 'the', 'popular', 'BERT', 'model', ',', 'finding', 'that', 'it', 'can', 'generally', 'distinguish', 'good', 'from', 'bad', 'completions', 'involving', 'shared', 'category', 'or', 'role', 'reversal', ',', 'albeit', 'with', 'less', 'sensitivity', 'than', 'humans', ',', 'and', 'it', 'robustly', 'retrieves', 'noun', 'hypernyms', ',', 'but', 'it', 'struggles', 'with', 'challenging', 'inference', 'and', 'role', '-', 'based', 'event', 'prediction', '\\u2014', 'and', ',', 'in', 'particular', ',', 'it', 'shows', 'clear', 'insensitivity', 'to', 'the', 'contextual', 'impacts', 'of', 'negation']\", \"split_anchor_span\": \"(64, 65)\", \"split_anchor_indices\": \"(373, 375)\", \"within_anchor_index\": -1.0}, {\"pc1\": -8.608392463650253, \"pc2\": 0.6026046496590314, \"word\": \"Is\", \"split_0\": \"What\", \"split_1\": \"BERT\", \"split_2\": \"Is Not : Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .\", \"averb\": \"Is\", \"averb_s\": \"['What', 'BERT']\", \"averb_o\": \"['Not', ': Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 10.0, \"averb_span1\": 13.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"Is\", \"root_full\": \"Is\", \"root_s\": \"['What', 'BERT']\", \"root_o\": \"['Not', ': Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models .']\", \"root_split\": 2, \"root_span0\": 10, \"root_span1\": 13, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"Is\", \"fword\": \"Is\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['BERT', 'Is']\", \"URL\": \"https://www.semanticscholar.org/paper/What-BERT-Is-Not%3A-Lessons-from-a-New-Suite-of-for-Ettinger/a0e49f65b6847437f262c59d0d399255101d0b75\", \"ID\": 23, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.\", \"split_tokens\": \"['What', 'BERT', 'Is', 'Not', ':', 'Lessons', 'from', 'a', 'New', 'Suite', 'of', 'Psycholinguistic', 'Diagnostics', 'for', 'Language', 'Models', '.']\", \"split_anchor_span\": \"(1, 2)\", \"split_anchor_indices\": \"(4, 8)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.603207360021225, \"pc2\": 1.574015391219314, \"word\": \"pre\", \"split_0\": \"Language model pre - training , such as\", \"split_1\": \"BERT\", \"split_2\": \", has achieved remarkable results in many NLP tasks .\", \"averb\": \"pre\", \"averb_s\": \"['model']\", \"averb_o\": \"['-', 'training , such as BERT ,']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 15.0, \"averb_span1\": 19.0, \"averb_cspan0\": 15.0, \"averb_cspan1\": 19.0, \"root\": \"achieved\", \"root_full\": \"has achieved\", \"root_s\": \"['Language model pre - training , such as BERT ,']\", \"root_o\": \"['remarkable results']\", \"root_split\": 2, \"root_span0\": 47, \"root_span1\": 60, \"root_cspan0\": 2, \"root_cspan1\": 15, \"fverb\": \"has\", \"fword\": \"has\", \"apos\": \"['PROPN', 'SCONJ', 'ADJ']\", \"apos_w\": \"['BERT', 'as', 'training']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks.\", \"split_tokens\": \"['Language', 'model', 'pre', '-', 'training', ',', 'such', 'as', 'BERT', ',', 'has', 'achieved', 'remarkable', 'results', 'in', 'many', 'NLP', 'tasks', '.']\", \"split_anchor_span\": \"(8, 9)\", \"split_anchor_indices\": \"(39, 43)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.610377766182556, \"pc2\": -0.8920662838769855, \"word\": \"tuning\", \"split_0\": \"In this paper , we propose to visualize loss landscapes and optimization trajectories of fine - tuning\", \"split_1\": \"BERT\", \"split_2\": \"on specific datasets .\", \"averb\": \"tuning\", \"averb_s\": \"['-']\", \"averb_o\": \"['BERT on specific datasets']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 96.0, \"averb_span1\": 103.0, \"averb_cspan0\": 96.0, \"averb_cspan1\": 103.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"[',', 'we']\", \"root_o\": \"['and']\", \"root_split\": 0, \"root_span0\": 19, \"root_span1\": 27, \"root_cspan0\": 19, \"root_cspan1\": 27, \"fverb\": null, \"fword\": \"on\", \"apos\": \"['NOUN', 'VERB', 'ADP']\", \"apos_w\": \"['BERT', 'tuning', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 2, \"Text\": \"In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets.\", \"split_tokens\": \"['In', 'this', 'paper', ',', 'we', 'propose', 'to', 'visualize', 'loss', 'landscapes', 'and', 'optimization', 'trajectories', 'of', 'fine', '-', 'tuning', 'BERT', 'on', 'specific', 'datasets', '.']\", \"split_anchor_span\": \"(17, 18)\", \"split_anchor_indices\": \"(102, 106)\", \"within_anchor_index\": 0.0}, {\"pc1\": -2.5152009999587994, \"pc2\": -2.0815569649187795, \"word\": \"demonstrate\", \"split_0\": \"We also demonstrate that the fine - tuning procedure is robust to overfitting , even though\", \"split_1\": \"BERT\", \"split_2\": \"is highly over - parameterized for downstream tasks .\", \"averb\": \"demonstrate\", \"averb_s\": \"['We']\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 8.0, \"averb_span1\": 20.0, \"averb_cspan0\": 8.0, \"averb_cspan1\": 20.0, \"root\": \"demonstrate\", \"root_full\": \"demonstrate\", \"root_s\": \"['We']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 8, \"root_span1\": 20, \"root_cspan0\": 8, \"root_cspan1\": 20, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'ADV', 'ADJ']\", \"apos_w\": \"['BERT', 'highly', 'robust']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks.\", \"split_tokens\": \"['We', 'also', 'demonstrate', 'that', 'the', 'fine', '-', 'tuning', 'procedure', 'is', 'robust', 'to', 'overfitting', ',', 'even', 'though', 'BERT', 'is', 'highly', 'over', '-', 'parameterized', 'for', 'downstream', 'tasks', '.']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(91, 95)\", \"within_anchor_index\": 0.0}, {\"pc1\": 4.322895009051766, \"pc2\": -2.1896502974093153, \"word\": \"tuning\", \"split_0\": \"Second , the visualization results indicate that fine - tuning\", \"split_1\": \"BERT\", \"split_2\": \"tends to generalize better because of the flat and wide optima , and the consistency between the training loss surface and the generalization error surface .\", \"averb\": \"tuning\", \"averb_s\": \"['-']\", \"averb_o\": \"['BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 56.0, \"averb_span1\": 63.0, \"averb_cspan0\": 56.0, \"averb_cspan1\": 63.0, \"root\": \"indicate\", \"root_full\": \"indicate\", \"root_s\": \"[', the visualization results']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 35, \"root_span1\": 44, \"root_cspan0\": 35, \"root_cspan1\": 44, \"fverb\": \"tends\", \"fword\": \"tends\", \"apos\": \"['PROPN', 'VERB', 'VERB']\", \"apos_w\": \"['BERT', 'tuning', 'tends']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface.\", \"split_tokens\": \"['Second', ',', 'the', 'visualization', 'results', 'indicate', 'that', 'fine', '-', 'tuning', 'BERT', 'tends', 'to', 'generalize', 'better', 'because', 'of', 'the', 'flat', 'and', 'wide', 'optima', ',', 'and', 'the', 'consistency', 'between', 'the', 'training', 'loss', 'surface', 'and', 'the', 'generalization', 'error', 'surface', '.']\", \"split_anchor_span\": \"(10, 11)\", \"split_anchor_indices\": \"(62, 66)\", \"within_anchor_index\": 0.0}, {\"pc1\": -10.525004311557936, \"pc2\": -0.5852306554561398, \"word\": \"Understanding\", \"split_0\": \"Visualizing and Understanding the Effectiveness of\", \"split_1\": \"BERT\", \"split_2\": \".\", \"averb\": \"Understanding\", \"averb_s\": \"[]\", \"averb_o\": \"['the Effectiveness of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 16.0, \"averb_span1\": 30.0, \"averb_cspan0\": 16.0, \"averb_cspan1\": 30.0, \"root\": \"Visualizing\", \"root_full\": \"Visualizing\", \"root_s\": \"[]\", \"root_o\": \"['Understanding the Effectiveness of BERT']\", \"root_split\": 0, \"root_span0\": 0, \"root_span1\": 12, \"root_cspan0\": 0, \"root_cspan1\": 12, \"fverb\": null, \"fword\": null, \"apos\": \"['PROPN', 'ADP', 'PROPN']\", \"apos_w\": \"['BERT', 'of', 'Effectiveness']\", \"URL\": \"https://www.semanticscholar.org/paper/Visualizing-and-Understanding-the-Effectiveness-of-Hao-Dong/d3cacb4806886eb2fe59c90d4b6f822c24ff1822\", \"ID\": 24, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"Visualizing and Understanding the Effectiveness of BERT.\", \"split_tokens\": \"['Visualizing', 'and', 'Understanding', 'the', 'Effectiveness', 'of', 'BERT', '.']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(50, 54)\", \"within_anchor_index\": 0.0}, {\"pc1\": -5.387652832822511, \"pc2\": 0.27859309014004324, \"word\": \"propose\", \"split_0\": \"We propose\", \"split_1\": \"a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation\", \"split_2\": \".\", \"averb\": \"propose\", \"averb_s\": \"['We']\", \"averb_o\": \"['a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 11.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 11.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['We']\", \"root_o\": \"['a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 11, \"root_cspan0\": 3, \"root_cspan1\": 11, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['method', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We propose a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation.\", \"split_tokens\": \"['We', 'propose', 'a', 'novel', 'data', 'augmentation', 'method', 'for', 'labeled', 'sentences', 'called', 'conditional', 'BERT', 'contextual', 'augmentation', '.']\", \"split_anchor_span\": \"(2, 15)\", \"split_anchor_indices\": \"(10, 112)\", \"within_anchor_index\": 74.0}, {\"pc1\": 9.410340676094554, \"pc2\": 12.362606552919383, \"word\": \"appeared\", \"split_0\": \"We retrofit BERT to conditional BERT by introducing a new conditional masked language model ( The term \\u201c conditional masked language model \\u201d appeared once in original\", \"split_1\": \"BERT\", \"split_2\": \"paper , which indicates context - conditional , is equivalent to term \\u201c masked language model \\u201d .\", \"averb\": \"appeared\", \"averb_s\": \"[]\", \"averb_o\": \"['once']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 141.0, \"averb_span1\": 150.0, \"averb_cspan0\": 141.0, \"averb_cspan1\": 150.0, \"root\": \"model\", \"root_full\": \"model\", \"root_s\": \"['\\u201c', 'masked']\", \"root_o\": \"['We retrofit BERT to conditional BERT by introducing a new conditional masked language model ( The term \\u201c conditional masked language model \\u201d appeared once in original BERT paper , term \\u201d .']\", \"root_split\": 2, \"root_span0\": 260, \"root_span1\": 266, \"root_cspan0\": 88, \"root_cspan1\": 94, \"fverb\": \"indicates\", \"fword\": \"paper\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'paper', 'in']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"We retrofit BERT to conditional BERT by introducing a new conditional masked language model (The term \\u201cconditional masked language model\\u201d appeared once in original BERT paper, which indicates context-conditional, is equivalent to term \\u201cmasked language model\\u201d.\", \"split_tokens\": \"['We', 'retrofit', 'BERT', 'to', 'conditional', 'BERT', 'by', 'introducing', 'a', 'new', 'conditional', 'masked', 'language', 'model', '(', 'The', 'term', '\\u201c', 'conditional', 'masked', 'language', 'model', '\\u201d', 'appeared', 'once', 'in', 'original', 'BERT', 'paper', ',', 'which', 'indicates', 'context', '-', 'conditional', ',', 'is', 'equivalent', 'to', 'term', '\\u201c', 'masked', 'language', 'model', '\\u201d', '.']\", \"split_anchor_span\": \"(27, 28)\", \"split_anchor_indices\": \"(166, 170)\", \"within_anchor_index\": 0.0}, {\"pc1\": -8.066551031360941, \"pc2\": -0.1481522552920512, \"word\": \"can\", \"split_0\": \"The well trained conditional\", \"split_1\": \"BERT\", \"split_2\": \"can be applied to enhance contextual augmentation .\", \"averb\": \"can\", \"averb_s\": \"['The well trained conditional BERT']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 34.0, \"averb_span1\": 38.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 4.0, \"root\": \"applied\", \"root_full\": \"can be applied\", \"root_s\": \"[]\", \"root_o\": \"['to enhance contextual augmentation']\", \"root_split\": 2, \"root_span0\": 34, \"root_span1\": 49, \"root_cspan0\": 0, \"root_cspan1\": 15, \"fverb\": \"be\", \"fword\": \"can\", \"apos\": \"['NOUN', 'ADJ', 'VERB']\", \"apos_w\": \"['BERT', 'conditional', 'can']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"The well trained conditional BERT can be applied to enhance contextual augmentation.\", \"split_tokens\": \"['The', 'well', 'trained', 'conditional', 'BERT', 'can', 'be', 'applied', 'to', 'enhance', 'contextual', 'augmentation', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(28, 32)\", \"within_anchor_index\": 0.0}, {\"pc1\": -1.380748181239962, \"pc2\": -4.470158789410915, \"word\": \"show\", \"split_0\": \"Experiments on six various different text classification tasks show that\", \"split_1\": \"our method\", \"split_2\": \"can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement\", \"averb\": \"show\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 63.0, \"averb_span1\": 68.0, \"averb_cspan0\": 63.0, \"averb_cspan1\": 68.0, \"root\": \"tasks\", \"root_full\": \"tasks\", \"root_s\": \"['various different classification']\", \"root_o\": \"['Experiments on show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement']\", \"root_split\": 0, \"root_span0\": 57, \"root_span1\": 63, \"root_cspan0\": 57, \"root_cspan1\": 63, \"fverb\": \"be\", \"fword\": \"can\", \"apos\": \"['NOUN', 'ADV', 'VERB']\", \"apos_w\": \"['method', 'easily', 'show']\", \"URL\": \"https://www.semanticscholar.org/paper/Conditional-BERT-Contextual-Augmentation-Wu-Lv/188024469a2443f262b3cbb5c5d4a96851949d68\", \"ID\": 25, \"Type\": \"Abstract\", \"Index\": 8, \"Text\": \"Experiments on six various different text classification tasks show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement\", \"split_tokens\": \"['Experiments', 'on', 'six', 'various', 'different', 'text', 'classification', 'tasks', 'show', 'that', 'our', 'method', 'can', 'be', 'easily', 'applied', 'to', 'both', 'convolutional', 'or', 'recurrent', 'neural', 'networks', 'classifier', 'to', 'obtain', 'improvement']\", \"split_anchor_span\": \"(10, 12)\", \"split_anchor_indices\": \"(72, 82)\", \"within_anchor_index\": -1.0}, {\"pc1\": -4.141153598514755, \"pc2\": 1.8122707697097777, \"word\": \"Starting\", \"split_0\": \"Starting from a public multilingual\", \"split_1\": \"BERT\", \"split_2\": \"checkpoint , our final model is 6x smaller and 27x faster , and has higher accuracy than a state-of-the-art multilingual baseline .\", \"averb\": \"Starting\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 9.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 9.0, \"root\": \"6x\", \"root_full\": \"is 6x\", \"root_s\": \"['Starting from a public multilingual BERT checkpoint , our final model', 'and']\", \"root_o\": \"['smaller', 'has higher accuracy than a state - of - the - art multilingual baseline .']\", \"root_split\": 2, \"root_span0\": 70, \"root_span1\": 76, \"root_cspan0\": 29, \"root_cspan1\": 35, \"fverb\": \"is\", \"fword\": \"checkpoint\", \"apos\": \"['PROPN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'checkpoint', 'from']\", \"URL\": \"https://www.semanticscholar.org/paper/Small-and-Practical-BERT-Models-for-Sequence-Tsai-Riesa/2f9d4887d0022400fc40c774c4c78350c3bc5390\", \"ID\": 26, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"Starting from a public multilingual BERT checkpoint, our final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-the-art multilingual baseline.\", \"split_tokens\": \"['Starting', 'from', 'a', 'public', 'multilingual', 'BERT', 'checkpoint', ',', 'our', 'final', 'model', 'is', '6x', 'smaller', 'and', '27x', 'faster', ',', 'and', 'has', 'higher', 'accuracy', 'than', 'a', 'state-of-the-art', 'multilingual', 'baseline', '.']\", \"split_anchor_span\": \"(5, 6)\", \"split_anchor_indices\": \"(35, 39)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.9310614883811903, \"pc2\": 1.8110395021061418, \"word\": \"using\", \"split_0\": \"Recently , a simple combination of passage retrieval using off-the-shelf IR techniques and a\", \"split_1\": \"BERT\", \"split_2\": \"reader was found to be very effective for question answering directly on Wikipedia , yielding a large improvement over the previous state of the art on a standard benchmark dataset .\", \"averb\": \"using\", \"averb_s\": \"[]\", \"averb_o\": \"['off', '- the - shelf IR techniques and a BERT reader']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 53.0, \"averb_span1\": 59.0, \"averb_cspan0\": 53.0, \"averb_cspan1\": 59.0, \"root\": \"found\", \"root_full\": \"was found\", \"root_s\": \"['a simple combination of passage retrieval using off - the - shelf IR techniques and a BERT reader', 'Recently ,']\", \"root_o\": \"['to be very effective for question answering directly on Wikipedia , yielding a large improvement over the previous state of the art on a standard benchmark dataset']\", \"root_split\": 2, \"root_span0\": 109, \"root_span1\": 119, \"root_cspan0\": 11, \"root_cspan1\": 21, \"fverb\": \"was\", \"fword\": \"reader\", \"apos\": \"['PROPN', 'NOUN', 'CCONJ']\", \"apos_w\": \"['BERT', 'reader', 'and']\", \"URL\": \"https://www.semanticscholar.org/paper/Data-Augmentation-for-BERT-Fine-Tuning-in-Question-Yang-Xie/f5eaf727b80240a13e9f631211c9ecec7e3b9feb\", \"ID\": 27, \"Type\": \"Abstract\", \"Index\": 0, \"Text\": \"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset.\", \"split_tokens\": \"['Recently', ',', 'a', 'simple', 'combination', 'of', 'passage', 'retrieval', 'using', 'off-the-shelf', 'IR', 'techniques', 'and', 'a', 'BERT', 'reader', 'was', 'found', 'to', 'be', 'very', 'effective', 'for', 'question', 'answering', 'directly', 'on', 'Wikipedia', ',', 'yielding', 'a', 'large', 'improvement', 'over', 'the', 'previous', 'state', 'of', 'the', 'art', 'on', 'a', 'standard', 'benchmark', 'dataset', '.']\", \"split_anchor_span\": \"(14, 15)\", \"split_anchor_indices\": \"(92, 96)\", \"within_anchor_index\": 0.0}, {\"pc1\": 3.8759135464315846, \"pc2\": -2.013769174310573, \"word\": \"argue\", \"split_0\": \"We take issue with this interpretation and argue that the performance of\", \"split_1\": \"BERT\", \"split_2\": \"is partly due to reasoning about ( the surface form of ) entity names , e.g. , guessing that a person with an Italian - sounding name speaks Italian .\", \"averb\": \"argue\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 43.0, \"averb_span1\": 49.0, \"averb_cspan0\": 43.0, \"averb_cspan1\": 49.0, \"root\": \"issue\", \"root_full\": \"issue\", \"root_s\": \"['take']\", \"root_o\": \"[]\", \"root_split\": 0, \"root_span0\": 8, \"root_span1\": 14, \"root_cspan0\": 8, \"root_cspan1\": 14, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'performance']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 3, \"Text\": \"We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.\", \"split_tokens\": \"['We', 'take', 'issue', 'with', 'this', 'interpretation', 'and', 'argue', 'that', 'the', 'performance', 'of', 'BERT', 'is', 'partly', 'due', 'to', 'reasoning', 'about', '(', 'the', 'surface', 'form', 'of', ')', 'entity', 'names', ',', 'e.g.', ',', 'guessing', 'that', 'a', 'person', 'with', 'an', 'Italian', '-', 'sounding', 'name', 'speaks', 'Italian', '.']\", \"split_anchor_span\": \"(12, 13)\", \"split_anchor_indices\": \"(72, 76)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.836786608842976, \"pc2\": -2.140540803552052, \"word\": \"drops\", \"split_0\": \"More specifically , we show that\", \"split_1\": \"BERT 's\", \"split_2\": \"precision drops dramatically when we filter certain easy - to - guess facts .\", \"averb\": \"drops\", \"averb_s\": \"[\\\"BERT 's precision\\\"]\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 51.0, \"averb_span1\": 57.0, \"averb_cspan0\": 10.0, \"averb_cspan1\": 16.0, \"root\": \"guess\", \"root_full\": \"guess\", \"root_s\": \"['when we filter certain easy -', 'to', '-']\", \"root_o\": \"[\\\"More specifically , we show that BERT 's precision drops dramatically facts\\\", '.']\", \"root_split\": 2, \"root_span0\": 105, \"root_span1\": 111, \"root_cspan0\": 64, \"root_cspan1\": 70, \"fverb\": \"filter\", \"fword\": \"precision\", \"apos\": \"['PROPN', 'NOUN', 'VERB']\", \"apos_w\": \"['BERT', 'precision', 'drops']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 4, \"Text\": \"More specifically, we show that BERT's precision drops dramatically when we filter certain easy-to-guess facts.\", \"split_tokens\": \"['More', 'specifically', ',', 'we', 'show', 'that', 'BERT', \\\"'s\\\", 'precision', 'drops', 'dramatically', 'when', 'we', 'filter', 'certain', 'easy', '-', 'to', '-', 'guess', 'facts', '.']\", \"split_anchor_span\": \"(6, 8)\", \"split_anchor_indices\": \"(32, 39)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.459375466018479, \"pc2\": 0.5689986877828528, \"word\": \"propose\", \"split_0\": \"As a remedy , we propose E - BERT , an extension of\", \"split_1\": \"BERT\", \"split_2\": \"that replaces entity mentions with symbolic entity embeddings .\", \"averb\": \"propose\", \"averb_s\": \"['we', 'As a remedy']\", \"averb_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 17.0, \"averb_span1\": 25.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 25.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we', 'As a remedy']\", \"root_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"root_split\": 0, \"root_span0\": 17, \"root_span1\": 25, \"root_cspan0\": 17, \"root_cspan1\": 25, \"fverb\": \"replaces\", \"fword\": \"that\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'extension']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings.\", \"split_tokens\": \"['As', 'a', 'remedy', ',', 'we', 'propose', 'E', '-', 'BERT', ',', 'an', 'extension', 'of', 'BERT', 'that', 'replaces', 'entity', 'mentions', 'with', 'symbolic', 'entity', 'embeddings', '.']\", \"split_anchor_span\": \"(13, 14)\", \"split_anchor_indices\": \"(51, 55)\", \"within_anchor_index\": 0.0}, {\"pc1\": -4.459375466018479, \"pc2\": 0.5689986877828535, \"word\": \"propose\", \"split_0\": \"As a remedy , we propose\", \"split_1\": \"E - BERT , an extension of BERT that replaces entity mentions with symbolic entity embeddings\", \"split_2\": \".\", \"averb\": \"propose\", \"averb_s\": \"['we', 'As a remedy']\", \"averb_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 17.0, \"averb_span1\": 25.0, \"averb_cspan0\": 17.0, \"averb_cspan1\": 25.0, \"root\": \"propose\", \"root_full\": \"propose\", \"root_s\": \"['we', 'As a remedy']\", \"root_o\": \"['E - BERT an extension of BERT that replaces entity mentions with symbolic entity embeddings']\", \"root_split\": 0, \"root_span0\": 17, \"root_span1\": 25, \"root_cspan0\": 17, \"root_cspan1\": 25, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'VERB']\", \"apos_w\": \"['E', 'propose']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 5, \"Text\": \"As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings.\", \"split_tokens\": \"['As', 'a', 'remedy', ',', 'we', 'propose', 'E', '-', 'BERT', ',', 'an', 'extension', 'of', 'BERT', 'that', 'replaces', 'entity', 'mentions', 'with', 'symbolic', 'entity', 'embeddings', '.']\", \"split_anchor_span\": \"(6, 22)\", \"split_anchor_indices\": \"(24, 117)\", \"within_anchor_index\": 4.0}, {\"pc1\": -3.3343767550126953, \"pc2\": -1.466341072489215, \"word\": \"show\", \"split_0\": \"We take this as evidence that E - BERT is richer in factual knowledge , and we show two ways of ensembling\", \"split_1\": \"BERT\", \"split_2\": \"and E - BERT\", \"averb\": \"show\", \"averb_s\": \"['we']\", \"averb_o\": \"['two ways of ensembling BERT and E - BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 79.0, \"averb_span1\": 84.0, \"averb_cspan0\": 79.0, \"averb_cspan1\": 84.0, \"root\": \"take\", \"root_full\": \"take\", \"root_s\": \"['We']\", \"root_o\": \"['this']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": null, \"fword\": \"and\", \"apos\": \"['NOUN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'ways']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT\", \"split_tokens\": \"['We', 'take', 'this', 'as', 'evidence', 'that', 'E', '-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E', '-', 'BERT']\", \"split_anchor_span\": \"(22, 23)\", \"split_anchor_indices\": \"(106, 110)\", \"within_anchor_index\": 0.0}, {\"pc1\": -3.139262953465423, \"pc2\": -1.4240839817661635, \"word\": \"take\", \"split_0\": \"We take this as evidence that\", \"split_1\": \"E - BERT\", \"split_2\": \"is richer in factual knowledge , and we show two ways of ensembling BERT and E - BERT\", \"averb\": \"take\", \"averb_s\": \"['We']\", \"averb_o\": \"['this']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 3.0, \"averb_span1\": 8.0, \"averb_cspan0\": 3.0, \"averb_cspan1\": 8.0, \"root\": \"take\", \"root_full\": \"take\", \"root_s\": \"['We']\", \"root_o\": \"['this']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['ADJ', 'NOUN', 'SCONJ']\", \"apos_w\": \"['richer', 'evidence', 'as']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT\", \"split_tokens\": \"['We', 'take', 'this', 'as', 'evidence', 'that', 'E', '-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E', '-', 'BERT']\", \"split_anchor_span\": \"(6, 9)\", \"split_anchor_indices\": \"(29, 37)\", \"within_anchor_index\": 4.0}, {\"pc1\": -3.3343767550126953, \"pc2\": -1.4663410724892152, \"word\": \"show\", \"split_0\": \"We take this as evidence that E - BERT is richer in factual knowledge , and we show two ways of ensembling BERT and\", \"split_1\": \"E - BERT\", \"split_2\": null, \"averb\": \"show\", \"averb_s\": \"['we']\", \"averb_o\": \"['two ways of ensembling BERT and E - BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 79.0, \"averb_span1\": 84.0, \"averb_cspan0\": 79.0, \"averb_cspan1\": 84.0, \"root\": \"take\", \"root_full\": \"take\", \"root_s\": \"['We']\", \"root_o\": \"['this']\", \"root_split\": 0, \"root_span0\": 3, \"root_span1\": 8, \"root_cspan0\": 3, \"root_cspan1\": 8, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'NOUN', 'ADP']\", \"apos_w\": \"['BERT', 'BERT', 'of']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Abstract\", \"Index\": 7, \"Text\": \"We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT\", \"split_tokens\": \"['We', 'take', 'this', 'as', 'evidence', 'that', 'E', '-', 'BERT', 'is', 'richer', 'in', 'factual', 'knowledge', ',', 'and', 'we', 'show', 'two', 'ways', 'of', 'ensembling', 'BERT', 'and', 'E', '-', 'BERT']\", \"split_anchor_span\": \"(24, 27)\", \"split_anchor_indices\": \"(115, 123)\", \"within_anchor_index\": 4.0}, {\"pc1\": -6.645523597512408, \"pc2\": 2.696641569821108, \"word\": \"is\", \"split_0\": null, \"split_1\": \"BERT\", \"split_2\": \"is Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA .\", \"averb\": \"is\", \"averb_s\": \"['BERT']\", \"averb_o\": \"['Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name - Based Reasoning in Unsupervised QA']\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 5.0, \"averb_span1\": 8.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 3.0, \"root\": \"is\", \"root_full\": \"is\", \"root_s\": \"['BERT']\", \"root_o\": \"['Not a Knowledge Base ( Yet ) : Factual Knowledge vs. Name - Based Reasoning in Unsupervised QA']\", \"root_split\": 2, \"root_span0\": 5, \"root_span1\": 8, \"root_cspan0\": 0, \"root_cspan1\": 3, \"fverb\": \"is\", \"fword\": \"is\", \"apos\": \"['PROPN', 'AUX']\", \"apos_w\": \"['BERT', 'is']\", \"URL\": \"https://www.semanticscholar.org/paper/BERT-is-Not-a-Knowledge-Base-(Yet)%3A-Factual-vs.-in-Poerner-Waltinger/9df6cc3bf35b70613abe95ad269ac74f169c9080\", \"ID\": 28, \"Type\": \"Title\", \"Index\": 0, \"Text\": \"BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA.\", \"split_tokens\": \"['BERT', 'is', 'Not', 'a', 'Knowledge', 'Base', '(', 'Yet', ')', ':', 'Factual', 'Knowledge', 'vs.', 'Name-Based', 'Reasoning', 'in', 'Unsupervised', 'QA', '.']\", \"split_anchor_span\": \"(0, 1)\", \"split_anchor_indices\": \"(0, 4)\", \"within_anchor_index\": 0.0}, {\"pc1\": -7.798840128720877, \"pc2\": -2.6163506451939527, \"word\": \"produced\", \"split_0\": \"However , just how contextual are the contextualized representations produced by models such as ELMo and\", \"split_1\": \"BERT\", \"split_2\": \"?\", \"averb\": \"produced\", \"averb_s\": \"[]\", \"averb_o\": \"[]\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 69.0, \"averb_span1\": 78.0, \"averb_cspan0\": 69.0, \"averb_cspan1\": 78.0, \"root\": \"representations\", \"root_full\": \"are representations\", \"root_s\": \"[]\", \"root_o\": \"['produced by models such as ELMo and BERT ?']\", \"root_split\": 0, \"root_span0\": 30, \"root_span1\": 69, \"root_cspan0\": 30, \"root_cspan1\": 69, \"fverb\": null, \"fword\": null, \"apos\": \"['NOUN', 'NOUN', 'SCONJ']\", \"apos_w\": \"['BERT', 'ELMo', 'as']\", \"URL\": \"https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39\", \"ID\": 29, \"Type\": \"Abstract\", \"Index\": 1, \"Text\": \"However, just how contextual are the contextualized representations produced by models such as ELMo and BERT?\", \"split_tokens\": \"['However', ',', 'just', 'how', 'contextual', 'are', 'the', 'contextualized', 'representations', 'produced', 'by', 'models', 'such', 'as', 'ELMo', 'and', 'BERT', '?']\", \"split_anchor_span\": \"(16, 17)\", \"split_anchor_indices\": \"(104, 108)\", \"within_anchor_index\": 0.0}, {\"pc1\": 5.187554042490798, \"pc2\": -1.2701228931195099, \"word\": \"be explained\", \"split_0\": \"In all layers of ELMo ,\", \"split_1\": \"BERT\", \"split_2\": \", and GPT-2 , on average , less than 5 % of the variance in a word 's contextualized representations can be explained by a static embedding for that word , providing some justification for the success of contextualized representations\", \"averb\": \"be explained\", \"averb_s\": \"['can']\", \"averb_o\": \"[]\", \"averb_relation\": 1.0, \"averb_split\": 2.0, \"averb_span0\": 134.0, \"averb_span1\": 147.0, \"averb_cspan0\": 105.0, \"averb_cspan1\": 118.0, \"root\": \"explained\", \"root_full\": \"be explained\", \"root_s\": \"['can']\", \"root_o\": \"[]\", \"root_split\": 2, \"root_span0\": 134, \"root_span1\": 147, \"root_cspan0\": 105, \"root_cspan1\": 118, \"fverb\": \"be\", \"fword\": \"and\", \"apos\": \"['PROPN', 'ADP', 'NOUN']\", \"apos_w\": \"['BERT', 'of', 'layers']\", \"URL\": \"https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39\", \"ID\": 29, \"Type\": \"Abstract\", \"Index\": 6, \"Text\": \"In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations\", \"split_tokens\": \"['In', 'all', 'layers', 'of', 'ELMo', ',', 'BERT', ',', 'and', 'GPT-2', ',', 'on', 'average', ',', 'less', 'than', '5', '%', 'of', 'the', 'variance', 'in', 'a', 'word', \\\"'s\\\", 'contextualized', 'representations', 'can', 'be', 'explained', 'by', 'a', 'static', 'embedding', 'for', 'that', 'word', ',', 'providing', 'some', 'justification', 'for', 'the', 'success', 'of', 'contextualized', 'representations']\", \"split_anchor_span\": \"(6, 7)\", \"split_anchor_indices\": \"(23, 27)\", \"within_anchor_index\": 0.0}, {\"pc1\": -8.556036381205166, \"pc2\": 0.09580501216312633, \"word\": \"Comparing\", \"split_0\": \"Comparing the Geometry of\", \"split_1\": \"BERT\", \"split_2\": \", ELMo , and GPT-2 Embeddings .\", \"averb\": \"Comparing\", \"averb_s\": \"[]\", \"averb_o\": \"['the', 'Geometry of BERT']\", \"averb_relation\": -1.0, \"averb_split\": 0.0, \"averb_span0\": 0.0, \"averb_span1\": 10.0, \"averb_cspan0\": 0.0, \"averb_cspan1\": 10.0, \"root\": \",\", \"root_full\": \",\", \"root_s\": \"[]\", \"root_o\": \"['Comparing the Geometry of BERT ELMo , and GPT-2 Embeddings .']\", \"root_split\": 2, \"root_span0\": 31, \"root_span1\": 33, \"root_cspan0\": 0, \"root_cspan1\": 2, \"fverb\": null, \"fword\": \"ELMo\", \"apos\": \"['PROPN', 'ADP', 'PROPN']\", \"apos_w\": \"['BERT', 'of', 'Geometry']\", \"URL\": \"https://www.semanticscholar.org/paper/How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh/9d7902e834d5d1d35179962c7a5b9d16623b0d39\", \"ID\": 29, \"Type\": \"Title\", \"Index\": 1, \"Text\": \"Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings.\", \"split_tokens\": \"['Comparing', 'the', 'Geometry', 'of', 'BERT', ',', 'ELMo', ',', 'and', 'GPT-2', 'Embeddings', '.']\", \"split_anchor_span\": \"(4, 5)\", \"split_anchor_indices\": \"(25, 29)\", \"within_anchor_index\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_embeds(output_elmo_inv, csv, tooltip=['word', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from\n",
    "# https://gmarti.gitlab.io/ml/2017/09/07/how-to-sort-distance-matrix.html\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "N = len(output_elmo)\n",
    "# data = output_w2v.iloc[:, 0:300]\n",
    "data = output_elmo.iloc[:, 0:1024]\n",
    "# Shuffle data for extra comparison\n",
    "# data = data.sample(frac=1)\n",
    "dist_mat = squareform(pdist(data))\n",
    "\n",
    "plt.pcolormesh(dist_mat)\n",
    "plt.colorbar()\n",
    "plt.xlim([0,N])\n",
    "plt.ylim([0,N])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from\n",
    "# https://gmarti.gitlab.io/ml/2017/09/07/how-to-sort-distance-matrix.html\n",
    "\n",
    "from fastcluster import linkage\n",
    "\n",
    "def seriation(Z,N,cur_index):\n",
    "    '''\n",
    "        input:\n",
    "            - Z is a hierarchical tree (dendrogram)\n",
    "            - N is the number of points given to the clustering process\n",
    "            - cur_index is the position in the tree for the recursive traversal\n",
    "        output:\n",
    "            - order implied by the hierarchical tree Z\n",
    "            \n",
    "        seriation computes the order implied by a hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    if cur_index < N:\n",
    "        return [cur_index]\n",
    "    else:\n",
    "        left = int(Z[cur_index-N,0])\n",
    "        right = int(Z[cur_index-N,1])\n",
    "        return (seriation(Z,N,left) + seriation(Z,N,right))\n",
    "    \n",
    "def compute_serial_matrix(dist_mat,method=\"ward\"):\n",
    "    '''\n",
    "        input:\n",
    "            - dist_mat is a distance matrix\n",
    "            - method = [\"ward\",\"single\",\"average\",\"complete\"]\n",
    "        output:\n",
    "            - seriated_dist is the input dist_mat,\n",
    "              but with re-ordered rows and columns\n",
    "              according to the seriation, i.e. the\n",
    "              order implied by the hierarchical tree\n",
    "            - res_order is the order implied by\n",
    "              the hierarchical tree\n",
    "            - res_linkage is the hierarchical tree (dendrogram)\n",
    "        \n",
    "        compute_serial_matrix transforms a distance matrix into \n",
    "        a sorted distance matrix according to the order implied \n",
    "        by the hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    N = len(dist_mat)\n",
    "    flat_dist_mat = squareform(dist_mat)\n",
    "    res_linkage = linkage(flat_dist_mat, method=method,preserve_input=True)\n",
    "    res_order = seriation(res_linkage, N, N + N-2)\n",
    "    seriated_dist = np.zeros((N,N))\n",
    "    a,b = np.triu_indices(N,k=1)\n",
    "    seriated_dist[a,b] = dist_mat[ [res_order[i] for i in a], [res_order[j] for j in b]]\n",
    "    seriated_dist[b,a] = seriated_dist[a,b]\n",
    "    \n",
    "    return seriated_dist, res_order, res_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"ward\",\"single\",\"average\",\"complete\"]\n",
    "for method in methods:\n",
    "    print(\"Method:\\t\",method)\n",
    "    \n",
    "    ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(dist_mat,method)\n",
    "    \n",
    "    plt.pcolormesh(ordered_dist_mat)\n",
    "    plt.xlim([0,N])\n",
    "    plt.ylim([0,N])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = output_elmo.join(\n",
    "    csv, \n",
    "    how='inner',\n",
    "    lsuffix='_embed', \n",
    "    rsuffix='_ref'\n",
    ")\n",
    "data = data.loc[csv['averb_relation'] == -1]\n",
    "# Shuffle data for extra comparison\n",
    "# data = data.sample(frac=1)\n",
    "dist_mat = squareform(pdist(data.iloc[:, 0:1024]))\n",
    "\n",
    "ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(\n",
    "    dist_mat,\n",
    "    'complete')\n",
    "reordered_data = data\n",
    "reordered_data['temp_index'] = data.index\n",
    "reordered_data = reordered_data.reset_index(drop=True)\n",
    "reordered_data = reordered_data.iloc[res_order].reset_index(drop=True)\n",
    "reordered_data['order_cluster'] = reordered_data.index\n",
    "reordered_data = reordered_data.set_index('temp_index')\n",
    "\n",
    "# reordered_data = csv.join(\n",
    "#     reordered_data, \n",
    "#     how='inner',\n",
    "#     lsuffix='_embed', \n",
    "#     rsuffix='_ref'\n",
    "# )\n",
    "# reordered_data.to_csv(f'temp/BERT_anchorverb_elmo_order_cluster.csv')\n",
    "# reordered_data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/cephcyn/ChatlogGrapher/blob/master/data_processing.ipynb\n",
    "# for cosine similarity; see also sklearn documentation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute a distance metric column from some origin phrase and given some column name to calculate diff of\n",
    "def distance_word2vec(row, origin, colname):\n",
    "    try:\n",
    "        sim = cosine_similarity(get_phrase_vector(origin), get_phrase_vector(row[colname]))[0][0]\n",
    "    except:\n",
    "        sim = -1\n",
    "    return dict(zip(['distance'], [sim]))\n",
    "\n",
    "base_averb = 'encodes'\n",
    "output = csv.apply(\n",
    "    lambda row: distance_word2vec(\n",
    "        row, \n",
    "        base_averb, \n",
    "        'averb'\n",
    "    ), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output = csv.join(output).sort_values(by=['distance'], ascending=False)\n",
    "output.to_csv(f'temp/BERT_similarity_encodes.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
