{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = f'data/ebm_nlp_2_00/annotations/aggregated/hierarchical_labels/interventions/test/crowd'\n",
    "directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/participants/test/crowd'\n",
    "directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/outcomes/test/crowd'\n",
    "directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/train/crowd'\n",
    "directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/test/crowd'\n",
    "directory = f'data/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/test/gold'\n",
    "for filename in os.scandir(directory):\n",
    "    print(filename.path)\n",
    "    # assume that the filepath does not contain .s except for the final extension...\n",
    "    print(filename.name.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sentence tokenization\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('data/ebm_nlp_2_00/documents/6989377.txt', 'r') as f:\n",
    "    alltext = ''.join(f.readlines())\n",
    "    sentences = tokenize.sent_tokenize(alltext)\n",
    "    df = pd.DataFrame({\n",
    "        'URL': ['https://pubmed.ncbi.nlm.nih.gov/6989377'] * (2),\n",
    "        'ID': [6989377] * (2),\n",
    "        'Type': ['Title', 'Abstract'],\n",
    "        'Text': [sentences[0], ' '.join(sentences[1:])],\n",
    "        'ann_section': ['test'] * (2),\n",
    "        'ann_source': ['gold'] * (2)\n",
    "    })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "def sentence_tokenize(group):\n",
    "    row = group.reset_index(drop=True).loc[0]\n",
    "    sentences = tokenize.sent_tokenize(row['Text'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * (len(sentences)),\n",
    "        'ID': [row['ID']] * (len(sentences)),\n",
    "        'Type': [row['Type']] * (len(sentences)),\n",
    "        'Index': list(range(len(sentences))),\n",
    "        'Text': sentences,\n",
    "        'ann_section': ['test'] * (len(sentences)),\n",
    "        'ann_source': ['gold'] * (len(sentences))\n",
    "    })\n",
    "\n",
    "df_sentences = df.groupby(['ID', 'Type'], group_keys=False).apply(\n",
    "    lambda group: sentence_tokenize(group)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labeled_terms(tokens, tags):\n",
    "    in_tag = False\n",
    "    hits = []\n",
    "    hit_indices = []\n",
    "    sofar = []\n",
    "    for i in range(len(tokens)):\n",
    "        if int(tags[i]) != 0:\n",
    "            if not in_tag:\n",
    "                in_tag = True\n",
    "            sofar.append(tokens[i])\n",
    "        elif in_tag:\n",
    "            in_tag = False\n",
    "            hits.append(sofar)\n",
    "            hit_indices.append(i - len(sofar))\n",
    "            sofar = []\n",
    "    if in_tag:\n",
    "        hits.append(sofar)\n",
    "        hit_indices.append(len(tokens) - len(sofar))\n",
    "    return hits, hit_indices\n",
    "\n",
    "test_extract_labeled_terms = extract_labeled_terms(\n",
    "    ['This', 'is', 'an', 'example', 'sentence', '.'],\n",
    "    ['1',    '0',  '0',  '1',       '1',        '0']\n",
    ")\n",
    "test_extract_labeled_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract tokenization, part-of-speech tags from EBMNLP dataset\n",
    "def ebmnlp_addtdata(group):\n",
    "    group = df_sentences\n",
    "    group = group.sort_values(by=['Index'])\n",
    "    group = group.sort_values(by=['Type'], ascending=False)\n",
    "    group = group.reset_index()\n",
    "    id_num = group.iloc[0]['ID']\n",
    "    ann_section = group.iloc[0]['ann_section']\n",
    "    ann_source = group.iloc[0]['ann_source']\n",
    "    # extract split_tokens data from .tokens file\n",
    "    with open(f'data/ebm_nlp_2_00/documents/{id_num}.tokens', 'r') as f:\n",
    "        tokens = [s.strip() for s in f.readlines()]\n",
    "    split_tokens = []\n",
    "    for i in range(len(group.index)):\n",
    "        row = group.iloc[i]\n",
    "        rowtext = row['Text'].strip()\n",
    "        i_tokens = []\n",
    "        while len(tokens) > 0 and rowtext.find(tokens[0]) == 0:\n",
    "            i_tokens.append(tokens[0])\n",
    "            rowtext = rowtext[len(tokens[0]):].strip()\n",
    "            tokens = tokens[1:]\n",
    "        split_tokens.append(i_tokens)\n",
    "    metadata = pd.DataFrame({'split_tokens': split_tokens})\n",
    "    # extract POS data from .pos file\n",
    "    with open(f'data/ebm_nlp_2_00/documents/{id_num}.pos', 'r') as f:\n",
    "        pos = [s.strip() for s in f.readlines()]\n",
    "    pos_labels = []\n",
    "    for i in range(len(group.index)):\n",
    "        sent_tok_len = len(metadata.iloc[i]['split_tokens'])\n",
    "        pos_labels.append(pos[:sent_tok_len])\n",
    "        pos = pos[sent_tok_len:]\n",
    "    metadata['split_tokens_pos'] = pos_labels\n",
    "    # Extract the direct annotations...\n",
    "    for annlevel in [('starting_spans', 'ss'), ('hierarchical_labels', 'hl')]:\n",
    "        for anntype in [('participants', 'p'), ('interventions', 'i'), ('outcomes', 'o')]:\n",
    "            # read an intersection of (ss/hl)*(p/i/o)\n",
    "            # (also get annotation clumps while we're doing that)\n",
    "            with open(f'data/ebm_nlp_2_00/annotations/aggregated/{annlevel[0]}/{anntype[0]}/{ann_section}/{ann_source}/{id_num}.AGGREGATED.ann', 'r') as f:\n",
    "                annotations = [s.strip() for s in f.readlines()]\n",
    "            ann_col = []\n",
    "            ann_col_clumps = []\n",
    "            for i in range(len(group.index)):\n",
    "                sent_tok = metadata.iloc[i]['split_tokens']\n",
    "                sent_tok_len = len(sent_tok)\n",
    "                ann_col.append(annotations[:sent_tok_len])\n",
    "                ann_col_clumps.append(extract_labeled_terms(sent_tok, annotations[:sent_tok_len]))\n",
    "                annotations = annotations[sent_tok_len:]\n",
    "            metadata[f'{annlevel[1]}_{anntype[1]}'] = ann_col\n",
    "            metadata[f'{annlevel[1]}_{anntype[1]}_clumps'] = ann_col_clumps\n",
    "    metadata['ID'] = group['ID']\n",
    "    metadata['Type'] = group['Type']\n",
    "    metadata['Index'] = group['Index']\n",
    "    metadata['Text'] = group['Text']\n",
    "    return metadata\n",
    "\n",
    "df_sentences.groupby(['ID'], group_keys=False).apply(\n",
    "    lambda group: ebmnlp_addtdata(group)\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
