{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
      "Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cephcyn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*projection.*weight\n",
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*bias_hh.*\n",
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*weight_ih.*\n",
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*projection.*bias\n",
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*weight_hh.*\n",
      "WARNING:allennlp.nn.initializers:Did not use initialization regex that was passed: .*bias_ih.*\n"
     ]
    }
   ],
   "source": [
    "# Do one-time large imports\n",
    "\n",
    "# For sentence tokenization\n",
    "from nltk import tokenize\n",
    "\n",
    "# For coreference resolution\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.coref\n",
    "coref_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n",
    ")\n",
    "\n",
    "# For part-of-speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# For dependency parsing\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "dependency_predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>What Does BERT Look At? An Analysis of BERT's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  ID      Type  \\\n",
       "0   https://www.semanticscholar.org/paper/BERT%3A-...   0     Title   \n",
       "1   https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract   \n",
       "2   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title   \n",
       "3   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract   \n",
       "4   https://www.semanticscholar.org/paper/DistilBE...   2     Title   \n",
       "5   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract   \n",
       "6   https://www.semanticscholar.org/paper/BERT-Red...   3     Title   \n",
       "7   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract   \n",
       "8   https://www.semanticscholar.org/paper/What-Doe...   4     Title   \n",
       "9   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract   \n",
       "10  https://www.semanticscholar.org/paper/Passage-...   5     Title   \n",
       "11  https://www.semanticscholar.org/paper/Passage-...   5  Abstract   \n",
       "12  https://www.semanticscholar.org/paper/Assessin...   6     Title   \n",
       "13  https://www.semanticscholar.org/paper/Assessin...   6  Abstract   \n",
       "14  https://www.semanticscholar.org/paper/Beto%2C-...   7     Title   \n",
       "15  https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract   \n",
       "16  https://www.semanticscholar.org/paper/TinyBERT...   8     Title   \n",
       "17  https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract   \n",
       "18  https://www.semanticscholar.org/paper/Fine-tun...   9     Title   \n",
       "19  https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract   \n",
       "20  https://www.semanticscholar.org/paper/BERT-Pos...  10     Title   \n",
       "21  https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract   \n",
       "22  https://www.semanticscholar.org/paper/How-to-F...  11     Title   \n",
       "23  https://www.semanticscholar.org/paper/How-to-F...  11  Abstract   \n",
       "24  https://www.semanticscholar.org/paper/BERT-has...  12     Title   \n",
       "25  https://www.semanticscholar.org/paper/BERT-has...  12  Abstract   \n",
       "26  https://www.semanticscholar.org/paper/Utilizin...  13     Title   \n",
       "27  https://www.semanticscholar.org/paper/Utilizin...  13  Abstract   \n",
       "28  https://www.semanticscholar.org/paper/Simple-A...  14     Title   \n",
       "29  https://www.semanticscholar.org/paper/Simple-A...  14  Abstract   \n",
       "30  https://www.semanticscholar.org/paper/Revealin...  15     Title   \n",
       "31  https://www.semanticscholar.org/paper/Revealin...  15  Abstract   \n",
       "32  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title   \n",
       "33  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract   \n",
       "34  https://www.semanticscholar.org/paper/Multi-pa...  17     Title   \n",
       "35  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract   \n",
       "36  https://www.semanticscholar.org/paper/BERT-for...  18     Title   \n",
       "37  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract   \n",
       "38  https://www.semanticscholar.org/paper/BERT-wit...  19     Title   \n",
       "39  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract   \n",
       "40  https://www.semanticscholar.org/paper/Understa...  20     Title   \n",
       "41  https://www.semanticscholar.org/paper/Understa...  20  Abstract   \n",
       "42  https://www.semanticscholar.org/paper/Simple-B...  21     Title   \n",
       "43  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract   \n",
       "44  https://www.semanticscholar.org/paper/BERT-and...  22     Title   \n",
       "45  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract   \n",
       "46  https://www.semanticscholar.org/paper/What-BER...  23     Title   \n",
       "47  https://www.semanticscholar.org/paper/What-BER...  23  Abstract   \n",
       "48  https://www.semanticscholar.org/paper/Visualiz...  24     Title   \n",
       "49  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract   \n",
       "50  https://www.semanticscholar.org/paper/Conditio...  25     Title   \n",
       "51  https://www.semanticscholar.org/paper/Conditio...  25  Abstract   \n",
       "52  https://www.semanticscholar.org/paper/Small-an...  26     Title   \n",
       "53  https://www.semanticscholar.org/paper/Small-an...  26  Abstract   \n",
       "54  https://www.semanticscholar.org/paper/Data-Aug...  27     Title   \n",
       "55  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract   \n",
       "56  https://www.semanticscholar.org/paper/BERT-is-...  28     Title   \n",
       "57  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract   \n",
       "58  https://www.semanticscholar.org/paper/How-Cont...  29     Title   \n",
       "59  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract   \n",
       "\n",
       "                                                 Text  \n",
       "0   BERT: Pre-training of Deep Bidirectional Trans...  \n",
       "1   We introduce a new language representation mod...  \n",
       "2   RoBERTa: A Robustly Optimized BERT Pretraining...  \n",
       "3   Language model pretraining has led to signific...  \n",
       "4   DistilBERT, a distilled version of BERT: small...  \n",
       "5   As Transfer Learning from large-scale pre-trai...  \n",
       "6         BERT Rediscovers the Classical NLP Pipeline  \n",
       "7   Pre-trained text encoders have rapidly advance...  \n",
       "8   What Does BERT Look At? An Analysis of BERT's ...  \n",
       "9   Large pre-trained neural networks such as BERT...  \n",
       "10                       Passage Re-ranking with BERT  \n",
       "11  Recently, neural models pretrained on a langua...  \n",
       "12               Assessing BERT's Syntactic Abilities  \n",
       "13  I assess the extent to which the recently intr...  \n",
       "14  Beto, Bentz, Becas: The Surprising Cross-Lingu...  \n",
       "15  Pretrained contextual representation models (P...  \n",
       "16  TinyBERT: Distilling BERT for Natural Language...  \n",
       "17  Language model pre-training, such as BERT, has...  \n",
       "18        Fine-tune BERT for Extractive Summarization  \n",
       "19  BERT, a pre-trained Transformer model, has ach...  \n",
       "20  BERT Post-Training for Review Reading Comprehe...  \n",
       "21  Question-answering plays an important role in ...  \n",
       "22     How to Fine-Tune BERT for Text Classification?  \n",
       "23  Language model pre-training has proven to be u...  \n",
       "24  BERT has a Mouth, and It Must Speak: BERT as a...  \n",
       "25  We show that BERT (Devlin et al., 2018) is a M...  \n",
       "26  Utilizing BERT for Aspect-Based Sentiment Anal...  \n",
       "27  Aspect-based sentiment analysis (ABSA), which ...  \n",
       "28  Simple Applications of BERT for Ad Hoc Documen...  \n",
       "29  Following recent successes in applying BERT to...  \n",
       "30                 Revealing the Dark Secrets of BERT  \n",
       "31  BERT-based architectures currently give state-...  \n",
       "32  Pre-Training with Whole Word Masking for Chine...  \n",
       "33  Bidirectional Encoder Representations from Tra...  \n",
       "34  Multi-passage BERT: A Globally Normalized BERT...  \n",
       "35  BERT model has been successfully applied to op...  \n",
       "36  BERT for Joint Intent Classification and Slot ...  \n",
       "37  Intent classification and slot filling are two...  \n",
       "38  BERT with History Answer Embedding for Convers...  \n",
       "39  Conversational search is an emerging topic in ...  \n",
       "40     Understanding the Behaviors of BERT in Ranking  \n",
       "41  This paper studies the performances and behavi...  \n",
       "42  Simple BERT Models for Relation Extraction and...  \n",
       "43  We present simple BERT-based models for relati...  \n",
       "44  BERT and PALs: Projected Attention Layers for ...  \n",
       "45  Multi-task learning allows the sharing of usef...  \n",
       "46  What BERT Is Not: Lessons from a New Suite of ...  \n",
       "47  Pre-training by language modeling has become a...  \n",
       "48  Visualizing and Understanding the Effectivenes...  \n",
       "49  Language model pre-training, such as BERT, has...  \n",
       "50           Conditional BERT Contextual Augmentation  \n",
       "51  Data augmentation methods are often applied to...  \n",
       "52  Small and Practical BERT Models for Sequence L...  \n",
       "53  We propose a practical scheme to train a singl...  \n",
       "54  Data Augmentation for BERT Fine-Tuning in Open...  \n",
       "55  Recently, a simple combination of passage retr...  \n",
       "56  BERT is Not a Knowledge Base (Yet): Factual Kn...  \n",
       "57  The BERT language model (LM) (Devlin et al., 2...  \n",
       "58  How Contextual are Contextualized Word Represe...  \n",
       "59  Replacing static word embeddings with contextu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Identify the term we are splitting on\n",
    "search_word = \"BERT\"\n",
    "\n",
    "# Read in the dataframe containing entire paper abstracts (NOT pre-split into sentences)\n",
    "df = pd.read_csv('data/nlp-align_BERT.csv')\n",
    "\n",
    "# Split apart the 'Title' and 'Abstract' columns\n",
    "def separate_title_abstract(group):\n",
    "    row = group.loc[0]\n",
    "    abs_text = tokenize.sent_tokenize(row['Abstract'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * 2,\n",
    "        'ID': [row['ID']] * 2,\n",
    "        'Type': ['Title', 'Abstract'],\n",
    "        'Text': [row['Title'], row['Abstract']]\n",
    "    })\n",
    "\n",
    "# Restructure the dataframe to be more usable...\n",
    "df = df.groupby('ID', group_keys=False).apply(\n",
    "    lambda row: separate_title_abstract(row)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Unlike recent language representation models, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a result, the pre-trained BERT model can be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We present a replication study of BERT pretrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We release our models and code.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We focus on one such model, BERT, and aim to q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT's attention heads exhibit patterns such a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What Does BERT Look At?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>An Analysis of BERT's Attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>The BERT model performs remarkably well on all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>A new release of BERT (Devlin, 2018) includes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We compare mBERT with the best-published metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>TinyBERT is also significantly better than sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe BERTSUM, a simple v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We fine-tune the pre-trained model from BERT a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, an upgraded version of BERT has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, previous work trains BERT by viewing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To tackle this issue, we propose a multi-passa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently a new language representation model, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>By using PALs in parallel with BERT layers, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Third, the lower layers of BERT are more invar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>task.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The well trained conditional BERT can be appli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Starting from a public multilingual BERT check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Petroni et al.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>(2019) take this as evidence that BERT memoriz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>More specifically, we show that BERT's precisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>As a remedy, we propose E-BERT, an extension o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>E-BERT outperforms both BERT and ERNIE (Zhang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In all layers of ELMo, BERT, and GPT-2, on ave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing the Geometry of BERT, ELMo, and GPT-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  ID      Type  Index  \\\n",
       "0    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      0   \n",
       "1    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      1   \n",
       "2    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      2   \n",
       "3    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      3   \n",
       "4    https://www.semanticscholar.org/paper/BERT%3A-...   0     Title      0   \n",
       "5    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      0   \n",
       "6    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      1   \n",
       "7    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      2   \n",
       "8    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      3   \n",
       "9    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      4   \n",
       "10   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      5   \n",
       "11   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      6   \n",
       "12   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title      0   \n",
       "13   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      0   \n",
       "14   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      1   \n",
       "15   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      2   \n",
       "16   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      3   \n",
       "17   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      4   \n",
       "18   https://www.semanticscholar.org/paper/DistilBE...   2     Title      0   \n",
       "19   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      0   \n",
       "20   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      1   \n",
       "21   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      2   \n",
       "22   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      3   \n",
       "23   https://www.semanticscholar.org/paper/BERT-Red...   3     Title      0   \n",
       "24   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      0   \n",
       "25   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      1   \n",
       "26   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      2   \n",
       "27   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      3   \n",
       "28   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      4   \n",
       "29   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      5   \n",
       "30   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      6   \n",
       "31   https://www.semanticscholar.org/paper/What-Doe...   4     Title      0   \n",
       "32   https://www.semanticscholar.org/paper/What-Doe...   4     Title      1   \n",
       "33   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      0   \n",
       "34   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      1   \n",
       "35   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      2   \n",
       "36   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      3   \n",
       "37   https://www.semanticscholar.org/paper/Passage-...   5     Title      0   \n",
       "38   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      0   \n",
       "39   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      1   \n",
       "40   https://www.semanticscholar.org/paper/Assessin...   6     Title      0   \n",
       "41   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      0   \n",
       "42   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      1   \n",
       "43   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      2   \n",
       "44   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      3   \n",
       "45   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      4   \n",
       "46   https://www.semanticscholar.org/paper/Beto%2C-...   7     Title      0   \n",
       "47   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      0   \n",
       "48   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      1   \n",
       "49   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      2   \n",
       "50   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      3   \n",
       "51   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      4   \n",
       "52   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      5   \n",
       "53   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      6   \n",
       "54   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      7   \n",
       "55   https://www.semanticscholar.org/paper/TinyBERT...   8     Title      0   \n",
       "56   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      0   \n",
       "57   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      1   \n",
       "58   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      2   \n",
       "59   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      3   \n",
       "60   https://www.semanticscholar.org/paper/Fine-tun...   9     Title      0   \n",
       "61   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      0   \n",
       "62   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      1   \n",
       "63   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      2   \n",
       "64   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      3   \n",
       "65   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      4   \n",
       "66   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      5   \n",
       "67   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      6   \n",
       "68   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      7   \n",
       "69   https://www.semanticscholar.org/paper/BERT-Pos...  10     Title      0   \n",
       "70   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      0   \n",
       "71   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      1   \n",
       "72   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      2   \n",
       "73   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      3   \n",
       "74   https://www.semanticscholar.org/paper/How-to-F...  11     Title      0   \n",
       "75   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      0   \n",
       "76   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      1   \n",
       "77   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      2   \n",
       "78   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      3   \n",
       "79   https://www.semanticscholar.org/paper/BERT-has...  12     Title      0   \n",
       "80   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      0   \n",
       "81   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      1   \n",
       "82   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      2   \n",
       "83   https://www.semanticscholar.org/paper/Utilizin...  13     Title      0   \n",
       "84   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      0   \n",
       "85   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      1   \n",
       "86   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      2   \n",
       "87   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      3   \n",
       "88   https://www.semanticscholar.org/paper/Simple-A...  14     Title      0   \n",
       "89   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      0   \n",
       "90   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      1   \n",
       "91   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      2   \n",
       "92   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      3   \n",
       "93   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      4   \n",
       "94   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      5   \n",
       "95   https://www.semanticscholar.org/paper/Revealin...  15     Title      0   \n",
       "96   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      0   \n",
       "97   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      1   \n",
       "98   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      2   \n",
       "99   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      3   \n",
       "100  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      4   \n",
       "101  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      5   \n",
       "102  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      6   \n",
       "103  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      7   \n",
       "104  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      8   \n",
       "105  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title      0   \n",
       "106  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      0   \n",
       "107  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      1   \n",
       "108  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      2   \n",
       "109  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      3   \n",
       "110  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      4   \n",
       "111  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      5   \n",
       "112  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      6   \n",
       "113  https://www.semanticscholar.org/paper/Multi-pa...  17     Title      0   \n",
       "114  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      0   \n",
       "115  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      1   \n",
       "116  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      2   \n",
       "117  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      3   \n",
       "118  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      4   \n",
       "119  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      5   \n",
       "120  https://www.semanticscholar.org/paper/BERT-for...  18     Title      0   \n",
       "121  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      0   \n",
       "122  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      1   \n",
       "123  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      2   \n",
       "124  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      3   \n",
       "125  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      4   \n",
       "126  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      5   \n",
       "127  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      6   \n",
       "128  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      7   \n",
       "129  https://www.semanticscholar.org/paper/BERT-wit...  19     Title      0   \n",
       "130  https://www.semanticscholar.org/paper/Understa...  20  Abstract      0   \n",
       "131  https://www.semanticscholar.org/paper/Understa...  20  Abstract      1   \n",
       "132  https://www.semanticscholar.org/paper/Understa...  20  Abstract      2   \n",
       "133  https://www.semanticscholar.org/paper/Understa...  20  Abstract      3   \n",
       "134  https://www.semanticscholar.org/paper/Understa...  20  Abstract      4   \n",
       "135  https://www.semanticscholar.org/paper/Understa...  20     Title      0   \n",
       "136  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      0   \n",
       "137  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      1   \n",
       "138  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      2   \n",
       "139  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      3   \n",
       "140  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      4   \n",
       "141  https://www.semanticscholar.org/paper/Simple-B...  21     Title      0   \n",
       "142  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      0   \n",
       "143  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      1   \n",
       "144  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      2   \n",
       "145  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      3   \n",
       "146  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      4   \n",
       "147  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      5   \n",
       "148  https://www.semanticscholar.org/paper/BERT-and...  22     Title      0   \n",
       "149  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      0   \n",
       "150  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      1   \n",
       "151  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      2   \n",
       "152  https://www.semanticscholar.org/paper/What-BER...  23     Title      0   \n",
       "153  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      0   \n",
       "154  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      1   \n",
       "155  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      2   \n",
       "156  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      3   \n",
       "157  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      4   \n",
       "158  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      5   \n",
       "159  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      6   \n",
       "160  https://www.semanticscholar.org/paper/Visualiz...  24     Title      0   \n",
       "161  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      0   \n",
       "162  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      1   \n",
       "163  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      2   \n",
       "164  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      3   \n",
       "165  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      4   \n",
       "166  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      5   \n",
       "167  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      6   \n",
       "168  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      7   \n",
       "169  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      8   \n",
       "170  https://www.semanticscholar.org/paper/Conditio...  25     Title      0   \n",
       "171  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      0   \n",
       "172  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      1   \n",
       "173  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      2   \n",
       "174  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      3   \n",
       "175  https://www.semanticscholar.org/paper/Small-an...  26     Title      0   \n",
       "176  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      0   \n",
       "177  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      1   \n",
       "178  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      2   \n",
       "179  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      3   \n",
       "180  https://www.semanticscholar.org/paper/Data-Aug...  27     Title      0   \n",
       "181  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      0   \n",
       "182  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      1   \n",
       "183  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      2   \n",
       "184  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      3   \n",
       "185  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      4   \n",
       "186  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      5   \n",
       "187  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      6   \n",
       "188  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      7   \n",
       "189  https://www.semanticscholar.org/paper/BERT-is-...  28     Title      0   \n",
       "190  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      0   \n",
       "191  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      1   \n",
       "192  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      2   \n",
       "193  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      3   \n",
       "194  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      4   \n",
       "195  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      5   \n",
       "196  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      6   \n",
       "197  https://www.semanticscholar.org/paper/How-Cont...  29     Title      0   \n",
       "198  https://www.semanticscholar.org/paper/How-Cont...  29     Title      1   \n",
       "\n",
       "                                                  Text  \n",
       "0    We introduce a new language representation mod...  \n",
       "1    Unlike recent language representation models, ...  \n",
       "2    As a result, the pre-trained BERT model can be...  \n",
       "3    It obtains new state-of-the-art results on ele...  \n",
       "4    BERT: Pre-training of Deep Bidirectional Trans...  \n",
       "5    Language model pretraining has led to signific...  \n",
       "6    Training is computationally expensive, often d...  \n",
       "7    We present a replication study of BERT pretrai...  \n",
       "8    We find that BERT was significantly undertrain...  \n",
       "9    Our best model achieves state-of-the-art resul...  \n",
       "10   These results highlight the importance of prev...  \n",
       "11                     We release our models and code.  \n",
       "12   RoBERTa: A Robustly Optimized BERT Pretraining...  \n",
       "13   As Transfer Learning from large-scale pre-trai...  \n",
       "14   In this work, we propose a method to pre-train...  \n",
       "15   While most prior work investigated the use of ...  \n",
       "16   To leverage the inductive biases learned by la...  \n",
       "17   Our smaller, faster and lighter model is cheap...  \n",
       "18   DistilBERT, a distilled version of BERT: small...  \n",
       "19   Pre-trained text encoders have rapidly advance...  \n",
       "20   We focus on one such model, BERT, and aim to q...  \n",
       "21   We find that the model represents the steps of...  \n",
       "22   Qualitative analysis reveals that the model ca...  \n",
       "23         BERT Rediscovers the Classical NLP Pipeline  \n",
       "24   Large pre-trained neural networks such as BERT...  \n",
       "25   Most recent analysis has focused on model outp...  \n",
       "26   Complementary to these works, we propose metho...  \n",
       "27   BERT's attention heads exhibit patterns such a...  \n",
       "28   We further show that certain attention heads c...  \n",
       "29   For example, we find heads that attend to the ...  \n",
       "30   Lastly, we propose an attention-based probing ...  \n",
       "31                             What Does BERT Look At?  \n",
       "32                     An Analysis of BERT's Attention  \n",
       "33   Recently, neural models pretrained on a langua...  \n",
       "34   In this paper, we describe a simple re-impleme...  \n",
       "35   Our system is the state of the art on the TREC...  \n",
       "36   The code to reproduce our results is available...  \n",
       "37                        Passage Re-ranking with BERT  \n",
       "38   I assess the extent to which the recently intr...  \n",
       "39   The BERT model performs remarkably well on all...  \n",
       "40                Assessing BERT's Syntactic Abilities  \n",
       "41   Pretrained contextual representation models (P...  \n",
       "42   A new release of BERT (Devlin, 2018) includes ...  \n",
       "43   This paper explores the broader cross-lingual ...  \n",
       "44   We compare mBERT with the best-published metho...  \n",
       "45   Additionally, we investigate the most effectiv...  \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...  \n",
       "47   Language model pre-training, such as BERT, has...  \n",
       "48   However, pre-trained language models are usual...  \n",
       "49   To accelerate inference and reduce model size ...  \n",
       "50   By leveraging this new KD method, the plenty o...  \n",
       "51   Moreover, we introduce a new two-stage learnin...  \n",
       "52   This framework ensures that TinyBERT can captu...  \n",
       "53   TinyBERT is empirically effective and achieves...  \n",
       "54   TinyBERT is also significantly better than sta...  \n",
       "55   TinyBERT: Distilling BERT for Natural Language...  \n",
       "56   BERT, a pre-trained Transformer model, has ach...  \n",
       "57   In this paper, we describe BERTSUM, a simple v...  \n",
       "58   Our system is the state of the art on the CNN/...  \n",
       "59   The codes to reproduce our results are availab...  \n",
       "60         Fine-tune BERT for Extractive Summarization  \n",
       "61   Question-answering plays an important role in ...  \n",
       "62   Inspired by the recent success of machine read...  \n",
       "63   To the best of our knowledge, no existing work...  \n",
       "64   In this work, we first build an RRC dataset ca...  \n",
       "65   Since ReviewRC has limited training examples f...  \n",
       "66   To show the generality of the approach, the pr...  \n",
       "67   Experimental results demonstrate that the prop...  \n",
       "68   The datasets and code are available at this ht...  \n",
       "69   BERT Post-Training for Review Reading Comprehe...  \n",
       "70   Language model pre-training has proven to be u...  \n",
       "71   As a state-of-the-art language model pre-train...  \n",
       "72   In this paper, we conduct exhaustive experimen...  \n",
       "73   Finally, the proposed solution obtains new sta...  \n",
       "74      How to Fine-Tune BERT for Text Classification?  \n",
       "75   We show that BERT (Devlin et al., 2018) is a M...  \n",
       "76   This formulation gives way to a natural proced...  \n",
       "77   We generate from BERT and find that it can pro...  \n",
       "78   Compared to the generations of a traditional l...  \n",
       "79   BERT has a Mouth, and It Must Speak: BERT as a...  \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...  \n",
       "81   In this paper, we construct an auxiliary sente...  \n",
       "82   We fine-tune the pre-trained model from BERT a...  \n",
       "83   Utilizing BERT for Aspect-Based Sentiment Anal...  \n",
       "84   Following recent successes in applying BERT to...  \n",
       "85   This required confronting the challenge posed ...  \n",
       "86   We address this issue by applying inference on...  \n",
       "87   Experiments on TREC microblog and newswire tes...  \n",
       "88   Simple Applications of BERT for Ad Hoc Documen...  \n",
       "89   BERT-based architectures currently give state-...  \n",
       "90   In the current work, we focus on the interpret...  \n",
       "91   Using a subset of GLUE tasks and a set of hand...  \n",
       "92   Our findings suggest that there is a limited s...  \n",
       "93   While different heads consistently use the sam...  \n",
       "94   We show that manually disabling attention in c...  \n",
       "95                  Revealing the Dark Secrets of BERT  \n",
       "96   Bidirectional Encoder Representations from Tra...  \n",
       "97   Recently, an upgraded version of BERT has been...  \n",
       "98   In this technical report, we adapt whole word ...  \n",
       "99   The model was trained on the latest Chinese Wi...  \n",
       "100  We aim to provide easy extensibility and bette...  \n",
       "101  The model is verified on various NLP tasks, ac...  \n",
       "102  Experimental results on these datasets show th...  \n",
       "103  Moreover, we also examine the effectiveness of...  \n",
       "104  We release the pre-trained model (both TensorF...  \n",
       "105  Pre-Training with Whole Word Masking for Chine...  \n",
       "106  BERT model has been successfully applied to op...  \n",
       "107  However, previous work trains BERT by viewing ...  \n",
       "108  To tackle this issue, we propose a multi-passa...  \n",
       "109  In addition, we find that splitting articles i...  \n",
       "110  By leveraging a passage ranker to select high-...  \n",
       "111  Experiments on four standard benchmarks showed...  \n",
       "112  In particular, on the OpenSQuAD dataset, our m...  \n",
       "113  Multi-passage BERT: A Globally Normalized BERT...  \n",
       "114  Intent classification and slot filling are two...  \n",
       "115  They often suffer from small-scale human-label...  \n",
       "116  Recently a new language representation model, ...  \n",
       "117  However, there has not been much effort on exp...  \n",
       "118  In this work, we propose a joint intent classi...  \n",
       "119  Experimental results demonstrate that our prop...  \n",
       "120  BERT for Joint Intent Classification and Slot ...  \n",
       "121  Conversational search is an emerging topic in ...  \n",
       "122  One of the major challenges to multi-turn conv...  \n",
       "123  Existing methods either prepend history turns ...  \n",
       "124  We propose a conceptually simple yet highly ef...  \n",
       "125  It enables seamless integration of conversatio...  \n",
       "126  We first explain our view that ConvQA is a sim...  \n",
       "127  We further demonstrate the effectiveness of ou...  \n",
       "128  Finally, we analyze the impact of different nu...  \n",
       "129  BERT with History Answer Embedding for Convers...  \n",
       "130  This paper studies the performances and behavi...  \n",
       "131  We explore several different ways to leverage ...  \n",
       "132  Experimental results on MS MARCO demonstrate t...  \n",
       "133  Experimental results on TREC show the gaps bet...  \n",
       "134  Analyses illustrate how BERT allocates its att...  \n",
       "135     Understanding the Behaviors of BERT in Ranking  \n",
       "136  We present simple BERT-based models for relati...  \n",
       "137  In recent years, state-of-the-art performance ...  \n",
       "138  In this paper, extensive experiments on datase...  \n",
       "139  To our knowledge, we are the first to successf...  \n",
       "140  Our models provide strong baselines for future...  \n",
       "141  Simple BERT Models for Relation Extraction and...  \n",
       "142  Multi-task learning allows the sharing of usef...  \n",
       "143  In natural language processing several recent ...  \n",
       "144  These results are based on fine-tuning on each...  \n",
       "145  We explore the multi-task learning setting for...  \n",
       "146  We introduce new adaptation modules, PALs or `...  \n",
       "147  By using PALs in parallel with BERT layers, we...  \n",
       "148  BERT and PALs: Projected Attention Layers for ...  \n",
       "149  Pre-training by language modeling has become a...  \n",
       "150  In this paper we introduce a suite of diagnost...  \n",
       "151  As a case study, we apply these diagnostics to...  \n",
       "152  What BERT Is Not: Lessons from a New Suite of ...  \n",
       "153  Language model pre-training, such as BERT, has...  \n",
       "154  However, it is unclear why the pre-training-th...  \n",
       "155  In this paper, we propose to visualize loss la...  \n",
       "156  First, we find that pre-training reaches a goo...  \n",
       "157  We also demonstrate that the fine-tuning proce...  \n",
       "158  Second, the visualization results indicate tha...  \n",
       "159  Third, the lower layers of BERT are more invar...  \n",
       "160  Visualizing and Understanding the Effectivenes...  \n",
       "161  Data augmentation methods are often applied to...  \n",
       "162  Recently proposed contextual augmentation augm...  \n",
       "163  Bidirectional Encoder Representations from Tra...  \n",
       "164  We propose a novel data augmentation method fo...  \n",
       "165  We retrofit BERT to conditional BERT by introd...  \n",
       "166  In our paper, “conditional masked language mod...  \n",
       "167                                              task.  \n",
       "168  The well trained conditional BERT can be appli...  \n",
       "169  Experiments on six various different text clas...  \n",
       "170           Conditional BERT Contextual Augmentation  \n",
       "171  We propose a practical scheme to train a singl...  \n",
       "172  Starting from a public multilingual BERT check...  \n",
       "173  We show that our model especially outperforms ...  \n",
       "174  We showcase the effectiveness of our method by...  \n",
       "175  Small and Practical BERT Models for Sequence L...  \n",
       "176  Recently, a simple combination of passage retr...  \n",
       "177  In this paper, we present a data augmentation ...  \n",
       "178  We apply a stage-wise approach to fine tuning ...  \n",
       "179  Experimental results show large gains in effec...  \n",
       "180  Data Augmentation for BERT Fine-Tuning in Open...  \n",
       "181  The BERT language model (LM) (Devlin et al., 2...  \n",
       "182                                     Petroni et al.  \n",
       "183  (2019) take this as evidence that BERT memoriz...  \n",
       "184  We take issue with this interpretation and arg...  \n",
       "185  More specifically, we show that BERT's precisi...  \n",
       "186  As a remedy, we propose E-BERT, an extension o...  \n",
       "187  E-BERT outperforms both BERT and ERNIE (Zhang ...  \n",
       "188  We take this as evidence that E-BERT is richer...  \n",
       "189  BERT is Not a Knowledge Base (Yet): Factual Kn...  \n",
       "190  Replacing static word embeddings with contextu...  \n",
       "191  However, just how contextual are the contextua...  \n",
       "192  Are there infinitely many context-specific rep...  \n",
       "193  For one, we find that the contextualized repre...  \n",
       "194  While representations of the same word in diff...  \n",
       "195  This suggests that upper layers of contextuali...  \n",
       "196  In all layers of ELMo, BERT, and GPT-2, on ave...  \n",
       "197  How Contextual are Contextualized Word Represe...  \n",
       "198  Comparing the Geometry of BERT, ELMo, and GPT-...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the full-abstract CSV into a CSV containing individual sentences instead\n",
    "def sentence_tokenize(group):\n",
    "    row = group.reset_index(drop=True).loc[0]\n",
    "    sentences = tokenize.sent_tokenize(row['Text'])\n",
    "    return pd.DataFrame({\n",
    "        'URL': [row['URL']] * (len(sentences)),\n",
    "        'ID': [row['ID']] * (len(sentences)),\n",
    "        'Type': [row['Type']] * (len(sentences)),\n",
    "        'Index': list(range(len(sentences))),\n",
    "        'Text': sentences\n",
    "    })\n",
    "\n",
    "df_sentences = df.groupby(['ID', 'Type'], group_keys=False).apply(\n",
    "    lambda row: sentence_tokenize(row)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a test dataframe so we can run models without taking impractically long\n",
    "# # TODO: this is causing some type inconsistencies, fix those?\n",
    "\n",
    "# temp_df = pd.DataFrame.from_dict(\n",
    "#     {'URL': 'abc', \n",
    "#      'ID': '0', \n",
    "#      'Title': 'Paper Title',\n",
    "#      'Abstract': 'The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. Petroni et al. (2019) take this as evidence that it memorizes factual knowledge during pre-training. We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian.'\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting functions\n",
    "\n",
    "Assume we have an input dataframe with some number of columns, at least one of which is titled `Text` and is the column containing each sentence of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_0 is the text literally preceding the anchor\n",
    "# split_1 is the text that the anchor consists of\n",
    "# split_2 is the text literally following the anchor\n",
    "# split_tokens is the list of tokens that split identifies\n",
    "# split_anchor_span is a tuple (anchor_first_token_index, anchor_last_token_index) or null if there is no anchor\n",
    "splitting_headers = ['split_0','split_1','split_2', 'split_tokens', 'split_anchor_span']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_0</th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>split_anchor_span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, which stands for Bidirectional Encoder Repre...</td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unlike recent language representation models,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre-train deep bidirectional re...</td>\n",
       "      <td>[Unlike, recent, language, representation, mod...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a result, the pre-trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine-tuned with just one addition...</td>\n",
       "      <td>[As, a, result,, the, pre-trained, BERT, model...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[It, obtains, new, state-of-the-art, results, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>: Pre-training of Deep Bidirectional Transform...</td>\n",
       "      <td>[BERT, :, Pre-training, of, Deep, Bidirectiona...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Training, is, computationally, expensive,, of...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining (Devlin et al., 2019) that careful...</td>\n",
       "      <td>[We, present, a, replication, study, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained, and can match ...</td>\n",
       "      <td>[We, find, that, BERT, was, significantly, und...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, best, model, achieves, state-of-the-art,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, highlight, the, importance, o...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, our, models, and, code.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ro</td>\n",
       "      <td>BERT</td>\n",
       "      <td>a: A Robustly Optimized BERT Pretraining Approach</td>\n",
       "      <td>[Ro, BERT, a:, A, Robustly, Optimized, BERT, P...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[As, Transfer, Learning, from, large-scale, pr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, which can then be fine-tuned with good perfo...</td>\n",
       "      <td>[In, this, work,, we, propose, a, method, to, ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model by 40%, while retaining 97% of its langu...</td>\n",
       "      <td>[While, most, prior, work, investigated, the, ...</td>\n",
       "      <td>(33, 34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, leverage, the, inductive, biases, learned...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, smaller,, faster, and, lighter, model, i...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Distil</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, a distilled version of BERT: smaller, faster...</td>\n",
       "      <td>[Distil, BERT, ,, a, distilled, version, of, B...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-trained, text, encoders, have, rapidly, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>We focus on one such model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "      <td>[We, focus, on, one, such, model,, BERT, ,, an...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, find, that, the, model, represents, the, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Qualitative, analysis, reveals, that, the, mo...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Large pre-trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP, motivati...</td>\n",
       "      <td>[Large, pre-trained, neural, networks, such, a...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Most, recent, analysis, has, focused, on, mod...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>[Complementary, to, these, works,, we, propose...</td>\n",
       "      <td>(19, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s attention heads exhibit patterns such as at...</td>\n",
       "      <td>[BERT, 's, attention, heads, exhibit, patterns...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, show, that, certain, attention, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, example,, we, find, heads, that, attend,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s attention.</td>\n",
       "      <td>[Lastly,, we, propose, an, attention-based, pr...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At?</td>\n",
       "      <td>[What, Does, BERT, Look, At?]</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>An Analysis of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s Attention</td>\n",
       "      <td>[An, Analysis, of, BERT, 's, Attention]</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "      <td>[Recently,, neural, models, pretrained, on, a,...</td>\n",
       "      <td>(23, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for query-based passage re-ranking.</td>\n",
       "      <td>[In, this, paper,, we, describe, a, simple, re...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, code, to, reproduce, our, results, is, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Passage, Re-ranking, with, BERT]</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model captures English syntactic phenomena, us...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model performs remarkably well on all cases.</td>\n",
       "      <td>[The, BERT, model, performs, remarkably, well,...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s Syntactic Abilities</td>\n",
       "      <td>[Assessing, BERT, 's, Syntactic, Abilities]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "      <td>[A, new, release, of, BERT, (Devlin,, 2018), i...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "      <td>[This, paper, explores, the, broader, cross-li...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>We compare m</td>\n",
       "      <td>BERT</td>\n",
       "      <td>with the best-published methods for zero-shot ...</td>\n",
       "      <td>[We, compare, m, BERT, with, the, best-publish...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner, determine to what extent mBERT...</td>\n",
       "      <td>[Additionally,, we, investigate, the, most, ef...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Beto,, Bentz,, Becas:, The, Surprising, Cross...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has significantly improved the performances ...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, pre-trained, language, models, are,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, accelerate, inference, and, reduce, model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be well transferred to a small student Tin...</td>\n",
       "      <td>[By, leveraging, this, new, KD, method,, the, ...</td>\n",
       "      <td>(15, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "      <td>[Moreover,, we, introduce, a, new, two-stage, ...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>This framework ensures that Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can capture both the general-domain and task-s...</td>\n",
       "      <td>[This, framework, ensures, that, Tiny, BERT, c...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "      <td>[Tiny, BERT, is, empirically, effective, and, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is also significantly better than state-of-the...</td>\n",
       "      <td>[Tiny, BERT, is, also, significantly, better, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Tiny</td>\n",
       "      <td>BERT</td>\n",
       "      <td>: Distilling BERT for Natural Language Underst...</td>\n",
       "      <td>[Tiny, BERT, :, Distilling, BERT, for, Natural...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>, a pre-trained Transformer model, has achieve...</td>\n",
       "      <td>[BERT, ,, a, pre-trained, Transformer, model,,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERT</td>\n",
       "      <td>SUM, a simple variant of BERT, for extractive ...</td>\n",
       "      <td>[In, this, paper,, we, describe, BERT, SUM,, a...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, codes, to, reproduce, our, results, are,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "      <td>[Fine-tune, BERT, for, Extractive, Summarization]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Question-answering, plays, an, important, rol...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Inspired, by, the, recent, success, of, machi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, the, best, of, our, knowledge,, no, exist...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work,, we, first, build, an, RRC, d...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to enhance the performance of fine-tuning of B...</td>\n",
       "      <td>[Since, ReviewRC, has, limited, training, exam...</td>\n",
       "      <td>(26, 27)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, show, the, generality, of, the, approach,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, the...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, datasets, and, code, are, available, at,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "      <td>[BERT, Post-Training, for, Review, Reading, Co...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pre-training, has, proven, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[As, a, state-of-the-art, language, model, pre...</td>\n",
       "      <td>(7, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "      <td>[In, this, paper,, we, conduct, exhaustive, ex...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, the, proposed, solution, obtains, n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "      <td>[How, to, Fine-Tune, BERT, for, Text, Classifi...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018) is a Markov random field...</td>\n",
       "      <td>[We, show, that, BERT, (Devlin, et, al.,, 2018...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>[This, formulation, gives, way, to, a, natural...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high-quality, flu...</td>\n",
       "      <td>[We, generate, from, BERT, and, find, that, it...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "      <td>[Compared, to, the, generations, of, a, tradit...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth, and It Must Speak: BERT as a Mark...</td>\n",
       "      <td>[BERT, has, a, Mouth,, and, It, Must, Speak:, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Aspect-based, sentiment, analysis, (ABSA),, w...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, construct, an, auxiliar...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "      <td>[We, fine-tune, the, pre-trained, model, from,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect-Based, Sentiment...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering, we explore simple appli...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle.</td>\n",
       "      <td>[This, required, confronting, the, challenge, ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, address, this, issue, by, applying, infer...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, TREC, microblog, and, newswi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based architectures currently give state-of-t...</td>\n",
       "      <td>[BERT, -based, architectures, currently, give,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>[In, the, current, work,, we, focus, on, the, ...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s heads.</td>\n",
       "      <td>[Using, a, subset, of, GLUE, tasks, and, a, se...</td>\n",
       "      <td>(31, 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, findings, suggest, that, there, is, a, l...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, different, heads, consistently, use, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models</td>\n",
       "      <td>[We, show, that, manually, disabling, attentio...</td>\n",
       "      <td>(18, 19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>) has shown marvelous improvements across vari...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Recently, an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking (WWM...</td>\n",
       "      <td>[Recently,, an, upgraded, version, of, BERT, h...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, technical, report,, we, adapt, whol...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, model, was, trained, on, the, latest, Ch...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "      <td>[We, aim, to, provide, easy, extensibility, an...</td>\n",
       "      <td>(11, 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, model, is, verified, on, various, NLP, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, on, these, datasets, s...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE, BERT-wwm.</td>\n",
       "      <td>[Moreover,, we, also, examine, the, effectiven...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, the, pre-trained, model, (both, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Pre-Training, with, Whole, Word, Masking, for...</td>\n",
       "      <td>(7, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open-do...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>However, previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "      <td>[However,, previous, work, trains, BERT, by, v...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "      <td>[To, tackle, this, issue,, we, propose, a, mul...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, addition,, we, find, that, splitting, art...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "      <td>[By, leveraging, a, passage, ranker, to, selec...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "      <td>[Experiments, on, four, standard, benchmarks, ...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models, and 5.8% EM and 6.5% $F_1$ over BERT-b...</td>\n",
       "      <td>[In, particular,, on, the, OpenSQuAD, dataset,...</td>\n",
       "      <td>(17, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>: A Globally Normalized BERT Model for Open-do...</td>\n",
       "      <td>[Multi-passage, BERT, :, A, Globally, Normaliz...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[They, often, suffer, from, small-scale, human...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[Recently, a, new, language, representation, m...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding.</td>\n",
       "      <td>[However,, there, has, not, been, much, effort...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>.</td>\n",
       "      <td>[In, this, work,, we, propose, a, joint, inten...</td>\n",
       "      <td>(15, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, our...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[One, of, the, major, challenges, to, multi-tu...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Existing, methods, either, prepend, history, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, conceptually, simple, yet, hi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[It, enables, seamless, integration, of, conve...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, first, explain, our, view, that, ConvQA, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, demonstrate, the, effectiveness,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, we, analyze, the, impact, of, diffe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks.</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "      <td>[We, explore, several, different, ways, to, le...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question-answering focused passage ranking ...</td>\n",
       "      <td>[Experimental, results, on, MS, MARCO, demonst...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "      <td>[Experimental, results, on, TREC, show, the, g...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query-documen...</td>\n",
       "      <td>[Analyses, illustrate, how, BERT, allocates, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>We present simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based models for relation extraction and sema...</td>\n",
       "      <td>[We, present, simple, BERT, -based, models, fo...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, recent, years,, state-of-the-art, perform...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>-based model can achieve state-of-the-art perf...</td>\n",
       "      <td>[In, this, paper,, extensive, experiments, on,...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "      <td>[To, our, knowledge,, we, are, the, first, to,...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, models, provide, strong, baselines, for,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Multi-task, learning, allows, the, sharing, o...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, natural, language, processing, several, r...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, are, based, on, fine-tuning, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "      <td>[We, explore, the, multi-task, learning, setti...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, new, adaptation, modules,, PAL...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "      <td>[By, using, PALs, in, parallel, with, BERT, la...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "      <td>[BERT, and, PALs:, Projected, Attention, Layer...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-training, by, language, modeling, has, be...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, we, introduce, a, suite, of,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model, finding that it can generally distingui...</td>\n",
       "      <td>[As, a, case, study,, we, apply, these, diagno...</td>\n",
       "      <td>(11, 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "      <td>[What, BERT, Is, Not:, Lessons, from, a, New, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, it, is, unclear, why, the, pre-trai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets.</td>\n",
       "      <td>[In, this, paper,, we, propose, to, visualize,...</td>\n",
       "      <td>(14, 15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[First,, we, find, that, pre-training, reaches...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over-parameterized for downstream ta...</td>\n",
       "      <td>[We, also, demonstrate, that, the, fine-tuning...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "      <td>[Second,, the, visualization, results, indicat...</td>\n",
       "      <td>(7, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Third, the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine-tuning, which s...</td>\n",
       "      <td>[Third,, the, lower, layers, of, BERT, are, mo...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Recently, proposed, contextual, augmentation,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>) demonstrates that a deep bidirectional langu...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "      <td>[We, propose, a, novel, data, augmentation, me...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "      <td>[We, retrofit, BERT, to, conditional, BERT, by...</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, our, paper,, “conditional, masked, langua...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>task.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[task.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "      <td>[The, well, trained, conditional, BERT, can, b...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, six, various, different, tex...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "      <td>[Starting, from, a, public, multilingual, BERT...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, our, model, especially, outpe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, showcase, the, effectiveness, of, our, me...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "      <td>[Recently,, a, simple, combination, of, passag...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, present, a, data, augme...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "      <td>[We, apply, a, stage-wise, approach, to, fine,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, show, large, gains, in...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine-Tuning, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>language model (LM) (Devlin et al., 2019) is s...</td>\n",
       "      <td>[The, BERT, language, model, (LM), (Devlin, et...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Petroni, et, al.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>(2019) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre-training.</td>\n",
       "      <td>[(2019), take, this, as, evidence, that, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about (the surface ...</td>\n",
       "      <td>[We, take, issue, with, this, interpretation, ...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>More specifically, we show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>'s precision drops dramatically when we filter...</td>\n",
       "      <td>[More, specifically,, we, show, that, BERT, 's...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>As a remedy, we propose E-</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, an extension of BERT that replaces entity me...</td>\n",
       "      <td>[As, a, remedy,, we, propose, E-, BERT, ,, an,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>E-</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms both BERT and ERNIE (Zhang et al.,...</td>\n",
       "      <td>[E-, BERT, outperforms, both, BERT, and, ERNIE...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>We take this as evidence that E-</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is richer in factual knowledge, and we show tw...</td>\n",
       "      <td>[We, take, this, as, evidence, that, E-, BERT,...</td>\n",
       "      <td>(7, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>?</td>\n",
       "      <td>[However,, just, how, contextual, are, the, co...</td>\n",
       "      <td>(15, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Are, there, infinitely, many, context-specifi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, one,, we, find, that, the, contextualize...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, representations, of, the, same, word, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[This, suggests, that, upper, layers, of, cont...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>In all layers of ELMo,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2, on average, less than 5% of the v...</td>\n",
       "      <td>[In, all, layers, of, ELMo,, BERT, ,, and, GPT...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Comparing the Geometry of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ELMo, and GPT-2 Embeddings</td>\n",
       "      <td>[Comparing, the, Geometry, of, BERT, ,, ELMo,,...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               split_0 split_1  \\\n",
       "0    We introduce a new language representation mod...    BERT   \n",
       "1        Unlike recent language representation models,    BERT   \n",
       "2                         As a result, the pre-trained    BERT   \n",
       "3    It obtains new state-of-the-art results on ele...           \n",
       "4                                                         BERT   \n",
       "5    Language model pretraining has led to signific...           \n",
       "6    Training is computationally expensive, often d...           \n",
       "7                    We present a replication study of    BERT   \n",
       "8                                         We find that    BERT   \n",
       "9    Our best model achieves state-of-the-art resul...           \n",
       "10   These results highlight the importance of prev...           \n",
       "11                     We release our models and code.           \n",
       "12                                                  Ro    BERT   \n",
       "13   As Transfer Learning from large-scale pre-trai...           \n",
       "14   In this work, we propose a method to pre-train...    BERT   \n",
       "15   While most prior work investigated the use of ...    BERT   \n",
       "16   To leverage the inductive biases learned by la...           \n",
       "17   Our smaller, faster and lighter model is cheap...           \n",
       "18                                              Distil    BERT   \n",
       "19   Pre-trained text encoders have rapidly advance...           \n",
       "20                         We focus on one such model,    BERT   \n",
       "21   We find that the model represents the steps of...           \n",
       "22   Qualitative analysis reveals that the model ca...           \n",
       "23                                                        BERT   \n",
       "24           Large pre-trained neural networks such as    BERT   \n",
       "25   Most recent analysis has focused on model outp...           \n",
       "26   Complementary to these works, we propose metho...    BERT   \n",
       "27                                                        BERT   \n",
       "28   We further show that certain attention heads c...           \n",
       "29   For example, we find heads that attend to the ...           \n",
       "30   Lastly, we propose an attention-based probing ...    BERT   \n",
       "31                                           What Does    BERT   \n",
       "32                                      An Analysis of    BERT   \n",
       "33   Recently, neural models pretrained on a langua...    BERT   \n",
       "34   In this paper, we describe a simple re-impleme...    BERT   \n",
       "35   Our system is the state of the art on the TREC...           \n",
       "36   The code to reproduce our results is available...           \n",
       "37                             Passage Re-ranking with    BERT   \n",
       "38   I assess the extent to which the recently intr...    BERT   \n",
       "39                                                 The    BERT   \n",
       "40                                           Assessing    BERT   \n",
       "41   Pretrained contextual representation models (P...           \n",
       "42                                    A new release of    BERT   \n",
       "43   This paper explores the broader cross-lingual ...    BERT   \n",
       "44                                        We compare m    BERT   \n",
       "45   Additionally, we investigate the most effectiv...    BERT   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...    BERT   \n",
       "47                Language model pre-training, such as    BERT   \n",
       "48   However, pre-trained language models are usual...           \n",
       "49   To accelerate inference and reduce model size ...           \n",
       "50   By leveraging this new KD method, the plenty o...    BERT   \n",
       "51   Moreover, we introduce a new two-stage learnin...    BERT   \n",
       "52                    This framework ensures that Tiny    BERT   \n",
       "53                                                Tiny    BERT   \n",
       "54                                                Tiny    BERT   \n",
       "55                                                Tiny    BERT   \n",
       "56                                                        BERT   \n",
       "57                          In this paper, we describe    BERT   \n",
       "58   Our system is the state of the art on the CNN/...           \n",
       "59   The codes to reproduce our results are availab...           \n",
       "60                                           Fine-tune    BERT   \n",
       "61   Question-answering plays an important role in ...           \n",
       "62   Inspired by the recent success of machine read...           \n",
       "63   To the best of our knowledge, no existing work...           \n",
       "64   In this work, we first build an RRC dataset ca...           \n",
       "65   Since ReviewRC has limited training examples f...    BERT   \n",
       "66   To show the generality of the approach, the pr...           \n",
       "67   Experimental results demonstrate that the prop...           \n",
       "68   The datasets and code are available at this ht...           \n",
       "69                                                        BERT   \n",
       "70   Language model pre-training has proven to be u...           \n",
       "71   As a state-of-the-art language model pre-train...    BERT   \n",
       "72   In this paper, we conduct exhaustive experimen...    BERT   \n",
       "73   Finally, the proposed solution obtains new sta...           \n",
       "74                                    How to Fine-Tune    BERT   \n",
       "75                                        We show that    BERT   \n",
       "76   This formulation gives way to a natural proced...    BERT   \n",
       "77                                    We generate from    BERT   \n",
       "78   Compared to the generations of a traditional l...    BERT   \n",
       "79                                                        BERT   \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...           \n",
       "81   In this paper, we construct an auxiliary sente...           \n",
       "82             We fine-tune the pre-trained model from    BERT   \n",
       "83                                           Utilizing    BERT   \n",
       "84              Following recent successes in applying    BERT   \n",
       "85   This required confronting the challenge posed ...    BERT   \n",
       "86   We address this issue by applying inference on...           \n",
       "87   Experiments on TREC microblog and newswire tes...           \n",
       "88                              Simple Applications of    BERT   \n",
       "89                                                        BERT   \n",
       "90   In the current work, we focus on the interpret...    BERT   \n",
       "91   Using a subset of GLUE tasks and a set of hand...    BERT   \n",
       "92   Our findings suggest that there is a limited s...           \n",
       "93   While different heads consistently use the sam...           \n",
       "94   We show that manually disabling attention in c...    BERT   \n",
       "95                       Revealing the Dark Secrets of    BERT   \n",
       "96   Bidirectional Encoder Representations from Tra...    BERT   \n",
       "97                    Recently, an upgraded version of    BERT   \n",
       "98   In this technical report, we adapt whole word ...           \n",
       "99   The model was trained on the latest Chinese Wi...           \n",
       "100  We aim to provide easy extensibility and bette...    BERT   \n",
       "101  The model is verified on various NLP tasks, ac...           \n",
       "102  Experimental results on these datasets show th...           \n",
       "103  Moreover, we also examine the effectiveness of...    BERT   \n",
       "104  We release the pre-trained model (both TensorF...           \n",
       "105   Pre-Training with Whole Word Masking for Chinese    BERT   \n",
       "106                                                       BERT   \n",
       "107                      However, previous work trains    BERT   \n",
       "108   To tackle this issue, we propose a multi-passage    BERT   \n",
       "109  In addition, we find that splitting articles i...           \n",
       "110  By leveraging a passage ranker to select high-...    BERT   \n",
       "111  Experiments on four standard benchmarks showed...    BERT   \n",
       "112  In particular, on the OpenSQuAD dataset, our m...    BERT   \n",
       "113                                      Multi-passage    BERT   \n",
       "114  Intent classification and slot filling are two...           \n",
       "115  They often suffer from small-scale human-label...           \n",
       "116      Recently a new language representation model,    BERT   \n",
       "117  However, there has not been much effort on exp...    BERT   \n",
       "118  In this work, we propose a joint intent classi...    BERT   \n",
       "119  Experimental results demonstrate that our prop...           \n",
       "120                                                       BERT   \n",
       "121  Conversational search is an emerging topic in ...           \n",
       "122  One of the major challenges to multi-turn conv...           \n",
       "123  Existing methods either prepend history turns ...           \n",
       "124  We propose a conceptually simple yet highly ef...           \n",
       "125  It enables seamless integration of conversatio...    BERT   \n",
       "126  We first explain our view that ConvQA is a sim...           \n",
       "127  We further demonstrate the effectiveness of ou...           \n",
       "128  Finally, we analyze the impact of different nu...           \n",
       "129                                                       BERT   \n",
       "130  This paper studies the performances and behavi...    BERT   \n",
       "131  We explore several different ways to leverage ...    BERT   \n",
       "132  Experimental results on MS MARCO demonstrate t...    BERT   \n",
       "133  Experimental results on TREC show the gaps bet...    BERT   \n",
       "134                            Analyses illustrate how    BERT   \n",
       "135                     Understanding the Behaviors of    BERT   \n",
       "136                                  We present simple    BERT   \n",
       "137  In recent years, state-of-the-art performance ...           \n",
       "138  In this paper, extensive experiments on datase...    BERT   \n",
       "139  To our knowledge, we are the first to successf...    BERT   \n",
       "140  Our models provide strong baselines for future...           \n",
       "141                                             Simple    BERT   \n",
       "142  Multi-task learning allows the sharing of usef...           \n",
       "143  In natural language processing several recent ...           \n",
       "144  These results are based on fine-tuning on each...           \n",
       "145  We explore the multi-task learning setting for...    BERT   \n",
       "146  We introduce new adaptation modules, PALs or `...           \n",
       "147                     By using PALs in parallel with    BERT   \n",
       "148                                                       BERT   \n",
       "149  Pre-training by language modeling has become a...           \n",
       "150  In this paper we introduce a suite of diagnost...           \n",
       "151  As a case study, we apply these diagnostics to...    BERT   \n",
       "152                                               What    BERT   \n",
       "153               Language model pre-training, such as    BERT   \n",
       "154  However, it is unclear why the pre-training-th...           \n",
       "155  In this paper, we propose to visualize loss la...    BERT   \n",
       "156  First, we find that pre-training reaches a goo...           \n",
       "157  We also demonstrate that the fine-tuning proce...    BERT   \n",
       "158  Second, the visualization results indicate tha...    BERT   \n",
       "159                         Third, the lower layers of    BERT   \n",
       "160  Visualizing and Understanding the Effectivenes...    BERT   \n",
       "161  Data augmentation methods are often applied to...           \n",
       "162  Recently proposed contextual augmentation augm...           \n",
       "163  Bidirectional Encoder Representations from Tra...    BERT   \n",
       "164  We propose a novel data augmentation method fo...    BERT   \n",
       "165                                        We retrofit    BERT   \n",
       "166  In our paper, “conditional masked language mod...           \n",
       "167                                              task.           \n",
       "168                       The well trained conditional    BERT   \n",
       "169  Experiments on six various different text clas...           \n",
       "170                                        Conditional    BERT   \n",
       "171  We propose a practical scheme to train a singl...           \n",
       "172                Starting from a public multilingual    BERT   \n",
       "173  We show that our model especially outperforms ...           \n",
       "174  We showcase the effectiveness of our method by...           \n",
       "175                                Small and Practical    BERT   \n",
       "176  Recently, a simple combination of passage retr...    BERT   \n",
       "177  In this paper, we present a data augmentation ...           \n",
       "178      We apply a stage-wise approach to fine tuning    BERT   \n",
       "179  Experimental results show large gains in effec...           \n",
       "180                              Data Augmentation for    BERT   \n",
       "181                                                The    BERT   \n",
       "182                                     Petroni et al.           \n",
       "183                  (2019) take this as evidence that    BERT   \n",
       "184  We take issue with this interpretation and arg...    BERT   \n",
       "185                    More specifically, we show that    BERT   \n",
       "186                         As a remedy, we propose E-    BERT   \n",
       "187                                                 E-    BERT   \n",
       "188                   We take this as evidence that E-    BERT   \n",
       "189                                                       BERT   \n",
       "190  Replacing static word embeddings with contextu...           \n",
       "191  However, just how contextual are the contextua...    BERT   \n",
       "192  Are there infinitely many context-specific rep...           \n",
       "193  For one, we find that the contextualized repre...           \n",
       "194  While representations of the same word in diff...           \n",
       "195  This suggests that upper layers of contextuali...           \n",
       "196                             In all layers of ELMo,    BERT   \n",
       "197  How Contextual are Contextualized Word Represe...           \n",
       "198                          Comparing the Geometry of    BERT   \n",
       "\n",
       "                                               split_2  \\\n",
       "0    , which stands for Bidirectional Encoder Repre...   \n",
       "1    is designed to pre-train deep bidirectional re...   \n",
       "2    model can be fine-tuned with just one addition...   \n",
       "3                                                        \n",
       "4    : Pre-training of Deep Bidirectional Transform...   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7    pretraining (Devlin et al., 2019) that careful...   \n",
       "8    was significantly undertrained, and can match ...   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12   a: A Robustly Optimized BERT Pretraining Approach   \n",
       "13                                                       \n",
       "14   , which can then be fine-tuned with good perfo...   \n",
       "15   model by 40%, while retaining 97% of its langu...   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18   , a distilled version of BERT: smaller, faster...   \n",
       "19                                                       \n",
       "20   , and aim to quantify where linguistic informa...   \n",
       "21                                                       \n",
       "22                                                       \n",
       "23              Rediscovers the Classical NLP Pipeline   \n",
       "24   have had great recent success in NLP, motivati...   \n",
       "25                                                       \n",
       "26                                                   .   \n",
       "27   's attention heads exhibit patterns such as at...   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                       's attention.   \n",
       "31                                            Look At?   \n",
       "32                                        's Attention   \n",
       "33   (Devlin et al., 2018), have achieved impressiv...   \n",
       "34                 for query-based passage re-ranking.   \n",
       "35                                                       \n",
       "36                                                       \n",
       "37                                                       \n",
       "38   model captures English syntactic phenomena, us...   \n",
       "39        model performs remarkably well on all cases.   \n",
       "40                              's Syntactic Abilities   \n",
       "41                                                       \n",
       "42   (Devlin, 2018) includes a model simultaneously...   \n",
       "43   (multilingual) as a zero shot language transfe...   \n",
       "44   with the best-published methods for zero-shot ...   \n",
       "45   in this manner, determine to what extent mBERT...   \n",
       "46                                                       \n",
       "47   , has significantly improved the performances ...   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50   can be well transferred to a small student Tin...   \n",
       "51   , which performs transformer distillation at b...   \n",
       "52   can capture both the general-domain and task-s...   \n",
       "53   is empirically effective and achieves comparab...   \n",
       "54   is also significantly better than state-of-the...   \n",
       "55   : Distilling BERT for Natural Language Underst...   \n",
       "56   , a pre-trained Transformer model, has achieve...   \n",
       "57   SUM, a simple variant of BERT, for extractive ...   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                        for Extractive Summarization   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65   to enhance the performance of fine-tuning of B...   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69   Post-Training for Review Reading Comprehension...   \n",
       "70                                                       \n",
       "71   (Bidirectional Encoder Representations from Tr...   \n",
       "72   on text classification task and provide a gene...   \n",
       "73                                                       \n",
       "74                            for Text Classification?   \n",
       "75   (Devlin et al., 2018) is a Markov random field...   \n",
       "76                                                   .   \n",
       "77   and find that it can produce high-quality, flu...   \n",
       "78   generates sentences that are more diverse but ...   \n",
       "79   has a Mouth, and It Must Speak: BERT as a Mark...   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82   and achieve new state-of-the-art results on Se...   \n",
       "83   for Aspect-Based Sentiment Analysis via Constr...   \n",
       "84   to question answering, we explore simple appli...   \n",
       "85                             was designed to handle.   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                       for Ad Hoc Document Retrieval   \n",
       "89   -based architectures currently give state-of-t...   \n",
       "90                                                   .   \n",
       "91                                           's heads.   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                              models   \n",
       "95                                                       \n",
       "96   ) has shown marvelous improvements across vari...   \n",
       "97   has been released with Whole Word Masking (WWM...   \n",
       "98                                                       \n",
       "99                                                       \n",
       "100  without changing any neural architecture or ev...   \n",
       "101                                                      \n",
       "102                                                      \n",
       "103                                 , ERNIE, BERT-wwm.   \n",
       "104                                                      \n",
       "105                                                      \n",
       "106  model has been successfully applied to open-do...   \n",
       "107  by viewing passages corresponding to the same ...   \n",
       "108  model to globally normalize answer scores acro...   \n",
       "109                                                      \n",
       "110                               gains additional 2%.   \n",
       "111  outperforms all state-of-the-art models on all...   \n",
       "112  models, and 5.8% EM and 6.5% $F_1$ over BERT-b...   \n",
       "113  : A Globally Normalized BERT Model for Open-do...   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116  (Bidirectional Encoder Representations from Tr...   \n",
       "117                for natural language understanding.   \n",
       "118                                                  .   \n",
       "119                                                      \n",
       "120   for Joint Intent Classification and Slot Filling   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125  (Bidirectional Encoder Representations from Tr...   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129  with History Answer Embedding for Conversation...   \n",
       "130                                  in ranking tasks.   \n",
       "131  and fine-tune it on two ranking tasks: MS MARC...   \n",
       "132  in question-answering focused passage ranking ...   \n",
       "133  pre-trained on surrounding contexts and the ne...   \n",
       "134  allocates its attentions between query-documen...   \n",
       "135                                         in Ranking   \n",
       "136  -based models for relation extraction and sema...   \n",
       "137                                                      \n",
       "138  -based model can achieve state-of-the-art perf...   \n",
       "139                                    in this manner.   \n",
       "140                                                      \n",
       "141  Models for Relation Extraction and Semantic Ro...   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145  model on the GLUE benchmark, and how to best a...   \n",
       "146                                                      \n",
       "147  layers, we match the performance of fine-tuned...   \n",
       "148  and PALs: Projected Attention Layers for Effic...   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151  model, finding that it can generally distingui...   \n",
       "152  Is Not: Lessons from a New Suite of Psycholing...   \n",
       "153  , has achieved remarkable results in many NLP ...   \n",
       "154                                                      \n",
       "155                              on specific datasets.   \n",
       "156                                                      \n",
       "157  is highly over-parameterized for downstream ta...   \n",
       "158  tends to generalize better because of the flat...   \n",
       "159  are more invariant during fine-tuning, which s...   \n",
       "160                                                      \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  ) demonstrates that a deep bidirectional langu...   \n",
       "164                           contextual augmentation.   \n",
       "165  to conditional BERT by introducing a new condi...   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168  can be applied to enhance contextual augmentat...   \n",
       "169                                                      \n",
       "170                            Contextual Augmentation   \n",
       "171                                                      \n",
       "172  checkpoint, our final model is 6x smaller and ...   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                       Models for Sequence Labeling   \n",
       "176  reader was found to be very effective for ques...   \n",
       "177                                                      \n",
       "178  on multiple datasets, starting with data that ...   \n",
       "179                                                      \n",
       "180      Fine-Tuning in Open-Domain Question Answering   \n",
       "181  language model (LM) (Devlin et al., 2019) is s...   \n",
       "182                                                      \n",
       "183   memorizes factual knowledge during pre-training.   \n",
       "184  is partly due to reasoning about (the surface ...   \n",
       "185  's precision drops dramatically when we filter...   \n",
       "186  , an extension of BERT that replaces entity me...   \n",
       "187  outperforms both BERT and ERNIE (Zhang et al.,...   \n",
       "188  is richer in factual knowledge, and we show tw...   \n",
       "189  is Not a Knowledge Base (Yet): Factual Knowled...   \n",
       "190                                                      \n",
       "191                                                  ?   \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196  , and GPT-2, on average, less than 5% of the v...   \n",
       "197                                                      \n",
       "198                       , ELMo, and GPT-2 Embeddings   \n",
       "\n",
       "                                          split_tokens split_anchor_span  \n",
       "0    [We, introduce, a, new, language, representati...            (8, 9)  \n",
       "1    [Unlike, recent, language, representation, mod...            (5, 6)  \n",
       "2    [As, a, result,, the, pre-trained, BERT, model...            (5, 6)  \n",
       "3    [It, obtains, new, state-of-the-art, results, ...              None  \n",
       "4    [BERT, :, Pre-training, of, Deep, Bidirectiona...            (0, 1)  \n",
       "5    [Language, model, pretraining, has, led, to, s...              None  \n",
       "6    [Training, is, computationally, expensive,, of...              None  \n",
       "7    [We, present, a, replication, study, of, BERT,...            (6, 7)  \n",
       "8    [We, find, that, BERT, was, significantly, und...            (3, 4)  \n",
       "9    [Our, best, model, achieves, state-of-the-art,...              None  \n",
       "10   [These, results, highlight, the, importance, o...              None  \n",
       "11              [We, release, our, models, and, code.]              None  \n",
       "12   [Ro, BERT, a:, A, Robustly, Optimized, BERT, P...            (1, 2)  \n",
       "13   [As, Transfer, Learning, from, large-scale, pr...              None  \n",
       "14   [In, this, work,, we, propose, a, method, to, ...          (17, 18)  \n",
       "15   [While, most, prior, work, investigated, the, ...          (33, 34)  \n",
       "16   [To, leverage, the, inductive, biases, learned...              None  \n",
       "17   [Our, smaller,, faster, and, lighter, model, i...              None  \n",
       "18   [Distil, BERT, ,, a, distilled, version, of, B...            (1, 2)  \n",
       "19   [Pre-trained, text, encoders, have, rapidly, a...              None  \n",
       "20   [We, focus, on, one, such, model,, BERT, ,, an...            (6, 7)  \n",
       "21   [We, find, that, the, model, represents, the, ...              None  \n",
       "22   [Qualitative, analysis, reveals, that, the, mo...              None  \n",
       "23   [BERT, Rediscovers, the, Classical, NLP, Pipel...            (0, 1)  \n",
       "24   [Large, pre-trained, neural, networks, such, a...            (6, 7)  \n",
       "25   [Most, recent, analysis, has, focused, on, mod...              None  \n",
       "26   [Complementary, to, these, works,, we, propose...          (19, 20)  \n",
       "27   [BERT, 's, attention, heads, exhibit, patterns...            (0, 1)  \n",
       "28   [We, further, show, that, certain, attention, ...              None  \n",
       "29   [For, example,, we, find, heads, that, attend,...              None  \n",
       "30   [Lastly,, we, propose, an, attention-based, pr...          (20, 21)  \n",
       "31                       [What, Does, BERT, Look, At?]            (2, 3)  \n",
       "32             [An, Analysis, of, BERT, 's, Attention]            (3, 4)  \n",
       "33   [Recently,, neural, models, pretrained, on, a,...          (23, 24)  \n",
       "34   [In, this, paper,, we, describe, a, simple, re...           (9, 10)  \n",
       "35   [Our, system, is, the, state, of, the, art, on...              None  \n",
       "36   [The, code, to, reproduce, our, results, is, a...              None  \n",
       "37                   [Passage, Re-ranking, with, BERT]            (3, 4)  \n",
       "38   [I, assess, the, extent, to, which, the, recen...           (9, 10)  \n",
       "39   [The, BERT, model, performs, remarkably, well,...            (1, 2)  \n",
       "40         [Assessing, BERT, 's, Syntactic, Abilities]            (1, 2)  \n",
       "41   [Pretrained, contextual, representation, model...              None  \n",
       "42   [A, new, release, of, BERT, (Devlin,, 2018), i...            (4, 5)  \n",
       "43   [This, paper, explores, the, broader, cross-li...           (9, 10)  \n",
       "44   [We, compare, m, BERT, with, the, best-publish...            (3, 4)  \n",
       "45   [Additionally,, we, investigate, the, most, ef...          (10, 11)  \n",
       "46   [Beto,, Bentz,, Becas:, The, Surprising, Cross...            (8, 9)  \n",
       "47   [Language, model, pre-training,, such, as, BER...            (5, 6)  \n",
       "48   [However,, pre-trained, language, models, are,...              None  \n",
       "49   [To, accelerate, inference, and, reduce, model...              None  \n",
       "50   [By, leveraging, this, new, KD, method,, the, ...          (15, 16)  \n",
       "51   [Moreover,, we, introduce, a, new, two-stage, ...          (10, 11)  \n",
       "52   [This, framework, ensures, that, Tiny, BERT, c...            (5, 6)  \n",
       "53   [Tiny, BERT, is, empirically, effective, and, ...            (1, 2)  \n",
       "54   [Tiny, BERT, is, also, significantly, better, ...            (1, 2)  \n",
       "55   [Tiny, BERT, :, Distilling, BERT, for, Natural...            (1, 2)  \n",
       "56   [BERT, ,, a, pre-trained, Transformer, model,,...            (0, 1)  \n",
       "57   [In, this, paper,, we, describe, BERT, SUM,, a...            (5, 6)  \n",
       "58   [Our, system, is, the, state, of, the, art, on...              None  \n",
       "59   [The, codes, to, reproduce, our, results, are,...              None  \n",
       "60   [Fine-tune, BERT, for, Extractive, Summarization]            (1, 2)  \n",
       "61   [Question-answering, plays, an, important, rol...              None  \n",
       "62   [Inspired, by, the, recent, success, of, machi...              None  \n",
       "63   [To, the, best, of, our, knowledge,, no, exist...              None  \n",
       "64   [In, this, work,, we, first, build, an, RRC, d...              None  \n",
       "65   [Since, ReviewRC, has, limited, training, exam...          (26, 27)  \n",
       "66   [To, show, the, generality, of, the, approach,...              None  \n",
       "67   [Experimental, results, demonstrate, that, the...              None  \n",
       "68   [The, datasets, and, code, are, available, at,...              None  \n",
       "69   [BERT, Post-Training, for, Review, Reading, Co...            (0, 1)  \n",
       "70   [Language, model, pre-training, has, proven, t...              None  \n",
       "71   [As, a, state-of-the-art, language, model, pre...            (7, 8)  \n",
       "72   [In, this, paper,, we, conduct, exhaustive, ex...          (13, 14)  \n",
       "73   [Finally,, the, proposed, solution, obtains, n...              None  \n",
       "74   [How, to, Fine-Tune, BERT, for, Text, Classifi...            (3, 4)  \n",
       "75   [We, show, that, BERT, (Devlin, et, al.,, 2018...            (3, 4)  \n",
       "76   [This, formulation, gives, way, to, a, natural...          (12, 13)  \n",
       "77   [We, generate, from, BERT, and, find, that, it...            (3, 4)  \n",
       "78   [Compared, to, the, generations, of, a, tradit...          (10, 11)  \n",
       "79   [BERT, has, a, Mouth,, and, It, Must, Speak:, ...            (0, 1)  \n",
       "80   [Aspect-based, sentiment, analysis, (ABSA),, w...              None  \n",
       "81   [In, this, paper,, we, construct, an, auxiliar...              None  \n",
       "82   [We, fine-tune, the, pre-trained, model, from,...            (6, 7)  \n",
       "83   [Utilizing, BERT, for, Aspect-Based, Sentiment...            (1, 2)  \n",
       "84   [Following, recent, successes, in, applying, B...            (5, 6)  \n",
       "85   [This, required, confronting, the, challenge, ...          (17, 18)  \n",
       "86   [We, address, this, issue, by, applying, infer...              None  \n",
       "87   [Experiments, on, TREC, microblog, and, newswi...              None  \n",
       "88   [Simple, Applications, of, BERT, for, Ad, Hoc,...            (3, 4)  \n",
       "89   [BERT, -based, architectures, currently, give,...            (0, 1)  \n",
       "90   [In, the, current, work,, we, focus, on, the, ...          (20, 21)  \n",
       "91   [Using, a, subset, of, GLUE, tasks, and, a, se...          (31, 32)  \n",
       "92   [Our, findings, suggest, that, there, is, a, l...              None  \n",
       "93   [While, different, heads, consistently, use, t...              None  \n",
       "94   [We, show, that, manually, disabling, attentio...          (18, 19)  \n",
       "95           [Revealing, the, Dark, Secrets, of, BERT]            (5, 6)  \n",
       "96   [Bidirectional, Encoder, Representations, from...            (6, 7)  \n",
       "97   [Recently,, an, upgraded, version, of, BERT, h...            (5, 6)  \n",
       "98   [In, this, technical, report,, we, adapt, whol...              None  \n",
       "99   [The, model, was, trained, on, the, latest, Ch...              None  \n",
       "100  [We, aim, to, provide, easy, extensibility, an...          (11, 12)  \n",
       "101  [The, model, is, verified, on, various, NLP, t...              None  \n",
       "102  [Experimental, results, on, these, datasets, s...              None  \n",
       "103  [Moreover,, we, also, examine, the, effectiven...          (10, 11)  \n",
       "104  [We, release, the, pre-trained, model, (both, ...              None  \n",
       "105  [Pre-Training, with, Whole, Word, Masking, for...            (7, 8)  \n",
       "106  [BERT, model, has, been, successfully, applied...            (0, 1)  \n",
       "107  [However,, previous, work, trains, BERT, by, v...            (4, 5)  \n",
       "108  [To, tackle, this, issue,, we, propose, a, mul...            (8, 9)  \n",
       "109  [In, addition,, we, find, that, splitting, art...              None  \n",
       "110  [By, leveraging, a, passage, ranker, to, selec...          (10, 11)  \n",
       "111  [Experiments, on, four, standard, benchmarks, ...           (9, 10)  \n",
       "112  [In, particular,, on, the, OpenSQuAD, dataset,...          (17, 18)  \n",
       "113  [Multi-passage, BERT, :, A, Globally, Normaliz...            (1, 2)  \n",
       "114  [Intent, classification, and, slot, filling, a...              None  \n",
       "115  [They, often, suffer, from, small-scale, human...              None  \n",
       "116  [Recently, a, new, language, representation, m...            (6, 7)  \n",
       "117  [However,, there, has, not, been, much, effort...           (9, 10)  \n",
       "118  [In, this, work,, we, propose, a, joint, inten...          (15, 16)  \n",
       "119  [Experimental, results, demonstrate, that, our...              None  \n",
       "120  [BERT, for, Joint, Intent, Classification, and...            (0, 1)  \n",
       "121  [Conversational, search, is, an, emerging, top...              None  \n",
       "122  [One, of, the, major, challenges, to, multi-tu...              None  \n",
       "123  [Existing, methods, either, prepend, history, ...              None  \n",
       "124  [We, propose, a, conceptually, simple, yet, hi...              None  \n",
       "125  [It, enables, seamless, integration, of, conve...          (16, 17)  \n",
       "126  [We, first, explain, our, view, that, ConvQA, ...              None  \n",
       "127  [We, further, demonstrate, the, effectiveness,...              None  \n",
       "128  [Finally,, we, analyze, the, impact, of, diffe...              None  \n",
       "129  [BERT, with, History, Answer, Embedding, for, ...            (0, 1)  \n",
       "130  [This, paper, studies, the, performances, and,...            (8, 9)  \n",
       "131  [We, explore, several, different, ways, to, le...           (9, 10)  \n",
       "132  [Experimental, results, on, MS, MARCO, demonst...          (10, 11)  \n",
       "133  [Experimental, results, on, TREC, show, the, g...           (9, 10)  \n",
       "134  [Analyses, illustrate, how, BERT, allocates, i...            (3, 4)  \n",
       "135  [Understanding, the, Behaviors, of, BERT, in, ...            (4, 5)  \n",
       "136  [We, present, simple, BERT, -based, models, fo...            (3, 4)  \n",
       "137  [In, recent, years,, state-of-the-art, perform...              None  \n",
       "138  [In, this, paper,, extensive, experiments, on,...          (20, 21)  \n",
       "139  [To, our, knowledge,, we, are, the, first, to,...          (10, 11)  \n",
       "140  [Our, models, provide, strong, baselines, for,...              None  \n",
       "141  [Simple, BERT, Models, for, Relation, Extracti...            (1, 2)  \n",
       "142  [Multi-task, learning, allows, the, sharing, o...              None  \n",
       "143  [In, natural, language, processing, several, r...              None  \n",
       "144  [These, results, are, based, on, fine-tuning, ...              None  \n",
       "145  [We, explore, the, multi-task, learning, setti...           (9, 10)  \n",
       "146  [We, introduce, new, adaptation, modules,, PAL...              None  \n",
       "147  [By, using, PALs, in, parallel, with, BERT, la...            (6, 7)  \n",
       "148  [BERT, and, PALs:, Projected, Attention, Layer...            (0, 1)  \n",
       "149  [Pre-training, by, language, modeling, has, be...              None  \n",
       "150  [In, this, paper, we, introduce, a, suite, of,...              None  \n",
       "151  [As, a, case, study,, we, apply, these, diagno...          (11, 12)  \n",
       "152  [What, BERT, Is, Not:, Lessons, from, a, New, ...            (1, 2)  \n",
       "153  [Language, model, pre-training,, such, as, BER...            (5, 6)  \n",
       "154  [However,, it, is, unclear, why, the, pre-trai...              None  \n",
       "155  [In, this, paper,, we, propose, to, visualize,...          (14, 15)  \n",
       "156  [First,, we, find, that, pre-training, reaches...              None  \n",
       "157  [We, also, demonstrate, that, the, fine-tuning...          (13, 14)  \n",
       "158  [Second,, the, visualization, results, indicat...            (7, 8)  \n",
       "159  [Third,, the, lower, layers, of, BERT, are, mo...            (5, 6)  \n",
       "160  [Visualizing, and, Understanding, the, Effecti...            (6, 7)  \n",
       "161  [Data, augmentation, methods, are, often, appl...              None  \n",
       "162  [Recently, proposed, contextual, augmentation,...              None  \n",
       "163  [Bidirectional, Encoder, Representations, from...            (6, 7)  \n",
       "164  [We, propose, a, novel, data, augmentation, me...          (12, 13)  \n",
       "165  [We, retrofit, BERT, to, conditional, BERT, by...            (2, 3)  \n",
       "166  [In, our, paper,, “conditional, masked, langua...              None  \n",
       "167                                            [task.]              None  \n",
       "168  [The, well, trained, conditional, BERT, can, b...            (4, 5)  \n",
       "169  [Experiments, on, six, various, different, tex...              None  \n",
       "170      [Conditional, BERT, Contextual, Augmentation]            (1, 2)  \n",
       "171  [We, propose, a, practical, scheme, to, train,...              None  \n",
       "172  [Starting, from, a, public, multilingual, BERT...            (5, 6)  \n",
       "173  [We, show, that, our, model, especially, outpe...              None  \n",
       "174  [We, showcase, the, effectiveness, of, our, me...              None  \n",
       "175  [Small, and, Practical, BERT, Models, for, Seq...            (3, 4)  \n",
       "176  [Recently,, a, simple, combination, of, passag...          (13, 14)  \n",
       "177  [In, this, paper,, we, present, a, data, augme...              None  \n",
       "178  [We, apply, a, stage-wise, approach, to, fine,...            (8, 9)  \n",
       "179  [Experimental, results, show, large, gains, in...              None  \n",
       "180  [Data, Augmentation, for, BERT, Fine-Tuning, i...            (3, 4)  \n",
       "181  [The, BERT, language, model, (LM), (Devlin, et...            (1, 2)  \n",
       "182                                 [Petroni, et, al.]              None  \n",
       "183  [(2019), take, this, as, evidence, that, BERT,...            (6, 7)  \n",
       "184  [We, take, issue, with, this, interpretation, ...          (12, 13)  \n",
       "185  [More, specifically,, we, show, that, BERT, 's...            (5, 6)  \n",
       "186  [As, a, remedy,, we, propose, E-, BERT, ,, an,...            (6, 7)  \n",
       "187  [E-, BERT, outperforms, both, BERT, and, ERNIE...            (1, 2)  \n",
       "188  [We, take, this, as, evidence, that, E-, BERT,...            (7, 8)  \n",
       "189  [BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...            (0, 1)  \n",
       "190  [Replacing, static, word, embeddings, with, co...              None  \n",
       "191  [However,, just, how, contextual, are, the, co...          (15, 16)  \n",
       "192  [Are, there, infinitely, many, context-specifi...              None  \n",
       "193  [For, one,, we, find, that, the, contextualize...              None  \n",
       "194  [While, representations, of, the, same, word, ...              None  \n",
       "195  [This, suggests, that, upper, layers, of, cont...              None  \n",
       "196  [In, all, layers, of, ELMo,, BERT, ,, and, GPT...            (5, 6)  \n",
       "197  [How, Contextual, are, Contextualized, Word, R...              None  \n",
       "198  [Comparing, the, Geometry, of, BERT, ,, ELMo,,...            (4, 5)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split on the search word, regardless of whitespace (if search word is A and we have word CAR, it slices it up)\n",
    "def split_term_literal(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        output = row['Text'].split(search_word, maxsplit=1)\n",
    "        output.insert(1, search_word)\n",
    "        output = [i.strip() for i in output]\n",
    "        # if the beginning string is empty don't include it in the tokens list\n",
    "        pre_split = output[0].split(' ') if output[0] != '' else []\n",
    "        post_split = output[2].split(' ') if output[2] != '' else []\n",
    "        output.append(pre_split + [search_word] + post_split)\n",
    "        output.append((len(pre_split), len(pre_split)+1))\n",
    "    else:\n",
    "        output = [row['Text'].strip(),'','',row['Text'].strip().split(' '),None]\n",
    "    return dict(zip(splitting_headers,output))\n",
    "\n",
    "literal_output = df_sentences.apply(\n",
    "    lambda row: split_term_literal(row, search_word), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "literal_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_0</th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>split_anchor_span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>which stands for Bidirectional Encoder Represe...</td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unlike recent language representation models,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre-train deep bidirectional re...</td>\n",
       "      <td>[Unlike, recent, language, representation, mod...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a result, the pre-trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine-tuned with just one addition...</td>\n",
       "      <td>[As, a, result,, the, pre-trained, BERT, model...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[It, obtains, new, state-of-the-art, results, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "      <td>[BERT:, Pre-training, of, Deep, Bidirectional,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Training, is, computationally, expensive,, of...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining (Devlin et al., 2019) that careful...</td>\n",
       "      <td>[We, present, a, replication, study, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained, and can match ...</td>\n",
       "      <td>[We, find, that, BERT, was, significantly, und...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, best, model, achieves, state-of-the-art,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, highlight, the, importance, o...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, our, models, and, code.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "      <td>[RoBERTa:, A, Robustly, Optimized, BERT, Pretr...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[As, Transfer, Learning, from, large-scale, pr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "      <td>[In, this, work,, we, propose, a, method, to, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model by 40%, while retaining 97% of its langu...</td>\n",
       "      <td>[While, most, prior, work, investigated, the, ...</td>\n",
       "      <td>(33, 34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, leverage, the, inductive, biases, learned...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, smaller,, faster, and, lighter, model, i...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "      <td>[DistilBERT,, a, distilled, version, of, BERT:...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-trained, text, encoders, have, rapidly, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>We focus on one such model,</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>and aim to quantify where linguistic informati...</td>\n",
       "      <td>[We, focus, on, one, such, model,, BERT,, and,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, find, that, the, model, represents, the, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Qualitative, analysis, reveals, that, the, mo...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Large pre-trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP, motivati...</td>\n",
       "      <td>[Large, pre-trained, neural, networks, such, a...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Most, recent, analysis, has, focused, on, mod...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "      <td>[Complementary, to, these, works,, we, propose...</td>\n",
       "      <td>(19, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>BERT's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "      <td>[BERT's, attention, heads, exhibit, patterns, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, show, that, certain, attention, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, example,, we, find, heads, that, attend,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>attention.</td>\n",
       "      <td>[Lastly,, we, propose, an, attention-based, pr...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At?</td>\n",
       "      <td>[What, Does, BERT, Look, At?]</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>An Analysis of</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Attention</td>\n",
       "      <td>[An, Analysis, of, BERT's, Attention]</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "      <td>[Recently,, neural, models, pretrained, on, a,...</td>\n",
       "      <td>(23, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for query-based passage re-ranking.</td>\n",
       "      <td>[In, this, paper,, we, describe, a, simple, re...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, code, to, reproduce, our, results, is, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Passage, Re-ranking, with, BERT]</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model captures English syntactic phenomena, us...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model performs remarkably well on all cases.</td>\n",
       "      <td>[The, BERT, model, performs, remarkably, well,...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "      <td>[Assessing, BERT's, Syntactic, Abilities]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "      <td>[A, new, release, of, BERT, (Devlin,, 2018), i...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "      <td>[This, paper, explores, the, broader, cross-li...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best-published methods for zero-shot ...</td>\n",
       "      <td>[We, compare, mBERT, with, the, best-published...</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner, determine to what extent mBERT...</td>\n",
       "      <td>[Additionally,, we, investigate, the, most, ef...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Beto,, Bentz,, Becas:, The, Surprising, Cross...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, pre-trained, language, models, are,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, accelerate, inference, and, reduce, model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be well transferred to a small student Tin...</td>\n",
       "      <td>[By, leveraging, this, new, KD, method,, the, ...</td>\n",
       "      <td>(15, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>TinyBERT,</td>\n",
       "      <td>which performs transformer distillation at bot...</td>\n",
       "      <td>[Moreover,, we, introduce, a, new, two-stage, ...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general-domain and task-s...</td>\n",
       "      <td>[This, framework, ensures, that, TinyBERT, can...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "      <td>[TinyBERT, is, empirically, effective, and, ac...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state-of-the...</td>\n",
       "      <td>[TinyBERT, is, also, significantly, better, th...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "      <td>[TinyBERT:, Distilling, BERT, for, Natural, La...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "      <td>[BERT,, a, pre-trained, Transformer, model,, h...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "      <td>[In, this, paper,, we, describe, BERTSUM,, a, ...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, codes, to, reproduce, our, results, are,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "      <td>[Fine-tune, BERT, for, Extractive, Summarization]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Question-answering, plays, an, important, rol...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Inspired, by, the, recent, success, of, machi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, the, best, of, our, knowledge,, no, exist...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work,, we, first, build, an, RRC, d...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to enhance the performance of fine-tuning of B...</td>\n",
       "      <td>[Since, ReviewRC, has, limited, training, exam...</td>\n",
       "      <td>(26, 27)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, show, the, generality, of, the, approach,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, the...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, datasets, and, code, are, available, at,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "      <td>[BERT, Post-Training, for, Review, Reading, Co...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pre-training, has, proven, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[As, a, state-of-the-art, language, model, pre...</td>\n",
       "      <td>(7, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "      <td>[In, this, paper,, we, conduct, exhaustive, ex...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, the, proposed, solution, obtains, n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "      <td>[How, to, Fine-Tune, BERT, for, Text, Classifi...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018) is a Markov random field...</td>\n",
       "      <td>[We, show, that, BERT, (Devlin, et, al.,, 2018...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "      <td>[This, formulation, gives, way, to, a, natural...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high-quality, flu...</td>\n",
       "      <td>[We, generate, from, BERT, and, find, that, it...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "      <td>[Compared, to, the, generations, of, a, tradit...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth, and It Must Speak: BERT as a Mark...</td>\n",
       "      <td>[BERT, has, a, Mouth,, and, It, Must, Speak:, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Aspect-based, sentiment, analysis, (ABSA),, w...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, construct, an, auxiliar...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "      <td>[We, fine-tune, the, pre-trained, model, from,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect-Based, Sentiment...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering, we explore simple appli...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle.</td>\n",
       "      <td>[This, required, confronting, the, challenge, ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, address, this, issue, by, applying, infer...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, TREC, microblog, and, newswi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "      <td>[BERT-based, architectures, currently, give, s...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "      <td>[In, the, current, work,, we, focus, on, the, ...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "      <td>[Using, a, subset, of, GLUE, tasks, and, a, se...</td>\n",
       "      <td>(31, 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, findings, suggest, that, there, is, a, l...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, different, heads, consistently, use, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models</td>\n",
       "      <td>[We, show, that, manually, disabling, attentio...</td>\n",
       "      <td>(18, 19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Recently, an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking (WWM...</td>\n",
       "      <td>[Recently,, an, upgraded, version, of, BERT, h...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, technical, report,, we, adapt, whol...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, model, was, trained, on, the, latest, Ch...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "      <td>[We, aim, to, provide, easy, extensibility, an...</td>\n",
       "      <td>(11, 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, model, is, verified, on, various, NLP, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, on, these, datasets, s...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ERNIE, BERT-wwm.</td>\n",
       "      <td>[Moreover,, we, also, examine, the, effectiven...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, the, pre-trained, model, (both, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Pre-Training, with, Whole, Word, Masking, for...</td>\n",
       "      <td>(7, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open-do...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>However, previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "      <td>[However,, previous, work, trains, BERT, by, v...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "      <td>[To, tackle, this, issue,, we, propose, a, mul...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, addition,, we, find, that, splitting, art...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "      <td>[By, leveraging, a, passage, ranker, to, selec...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "      <td>[Experiments, on, four, standard, benchmarks, ...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>non-BERT</td>\n",
       "      <td>models, and 5.8% EM and 6.5% $F_1$ over BERT-b...</td>\n",
       "      <td>[In, particular,, on, the, OpenSQuAD, dataset,...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "      <td>[Multi-passage, BERT:, A, Globally, Normalized...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[They, often, suffer, from, small-scale, human...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[Recently, a, new, language, representation, m...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding.</td>\n",
       "      <td>[However,, there, has, not, been, much, effort...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>BERT.</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work,, we, propose, a, joint, inten...</td>\n",
       "      <td>(15, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, our...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[One, of, the, major, challenges, to, multi-tu...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Existing, methods, either, prepend, history, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, conceptually, simple, yet, hi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[It, enables, seamless, integration, of, conve...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, first, explain, our, view, that, ConvQA, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, demonstrate, the, effectiveness,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, we, analyze, the, impact, of, diffe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks.</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "      <td>[We, explore, several, different, ways, to, le...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question-answering focused passage ranking ...</td>\n",
       "      <td>[Experimental, results, on, MS, MARCO, demonst...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "      <td>[Experimental, results, on, TREC, show, the, g...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query-documen...</td>\n",
       "      <td>[Analyses, illustrate, how, BERT, allocates, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>We present simple</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>models for relation extraction and semantic ro...</td>\n",
       "      <td>[We, present, simple, BERT-based, models, for,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, recent, years,, state-of-the-art, perform...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "      <td>[In, this, paper,, extensive, experiments, on,...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "      <td>[To, our, knowledge,, we, are, the, first, to,...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, models, provide, strong, baselines, for,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Multi-task, learning, allows, the, sharing, o...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, natural, language, processing, several, r...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, are, based, on, fine-tuning, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "      <td>[We, explore, the, multi-task, learning, setti...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, new, adaptation, modules,, PAL...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "      <td>[By, using, PALs, in, parallel, with, BERT, la...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "      <td>[BERT, and, PALs:, Projected, Attention, Layer...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-training, by, language, modeling, has, be...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, we, introduce, a, suite, of,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model, finding that it can generally distingui...</td>\n",
       "      <td>[As, a, case, study,, we, apply, these, diagno...</td>\n",
       "      <td>(11, 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "      <td>[What, BERT, Is, Not:, Lessons, from, a, New, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has achieved remarkable results in many NLP ta...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, it, is, unclear, why, the, pre-trai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets.</td>\n",
       "      <td>[In, this, paper,, we, propose, to, visualize,...</td>\n",
       "      <td>(14, 15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[First,, we, find, that, pre-training, reaches...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over-parameterized for downstream ta...</td>\n",
       "      <td>[We, also, demonstrate, that, the, fine-tuning...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "      <td>[Second,, the, visualization, results, indicat...</td>\n",
       "      <td>(7, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Third, the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine-tuning, which s...</td>\n",
       "      <td>[Third,, the, lower, layers, of, BERT, are, mo...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Recently, proposed, contextual, augmentation,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "      <td>[We, propose, a, novel, data, augmentation, me...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "      <td>[We, retrofit, BERT, to, conditional, BERT, by...</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, our, paper,, “conditional, masked, langua...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>task.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[task.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "      <td>[The, well, trained, conditional, BERT, can, b...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, six, various, different, tex...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "      <td>[Starting, from, a, public, multilingual, BERT...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, our, model, especially, outpe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, showcase, the, effectiveness, of, our, me...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "      <td>[Recently,, a, simple, combination, of, passag...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, present, a, data, augme...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "      <td>[We, apply, a, stage-wise, approach, to, fine,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, show, large, gains, in...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine-Tuning, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>The</td>\n",
       "      <td>BERT</td>\n",
       "      <td>language model (LM) (Devlin et al., 2019) is s...</td>\n",
       "      <td>[The, BERT, language, model, (LM), (Devlin, et...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Petroni, et, al.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>(2019) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre-training.</td>\n",
       "      <td>[(2019), take, this, as, evidence, that, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about (the surface ...</td>\n",
       "      <td>[We, take, issue, with, this, interpretation, ...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>More specifically, we show that</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "      <td>[More, specifically,, we, show, that, BERT's, ...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>As a remedy, we propose</td>\n",
       "      <td>E-BERT,</td>\n",
       "      <td>an extension of BERT that replaces entity ment...</td>\n",
       "      <td>[As, a, remedy,, we, propose, E-BERT,, an, ext...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td></td>\n",
       "      <td>E-BERT</td>\n",
       "      <td>outperforms both BERT and ERNIE (Zhang et al.,...</td>\n",
       "      <td>[E-BERT, outperforms, both, BERT, and, ERNIE, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>We take this as evidence that</td>\n",
       "      <td>E-BERT</td>\n",
       "      <td>is richer in factual knowledge, and we show tw...</td>\n",
       "      <td>[We, take, this, as, evidence, that, E-BERT, i...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>BERT?</td>\n",
       "      <td></td>\n",
       "      <td>[However,, just, how, contextual, are, the, co...</td>\n",
       "      <td>(15, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Are, there, infinitely, many, context-specifi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, one,, we, find, that, the, contextualize...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, representations, of, the, same, word, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[This, suggests, that, upper, layers, of, cont...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>In all layers of ELMo,</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>and GPT-2, on average, less than 5% of the var...</td>\n",
       "      <td>[In, all, layers, of, ELMo,, BERT,, and, GPT-2...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Comparing the Geometry of</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "      <td>[Comparing, the, Geometry, of, BERT,, ELMo,, a...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               split_0      split_1  \\\n",
       "0    We introduce a new language representation mod...        BERT,   \n",
       "1        Unlike recent language representation models,         BERT   \n",
       "2                         As a result, the pre-trained         BERT   \n",
       "3    It obtains new state-of-the-art results on ele...                \n",
       "4                                                             BERT:   \n",
       "5    Language model pretraining has led to signific...                \n",
       "6    Training is computationally expensive, often d...                \n",
       "7                    We present a replication study of         BERT   \n",
       "8                                         We find that         BERT   \n",
       "9    Our best model achieves state-of-the-art resul...                \n",
       "10   These results highlight the importance of prev...                \n",
       "11                     We release our models and code.                \n",
       "12                                                         RoBERTa:   \n",
       "13   As Transfer Learning from large-scale pre-trai...                \n",
       "14   In this work, we propose a method to pre-train...  DistilBERT,   \n",
       "15   While most prior work investigated the use of ...         BERT   \n",
       "16   To leverage the inductive biases learned by la...                \n",
       "17   Our smaller, faster and lighter model is cheap...                \n",
       "18                                                      DistilBERT,   \n",
       "19   Pre-trained text encoders have rapidly advance...                \n",
       "20                         We focus on one such model,        BERT,   \n",
       "21   We find that the model represents the steps of...                \n",
       "22   Qualitative analysis reveals that the model ca...                \n",
       "23                                                             BERT   \n",
       "24           Large pre-trained neural networks such as         BERT   \n",
       "25   Most recent analysis has focused on model outp...                \n",
       "26   Complementary to these works, we propose metho...        BERT.   \n",
       "27                                                           BERT's   \n",
       "28   We further show that certain attention heads c...                \n",
       "29   For example, we find heads that attend to the ...                \n",
       "30   Lastly, we propose an attention-based probing ...       BERT's   \n",
       "31                                           What Does         BERT   \n",
       "32                                      An Analysis of       BERT's   \n",
       "33   Recently, neural models pretrained on a langua...         BERT   \n",
       "34   In this paper, we describe a simple re-impleme...         BERT   \n",
       "35   Our system is the state of the art on the TREC...                \n",
       "36   The code to reproduce our results is available...                \n",
       "37                             Passage Re-ranking with         BERT   \n",
       "38   I assess the extent to which the recently intr...         BERT   \n",
       "39                                                 The         BERT   \n",
       "40                                           Assessing       BERT's   \n",
       "41   Pretrained contextual representation models (P...                \n",
       "42                                    A new release of         BERT   \n",
       "43   This paper explores the broader cross-lingual ...        mBERT   \n",
       "44                                          We compare        mBERT   \n",
       "45   Additionally, we investigate the most effectiv...        mBERT   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...         BERT   \n",
       "47                Language model pre-training, such as        BERT,   \n",
       "48   However, pre-trained language models are usual...                \n",
       "49   To accelerate inference and reduce model size ...                \n",
       "50   By leveraging this new KD method, the plenty o...         BERT   \n",
       "51   Moreover, we introduce a new two-stage learnin...    TinyBERT,   \n",
       "52                         This framework ensures that     TinyBERT   \n",
       "53                                                         TinyBERT   \n",
       "54                                                         TinyBERT   \n",
       "55                                                        TinyBERT:   \n",
       "56                                                            BERT,   \n",
       "57                          In this paper, we describe     BERTSUM,   \n",
       "58   Our system is the state of the art on the CNN/...                \n",
       "59   The codes to reproduce our results are availab...                \n",
       "60                                           Fine-tune         BERT   \n",
       "61   Question-answering plays an important role in ...                \n",
       "62   Inspired by the recent success of machine read...                \n",
       "63   To the best of our knowledge, no existing work...                \n",
       "64   In this work, we first build an RRC dataset ca...                \n",
       "65   Since ReviewRC has limited training examples f...         BERT   \n",
       "66   To show the generality of the approach, the pr...                \n",
       "67   Experimental results demonstrate that the prop...                \n",
       "68   The datasets and code are available at this ht...                \n",
       "69                                                             BERT   \n",
       "70   Language model pre-training has proven to be u...                \n",
       "71   As a state-of-the-art language model pre-train...         BERT   \n",
       "72   In this paper, we conduct exhaustive experimen...         BERT   \n",
       "73   Finally, the proposed solution obtains new sta...                \n",
       "74                                    How to Fine-Tune         BERT   \n",
       "75                                        We show that         BERT   \n",
       "76   This formulation gives way to a natural proced...        BERT.   \n",
       "77                                    We generate from         BERT   \n",
       "78   Compared to the generations of a traditional l...         BERT   \n",
       "79                                                             BERT   \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...                \n",
       "81   In this paper, we construct an auxiliary sente...                \n",
       "82             We fine-tune the pre-trained model from         BERT   \n",
       "83                                           Utilizing         BERT   \n",
       "84              Following recent successes in applying         BERT   \n",
       "85   This required confronting the challenge posed ...         BERT   \n",
       "86   We address this issue by applying inference on...                \n",
       "87   Experiments on TREC microblog and newswire tes...                \n",
       "88                              Simple Applications of         BERT   \n",
       "89                                                       BERT-based   \n",
       "90   In the current work, we focus on the interpret...        BERT.   \n",
       "91   Using a subset of GLUE tasks and a set of hand...       BERT's   \n",
       "92   Our findings suggest that there is a limited s...                \n",
       "93   While different heads consistently use the sam...                \n",
       "94   We show that manually disabling attention in c...         BERT   \n",
       "95                       Revealing the Dark Secrets of         BERT   \n",
       "96   Bidirectional Encoder Representations from Tra...       (BERT)   \n",
       "97                    Recently, an upgraded version of         BERT   \n",
       "98   In this technical report, we adapt whole word ...                \n",
       "99   The model was trained on the latest Chinese Wi...                \n",
       "100  We aim to provide easy extensibility and bette...         BERT   \n",
       "101  The model is verified on various NLP tasks, ac...                \n",
       "102  Experimental results on these datasets show th...                \n",
       "103  Moreover, we also examine the effectiveness of...        BERT,   \n",
       "104  We release the pre-trained model (both TensorF...                \n",
       "105   Pre-Training with Whole Word Masking for Chinese         BERT   \n",
       "106                                                            BERT   \n",
       "107                      However, previous work trains         BERT   \n",
       "108   To tackle this issue, we propose a multi-passage         BERT   \n",
       "109  In addition, we find that splitting articles i...                \n",
       "110  By leveraging a passage ranker to select high-...         BERT   \n",
       "111  Experiments on four standard benchmarks showed...         BERT   \n",
       "112  In particular, on the OpenSQuAD dataset, our m...     non-BERT   \n",
       "113                                      Multi-passage        BERT:   \n",
       "114  Intent classification and slot filling are two...                \n",
       "115  They often suffer from small-scale human-label...                \n",
       "116      Recently a new language representation model,         BERT   \n",
       "117  However, there has not been much effort on exp...         BERT   \n",
       "118  In this work, we propose a joint intent classi...        BERT.   \n",
       "119  Experimental results demonstrate that our prop...                \n",
       "120                                                            BERT   \n",
       "121  Conversational search is an emerging topic in ...                \n",
       "122  One of the major challenges to multi-turn conv...                \n",
       "123  Existing methods either prepend history turns ...                \n",
       "124  We propose a conceptually simple yet highly ef...                \n",
       "125  It enables seamless integration of conversatio...         BERT   \n",
       "126  We first explain our view that ConvQA is a sim...                \n",
       "127  We further demonstrate the effectiveness of ou...                \n",
       "128  Finally, we analyze the impact of different nu...                \n",
       "129                                                            BERT   \n",
       "130  This paper studies the performances and behavi...         BERT   \n",
       "131  We explore several different ways to leverage ...         BERT   \n",
       "132  Experimental results on MS MARCO demonstrate t...         BERT   \n",
       "133  Experimental results on TREC show the gaps bet...         BERT   \n",
       "134                            Analyses illustrate how         BERT   \n",
       "135                     Understanding the Behaviors of         BERT   \n",
       "136                                  We present simple   BERT-based   \n",
       "137  In recent years, state-of-the-art performance ...                \n",
       "138  In this paper, extensive experiments on datase...   BERT-based   \n",
       "139  To our knowledge, we are the first to successf...         BERT   \n",
       "140  Our models provide strong baselines for future...                \n",
       "141                                             Simple         BERT   \n",
       "142  Multi-task learning allows the sharing of usef...                \n",
       "143  In natural language processing several recent ...                \n",
       "144  These results are based on fine-tuning on each...                \n",
       "145  We explore the multi-task learning setting for...         BERT   \n",
       "146  We introduce new adaptation modules, PALs or `...                \n",
       "147                     By using PALs in parallel with         BERT   \n",
       "148                                                            BERT   \n",
       "149  Pre-training by language modeling has become a...                \n",
       "150  In this paper we introduce a suite of diagnost...                \n",
       "151  As a case study, we apply these diagnostics to...         BERT   \n",
       "152                                               What         BERT   \n",
       "153               Language model pre-training, such as        BERT,   \n",
       "154  However, it is unclear why the pre-training-th...                \n",
       "155  In this paper, we propose to visualize loss la...         BERT   \n",
       "156  First, we find that pre-training reaches a goo...                \n",
       "157  We also demonstrate that the fine-tuning proce...         BERT   \n",
       "158  Second, the visualization results indicate tha...         BERT   \n",
       "159                         Third, the lower layers of         BERT   \n",
       "160  Visualizing and Understanding the Effectivenes...         BERT   \n",
       "161  Data augmentation methods are often applied to...                \n",
       "162  Recently proposed contextual augmentation augm...                \n",
       "163  Bidirectional Encoder Representations from Tra...       (BERT)   \n",
       "164  We propose a novel data augmentation method fo...         BERT   \n",
       "165                                        We retrofit         BERT   \n",
       "166  In our paper, “conditional masked language mod...                \n",
       "167                                              task.                \n",
       "168                       The well trained conditional         BERT   \n",
       "169  Experiments on six various different text clas...                \n",
       "170                                        Conditional         BERT   \n",
       "171  We propose a practical scheme to train a singl...                \n",
       "172                Starting from a public multilingual         BERT   \n",
       "173  We show that our model especially outperforms ...                \n",
       "174  We showcase the effectiveness of our method by...                \n",
       "175                                Small and Practical         BERT   \n",
       "176  Recently, a simple combination of passage retr...         BERT   \n",
       "177  In this paper, we present a data augmentation ...                \n",
       "178      We apply a stage-wise approach to fine tuning         BERT   \n",
       "179  Experimental results show large gains in effec...                \n",
       "180                              Data Augmentation for         BERT   \n",
       "181                                                The         BERT   \n",
       "182                                     Petroni et al.                \n",
       "183                  (2019) take this as evidence that         BERT   \n",
       "184  We take issue with this interpretation and arg...         BERT   \n",
       "185                    More specifically, we show that       BERT's   \n",
       "186                            As a remedy, we propose      E-BERT,   \n",
       "187                                                          E-BERT   \n",
       "188                      We take this as evidence that       E-BERT   \n",
       "189                                                            BERT   \n",
       "190  Replacing static word embeddings with contextu...                \n",
       "191  However, just how contextual are the contextua...        BERT?   \n",
       "192  Are there infinitely many context-specific rep...                \n",
       "193  For one, we find that the contextualized repre...                \n",
       "194  While representations of the same word in diff...                \n",
       "195  This suggests that upper layers of contextuali...                \n",
       "196                             In all layers of ELMo,        BERT,   \n",
       "197  How Contextual are Contextualized Word Represe...                \n",
       "198                          Comparing the Geometry of        BERT,   \n",
       "\n",
       "                                               split_2  \\\n",
       "0    which stands for Bidirectional Encoder Represe...   \n",
       "1    is designed to pre-train deep bidirectional re...   \n",
       "2    model can be fine-tuned with just one addition...   \n",
       "3                                                        \n",
       "4    Pre-training of Deep Bidirectional Transformer...   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7    pretraining (Devlin et al., 2019) that careful...   \n",
       "8    was significantly undertrained, and can match ...   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12      A Robustly Optimized BERT Pretraining Approach   \n",
       "13                                                       \n",
       "14   which can then be fine-tuned with good perform...   \n",
       "15   model by 40%, while retaining 97% of its langu...   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18   a distilled version of BERT: smaller, faster, ...   \n",
       "19                                                       \n",
       "20   and aim to quantify where linguistic informati...   \n",
       "21                                                       \n",
       "22                                                       \n",
       "23              Rediscovers the Classical NLP Pipeline   \n",
       "24   have had great recent success in NLP, motivati...   \n",
       "25                                                       \n",
       "26                                                       \n",
       "27   attention heads exhibit patterns such as atten...   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                          attention.   \n",
       "31                                            Look At?   \n",
       "32                                           Attention   \n",
       "33   (Devlin et al., 2018), have achieved impressiv...   \n",
       "34                 for query-based passage re-ranking.   \n",
       "35                                                       \n",
       "36                                                       \n",
       "37                                                       \n",
       "38   model captures English syntactic phenomena, us...   \n",
       "39        model performs remarkably well on all cases.   \n",
       "40                                 Syntactic Abilities   \n",
       "41                                                       \n",
       "42   (Devlin, 2018) includes a model simultaneously...   \n",
       "43   (multilingual) as a zero shot language transfe...   \n",
       "44   with the best-published methods for zero-shot ...   \n",
       "45   in this manner, determine to what extent mBERT...   \n",
       "46                                                       \n",
       "47   has significantly improved the performances of...   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50   can be well transferred to a small student Tin...   \n",
       "51   which performs transformer distillation at bot...   \n",
       "52   can capture both the general-domain and task-s...   \n",
       "53   is empirically effective and achieves comparab...   \n",
       "54   is also significantly better than state-of-the...   \n",
       "55   Distilling BERT for Natural Language Understan...   \n",
       "56   a pre-trained Transformer model, has achieved ...   \n",
       "57   a simple variant of BERT, for extractive summa...   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                        for Extractive Summarization   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65   to enhance the performance of fine-tuning of B...   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69   Post-Training for Review Reading Comprehension...   \n",
       "70                                                       \n",
       "71   (Bidirectional Encoder Representations from Tr...   \n",
       "72   on text classification task and provide a gene...   \n",
       "73                                                       \n",
       "74                            for Text Classification?   \n",
       "75   (Devlin et al., 2018) is a Markov random field...   \n",
       "76                                                       \n",
       "77   and find that it can produce high-quality, flu...   \n",
       "78   generates sentences that are more diverse but ...   \n",
       "79   has a Mouth, and It Must Speak: BERT as a Mark...   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82   and achieve new state-of-the-art results on Se...   \n",
       "83   for Aspect-Based Sentiment Analysis via Constr...   \n",
       "84   to question answering, we explore simple appli...   \n",
       "85                             was designed to handle.   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                       for Ad Hoc Document Retrieval   \n",
       "89   architectures currently give state-of-the-art ...   \n",
       "90                                                       \n",
       "91                                              heads.   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                              models   \n",
       "95                                                       \n",
       "96   has shown marvelous improvements across variou...   \n",
       "97   has been released with Whole Word Masking (WWM...   \n",
       "98                                                       \n",
       "99                                                       \n",
       "100  without changing any neural architecture or ev...   \n",
       "101                                                      \n",
       "102                                                      \n",
       "103                                   ERNIE, BERT-wwm.   \n",
       "104                                                      \n",
       "105                                                      \n",
       "106  model has been successfully applied to open-do...   \n",
       "107  by viewing passages corresponding to the same ...   \n",
       "108  model to globally normalize answer scores acro...   \n",
       "109                                                      \n",
       "110                               gains additional 2%.   \n",
       "111  outperforms all state-of-the-art models on all...   \n",
       "112  models, and 5.8% EM and 6.5% $F_1$ over BERT-b...   \n",
       "113  A Globally Normalized BERT Model for Open-doma...   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116  (Bidirectional Encoder Representations from Tr...   \n",
       "117                for natural language understanding.   \n",
       "118                                                      \n",
       "119                                                      \n",
       "120   for Joint Intent Classification and Slot Filling   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125  (Bidirectional Encoder Representations from Tr...   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129  with History Answer Embedding for Conversation...   \n",
       "130                                  in ranking tasks.   \n",
       "131  and fine-tune it on two ranking tasks: MS MARC...   \n",
       "132  in question-answering focused passage ranking ...   \n",
       "133  pre-trained on surrounding contexts and the ne...   \n",
       "134  allocates its attentions between query-documen...   \n",
       "135                                         in Ranking   \n",
       "136  models for relation extraction and semantic ro...   \n",
       "137                                                      \n",
       "138    model can achieve state-of-the-art performance.   \n",
       "139                                    in this manner.   \n",
       "140                                                      \n",
       "141  Models for Relation Extraction and Semantic Ro...   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145  model on the GLUE benchmark, and how to best a...   \n",
       "146                                                      \n",
       "147  layers, we match the performance of fine-tuned...   \n",
       "148  and PALs: Projected Attention Layers for Effic...   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151  model, finding that it can generally distingui...   \n",
       "152  Is Not: Lessons from a New Suite of Psycholing...   \n",
       "153  has achieved remarkable results in many NLP ta...   \n",
       "154                                                      \n",
       "155                              on specific datasets.   \n",
       "156                                                      \n",
       "157  is highly over-parameterized for downstream ta...   \n",
       "158  tends to generalize better because of the flat...   \n",
       "159  are more invariant during fine-tuning, which s...   \n",
       "160                                                      \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  demonstrates that a deep bidirectional languag...   \n",
       "164                           contextual augmentation.   \n",
       "165  to conditional BERT by introducing a new condi...   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168  can be applied to enhance contextual augmentat...   \n",
       "169                                                      \n",
       "170                            Contextual Augmentation   \n",
       "171                                                      \n",
       "172  checkpoint, our final model is 6x smaller and ...   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                       Models for Sequence Labeling   \n",
       "176  reader was found to be very effective for ques...   \n",
       "177                                                      \n",
       "178  on multiple datasets, starting with data that ...   \n",
       "179                                                      \n",
       "180      Fine-Tuning in Open-Domain Question Answering   \n",
       "181  language model (LM) (Devlin et al., 2019) is s...   \n",
       "182                                                      \n",
       "183   memorizes factual knowledge during pre-training.   \n",
       "184  is partly due to reasoning about (the surface ...   \n",
       "185  precision drops dramatically when we filter ce...   \n",
       "186  an extension of BERT that replaces entity ment...   \n",
       "187  outperforms both BERT and ERNIE (Zhang et al.,...   \n",
       "188  is richer in factual knowledge, and we show tw...   \n",
       "189  is Not a Knowledge Base (Yet): Factual Knowled...   \n",
       "190                                                      \n",
       "191                                                      \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196  and GPT-2, on average, less than 5% of the var...   \n",
       "197                                                      \n",
       "198                         ELMo, and GPT-2 Embeddings   \n",
       "\n",
       "                                          split_tokens split_anchor_span  \n",
       "0    [We, introduce, a, new, language, representati...            (8, 9)  \n",
       "1    [Unlike, recent, language, representation, mod...            (5, 6)  \n",
       "2    [As, a, result,, the, pre-trained, BERT, model...            (5, 6)  \n",
       "3    [It, obtains, new, state-of-the-art, results, ...              None  \n",
       "4    [BERT:, Pre-training, of, Deep, Bidirectional,...            (0, 1)  \n",
       "5    [Language, model, pretraining, has, led, to, s...              None  \n",
       "6    [Training, is, computationally, expensive,, of...              None  \n",
       "7    [We, present, a, replication, study, of, BERT,...            (6, 7)  \n",
       "8    [We, find, that, BERT, was, significantly, und...            (3, 4)  \n",
       "9    [Our, best, model, achieves, state-of-the-art,...              None  \n",
       "10   [These, results, highlight, the, importance, o...              None  \n",
       "11              [We, release, our, models, and, code.]              None  \n",
       "12   [RoBERTa:, A, Robustly, Optimized, BERT, Pretr...            (0, 1)  \n",
       "13   [As, Transfer, Learning, from, large-scale, pr...              None  \n",
       "14   [In, this, work,, we, propose, a, method, to, ...          (16, 17)  \n",
       "15   [While, most, prior, work, investigated, the, ...          (33, 34)  \n",
       "16   [To, leverage, the, inductive, biases, learned...              None  \n",
       "17   [Our, smaller,, faster, and, lighter, model, i...              None  \n",
       "18   [DistilBERT,, a, distilled, version, of, BERT:...            (0, 1)  \n",
       "19   [Pre-trained, text, encoders, have, rapidly, a...              None  \n",
       "20   [We, focus, on, one, such, model,, BERT,, and,...            (6, 7)  \n",
       "21   [We, find, that, the, model, represents, the, ...              None  \n",
       "22   [Qualitative, analysis, reveals, that, the, mo...              None  \n",
       "23   [BERT, Rediscovers, the, Classical, NLP, Pipel...            (0, 1)  \n",
       "24   [Large, pre-trained, neural, networks, such, a...            (6, 7)  \n",
       "25   [Most, recent, analysis, has, focused, on, mod...              None  \n",
       "26   [Complementary, to, these, works,, we, propose...          (19, 20)  \n",
       "27   [BERT's, attention, heads, exhibit, patterns, ...            (0, 1)  \n",
       "28   [We, further, show, that, certain, attention, ...              None  \n",
       "29   [For, example,, we, find, heads, that, attend,...              None  \n",
       "30   [Lastly,, we, propose, an, attention-based, pr...          (20, 21)  \n",
       "31                       [What, Does, BERT, Look, At?]            (2, 3)  \n",
       "32               [An, Analysis, of, BERT's, Attention]            (3, 4)  \n",
       "33   [Recently,, neural, models, pretrained, on, a,...          (23, 24)  \n",
       "34   [In, this, paper,, we, describe, a, simple, re...           (9, 10)  \n",
       "35   [Our, system, is, the, state, of, the, art, on...              None  \n",
       "36   [The, code, to, reproduce, our, results, is, a...              None  \n",
       "37                   [Passage, Re-ranking, with, BERT]            (3, 4)  \n",
       "38   [I, assess, the, extent, to, which, the, recen...           (9, 10)  \n",
       "39   [The, BERT, model, performs, remarkably, well,...            (1, 2)  \n",
       "40           [Assessing, BERT's, Syntactic, Abilities]            (1, 2)  \n",
       "41   [Pretrained, contextual, representation, model...              None  \n",
       "42   [A, new, release, of, BERT, (Devlin,, 2018), i...            (4, 5)  \n",
       "43   [This, paper, explores, the, broader, cross-li...            (8, 9)  \n",
       "44   [We, compare, mBERT, with, the, best-published...            (2, 3)  \n",
       "45   [Additionally,, we, investigate, the, most, ef...           (9, 10)  \n",
       "46   [Beto,, Bentz,, Becas:, The, Surprising, Cross...            (8, 9)  \n",
       "47   [Language, model, pre-training,, such, as, BER...            (5, 6)  \n",
       "48   [However,, pre-trained, language, models, are,...              None  \n",
       "49   [To, accelerate, inference, and, reduce, model...              None  \n",
       "50   [By, leveraging, this, new, KD, method,, the, ...          (15, 16)  \n",
       "51   [Moreover,, we, introduce, a, new, two-stage, ...           (9, 10)  \n",
       "52   [This, framework, ensures, that, TinyBERT, can...            (4, 5)  \n",
       "53   [TinyBERT, is, empirically, effective, and, ac...            (0, 1)  \n",
       "54   [TinyBERT, is, also, significantly, better, th...            (0, 1)  \n",
       "55   [TinyBERT:, Distilling, BERT, for, Natural, La...            (0, 1)  \n",
       "56   [BERT,, a, pre-trained, Transformer, model,, h...            (0, 1)  \n",
       "57   [In, this, paper,, we, describe, BERTSUM,, a, ...            (5, 6)  \n",
       "58   [Our, system, is, the, state, of, the, art, on...              None  \n",
       "59   [The, codes, to, reproduce, our, results, are,...              None  \n",
       "60   [Fine-tune, BERT, for, Extractive, Summarization]            (1, 2)  \n",
       "61   [Question-answering, plays, an, important, rol...              None  \n",
       "62   [Inspired, by, the, recent, success, of, machi...              None  \n",
       "63   [To, the, best, of, our, knowledge,, no, exist...              None  \n",
       "64   [In, this, work,, we, first, build, an, RRC, d...              None  \n",
       "65   [Since, ReviewRC, has, limited, training, exam...          (26, 27)  \n",
       "66   [To, show, the, generality, of, the, approach,...              None  \n",
       "67   [Experimental, results, demonstrate, that, the...              None  \n",
       "68   [The, datasets, and, code, are, available, at,...              None  \n",
       "69   [BERT, Post-Training, for, Review, Reading, Co...            (0, 1)  \n",
       "70   [Language, model, pre-training, has, proven, t...              None  \n",
       "71   [As, a, state-of-the-art, language, model, pre...            (7, 8)  \n",
       "72   [In, this, paper,, we, conduct, exhaustive, ex...          (13, 14)  \n",
       "73   [Finally,, the, proposed, solution, obtains, n...              None  \n",
       "74   [How, to, Fine-Tune, BERT, for, Text, Classifi...            (3, 4)  \n",
       "75   [We, show, that, BERT, (Devlin, et, al.,, 2018...            (3, 4)  \n",
       "76   [This, formulation, gives, way, to, a, natural...          (12, 13)  \n",
       "77   [We, generate, from, BERT, and, find, that, it...            (3, 4)  \n",
       "78   [Compared, to, the, generations, of, a, tradit...          (10, 11)  \n",
       "79   [BERT, has, a, Mouth,, and, It, Must, Speak:, ...            (0, 1)  \n",
       "80   [Aspect-based, sentiment, analysis, (ABSA),, w...              None  \n",
       "81   [In, this, paper,, we, construct, an, auxiliar...              None  \n",
       "82   [We, fine-tune, the, pre-trained, model, from,...            (6, 7)  \n",
       "83   [Utilizing, BERT, for, Aspect-Based, Sentiment...            (1, 2)  \n",
       "84   [Following, recent, successes, in, applying, B...            (5, 6)  \n",
       "85   [This, required, confronting, the, challenge, ...          (17, 18)  \n",
       "86   [We, address, this, issue, by, applying, infer...              None  \n",
       "87   [Experiments, on, TREC, microblog, and, newswi...              None  \n",
       "88   [Simple, Applications, of, BERT, for, Ad, Hoc,...            (3, 4)  \n",
       "89   [BERT-based, architectures, currently, give, s...            (0, 1)  \n",
       "90   [In, the, current, work,, we, focus, on, the, ...          (20, 21)  \n",
       "91   [Using, a, subset, of, GLUE, tasks, and, a, se...          (31, 32)  \n",
       "92   [Our, findings, suggest, that, there, is, a, l...              None  \n",
       "93   [While, different, heads, consistently, use, t...              None  \n",
       "94   [We, show, that, manually, disabling, attentio...          (18, 19)  \n",
       "95           [Revealing, the, Dark, Secrets, of, BERT]            (5, 6)  \n",
       "96   [Bidirectional, Encoder, Representations, from...            (5, 6)  \n",
       "97   [Recently,, an, upgraded, version, of, BERT, h...            (5, 6)  \n",
       "98   [In, this, technical, report,, we, adapt, whol...              None  \n",
       "99   [The, model, was, trained, on, the, latest, Ch...              None  \n",
       "100  [We, aim, to, provide, easy, extensibility, an...          (11, 12)  \n",
       "101  [The, model, is, verified, on, various, NLP, t...              None  \n",
       "102  [Experimental, results, on, these, datasets, s...              None  \n",
       "103  [Moreover,, we, also, examine, the, effectiven...          (10, 11)  \n",
       "104  [We, release, the, pre-trained, model, (both, ...              None  \n",
       "105  [Pre-Training, with, Whole, Word, Masking, for...            (7, 8)  \n",
       "106  [BERT, model, has, been, successfully, applied...            (0, 1)  \n",
       "107  [However,, previous, work, trains, BERT, by, v...            (4, 5)  \n",
       "108  [To, tackle, this, issue,, we, propose, a, mul...            (8, 9)  \n",
       "109  [In, addition,, we, find, that, splitting, art...              None  \n",
       "110  [By, leveraging, a, passage, ranker, to, selec...          (10, 11)  \n",
       "111  [Experiments, on, four, standard, benchmarks, ...           (9, 10)  \n",
       "112  [In, particular,, on, the, OpenSQuAD, dataset,...          (16, 17)  \n",
       "113  [Multi-passage, BERT:, A, Globally, Normalized...            (1, 2)  \n",
       "114  [Intent, classification, and, slot, filling, a...              None  \n",
       "115  [They, often, suffer, from, small-scale, human...              None  \n",
       "116  [Recently, a, new, language, representation, m...            (6, 7)  \n",
       "117  [However,, there, has, not, been, much, effort...           (9, 10)  \n",
       "118  [In, this, work,, we, propose, a, joint, inten...          (15, 16)  \n",
       "119  [Experimental, results, demonstrate, that, our...              None  \n",
       "120  [BERT, for, Joint, Intent, Classification, and...            (0, 1)  \n",
       "121  [Conversational, search, is, an, emerging, top...              None  \n",
       "122  [One, of, the, major, challenges, to, multi-tu...              None  \n",
       "123  [Existing, methods, either, prepend, history, ...              None  \n",
       "124  [We, propose, a, conceptually, simple, yet, hi...              None  \n",
       "125  [It, enables, seamless, integration, of, conve...          (16, 17)  \n",
       "126  [We, first, explain, our, view, that, ConvQA, ...              None  \n",
       "127  [We, further, demonstrate, the, effectiveness,...              None  \n",
       "128  [Finally,, we, analyze, the, impact, of, diffe...              None  \n",
       "129  [BERT, with, History, Answer, Embedding, for, ...            (0, 1)  \n",
       "130  [This, paper, studies, the, performances, and,...            (8, 9)  \n",
       "131  [We, explore, several, different, ways, to, le...           (9, 10)  \n",
       "132  [Experimental, results, on, MS, MARCO, demonst...          (10, 11)  \n",
       "133  [Experimental, results, on, TREC, show, the, g...           (9, 10)  \n",
       "134  [Analyses, illustrate, how, BERT, allocates, i...            (3, 4)  \n",
       "135  [Understanding, the, Behaviors, of, BERT, in, ...            (4, 5)  \n",
       "136  [We, present, simple, BERT-based, models, for,...            (3, 4)  \n",
       "137  [In, recent, years,, state-of-the-art, perform...              None  \n",
       "138  [In, this, paper,, extensive, experiments, on,...          (20, 21)  \n",
       "139  [To, our, knowledge,, we, are, the, first, to,...          (10, 11)  \n",
       "140  [Our, models, provide, strong, baselines, for,...              None  \n",
       "141  [Simple, BERT, Models, for, Relation, Extracti...            (1, 2)  \n",
       "142  [Multi-task, learning, allows, the, sharing, o...              None  \n",
       "143  [In, natural, language, processing, several, r...              None  \n",
       "144  [These, results, are, based, on, fine-tuning, ...              None  \n",
       "145  [We, explore, the, multi-task, learning, setti...           (9, 10)  \n",
       "146  [We, introduce, new, adaptation, modules,, PAL...              None  \n",
       "147  [By, using, PALs, in, parallel, with, BERT, la...            (6, 7)  \n",
       "148  [BERT, and, PALs:, Projected, Attention, Layer...            (0, 1)  \n",
       "149  [Pre-training, by, language, modeling, has, be...              None  \n",
       "150  [In, this, paper, we, introduce, a, suite, of,...              None  \n",
       "151  [As, a, case, study,, we, apply, these, diagno...          (11, 12)  \n",
       "152  [What, BERT, Is, Not:, Lessons, from, a, New, ...            (1, 2)  \n",
       "153  [Language, model, pre-training,, such, as, BER...            (5, 6)  \n",
       "154  [However,, it, is, unclear, why, the, pre-trai...              None  \n",
       "155  [In, this, paper,, we, propose, to, visualize,...          (14, 15)  \n",
       "156  [First,, we, find, that, pre-training, reaches...              None  \n",
       "157  [We, also, demonstrate, that, the, fine-tuning...          (13, 14)  \n",
       "158  [Second,, the, visualization, results, indicat...            (7, 8)  \n",
       "159  [Third,, the, lower, layers, of, BERT, are, mo...            (5, 6)  \n",
       "160  [Visualizing, and, Understanding, the, Effecti...            (6, 7)  \n",
       "161  [Data, augmentation, methods, are, often, appl...              None  \n",
       "162  [Recently, proposed, contextual, augmentation,...              None  \n",
       "163  [Bidirectional, Encoder, Representations, from...            (5, 6)  \n",
       "164  [We, propose, a, novel, data, augmentation, me...          (12, 13)  \n",
       "165  [We, retrofit, BERT, to, conditional, BERT, by...            (2, 3)  \n",
       "166  [In, our, paper,, “conditional, masked, langua...              None  \n",
       "167                                            [task.]              None  \n",
       "168  [The, well, trained, conditional, BERT, can, b...            (4, 5)  \n",
       "169  [Experiments, on, six, various, different, tex...              None  \n",
       "170      [Conditional, BERT, Contextual, Augmentation]            (1, 2)  \n",
       "171  [We, propose, a, practical, scheme, to, train,...              None  \n",
       "172  [Starting, from, a, public, multilingual, BERT...            (5, 6)  \n",
       "173  [We, show, that, our, model, especially, outpe...              None  \n",
       "174  [We, showcase, the, effectiveness, of, our, me...              None  \n",
       "175  [Small, and, Practical, BERT, Models, for, Seq...            (3, 4)  \n",
       "176  [Recently,, a, simple, combination, of, passag...          (13, 14)  \n",
       "177  [In, this, paper,, we, present, a, data, augme...              None  \n",
       "178  [We, apply, a, stage-wise, approach, to, fine,...            (8, 9)  \n",
       "179  [Experimental, results, show, large, gains, in...              None  \n",
       "180  [Data, Augmentation, for, BERT, Fine-Tuning, i...            (3, 4)  \n",
       "181  [The, BERT, language, model, (LM), (Devlin, et...            (1, 2)  \n",
       "182                                 [Petroni, et, al.]              None  \n",
       "183  [(2019), take, this, as, evidence, that, BERT,...            (6, 7)  \n",
       "184  [We, take, issue, with, this, interpretation, ...          (12, 13)  \n",
       "185  [More, specifically,, we, show, that, BERT's, ...            (5, 6)  \n",
       "186  [As, a, remedy,, we, propose, E-BERT,, an, ext...            (5, 6)  \n",
       "187  [E-BERT, outperforms, both, BERT, and, ERNIE, ...            (0, 1)  \n",
       "188  [We, take, this, as, evidence, that, E-BERT, i...            (6, 7)  \n",
       "189  [BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...            (0, 1)  \n",
       "190  [Replacing, static, word, embeddings, with, co...              None  \n",
       "191  [However,, just, how, contextual, are, the, co...          (15, 16)  \n",
       "192  [Are, there, infinitely, many, context-specifi...              None  \n",
       "193  [For, one,, we, find, that, the, contextualize...              None  \n",
       "194  [While, representations, of, the, same, word, ...              None  \n",
       "195  [This, suggests, that, upper, layers, of, cont...              None  \n",
       "196  [In, all, layers, of, ELMo,, BERT,, and, GPT-2...            (5, 6)  \n",
       "197  [How, Contextual, are, Contextualized, Word, R...              None  \n",
       "198  [Comparing, the, Geometry, of, BERT,, ELMo,, a...            (4, 5)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split on the search word, taking care to only split on whitespace\n",
    "def split_term_whitespace(row, search_word):\n",
    "    # Splits on first instance ONLY\n",
    "    output = []\n",
    "    if search_word in row['Text']:\n",
    "        tokens = row['Text'].split(' ')\n",
    "        for i in range(len(tokens)):\n",
    "            if search_word in tokens[i]:\n",
    "                output=[' '.join(tokens[:i]), tokens[i], ' '.join(tokens[i+1:])]\n",
    "                break\n",
    "        output = [i.strip() for i in output]\n",
    "        # if the beginning string is empty don't include it in the tokens list\n",
    "        pre_split = output[0].split(' ') if output[0] != '' else []\n",
    "        post_split = output[2].split(' ') if output[2] != '' else []\n",
    "        output.append(pre_split + [output[1]] + post_split)\n",
    "        output.append((len(pre_split), len(pre_split)+1))\n",
    "    else:\n",
    "        output = [row['Text'].strip(),'','',row['Text'].strip().split(' '),None]\n",
    "    return dict(zip(splitting_headers,output))\n",
    "\n",
    "whitespace_output = df_sentences.apply(\n",
    "    lambda row: split_term_whitespace(row, search_word), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "whitespace_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>top_spans</th>\n",
       "      <th>antecedent_indices</th>\n",
       "      <th>predicted_antecedents</th>\n",
       "      <th>document</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td>[[0, 0], [2, 11], [6, 8], [6, 11]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[BERT, :, Pre, -, training, of, Deep, Bidirect...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>[[0, 0], [1, 1], [2, 17], [11, 11], [13, 17], ...</td>\n",
       "      <td>[[2, 3, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1...</td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>[[[2, 17], [25, 25], [59, 59], [105, 105], [11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td>[[0, 0], [2, 7], [5, 5]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[RoBERTa, :, A, Robustly, Optimized, BERT, Pre...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>[[0, 2], [4, 4], [6, 8], [10, 14], [13, 14], [...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>[[[49, 49], [79, 79], [101, 101], [142, 142], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>[[0, 0], [0, 6], [0, 7], [0, 14], [6, 6], [8, ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[DistilBERT, ,, a, distilled, version, of, BER...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>[[1, 10], [4, 10], [15, 18], [15, 19], [15, 20...</td>\n",
       "      <td>[[12, 13, 14, 15, 16, 17, 19, 43, 44, 45, 46, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1...</td>\n",
       "      <td>[As, Transfer, Learning, from, large, -, scale...</td>\n",
       "      <td>[[[4, 10], [23, 25]], [[47, 47], [104, 104], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[[0, 0], [2, 5]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td>[[0, 4], [7, 7], [8, 16], [11, 11], [11, 12], ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Pre, -, trained, text, encoders, have, rapidl...</td>\n",
       "      <td>[[[21, 25], [43, 44], [91, 92]], [[15, 15], [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>What Does BERT Look At? An Analysis of BERT's ...</td>\n",
       "      <td>[[2, 2], [3, 3], [9, 10], [9, 11]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, 0, -1]</td>\n",
       "      <td>[What, Does, BERT, Look, At, ?, An, Analysis, ...</td>\n",
       "      <td>[[[2, 2], [9, 10]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>[[0, 8], [8, 8], [11, 15], [11, 35], [15, 15],...</td>\n",
       "      <td>[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1...</td>\n",
       "      <td>[Large, pre, -, trained, neural, networks, suc...</td>\n",
       "      <td>[[[0, 8], [28, 28]], [[70, 80], [83, 83]], [[8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "      <td>[[1, 1], [5, 5]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Passage, Re, -, ranking, with, BERT]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>[[6, 9], [15, 20], [15, 21], [17, 17], [18, 18...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Recently, ,, neural, models, pretrained, on, ...</td>\n",
       "      <td>[[[69, 69], [87, 87], [136, 136]], [[71, 85], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "      <td>[[1, 2], [1, 4]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Assessing, BERT, 's, Syntactic, Abilities]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>[[0, 0], [0, 15], [2, 10], [2, 14], [6, 10], [...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>[[[6, 10], [83, 85]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>[[0, 0], [2, 2], [4, 4], [6, 13], [13, 13]]</td>\n",
       "      <td>[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>[Beto, ,, Bentz, ,, Becas, :, The, Surprising,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td>[[0, 3], [0, 17], [0, 18], [1, 3], [5, 8], [5,...</td>\n",
       "      <td>[[2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>[[[17, 17], [43, 43]], [[32, 32], [94, 94]], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "      <td>[[0, 0], [2, 7], [5, 7]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[TinyBERT, :, Distilling, BERT, for, Natural, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>[[0, 2], [0, 4], [0, 8], [0, 9], [8, 8], [8, 9...</td>\n",
       "      <td>[[12, 13, 14, 15, 16, 17, 44, 45, 46, 47, 48, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>[[[24, 28], [44, 44]], [[66, 86], [90, 93]], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "      <td>[[0, 3], [5, 6]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Fine, -, tune, BERT, for, Extractive, Summari...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "      <td>[[10, 10], [11, 18], [16, 18], [17, 17], [21, ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1...</td>\n",
       "      <td>[BERT, ,, a, pre, -, trained, Transformer, mod...</td>\n",
       "      <td>[[[24, 24], [38, 38], [70, 70]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "      <td>[[0, 0], [1, 1], [5, 7], [5, 13], [9, 13]]</td>\n",
       "      <td>[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>[BERT, Post, -, Training, for, Review, Reading...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td>[[0, 2], [3, 3], [4, 4], [4, 6], [4, 10], [8, ...</td>\n",
       "      <td>[[14, 15, 16, 17, 45, 46, 47, 48, 49, 50, 51, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1...</td>\n",
       "      <td>[Question, -, answering, plays, an, important,...</td>\n",
       "      <td>[[[0, 2], [12, 12]], [[14, 15], [27, 27]], [[7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "      <td>[[2, 2], [5, 5], [7, 8], [9, 9]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[How, to, Fine, -, Tune, BERT, for, Text, Clas...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td>[[0, 1], [0, 2], [6, 6], [9, 9], [11, 11], [12...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Language, model, pre, -, training, has, prove...</td>\n",
       "      <td>[[[32, 39], [66, 66], [77, 77]], [[73, 80], [8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "      <td>[[0, 0], [1, 1], [2, 3], [6, 6], [8, 8], [9, 9...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6],...</td>\n",
       "      <td>[-1, -1, -1, 0, -1, -1, 0]</td>\n",
       "      <td>[BERT, has, a, Mouth, ,, and, It, Must, Speak,...</td>\n",
       "      <td>[[[0, 0], [6, 6], [10, 17]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "      <td>[[0, 0], [3, 3], [3, 11], [5, 7], [5, 8], [5, ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, 7, -1, -1...</td>\n",
       "      <td>[We, show, that, BERT, (, Devlin, et, al, ., ,...</td>\n",
       "      <td>[[[12, 12], [20, 21]], [[3, 3], [32, 32], [37,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "      <td>[[1, 1], [3, 7], [3, 11], [10, 11]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect, -, Based, Senti...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td>[[0, 21], [0, 22], [10, 10], [12, 12], [13, 21...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Aspect, -, based, sentiment, analysis, (, ABS...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "      <td>[[0, 8], [3, 3], [5, 8]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "      <td>[[1, 8], [1, 9], [1, 18], [5, 5], [7, 8], [10,...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, 6, -1, -1...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>[[[11, 11], [20, 20]], [[5, 5], [37, 37]], [[1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "      <td>[[1, 5], [5, 5]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "      <td>[[0, 2], [0, 3], [5, 5], [6, 17], [6, 31], [15...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1...</td>\n",
       "      <td>[BERT, -, based, architectures, currently, giv...</td>\n",
       "      <td>[[[0, 3], [30, 30]], [[38, 38], [76, 76], [99,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "      <td>[[0, 9], [4, 6], [4, 9], [8, 9]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[Pre, -, Training, with, Whole, Word, Masking,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>[[13, 15], [14, 14], [19, 23], [23, 23], [26, ...</td>\n",
       "      <td>[[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>[[[23, 23], [48, 48], [92, 93], [113, 114], [1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "      <td>[[0, 3], [0, 4], [5, 15], [8, 8], [11, 15], [1...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[Multi, -, passage, BERT, :, A, Globally, Norm...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "      <td>[[0, 0], [5, 5], [7, 11], [10, 10], [15, 16], ...</td>\n",
       "      <td>[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>[[[0, 0], [18, 18], [180, 180]], [[58, 58], [7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "      <td>[[2, 3], [2, 4], [2, 7]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td>[[0, 1], [0, 4], [3, 4], [6, 12], [10, 12], [1...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 14, 15,...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1...</td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>[[[0, 4], [14, 14]], [[104, 104], [126, 126]],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "      <td>[[2, 3], [2, 4], [6, 8]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td>[[0, 1], [3, 10], [7, 10], [12, 22], [14, 22],...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>[[[31, 33], [42, 44]], [[26, 28], [52, 53]], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "      <td>[[1, 4], [4, 4]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>[[0, 1], [2, 2], [3, 8], [3, 11], [8, 8], [10,...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>[[[20, 24], [29, 29]], [[35, 36], [51, 52]], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "      <td>[[0, 9], [4, 5], [4, 9], [7, 9]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "      <td>[[0, 0], [1, 1], [2, 13], [3, 3], [3, 5], [8, ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[We, present, simple, BERT, -, based, models, ...</td>\n",
       "      <td>[[[8, 13], [60, 62]], [[89, 89], [92, 92], [10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "      <td>[[0, 0], [0, 2], [0, 14], [2, 2], [4, 14], [11...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[BERT, and, PALs, :, Projected, Attention, Lay...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td>[[0, 3], [5, 13], [8, 9], [11, 13], [16, 18], ...</td>\n",
       "      <td>[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Multi, -, task, learning, allows, the, sharin...</td>\n",
       "      <td>[[[45, 47], [76, 78], [172, 174]], [[109, 119]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "      <td>[[1, 1], [2, 2], [4, 4], [5, 15], [7, 15], [14...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[What, BERT, Is, Not, :, Lessons, from, a, New...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td>[[0, 5], [4, 5], [7, 7], [8, 15], [8, 31], [8,...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0...</td>\n",
       "      <td>[Pre, -, training, by, language, modeling, has...</td>\n",
       "      <td>[[[0, 5], [27, 31]], [[39, 39], [75, 75]], [[4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>[[3, 6], [6, 6]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>[[0, 2], [0, 8], [8, 8], [11, 11], [12, 17], [...</td>\n",
       "      <td>[[2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 38...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>[[[8, 8], [63, 63], [115, 115], [135, 135], [1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "      <td>[[0, 1]]</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td>[[0, 2], [5, 5], [7, 7], [8, 8], [13, 16], [18...</td>\n",
       "      <td>[[10, 11, 12, 13, 14, 15, 17, 18, 19, 40, 41, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>[[[18, 21], [75, 87], [93, 94], [187, 188]], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "      <td>[[0, 7], [3, 3], [6, 7]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td>[[0, 0], [2, 31], [6, 6], [7, 31], [14, 14], [...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>[[[0, 0], [41, 41], [67, 67], [70, 70], [94, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "      <td>[[0, 1], [0, 12], [3, 3], [3, 12], [8, 12]]</td>\n",
       "      <td>[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine, -, Tunin...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>[[2, 19], [6, 15], [6, 19], [9, 14], [9, 15], ...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Recently, ,, a, simple, combination, of, pass...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "      <td>[[0, 0], [3, 5], [10, 11], [10, 19], [12, 12],...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (, Yet, ),...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "      <td>[[0, 6], [0, 14], [8, 13], [11, 11], [13, 13],...</td>\n",
       "      <td>[[2, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5...</td>\n",
       "      <td>[The, BERT, language, model, (, LM, ), (, Devl...</td>\n",
       "      <td>[[[15, 15], [36, 36]], [[0, 6], [40, 40], [61,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>[[3, 5], [7, 7], [8, 17], [11, 11], [11, 17], ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6],...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td>[[0, 0], [1, 3], [5, 7], [9, 9], [10, 15], [13...</td>\n",
       "      <td>[[2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 3...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>[[[97, 100], [108, 111]], [[112, 112], [119, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  ID      Type  \\\n",
       "0   https://www.semanticscholar.org/paper/BERT%3A-...   0     Title   \n",
       "1   https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract   \n",
       "2   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title   \n",
       "3   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract   \n",
       "4   https://www.semanticscholar.org/paper/DistilBE...   2     Title   \n",
       "5   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract   \n",
       "6   https://www.semanticscholar.org/paper/BERT-Red...   3     Title   \n",
       "7   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract   \n",
       "8   https://www.semanticscholar.org/paper/What-Doe...   4     Title   \n",
       "9   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract   \n",
       "10  https://www.semanticscholar.org/paper/Passage-...   5     Title   \n",
       "11  https://www.semanticscholar.org/paper/Passage-...   5  Abstract   \n",
       "12  https://www.semanticscholar.org/paper/Assessin...   6     Title   \n",
       "13  https://www.semanticscholar.org/paper/Assessin...   6  Abstract   \n",
       "14  https://www.semanticscholar.org/paper/Beto%2C-...   7     Title   \n",
       "15  https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract   \n",
       "16  https://www.semanticscholar.org/paper/TinyBERT...   8     Title   \n",
       "17  https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract   \n",
       "18  https://www.semanticscholar.org/paper/Fine-tun...   9     Title   \n",
       "19  https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract   \n",
       "20  https://www.semanticscholar.org/paper/BERT-Pos...  10     Title   \n",
       "21  https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract   \n",
       "22  https://www.semanticscholar.org/paper/How-to-F...  11     Title   \n",
       "23  https://www.semanticscholar.org/paper/How-to-F...  11  Abstract   \n",
       "24  https://www.semanticscholar.org/paper/BERT-has...  12     Title   \n",
       "25  https://www.semanticscholar.org/paper/BERT-has...  12  Abstract   \n",
       "26  https://www.semanticscholar.org/paper/Utilizin...  13     Title   \n",
       "27  https://www.semanticscholar.org/paper/Utilizin...  13  Abstract   \n",
       "28  https://www.semanticscholar.org/paper/Simple-A...  14     Title   \n",
       "29  https://www.semanticscholar.org/paper/Simple-A...  14  Abstract   \n",
       "30  https://www.semanticscholar.org/paper/Revealin...  15     Title   \n",
       "31  https://www.semanticscholar.org/paper/Revealin...  15  Abstract   \n",
       "32  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title   \n",
       "33  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract   \n",
       "34  https://www.semanticscholar.org/paper/Multi-pa...  17     Title   \n",
       "35  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract   \n",
       "36  https://www.semanticscholar.org/paper/BERT-for...  18     Title   \n",
       "37  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract   \n",
       "38  https://www.semanticscholar.org/paper/BERT-wit...  19     Title   \n",
       "39  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract   \n",
       "40  https://www.semanticscholar.org/paper/Understa...  20     Title   \n",
       "41  https://www.semanticscholar.org/paper/Understa...  20  Abstract   \n",
       "42  https://www.semanticscholar.org/paper/Simple-B...  21     Title   \n",
       "43  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract   \n",
       "44  https://www.semanticscholar.org/paper/BERT-and...  22     Title   \n",
       "45  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract   \n",
       "46  https://www.semanticscholar.org/paper/What-BER...  23     Title   \n",
       "47  https://www.semanticscholar.org/paper/What-BER...  23  Abstract   \n",
       "48  https://www.semanticscholar.org/paper/Visualiz...  24     Title   \n",
       "49  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract   \n",
       "50  https://www.semanticscholar.org/paper/Conditio...  25     Title   \n",
       "51  https://www.semanticscholar.org/paper/Conditio...  25  Abstract   \n",
       "52  https://www.semanticscholar.org/paper/Small-an...  26     Title   \n",
       "53  https://www.semanticscholar.org/paper/Small-an...  26  Abstract   \n",
       "54  https://www.semanticscholar.org/paper/Data-Aug...  27     Title   \n",
       "55  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract   \n",
       "56  https://www.semanticscholar.org/paper/BERT-is-...  28     Title   \n",
       "57  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract   \n",
       "58  https://www.semanticscholar.org/paper/How-Cont...  29     Title   \n",
       "59  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract   \n",
       "\n",
       "                                                 Text  \\\n",
       "0   BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "1   We introduce a new language representation mod...   \n",
       "2   RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "3   Language model pretraining has led to signific...   \n",
       "4   DistilBERT, a distilled version of BERT: small...   \n",
       "5   As Transfer Learning from large-scale pre-trai...   \n",
       "6         BERT Rediscovers the Classical NLP Pipeline   \n",
       "7   Pre-trained text encoders have rapidly advance...   \n",
       "8   What Does BERT Look At? An Analysis of BERT's ...   \n",
       "9   Large pre-trained neural networks such as BERT...   \n",
       "10                       Passage Re-ranking with BERT   \n",
       "11  Recently, neural models pretrained on a langua...   \n",
       "12               Assessing BERT's Syntactic Abilities   \n",
       "13  I assess the extent to which the recently intr...   \n",
       "14  Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "15  Pretrained contextual representation models (P...   \n",
       "16  TinyBERT: Distilling BERT for Natural Language...   \n",
       "17  Language model pre-training, such as BERT, has...   \n",
       "18        Fine-tune BERT for Extractive Summarization   \n",
       "19  BERT, a pre-trained Transformer model, has ach...   \n",
       "20  BERT Post-Training for Review Reading Comprehe...   \n",
       "21  Question-answering plays an important role in ...   \n",
       "22     How to Fine-Tune BERT for Text Classification?   \n",
       "23  Language model pre-training has proven to be u...   \n",
       "24  BERT has a Mouth, and It Must Speak: BERT as a...   \n",
       "25  We show that BERT (Devlin et al., 2018) is a M...   \n",
       "26  Utilizing BERT for Aspect-Based Sentiment Anal...   \n",
       "27  Aspect-based sentiment analysis (ABSA), which ...   \n",
       "28  Simple Applications of BERT for Ad Hoc Documen...   \n",
       "29  Following recent successes in applying BERT to...   \n",
       "30                 Revealing the Dark Secrets of BERT   \n",
       "31  BERT-based architectures currently give state-...   \n",
       "32  Pre-Training with Whole Word Masking for Chine...   \n",
       "33  Bidirectional Encoder Representations from Tra...   \n",
       "34  Multi-passage BERT: A Globally Normalized BERT...   \n",
       "35  BERT model has been successfully applied to op...   \n",
       "36  BERT for Joint Intent Classification and Slot ...   \n",
       "37  Intent classification and slot filling are two...   \n",
       "38  BERT with History Answer Embedding for Convers...   \n",
       "39  Conversational search is an emerging topic in ...   \n",
       "40     Understanding the Behaviors of BERT in Ranking   \n",
       "41  This paper studies the performances and behavi...   \n",
       "42  Simple BERT Models for Relation Extraction and...   \n",
       "43  We present simple BERT-based models for relati...   \n",
       "44  BERT and PALs: Projected Attention Layers for ...   \n",
       "45  Multi-task learning allows the sharing of usef...   \n",
       "46  What BERT Is Not: Lessons from a New Suite of ...   \n",
       "47  Pre-training by language modeling has become a...   \n",
       "48  Visualizing and Understanding the Effectivenes...   \n",
       "49  Language model pre-training, such as BERT, has...   \n",
       "50           Conditional BERT Contextual Augmentation   \n",
       "51  Data augmentation methods are often applied to...   \n",
       "52  Small and Practical BERT Models for Sequence L...   \n",
       "53  We propose a practical scheme to train a singl...   \n",
       "54  Data Augmentation for BERT Fine-Tuning in Open...   \n",
       "55  Recently, a simple combination of passage retr...   \n",
       "56  BERT is Not a Knowledge Base (Yet): Factual Kn...   \n",
       "57  The BERT language model (LM) (Devlin et al., 2...   \n",
       "58  How Contextual are Contextualized Word Represe...   \n",
       "59  Replacing static word embeddings with contextu...   \n",
       "\n",
       "                                            top_spans  \\\n",
       "0                  [[0, 0], [2, 11], [6, 8], [6, 11]]   \n",
       "1   [[0, 0], [1, 1], [2, 17], [11, 11], [13, 17], ...   \n",
       "2                            [[0, 0], [2, 7], [5, 5]]   \n",
       "3   [[0, 2], [4, 4], [6, 8], [10, 14], [13, 14], [...   \n",
       "4   [[0, 0], [0, 6], [0, 7], [0, 14], [6, 6], [8, ...   \n",
       "5   [[1, 10], [4, 10], [15, 18], [15, 19], [15, 20...   \n",
       "6                                    [[0, 0], [2, 5]]   \n",
       "7   [[0, 4], [7, 7], [8, 16], [11, 11], [11, 12], ...   \n",
       "8                  [[2, 2], [3, 3], [9, 10], [9, 11]]   \n",
       "9   [[0, 8], [8, 8], [11, 15], [11, 35], [15, 15],...   \n",
       "10                                   [[1, 1], [5, 5]]   \n",
       "11  [[6, 9], [15, 20], [15, 21], [17, 17], [18, 18...   \n",
       "12                                   [[1, 2], [1, 4]]   \n",
       "13  [[0, 0], [0, 15], [2, 10], [2, 14], [6, 10], [...   \n",
       "14        [[0, 0], [2, 2], [4, 4], [6, 13], [13, 13]]   \n",
       "15  [[0, 3], [0, 17], [0, 18], [1, 3], [5, 8], [5,...   \n",
       "16                           [[0, 0], [2, 7], [5, 7]]   \n",
       "17  [[0, 2], [0, 4], [0, 8], [0, 9], [8, 8], [8, 9...   \n",
       "18                                   [[0, 3], [5, 6]]   \n",
       "19  [[10, 10], [11, 18], [16, 18], [17, 17], [21, ...   \n",
       "20         [[0, 0], [1, 1], [5, 7], [5, 13], [9, 13]]   \n",
       "21  [[0, 2], [3, 3], [4, 4], [4, 6], [4, 10], [8, ...   \n",
       "22                   [[2, 2], [5, 5], [7, 8], [9, 9]]   \n",
       "23  [[0, 1], [0, 2], [6, 6], [9, 9], [11, 11], [12...   \n",
       "24  [[0, 0], [1, 1], [2, 3], [6, 6], [8, 8], [9, 9...   \n",
       "25  [[0, 0], [3, 3], [3, 11], [5, 7], [5, 8], [5, ...   \n",
       "26                [[1, 1], [3, 7], [3, 11], [10, 11]]   \n",
       "27  [[0, 21], [0, 22], [10, 10], [12, 12], [13, 21...   \n",
       "28                           [[0, 8], [3, 3], [5, 8]]   \n",
       "29  [[1, 8], [1, 9], [1, 18], [5, 5], [7, 8], [10,...   \n",
       "30                                   [[1, 5], [5, 5]]   \n",
       "31  [[0, 2], [0, 3], [5, 5], [6, 17], [6, 31], [15...   \n",
       "32                   [[0, 9], [4, 6], [4, 9], [8, 9]]   \n",
       "33  [[13, 15], [14, 14], [19, 23], [23, 23], [26, ...   \n",
       "34  [[0, 3], [0, 4], [5, 15], [8, 8], [11, 15], [1...   \n",
       "35  [[0, 0], [5, 5], [7, 11], [10, 10], [15, 16], ...   \n",
       "36                           [[2, 3], [2, 4], [2, 7]]   \n",
       "37  [[0, 1], [0, 4], [3, 4], [6, 12], [10, 12], [1...   \n",
       "38                           [[2, 3], [2, 4], [6, 8]]   \n",
       "39  [[0, 1], [3, 10], [7, 10], [12, 22], [14, 22],...   \n",
       "40                                   [[1, 4], [4, 4]]   \n",
       "41  [[0, 1], [2, 2], [3, 8], [3, 11], [8, 8], [10,...   \n",
       "42                   [[0, 9], [4, 5], [4, 9], [7, 9]]   \n",
       "43  [[0, 0], [1, 1], [2, 13], [3, 3], [3, 5], [8, ...   \n",
       "44  [[0, 0], [0, 2], [0, 14], [2, 2], [4, 14], [11...   \n",
       "45  [[0, 3], [5, 13], [8, 9], [11, 13], [16, 18], ...   \n",
       "46  [[1, 1], [2, 2], [4, 4], [5, 15], [7, 15], [14...   \n",
       "47  [[0, 5], [4, 5], [7, 7], [8, 15], [8, 31], [8,...   \n",
       "48                                   [[3, 6], [6, 6]]   \n",
       "49  [[0, 2], [0, 8], [8, 8], [11, 11], [12, 17], [...   \n",
       "50                                           [[0, 1]]   \n",
       "51  [[0, 2], [5, 5], [7, 7], [8, 8], [13, 16], [18...   \n",
       "52                           [[0, 7], [3, 3], [6, 7]]   \n",
       "53  [[0, 0], [2, 31], [6, 6], [7, 31], [14, 14], [...   \n",
       "54        [[0, 1], [0, 12], [3, 3], [3, 12], [8, 12]]   \n",
       "55  [[2, 19], [6, 15], [6, 19], [9, 14], [9, 15], ...   \n",
       "56  [[0, 0], [3, 5], [10, 11], [10, 19], [12, 12],...   \n",
       "57  [[0, 6], [0, 14], [8, 13], [11, 11], [13, 13],...   \n",
       "58  [[3, 5], [7, 7], [8, 17], [11, 11], [11, 17], ...   \n",
       "59  [[0, 0], [1, 3], [5, 7], [9, 9], [10, 15], [13...   \n",
       "\n",
       "                                   antecedent_indices  \\\n",
       "0   [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "1   [[2, 3, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, ...   \n",
       "2                   [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "3   [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "4   [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...   \n",
       "5   [[12, 13, 14, 15, 16, 17, 19, 43, 44, 45, 46, ...   \n",
       "6                                    [[0, 1], [0, 1]]   \n",
       "7   [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "8   [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "9   [[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...   \n",
       "10                                   [[0, 1], [0, 1]]   \n",
       "11  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "12                                   [[0, 1], [0, 1]]   \n",
       "13  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "14  [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...   \n",
       "15  [[2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 1...   \n",
       "16                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "17  [[12, 13, 14, 15, 16, 17, 44, 45, 46, 47, 48, ...   \n",
       "18                                   [[0, 1], [0, 1]]   \n",
       "19  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "20  [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...   \n",
       "21  [[14, 15, 16, 17, 45, 46, 47, 48, 49, 50, 51, ...   \n",
       "22  [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "23  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "24  [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6],...   \n",
       "25  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "26  [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "27  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "28                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "29  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "30                                   [[0, 1], [0, 1]]   \n",
       "31  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "32  [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "33  [[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, ...   \n",
       "34  [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...   \n",
       "35  [[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...   \n",
       "36                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "37  [[1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 14, 15,...   \n",
       "38                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "39  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "40                                   [[0, 1], [0, 1]]   \n",
       "41  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "42  [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "43  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "44  [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...   \n",
       "45  [[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...   \n",
       "46  [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...   \n",
       "47  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "48                                   [[0, 1], [0, 1]]   \n",
       "49  [[2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 38...   \n",
       "50                                              [[0]]   \n",
       "51  [[10, 11, 12, 13, 14, 15, 17, 18, 19, 40, 41, ...   \n",
       "52                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "53  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "54  [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...   \n",
       "55  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "56  [[0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, ...   \n",
       "57  [[2, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,...   \n",
       "58  [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6],...   \n",
       "59  [[2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 3...   \n",
       "\n",
       "                                predicted_antecedents  \\\n",
       "0                                    [-1, -1, -1, -1]   \n",
       "1   [-1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1...   \n",
       "2                                        [-1, -1, -1]   \n",
       "3   [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "4                            [-1, -1, -1, -1, -1, -1]   \n",
       "5   [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1...   \n",
       "6                                            [-1, -1]   \n",
       "7   [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "8                                     [-1, -1, 0, -1]   \n",
       "9   [-1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1...   \n",
       "10                                           [-1, -1]   \n",
       "11  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "12                                           [-1, -1]   \n",
       "13  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "14                               [-1, -1, -1, -1, -1]   \n",
       "15  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "16                                       [-1, -1, -1]   \n",
       "17  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "18                                           [-1, -1]   \n",
       "19  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1...   \n",
       "20                               [-1, -1, -1, -1, -1]   \n",
       "21  [-1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1...   \n",
       "22                                   [-1, -1, -1, -1]   \n",
       "23  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "24                         [-1, -1, -1, 0, -1, -1, 0]   \n",
       "25  [-1, -1, -1, -1, -1, -1, -1, -1, -1, 7, -1, -1...   \n",
       "26                                   [-1, -1, -1, -1]   \n",
       "27  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "28                                       [-1, -1, -1]   \n",
       "29  [-1, -1, -1, -1, -1, -1, -1, -1, -1, 6, -1, -1...   \n",
       "30                                           [-1, -1]   \n",
       "31  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1...   \n",
       "32                                   [-1, -1, -1, -1]   \n",
       "33  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "34                           [-1, -1, -1, -1, -1, -1]   \n",
       "35  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1...   \n",
       "36                                       [-1, -1, -1]   \n",
       "37  [-1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1...   \n",
       "38                                       [-1, -1, -1]   \n",
       "39  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "40                                           [-1, -1]   \n",
       "41  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "42                                   [-1, -1, -1, -1]   \n",
       "43  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "44                           [-1, -1, -1, -1, -1, -1]   \n",
       "45  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "46                           [-1, -1, -1, -1, -1, -1]   \n",
       "47  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0...   \n",
       "48                                           [-1, -1]   \n",
       "49  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "50                                               [-1]   \n",
       "51  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "52                                       [-1, -1, -1]   \n",
       "53  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "54                               [-1, -1, -1, -1, -1]   \n",
       "55  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "56                   [-1, -1, -1, -1, -1, -1, -1, -1]   \n",
       "57  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5...   \n",
       "58                       [-1, -1, -1, -1, -1, -1, -1]   \n",
       "59  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "\n",
       "                                             document  \\\n",
       "0   [BERT, :, Pre, -, training, of, Deep, Bidirect...   \n",
       "1   [We, introduce, a, new, language, representati...   \n",
       "2   [RoBERTa, :, A, Robustly, Optimized, BERT, Pre...   \n",
       "3   [Language, model, pretraining, has, led, to, s...   \n",
       "4   [DistilBERT, ,, a, distilled, version, of, BER...   \n",
       "5   [As, Transfer, Learning, from, large, -, scale...   \n",
       "6   [BERT, Rediscovers, the, Classical, NLP, Pipel...   \n",
       "7   [Pre, -, trained, text, encoders, have, rapidl...   \n",
       "8   [What, Does, BERT, Look, At, ?, An, Analysis, ...   \n",
       "9   [Large, pre, -, trained, neural, networks, suc...   \n",
       "10              [Passage, Re, -, ranking, with, BERT]   \n",
       "11  [Recently, ,, neural, models, pretrained, on, ...   \n",
       "12        [Assessing, BERT, 's, Syntactic, Abilities]   \n",
       "13  [I, assess, the, extent, to, which, the, recen...   \n",
       "14  [Beto, ,, Bentz, ,, Becas, :, The, Surprising,...   \n",
       "15  [Pretrained, contextual, representation, model...   \n",
       "16  [TinyBERT, :, Distilling, BERT, for, Natural, ...   \n",
       "17  [Language, model, pre, -, training, ,, such, a...   \n",
       "18  [Fine, -, tune, BERT, for, Extractive, Summari...   \n",
       "19  [BERT, ,, a, pre, -, trained, Transformer, mod...   \n",
       "20  [BERT, Post, -, Training, for, Review, Reading...   \n",
       "21  [Question, -, answering, plays, an, important,...   \n",
       "22  [How, to, Fine, -, Tune, BERT, for, Text, Clas...   \n",
       "23  [Language, model, pre, -, training, has, prove...   \n",
       "24  [BERT, has, a, Mouth, ,, and, It, Must, Speak,...   \n",
       "25  [We, show, that, BERT, (, Devlin, et, al, ., ,...   \n",
       "26  [Utilizing, BERT, for, Aspect, -, Based, Senti...   \n",
       "27  [Aspect, -, based, sentiment, analysis, (, ABS...   \n",
       "28  [Simple, Applications, of, BERT, for, Ad, Hoc,...   \n",
       "29  [Following, recent, successes, in, applying, B...   \n",
       "30          [Revealing, the, Dark, Secrets, of, BERT]   \n",
       "31  [BERT, -, based, architectures, currently, giv...   \n",
       "32  [Pre, -, Training, with, Whole, Word, Masking,...   \n",
       "33  [Bidirectional, Encoder, Representations, from...   \n",
       "34  [Multi, -, passage, BERT, :, A, Globally, Norm...   \n",
       "35  [BERT, model, has, been, successfully, applied...   \n",
       "36  [BERT, for, Joint, Intent, Classification, and...   \n",
       "37  [Intent, classification, and, slot, filling, a...   \n",
       "38  [BERT, with, History, Answer, Embedding, for, ...   \n",
       "39  [Conversational, search, is, an, emerging, top...   \n",
       "40  [Understanding, the, Behaviors, of, BERT, in, ...   \n",
       "41  [This, paper, studies, the, performances, and,...   \n",
       "42  [Simple, BERT, Models, for, Relation, Extracti...   \n",
       "43  [We, present, simple, BERT, -, based, models, ...   \n",
       "44  [BERT, and, PALs, :, Projected, Attention, Lay...   \n",
       "45  [Multi, -, task, learning, allows, the, sharin...   \n",
       "46  [What, BERT, Is, Not, :, Lessons, from, a, New...   \n",
       "47  [Pre, -, training, by, language, modeling, has...   \n",
       "48  [Visualizing, and, Understanding, the, Effecti...   \n",
       "49  [Language, model, pre, -, training, ,, such, a...   \n",
       "50      [Conditional, BERT, Contextual, Augmentation]   \n",
       "51  [Data, augmentation, methods, are, often, appl...   \n",
       "52  [Small, and, Practical, BERT, Models, for, Seq...   \n",
       "53  [We, propose, a, practical, scheme, to, train,...   \n",
       "54  [Data, Augmentation, for, BERT, Fine, -, Tunin...   \n",
       "55  [Recently, ,, a, simple, combination, of, pass...   \n",
       "56  [BERT, is, Not, a, Knowledge, Base, (, Yet, ),...   \n",
       "57  [The, BERT, language, model, (, LM, ), (, Devl...   \n",
       "58  [How, Contextual, are, Contextualized, Word, R...   \n",
       "59  [Replacing, static, word, embeddings, with, co...   \n",
       "\n",
       "                                             clusters  \n",
       "0                                                  []  \n",
       "1   [[[2, 17], [25, 25], [59, 59], [105, 105], [11...  \n",
       "2                                                  []  \n",
       "3   [[[49, 49], [79, 79], [101, 101], [142, 142], ...  \n",
       "4                                                  []  \n",
       "5   [[[4, 10], [23, 25]], [[47, 47], [104, 104], [...  \n",
       "6                                                  []  \n",
       "7   [[[21, 25], [43, 44], [91, 92]], [[15, 15], [5...  \n",
       "8                                 [[[2, 2], [9, 10]]]  \n",
       "9   [[[0, 8], [28, 28]], [[70, 80], [83, 83]], [[8...  \n",
       "10                                                 []  \n",
       "11  [[[69, 69], [87, 87], [136, 136]], [[71, 85], ...  \n",
       "12                                                 []  \n",
       "13                              [[[6, 10], [83, 85]]]  \n",
       "14                                                 []  \n",
       "15  [[[17, 17], [43, 43]], [[32, 32], [94, 94]], [...  \n",
       "16                                                 []  \n",
       "17  [[[24, 28], [44, 44]], [[66, 86], [90, 93]], [...  \n",
       "18                                                 []  \n",
       "19                   [[[24, 24], [38, 38], [70, 70]]]  \n",
       "20                                                 []  \n",
       "21  [[[0, 2], [12, 12]], [[14, 15], [27, 27]], [[7...  \n",
       "22                                                 []  \n",
       "23  [[[32, 39], [66, 66], [77, 77]], [[73, 80], [8...  \n",
       "24                       [[[0, 0], [6, 6], [10, 17]]]  \n",
       "25  [[[12, 12], [20, 21]], [[3, 3], [32, 32], [37,...  \n",
       "26                                                 []  \n",
       "27                                                 []  \n",
       "28                                                 []  \n",
       "29  [[[11, 11], [20, 20]], [[5, 5], [37, 37]], [[1...  \n",
       "30                                                 []  \n",
       "31  [[[0, 3], [30, 30]], [[38, 38], [76, 76], [99,...  \n",
       "32                                                 []  \n",
       "33  [[[23, 23], [48, 48], [92, 93], [113, 114], [1...  \n",
       "34                                                 []  \n",
       "35  [[[0, 0], [18, 18], [180, 180]], [[58, 58], [7...  \n",
       "36                                                 []  \n",
       "37  [[[0, 4], [14, 14]], [[104, 104], [126, 126]],...  \n",
       "38                                                 []  \n",
       "39  [[[31, 33], [42, 44]], [[26, 28], [52, 53]], [...  \n",
       "40                                                 []  \n",
       "41  [[[20, 24], [29, 29]], [[35, 36], [51, 52]], [...  \n",
       "42                                                 []  \n",
       "43  [[[8, 13], [60, 62]], [[89, 89], [92, 92], [10...  \n",
       "44                                                 []  \n",
       "45  [[[45, 47], [76, 78], [172, 174]], [[109, 119]...  \n",
       "46                                                 []  \n",
       "47  [[[0, 5], [27, 31]], [[39, 39], [75, 75]], [[4...  \n",
       "48                                                 []  \n",
       "49  [[[8, 8], [63, 63], [115, 115], [135, 135], [1...  \n",
       "50                                                 []  \n",
       "51  [[[18, 21], [75, 87], [93, 94], [187, 188]], [...  \n",
       "52                                                 []  \n",
       "53  [[[0, 0], [41, 41], [67, 67], [70, 70], [94, 9...  \n",
       "54                                                 []  \n",
       "55                                                 []  \n",
       "56                                                 []  \n",
       "57  [[[15, 15], [36, 36]], [[0, 6], [40, 40], [61,...  \n",
       "58                                                 []  \n",
       "59  [[[97, 100], [108, 111]], [[112, 112], [119, 1...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run coreference resolution over the entire abstract, not individual sentences\n",
    "output = df.apply(\n",
    "    lambda row: coref_predictor.predict(row['Text']), axis=1, result_type='expand')\n",
    "df_merged = df.join(output)\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>top_spans</th>\n",
       "      <th>antecedent_indices</th>\n",
       "      <th>predicted_antecedents</th>\n",
       "      <th>document</th>\n",
       "      <th>clusters</th>\n",
       "      <th>sent_mapping</th>\n",
       "      <th>sent_content</th>\n",
       "      <th>doct_mapping</th>\n",
       "      <th>selcluster_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td>[[0, 0], [2, 11], [6, 8], [6, 11]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[BERT, :, Pre, -, training, of, Deep, Bidirect...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>[[0, 0], [1, 1], [2, 17], [11, 11], [13, 17], ...</td>\n",
       "      <td>[[2, 3, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1...</td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>[[[2, 17], [25, 25], [59, 59], [105, 105], [11...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td>[[0, 0], [2, 7], [5, 5]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[RoBERTa, :, A, Robustly, Optimized, BERT, Pre...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>[[0, 2], [4, 4], [6, 8], [10, 14], [13, 14], [...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>[[[49, 49], [79, 79], [101, 101], [142, 142], ...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>[[0, 0], [0, 6], [0, 7], [0, 14], [6, 6], [8, ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[DistilBERT, ,, a, distilled, version, of, BER...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>[[1, 10], [4, 10], [15, 18], [15, 19], [15, 20...</td>\n",
       "      <td>[[12, 13, 14, 15, 16, 17, 19, 43, 44, 45, 46, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1...</td>\n",
       "      <td>[As, Transfer, Learning, from, large, -, scale...</td>\n",
       "      <td>[[[4, 10], [23, 25]], [[47, 47], [104, 104], [...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[[0, 0], [2, 5]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td>[[0, 4], [7, 7], [8, 16], [11, 11], [11, 12], ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Pre, -, trained, text, encoders, have, rapidl...</td>\n",
       "      <td>[[[21, 25], [43, 44], [91, 92]], [[15, 15], [5...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>What Does BERT Look At? An Analysis of BERT's ...</td>\n",
       "      <td>[[2, 2], [3, 3], [9, 10], [9, 11]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, 0, -1]</td>\n",
       "      <td>[What, Does, BERT, Look, At, ?, An, Analysis, ...</td>\n",
       "      <td>[[[2, 2], [9, 10]]]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5], 1: [6, 7, 8, 9, 10, 11]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 0, 7: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>[[0, 8], [8, 8], [11, 15], [11, 35], [15, 15],...</td>\n",
       "      <td>[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1...</td>\n",
       "      <td>[Large, pre, -, trained, neural, networks, suc...</td>\n",
       "      <td>[[[0, 8], [28, 28]], [[70, 80], [83, 83]], [[8...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "      <td>[[1, 1], [5, 5]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Passage, Re, -, ranking, with, BERT]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>[[6, 9], [15, 20], [15, 21], [17, 17], [18, 18...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Recently, ,, neural, models, pretrained, on, ...</td>\n",
       "      <td>[[[69, 69], [87, 87], [136, 136]], [[71, 85], ...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "      <td>[[1, 2], [1, 4]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Assessing, BERT, 's, Syntactic, Abilities]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>[[0, 0], [0, 15], [2, 10], [2, 14], [6, 10], [...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>[[[6, 10], [83, 85]]]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>[[0, 0], [2, 2], [4, 4], [6, 13], [13, 13]]</td>\n",
       "      <td>[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>[Beto, ,, Bentz, ,, Becas, :, The, Surprising,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td>[[0, 3], [0, 17], [0, 18], [1, 3], [5, 8], [5,...</td>\n",
       "      <td>[[2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>[[[17, 17], [43, 43]], [[32, 32], [94, 94]], [...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "      <td>[[0, 0], [2, 7], [5, 7]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[TinyBERT, :, Distilling, BERT, for, Natural, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>[[0, 2], [0, 4], [0, 8], [0, 9], [8, 8], [8, 9...</td>\n",
       "      <td>[[12, 13, 14, 15, 16, 17, 44, 45, 46, 47, 48, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>[[[24, 28], [44, 44]], [[66, 86], [90, 93]], [...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "      <td>[[0, 3], [5, 6]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Fine, -, tune, BERT, for, Extractive, Summari...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "      <td>[[10, 10], [11, 18], [16, 18], [17, 17], [21, ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1...</td>\n",
       "      <td>[BERT, ,, a, pre, -, trained, Transformer, mod...</td>\n",
       "      <td>[[[24, 24], [38, 38], [70, 70]]]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "      <td>[[0, 0], [1, 1], [5, 7], [5, 13], [9, 13]]</td>\n",
       "      <td>[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>[BERT, Post, -, Training, for, Review, Reading...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td>[[0, 2], [3, 3], [4, 4], [4, 6], [4, 10], [8, ...</td>\n",
       "      <td>[[14, 15, 16, 17, 45, 46, 47, 48, 49, 50, 51, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1...</td>\n",
       "      <td>[Question, -, answering, plays, an, important,...</td>\n",
       "      <td>[[[0, 2], [12, 12]], [[14, 15], [27, 27]], [[7...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "      <td>[[2, 2], [5, 5], [7, 8], [9, 9]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[How, to, Fine, -, Tune, BERT, for, Text, Clas...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td>[[0, 1], [0, 2], [6, 6], [9, 9], [11, 11], [12...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Language, model, pre, -, training, has, prove...</td>\n",
       "      <td>[[[32, 39], [66, 66], [77, 77]], [[73, 80], [8...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "      <td>[[0, 0], [1, 1], [2, 3], [6, 6], [8, 8], [9, 9...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6],...</td>\n",
       "      <td>[-1, -1, -1, 0, -1, -1, 0]</td>\n",
       "      <td>[BERT, has, a, Mouth, ,, and, It, Must, Speak,...</td>\n",
       "      <td>[[[0, 0], [6, 6], [10, 17]]]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "      <td>[[0, 0], [3, 3], [3, 11], [5, 7], [5, 8], [5, ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, 7, -1, -1...</td>\n",
       "      <td>[We, show, that, BERT, (, Devlin, et, al, ., ,...</td>\n",
       "      <td>[[[12, 12], [20, 21]], [[3, 3], [32, 32], [37,...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "      <td>[[1, 1], [3, 7], [3, 11], [10, 11]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect, -, Based, Senti...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td>[[0, 21], [0, 22], [10, 10], [12, 12], [13, 21...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Aspect, -, based, sentiment, analysis, (, ABS...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "      <td>[[0, 8], [3, 3], [5, 8]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "      <td>[[1, 8], [1, 9], [1, 18], [5, 5], [7, 8], [10,...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, 6, -1, -1...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>[[[11, 11], [20, 20]], [[5, 5], [37, 37]], [[1...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "      <td>[[1, 5], [5, 5]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "      <td>[[0, 2], [0, 3], [5, 5], [6, 17], [6, 31], [15...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1...</td>\n",
       "      <td>[BERT, -, based, architectures, currently, giv...</td>\n",
       "      <td>[[[0, 3], [30, 30]], [[38, 38], [76, 76], [99,...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "      <td>[[0, 9], [4, 6], [4, 9], [8, 9]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[Pre, -, Training, with, Whole, Word, Masking,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>[[13, 15], [14, 14], [19, 23], [23, 23], [26, ...</td>\n",
       "      <td>[[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>[[[23, 23], [48, 48], [92, 93], [113, 114], [1...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "      <td>[[0, 3], [0, 4], [5, 15], [8, 8], [11, 15], [1...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[Multi, -, passage, BERT, :, A, Globally, Norm...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "      <td>[[0, 0], [5, 5], [7, 11], [10, 10], [15, 16], ...</td>\n",
       "      <td>[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>[[[0, 0], [18, 18], [180, 180]], [[58, 58], [7...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "      <td>[[2, 3], [2, 4], [2, 7]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td>[[0, 1], [0, 4], [3, 4], [6, 12], [10, 12], [1...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 14, 15,...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1...</td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>[[[0, 4], [14, 14]], [[104, 104], [126, 126]],...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "      <td>[[2, 3], [2, 4], [6, 8]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td>[[0, 1], [3, 10], [7, 10], [12, 22], [14, 22],...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>[[[31, 33], [42, 44]], [[26, 28], [52, 53]], [...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 1:...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "      <td>[[1, 4], [4, 4]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>[[0, 1], [2, 2], [3, 8], [3, 11], [8, 8], [10,...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>[[[20, 24], [29, 29]], [[35, 36], [51, 52]], [...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "      <td>[[0, 9], [4, 5], [4, 9], [7, 9]]</td>\n",
       "      <td>[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...</td>\n",
       "      <td>[-1, -1, -1, -1]</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "      <td>[[0, 0], [1, 1], [2, 13], [3, 3], [3, 5], [8, ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[We, present, simple, BERT, -, based, models, ...</td>\n",
       "      <td>[[[8, 13], [60, 62]], [[89, 89], [92, 92], [10...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "      <td>[[0, 0], [0, 2], [0, 14], [2, 2], [4, 14], [11...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[BERT, and, PALs, :, Projected, Attention, Lay...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td>[[0, 3], [5, 13], [8, 9], [11, 13], [16, 18], ...</td>\n",
       "      <td>[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Multi, -, task, learning, allows, the, sharin...</td>\n",
       "      <td>[[[45, 47], [76, 78], [172, 174]], [[109, 119]...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "      <td>[[1, 1], [2, 2], [4, 4], [5, 15], [7, 15], [14...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[What, BERT, Is, Not, :, Lessons, from, a, New...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td>[[0, 5], [4, 5], [7, 7], [8, 15], [8, 31], [8,...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0...</td>\n",
       "      <td>[Pre, -, training, by, language, modeling, has...</td>\n",
       "      <td>[[[0, 5], [27, 31]], [[39, 39], [75, 75]], [[4...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>[[3, 6], [6, 6]]</td>\n",
       "      <td>[[0, 1], [0, 1]]</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>[[0, 2], [0, 8], [8, 8], [11, 11], [12, 17], [...</td>\n",
       "      <td>[[2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 38...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>[[[8, 8], [63, 63], [115, 115], [135, 135], [1...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "      <td>[[0, 1]]</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>[-1]</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td>[[0, 2], [5, 5], [7, 7], [8, 8], [13, 16], [18...</td>\n",
       "      <td>[[10, 11, 12, 13, 14, 15, 17, 18, 19, 40, 41, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>[[[18, 21], [75, 87], [93, 94], [187, 188]], [...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "      <td>[[0, 7], [3, 3], [6, 7]]</td>\n",
       "      <td>[[0, 1, 2], [0, 1, 2], [0, 1, 2]]</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td>[[0, 0], [2, 31], [6, 6], [7, 31], [14, 14], [...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>[[[0, 0], [41, 41], [67, 67], [70, 70], [94, 9...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "      <td>[[0, 1], [0, 12], [3, 3], [3, 12], [8, 12]]</td>\n",
       "      <td>[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine, -, Tunin...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>[[2, 19], [6, 15], [6, 19], [9, 14], [9, 15], ...</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Recently, ,, a, simple, combination, of, pass...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "      <td>[[0, 0], [3, 5], [10, 11], [10, 19], [12, 12],...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (, Yet, ),...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "      <td>[[0, 6], [0, 14], [8, 13], [11, 11], [13, 13],...</td>\n",
       "      <td>[[2, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5...</td>\n",
       "      <td>[The, BERT, language, model, (, LM, ), (, Devl...</td>\n",
       "      <td>[[[15, 15], [36, 36]], [[0, 6], [40, 40], [61,...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>[[3, 5], [7, 7], [8, 17], [11, 11], [11, 17], ...</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6],...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1]</td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6], 1: [7, 8, 9, 10, 11...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td>[[0, 0], [1, 3], [5, 7], [9, 9], [10, 15], [13...</td>\n",
       "      <td>[[2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 3...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>[[[97, 100], [108, 111]], [[112, 112], [119, 1...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>{0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  ID      Type  \\\n",
       "0   https://www.semanticscholar.org/paper/BERT%3A-...   0     Title   \n",
       "1   https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract   \n",
       "2   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title   \n",
       "3   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract   \n",
       "4   https://www.semanticscholar.org/paper/DistilBE...   2     Title   \n",
       "5   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract   \n",
       "6   https://www.semanticscholar.org/paper/BERT-Red...   3     Title   \n",
       "7   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract   \n",
       "8   https://www.semanticscholar.org/paper/What-Doe...   4     Title   \n",
       "9   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract   \n",
       "10  https://www.semanticscholar.org/paper/Passage-...   5     Title   \n",
       "11  https://www.semanticscholar.org/paper/Passage-...   5  Abstract   \n",
       "12  https://www.semanticscholar.org/paper/Assessin...   6     Title   \n",
       "13  https://www.semanticscholar.org/paper/Assessin...   6  Abstract   \n",
       "14  https://www.semanticscholar.org/paper/Beto%2C-...   7     Title   \n",
       "15  https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract   \n",
       "16  https://www.semanticscholar.org/paper/TinyBERT...   8     Title   \n",
       "17  https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract   \n",
       "18  https://www.semanticscholar.org/paper/Fine-tun...   9     Title   \n",
       "19  https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract   \n",
       "20  https://www.semanticscholar.org/paper/BERT-Pos...  10     Title   \n",
       "21  https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract   \n",
       "22  https://www.semanticscholar.org/paper/How-to-F...  11     Title   \n",
       "23  https://www.semanticscholar.org/paper/How-to-F...  11  Abstract   \n",
       "24  https://www.semanticscholar.org/paper/BERT-has...  12     Title   \n",
       "25  https://www.semanticscholar.org/paper/BERT-has...  12  Abstract   \n",
       "26  https://www.semanticscholar.org/paper/Utilizin...  13     Title   \n",
       "27  https://www.semanticscholar.org/paper/Utilizin...  13  Abstract   \n",
       "28  https://www.semanticscholar.org/paper/Simple-A...  14     Title   \n",
       "29  https://www.semanticscholar.org/paper/Simple-A...  14  Abstract   \n",
       "30  https://www.semanticscholar.org/paper/Revealin...  15     Title   \n",
       "31  https://www.semanticscholar.org/paper/Revealin...  15  Abstract   \n",
       "32  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title   \n",
       "33  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract   \n",
       "34  https://www.semanticscholar.org/paper/Multi-pa...  17     Title   \n",
       "35  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract   \n",
       "36  https://www.semanticscholar.org/paper/BERT-for...  18     Title   \n",
       "37  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract   \n",
       "38  https://www.semanticscholar.org/paper/BERT-wit...  19     Title   \n",
       "39  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract   \n",
       "40  https://www.semanticscholar.org/paper/Understa...  20     Title   \n",
       "41  https://www.semanticscholar.org/paper/Understa...  20  Abstract   \n",
       "42  https://www.semanticscholar.org/paper/Simple-B...  21     Title   \n",
       "43  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract   \n",
       "44  https://www.semanticscholar.org/paper/BERT-and...  22     Title   \n",
       "45  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract   \n",
       "46  https://www.semanticscholar.org/paper/What-BER...  23     Title   \n",
       "47  https://www.semanticscholar.org/paper/What-BER...  23  Abstract   \n",
       "48  https://www.semanticscholar.org/paper/Visualiz...  24     Title   \n",
       "49  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract   \n",
       "50  https://www.semanticscholar.org/paper/Conditio...  25     Title   \n",
       "51  https://www.semanticscholar.org/paper/Conditio...  25  Abstract   \n",
       "52  https://www.semanticscholar.org/paper/Small-an...  26     Title   \n",
       "53  https://www.semanticscholar.org/paper/Small-an...  26  Abstract   \n",
       "54  https://www.semanticscholar.org/paper/Data-Aug...  27     Title   \n",
       "55  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract   \n",
       "56  https://www.semanticscholar.org/paper/BERT-is-...  28     Title   \n",
       "57  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract   \n",
       "58  https://www.semanticscholar.org/paper/How-Cont...  29     Title   \n",
       "59  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract   \n",
       "\n",
       "                                                 Text  \\\n",
       "0   BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "1   We introduce a new language representation mod...   \n",
       "2   RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "3   Language model pretraining has led to signific...   \n",
       "4   DistilBERT, a distilled version of BERT: small...   \n",
       "5   As Transfer Learning from large-scale pre-trai...   \n",
       "6         BERT Rediscovers the Classical NLP Pipeline   \n",
       "7   Pre-trained text encoders have rapidly advance...   \n",
       "8   What Does BERT Look At? An Analysis of BERT's ...   \n",
       "9   Large pre-trained neural networks such as BERT...   \n",
       "10                       Passage Re-ranking with BERT   \n",
       "11  Recently, neural models pretrained on a langua...   \n",
       "12               Assessing BERT's Syntactic Abilities   \n",
       "13  I assess the extent to which the recently intr...   \n",
       "14  Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "15  Pretrained contextual representation models (P...   \n",
       "16  TinyBERT: Distilling BERT for Natural Language...   \n",
       "17  Language model pre-training, such as BERT, has...   \n",
       "18        Fine-tune BERT for Extractive Summarization   \n",
       "19  BERT, a pre-trained Transformer model, has ach...   \n",
       "20  BERT Post-Training for Review Reading Comprehe...   \n",
       "21  Question-answering plays an important role in ...   \n",
       "22     How to Fine-Tune BERT for Text Classification?   \n",
       "23  Language model pre-training has proven to be u...   \n",
       "24  BERT has a Mouth, and It Must Speak: BERT as a...   \n",
       "25  We show that BERT (Devlin et al., 2018) is a M...   \n",
       "26  Utilizing BERT for Aspect-Based Sentiment Anal...   \n",
       "27  Aspect-based sentiment analysis (ABSA), which ...   \n",
       "28  Simple Applications of BERT for Ad Hoc Documen...   \n",
       "29  Following recent successes in applying BERT to...   \n",
       "30                 Revealing the Dark Secrets of BERT   \n",
       "31  BERT-based architectures currently give state-...   \n",
       "32  Pre-Training with Whole Word Masking for Chine...   \n",
       "33  Bidirectional Encoder Representations from Tra...   \n",
       "34  Multi-passage BERT: A Globally Normalized BERT...   \n",
       "35  BERT model has been successfully applied to op...   \n",
       "36  BERT for Joint Intent Classification and Slot ...   \n",
       "37  Intent classification and slot filling are two...   \n",
       "38  BERT with History Answer Embedding for Convers...   \n",
       "39  Conversational search is an emerging topic in ...   \n",
       "40     Understanding the Behaviors of BERT in Ranking   \n",
       "41  This paper studies the performances and behavi...   \n",
       "42  Simple BERT Models for Relation Extraction and...   \n",
       "43  We present simple BERT-based models for relati...   \n",
       "44  BERT and PALs: Projected Attention Layers for ...   \n",
       "45  Multi-task learning allows the sharing of usef...   \n",
       "46  What BERT Is Not: Lessons from a New Suite of ...   \n",
       "47  Pre-training by language modeling has become a...   \n",
       "48  Visualizing and Understanding the Effectivenes...   \n",
       "49  Language model pre-training, such as BERT, has...   \n",
       "50           Conditional BERT Contextual Augmentation   \n",
       "51  Data augmentation methods are often applied to...   \n",
       "52  Small and Practical BERT Models for Sequence L...   \n",
       "53  We propose a practical scheme to train a singl...   \n",
       "54  Data Augmentation for BERT Fine-Tuning in Open...   \n",
       "55  Recently, a simple combination of passage retr...   \n",
       "56  BERT is Not a Knowledge Base (Yet): Factual Kn...   \n",
       "57  The BERT language model (LM) (Devlin et al., 2...   \n",
       "58  How Contextual are Contextualized Word Represe...   \n",
       "59  Replacing static word embeddings with contextu...   \n",
       "\n",
       "                                            top_spans  \\\n",
       "0                  [[0, 0], [2, 11], [6, 8], [6, 11]]   \n",
       "1   [[0, 0], [1, 1], [2, 17], [11, 11], [13, 17], ...   \n",
       "2                            [[0, 0], [2, 7], [5, 5]]   \n",
       "3   [[0, 2], [4, 4], [6, 8], [10, 14], [13, 14], [...   \n",
       "4   [[0, 0], [0, 6], [0, 7], [0, 14], [6, 6], [8, ...   \n",
       "5   [[1, 10], [4, 10], [15, 18], [15, 19], [15, 20...   \n",
       "6                                    [[0, 0], [2, 5]]   \n",
       "7   [[0, 4], [7, 7], [8, 16], [11, 11], [11, 12], ...   \n",
       "8                  [[2, 2], [3, 3], [9, 10], [9, 11]]   \n",
       "9   [[0, 8], [8, 8], [11, 15], [11, 35], [15, 15],...   \n",
       "10                                   [[1, 1], [5, 5]]   \n",
       "11  [[6, 9], [15, 20], [15, 21], [17, 17], [18, 18...   \n",
       "12                                   [[1, 2], [1, 4]]   \n",
       "13  [[0, 0], [0, 15], [2, 10], [2, 14], [6, 10], [...   \n",
       "14        [[0, 0], [2, 2], [4, 4], [6, 13], [13, 13]]   \n",
       "15  [[0, 3], [0, 17], [0, 18], [1, 3], [5, 8], [5,...   \n",
       "16                           [[0, 0], [2, 7], [5, 7]]   \n",
       "17  [[0, 2], [0, 4], [0, 8], [0, 9], [8, 8], [8, 9...   \n",
       "18                                   [[0, 3], [5, 6]]   \n",
       "19  [[10, 10], [11, 18], [16, 18], [17, 17], [21, ...   \n",
       "20         [[0, 0], [1, 1], [5, 7], [5, 13], [9, 13]]   \n",
       "21  [[0, 2], [3, 3], [4, 4], [4, 6], [4, 10], [8, ...   \n",
       "22                   [[2, 2], [5, 5], [7, 8], [9, 9]]   \n",
       "23  [[0, 1], [0, 2], [6, 6], [9, 9], [11, 11], [12...   \n",
       "24  [[0, 0], [1, 1], [2, 3], [6, 6], [8, 8], [9, 9...   \n",
       "25  [[0, 0], [3, 3], [3, 11], [5, 7], [5, 8], [5, ...   \n",
       "26                [[1, 1], [3, 7], [3, 11], [10, 11]]   \n",
       "27  [[0, 21], [0, 22], [10, 10], [12, 12], [13, 21...   \n",
       "28                           [[0, 8], [3, 3], [5, 8]]   \n",
       "29  [[1, 8], [1, 9], [1, 18], [5, 5], [7, 8], [10,...   \n",
       "30                                   [[1, 5], [5, 5]]   \n",
       "31  [[0, 2], [0, 3], [5, 5], [6, 17], [6, 31], [15...   \n",
       "32                   [[0, 9], [4, 6], [4, 9], [8, 9]]   \n",
       "33  [[13, 15], [14, 14], [19, 23], [23, 23], [26, ...   \n",
       "34  [[0, 3], [0, 4], [5, 15], [8, 8], [11, 15], [1...   \n",
       "35  [[0, 0], [5, 5], [7, 11], [10, 10], [15, 16], ...   \n",
       "36                           [[2, 3], [2, 4], [2, 7]]   \n",
       "37  [[0, 1], [0, 4], [3, 4], [6, 12], [10, 12], [1...   \n",
       "38                           [[2, 3], [2, 4], [6, 8]]   \n",
       "39  [[0, 1], [3, 10], [7, 10], [12, 22], [14, 22],...   \n",
       "40                                   [[1, 4], [4, 4]]   \n",
       "41  [[0, 1], [2, 2], [3, 8], [3, 11], [8, 8], [10,...   \n",
       "42                   [[0, 9], [4, 5], [4, 9], [7, 9]]   \n",
       "43  [[0, 0], [1, 1], [2, 13], [3, 3], [3, 5], [8, ...   \n",
       "44  [[0, 0], [0, 2], [0, 14], [2, 2], [4, 14], [11...   \n",
       "45  [[0, 3], [5, 13], [8, 9], [11, 13], [16, 18], ...   \n",
       "46  [[1, 1], [2, 2], [4, 4], [5, 15], [7, 15], [14...   \n",
       "47  [[0, 5], [4, 5], [7, 7], [8, 15], [8, 31], [8,...   \n",
       "48                                   [[3, 6], [6, 6]]   \n",
       "49  [[0, 2], [0, 8], [8, 8], [11, 11], [12, 17], [...   \n",
       "50                                           [[0, 1]]   \n",
       "51  [[0, 2], [5, 5], [7, 7], [8, 8], [13, 16], [18...   \n",
       "52                           [[0, 7], [3, 3], [6, 7]]   \n",
       "53  [[0, 0], [2, 31], [6, 6], [7, 31], [14, 14], [...   \n",
       "54        [[0, 1], [0, 12], [3, 3], [3, 12], [8, 12]]   \n",
       "55  [[2, 19], [6, 15], [6, 19], [9, 14], [9, 15], ...   \n",
       "56  [[0, 0], [3, 5], [10, 11], [10, 19], [12, 12],...   \n",
       "57  [[0, 6], [0, 14], [8, 13], [11, 11], [13, 13],...   \n",
       "58  [[3, 5], [7, 7], [8, 17], [11, 11], [11, 17], ...   \n",
       "59  [[0, 0], [1, 3], [5, 7], [9, 9], [10, 15], [13...   \n",
       "\n",
       "                                   antecedent_indices  \\\n",
       "0   [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "1   [[2, 3, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, ...   \n",
       "2                   [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "3   [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "4   [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...   \n",
       "5   [[12, 13, 14, 15, 16, 17, 19, 43, 44, 45, 46, ...   \n",
       "6                                    [[0, 1], [0, 1]]   \n",
       "7   [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "8   [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "9   [[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...   \n",
       "10                                   [[0, 1], [0, 1]]   \n",
       "11  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "12                                   [[0, 1], [0, 1]]   \n",
       "13  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "14  [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...   \n",
       "15  [[2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 1...   \n",
       "16                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "17  [[12, 13, 14, 15, 16, 17, 44, 45, 46, 47, 48, ...   \n",
       "18                                   [[0, 1], [0, 1]]   \n",
       "19  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "20  [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...   \n",
       "21  [[14, 15, 16, 17, 45, 46, 47, 48, 49, 50, 51, ...   \n",
       "22  [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "23  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "24  [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6],...   \n",
       "25  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "26  [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "27  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "28                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "29  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "30                                   [[0, 1], [0, 1]]   \n",
       "31  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "32  [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "33  [[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, ...   \n",
       "34  [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...   \n",
       "35  [[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...   \n",
       "36                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "37  [[1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 14, 15,...   \n",
       "38                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "39  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "40                                   [[0, 1], [0, 1]]   \n",
       "41  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "42  [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0,...   \n",
       "43  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "44  [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...   \n",
       "45  [[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, ...   \n",
       "46  [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1...   \n",
       "47  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "48                                   [[0, 1], [0, 1]]   \n",
       "49  [[2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 38...   \n",
       "50                                              [[0]]   \n",
       "51  [[10, 11, 12, 13, 14, 15, 17, 18, 19, 40, 41, ...   \n",
       "52                  [[0, 1, 2], [0, 1, 2], [0, 1, 2]]   \n",
       "53  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n",
       "54  [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3...   \n",
       "55  [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...   \n",
       "56  [[0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, ...   \n",
       "57  [[2, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,...   \n",
       "58  [[0, 1, 2, 3, 4, 5, 6], [0, 1, 2, 3, 4, 5, 6],...   \n",
       "59  [[2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 3...   \n",
       "\n",
       "                                predicted_antecedents  \\\n",
       "0                                    [-1, -1, -1, -1]   \n",
       "1   [-1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1...   \n",
       "2                                        [-1, -1, -1]   \n",
       "3   [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "4                            [-1, -1, -1, -1, -1, -1]   \n",
       "5   [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1...   \n",
       "6                                            [-1, -1]   \n",
       "7   [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "8                                     [-1, -1, 0, -1]   \n",
       "9   [-1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1...   \n",
       "10                                           [-1, -1]   \n",
       "11  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "12                                           [-1, -1]   \n",
       "13  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "14                               [-1, -1, -1, -1, -1]   \n",
       "15  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "16                                       [-1, -1, -1]   \n",
       "17  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "18                                           [-1, -1]   \n",
       "19  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1...   \n",
       "20                               [-1, -1, -1, -1, -1]   \n",
       "21  [-1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1...   \n",
       "22                                   [-1, -1, -1, -1]   \n",
       "23  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "24                         [-1, -1, -1, 0, -1, -1, 0]   \n",
       "25  [-1, -1, -1, -1, -1, -1, -1, -1, -1, 7, -1, -1...   \n",
       "26                                   [-1, -1, -1, -1]   \n",
       "27  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "28                                       [-1, -1, -1]   \n",
       "29  [-1, -1, -1, -1, -1, -1, -1, -1, -1, 6, -1, -1...   \n",
       "30                                           [-1, -1]   \n",
       "31  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1...   \n",
       "32                                   [-1, -1, -1, -1]   \n",
       "33  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "34                           [-1, -1, -1, -1, -1, -1]   \n",
       "35  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1...   \n",
       "36                                       [-1, -1, -1]   \n",
       "37  [-1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1...   \n",
       "38                                       [-1, -1, -1]   \n",
       "39  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "40                                           [-1, -1]   \n",
       "41  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "42                                   [-1, -1, -1, -1]   \n",
       "43  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "44                           [-1, -1, -1, -1, -1, -1]   \n",
       "45  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "46                           [-1, -1, -1, -1, -1, -1]   \n",
       "47  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0...   \n",
       "48                                           [-1, -1]   \n",
       "49  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "50                                               [-1]   \n",
       "51  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "52                                       [-1, -1, -1]   \n",
       "53  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "54                               [-1, -1, -1, -1, -1]   \n",
       "55  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "56                   [-1, -1, -1, -1, -1, -1, -1, -1]   \n",
       "57  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5...   \n",
       "58                       [-1, -1, -1, -1, -1, -1, -1]   \n",
       "59  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "\n",
       "                                             document  \\\n",
       "0   [BERT, :, Pre, -, training, of, Deep, Bidirect...   \n",
       "1   [We, introduce, a, new, language, representati...   \n",
       "2   [RoBERTa, :, A, Robustly, Optimized, BERT, Pre...   \n",
       "3   [Language, model, pretraining, has, led, to, s...   \n",
       "4   [DistilBERT, ,, a, distilled, version, of, BER...   \n",
       "5   [As, Transfer, Learning, from, large, -, scale...   \n",
       "6   [BERT, Rediscovers, the, Classical, NLP, Pipel...   \n",
       "7   [Pre, -, trained, text, encoders, have, rapidl...   \n",
       "8   [What, Does, BERT, Look, At, ?, An, Analysis, ...   \n",
       "9   [Large, pre, -, trained, neural, networks, suc...   \n",
       "10              [Passage, Re, -, ranking, with, BERT]   \n",
       "11  [Recently, ,, neural, models, pretrained, on, ...   \n",
       "12        [Assessing, BERT, 's, Syntactic, Abilities]   \n",
       "13  [I, assess, the, extent, to, which, the, recen...   \n",
       "14  [Beto, ,, Bentz, ,, Becas, :, The, Surprising,...   \n",
       "15  [Pretrained, contextual, representation, model...   \n",
       "16  [TinyBERT, :, Distilling, BERT, for, Natural, ...   \n",
       "17  [Language, model, pre, -, training, ,, such, a...   \n",
       "18  [Fine, -, tune, BERT, for, Extractive, Summari...   \n",
       "19  [BERT, ,, a, pre, -, trained, Transformer, mod...   \n",
       "20  [BERT, Post, -, Training, for, Review, Reading...   \n",
       "21  [Question, -, answering, plays, an, important,...   \n",
       "22  [How, to, Fine, -, Tune, BERT, for, Text, Clas...   \n",
       "23  [Language, model, pre, -, training, has, prove...   \n",
       "24  [BERT, has, a, Mouth, ,, and, It, Must, Speak,...   \n",
       "25  [We, show, that, BERT, (, Devlin, et, al, ., ,...   \n",
       "26  [Utilizing, BERT, for, Aspect, -, Based, Senti...   \n",
       "27  [Aspect, -, based, sentiment, analysis, (, ABS...   \n",
       "28  [Simple, Applications, of, BERT, for, Ad, Hoc,...   \n",
       "29  [Following, recent, successes, in, applying, B...   \n",
       "30          [Revealing, the, Dark, Secrets, of, BERT]   \n",
       "31  [BERT, -, based, architectures, currently, giv...   \n",
       "32  [Pre, -, Training, with, Whole, Word, Masking,...   \n",
       "33  [Bidirectional, Encoder, Representations, from...   \n",
       "34  [Multi, -, passage, BERT, :, A, Globally, Norm...   \n",
       "35  [BERT, model, has, been, successfully, applied...   \n",
       "36  [BERT, for, Joint, Intent, Classification, and...   \n",
       "37  [Intent, classification, and, slot, filling, a...   \n",
       "38  [BERT, with, History, Answer, Embedding, for, ...   \n",
       "39  [Conversational, search, is, an, emerging, top...   \n",
       "40  [Understanding, the, Behaviors, of, BERT, in, ...   \n",
       "41  [This, paper, studies, the, performances, and,...   \n",
       "42  [Simple, BERT, Models, for, Relation, Extracti...   \n",
       "43  [We, present, simple, BERT, -, based, models, ...   \n",
       "44  [BERT, and, PALs, :, Projected, Attention, Lay...   \n",
       "45  [Multi, -, task, learning, allows, the, sharin...   \n",
       "46  [What, BERT, Is, Not, :, Lessons, from, a, New...   \n",
       "47  [Pre, -, training, by, language, modeling, has...   \n",
       "48  [Visualizing, and, Understanding, the, Effecti...   \n",
       "49  [Language, model, pre, -, training, ,, such, a...   \n",
       "50      [Conditional, BERT, Contextual, Augmentation]   \n",
       "51  [Data, augmentation, methods, are, often, appl...   \n",
       "52  [Small, and, Practical, BERT, Models, for, Seq...   \n",
       "53  [We, propose, a, practical, scheme, to, train,...   \n",
       "54  [Data, Augmentation, for, BERT, Fine, -, Tunin...   \n",
       "55  [Recently, ,, a, simple, combination, of, pass...   \n",
       "56  [BERT, is, Not, a, Knowledge, Base, (, Yet, ),...   \n",
       "57  [The, BERT, language, model, (, LM, ), (, Devl...   \n",
       "58  [How, Contextual, are, Contextualized, Word, R...   \n",
       "59  [Replacing, static, word, embeddings, with, co...   \n",
       "\n",
       "                                             clusters  \\\n",
       "0                                                  []   \n",
       "1   [[[2, 17], [25, 25], [59, 59], [105, 105], [11...   \n",
       "2                                                  []   \n",
       "3   [[[49, 49], [79, 79], [101, 101], [142, 142], ...   \n",
       "4                                                  []   \n",
       "5   [[[4, 10], [23, 25]], [[47, 47], [104, 104], [...   \n",
       "6                                                  []   \n",
       "7   [[[21, 25], [43, 44], [91, 92]], [[15, 15], [5...   \n",
       "8                                 [[[2, 2], [9, 10]]]   \n",
       "9   [[[0, 8], [28, 28]], [[70, 80], [83, 83]], [[8...   \n",
       "10                                                 []   \n",
       "11  [[[69, 69], [87, 87], [136, 136]], [[71, 85], ...   \n",
       "12                                                 []   \n",
       "13                              [[[6, 10], [83, 85]]]   \n",
       "14                                                 []   \n",
       "15  [[[17, 17], [43, 43]], [[32, 32], [94, 94]], [...   \n",
       "16                                                 []   \n",
       "17  [[[24, 28], [44, 44]], [[66, 86], [90, 93]], [...   \n",
       "18                                                 []   \n",
       "19                   [[[24, 24], [38, 38], [70, 70]]]   \n",
       "20                                                 []   \n",
       "21  [[[0, 2], [12, 12]], [[14, 15], [27, 27]], [[7...   \n",
       "22                                                 []   \n",
       "23  [[[32, 39], [66, 66], [77, 77]], [[73, 80], [8...   \n",
       "24                       [[[0, 0], [6, 6], [10, 17]]]   \n",
       "25  [[[12, 12], [20, 21]], [[3, 3], [32, 32], [37,...   \n",
       "26                                                 []   \n",
       "27                                                 []   \n",
       "28                                                 []   \n",
       "29  [[[11, 11], [20, 20]], [[5, 5], [37, 37]], [[1...   \n",
       "30                                                 []   \n",
       "31  [[[0, 3], [30, 30]], [[38, 38], [76, 76], [99,...   \n",
       "32                                                 []   \n",
       "33  [[[23, 23], [48, 48], [92, 93], [113, 114], [1...   \n",
       "34                                                 []   \n",
       "35  [[[0, 0], [18, 18], [180, 180]], [[58, 58], [7...   \n",
       "36                                                 []   \n",
       "37  [[[0, 4], [14, 14]], [[104, 104], [126, 126]],...   \n",
       "38                                                 []   \n",
       "39  [[[31, 33], [42, 44]], [[26, 28], [52, 53]], [...   \n",
       "40                                                 []   \n",
       "41  [[[20, 24], [29, 29]], [[35, 36], [51, 52]], [...   \n",
       "42                                                 []   \n",
       "43  [[[8, 13], [60, 62]], [[89, 89], [92, 92], [10...   \n",
       "44                                                 []   \n",
       "45  [[[45, 47], [76, 78], [172, 174]], [[109, 119]...   \n",
       "46                                                 []   \n",
       "47  [[[0, 5], [27, 31]], [[39, 39], [75, 75]], [[4...   \n",
       "48                                                 []   \n",
       "49  [[[8, 8], [63, 63], [115, 115], [135, 135], [1...   \n",
       "50                                                 []   \n",
       "51  [[[18, 21], [75, 87], [93, 94], [187, 188]], [...   \n",
       "52                                                 []   \n",
       "53  [[[0, 0], [41, 41], [67, 67], [70, 70], [94, 9...   \n",
       "54                                                 []   \n",
       "55                                                 []   \n",
       "56                                                 []   \n",
       "57  [[[15, 15], [36, 36]], [[0, 6], [40, 40], [61,...   \n",
       "58                                                 []   \n",
       "59  [[[97, 100], [108, 111]], [[112, 112], [119, 1...   \n",
       "\n",
       "                                         sent_mapping  \\\n",
       "0   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "1   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "2    {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}   \n",
       "3   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "4   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "5   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "6                {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}   \n",
       "7   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "8   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: ...   \n",
       "9   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "10               {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}   \n",
       "11  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "12                     {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}   \n",
       "13  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "14  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "15  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "16   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}   \n",
       "17  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "18         {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}   \n",
       "19  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "20  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "21  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "22  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "23  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "24  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "25  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "26  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "27  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "28  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "29  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "30               {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}   \n",
       "31  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "32  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "33  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "34  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "35  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "36   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}   \n",
       "37  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "38  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "39  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "40         {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}   \n",
       "41  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "42  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "43  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "44  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "45  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "46  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "47  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "48         {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}   \n",
       "49  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "50                           {0: 0, 1: 0, 2: 0, 3: 0}   \n",
       "51  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "52   {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}   \n",
       "53  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "54  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "55  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "56  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "57  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "58  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "59  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   \n",
       "\n",
       "                                         sent_content  \\\n",
       "0         {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}   \n",
       "1   {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "2                       {0: [0, 1, 2, 3, 4, 5, 6, 7]}   \n",
       "3   {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "4   {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "5   {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "6                             {0: [0, 1, 2, 3, 4, 5]}   \n",
       "7   {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "8    {0: [0, 1, 2, 3, 4, 5], 1: [6, 7, 8, 9, 10, 11]}   \n",
       "9   {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "10                            {0: [0, 1, 2, 3, 4, 5]}   \n",
       "11  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "12                               {0: [0, 1, 2, 3, 4]}   \n",
       "13  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "14  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "15  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "16                      {0: [0, 1, 2, 3, 4, 5, 6, 7]}   \n",
       "17  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "18                         {0: [0, 1, 2, 3, 4, 5, 6]}   \n",
       "19  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "20  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "21  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "22                {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}   \n",
       "23  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "24  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "25  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "26        {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}   \n",
       "27  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "28                   {0: [0, 1, 2, 3, 4, 5, 6, 7, 8]}   \n",
       "29  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "30                            {0: [0, 1, 2, 3, 4, 5]}   \n",
       "31  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "32                {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}   \n",
       "33  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "34  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "35  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]...   \n",
       "36                      {0: [0, 1, 2, 3, 4, 5, 6, 7]}   \n",
       "37  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "38                   {0: [0, 1, 2, 3, 4, 5, 6, 7, 8]}   \n",
       "39  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 1:...   \n",
       "40                         {0: [0, 1, 2, 3, 4, 5, 6]}   \n",
       "41  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]...   \n",
       "42                {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}   \n",
       "43  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "44  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "45  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "46  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "47  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "48                         {0: [0, 1, 2, 3, 4, 5, 6]}   \n",
       "49  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "50                                  {0: [0, 1, 2, 3]}   \n",
       "51  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "52                      {0: [0, 1, 2, 3, 4, 5, 6, 7]}   \n",
       "53  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "54    {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}   \n",
       "55  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "56  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "57  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "58  {0: [0, 1, 2, 3, 4, 5, 6], 1: [7, 8, 9, 10, 11...   \n",
       "59  {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "\n",
       "                                         doct_mapping  selcluster_idx  \n",
       "0   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "1   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               0  \n",
       "2    {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}              -1  \n",
       "3   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               1  \n",
       "4   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "5   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               2  \n",
       "6                {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}              -1  \n",
       "7   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               0  \n",
       "8   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 0, 7: ...               0  \n",
       "9   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               2  \n",
       "10               {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}              -1  \n",
       "11  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               1  \n",
       "12                     {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}              -1  \n",
       "13  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               0  \n",
       "14  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "15  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               2  \n",
       "16   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}              -1  \n",
       "17  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               2  \n",
       "18         {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}              -1  \n",
       "19  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "20  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "21  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               4  \n",
       "22  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "23  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               0  \n",
       "24  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               0  \n",
       "25  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               1  \n",
       "26  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "27  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "28  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "29  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               1  \n",
       "30               {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}              -1  \n",
       "31  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               3  \n",
       "32  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "33  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               0  \n",
       "34  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "35  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               0  \n",
       "36   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}              -1  \n",
       "37  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               1  \n",
       "38  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "39  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "40         {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}              -1  \n",
       "41  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               2  \n",
       "42  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "43  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               2  \n",
       "44  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "45  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "46  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "47  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               3  \n",
       "48         {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}              -1  \n",
       "49  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               0  \n",
       "50                           {0: 0, 1: 1, 2: 2, 3: 3}              -1  \n",
       "51  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               2  \n",
       "52   {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7}              -1  \n",
       "53  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "54  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "55  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "56  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "57  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               1  \n",
       "58  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...              -1  \n",
       "59  {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...               3  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# transform the output of coreference resolution into something that is more easily manipulated\n",
    "# split it across multiple sentences so each indiv sentence row can still work\n",
    "def reinterpret_coref_clusters(row, search_word, sentences):\n",
    "    # Create dicts to map full-document to indiv sentence data\n",
    "    src = sentences.loc[sentences['ID'] == row['ID']].loc[sentences['Type'] == row['Type']]['Text']\n",
    "    curr_sentence = 0\n",
    "    consumed = 0\n",
    "    sent_mapping = {}\n",
    "    sent_content = {}\n",
    "    last_sent_end = 0\n",
    "    doct_mapping = {}\n",
    "    doct_split = []\n",
    "    for i in range(len(row['document'])):\n",
    "        if row['document'][i].strip() != '':\n",
    "            if row['document'][i] not in src.iloc[curr_sentence][consumed:]:\n",
    "                doct_split.append(row['document'][last_sent_end:i])\n",
    "                last_sent_end = i\n",
    "                curr_sentence += 1\n",
    "                consumed = 0\n",
    "            offset = src.iloc[curr_sentence][consumed:].index(row['document'][i])\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            if curr_sentence not in sent_content:\n",
    "                sent_content[curr_sentence] = []\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "            consumed += offset + len(row['document'][i])\n",
    "        else:\n",
    "            sent_mapping[i] = curr_sentence\n",
    "            sent_content[curr_sentence].append(i)\n",
    "            doct_mapping[i] = i - last_sent_end\n",
    "        doct_split.append(row['document'][last_sent_end:])\n",
    "    # Select the cluster that best matches the search word\n",
    "    selcluster_idx = -1\n",
    "    selcluster_ct = 0\n",
    "    for i in range(len(row['clusters'])):\n",
    "        currcluster_ct = 0\n",
    "        for c in row['clusters'][i]:\n",
    "            currcluster_ct += len(re.findall(f'{search_word}', ' '.join(row['document'][c[0]:c[1]+1])))\n",
    "        if currcluster_ct > selcluster_ct:\n",
    "            selcluster_idx = i\n",
    "            selcluster_ct = currcluster_ct\n",
    "    # Build the output row\n",
    "    output = [sent_mapping, sent_content, doct_mapping, selcluster_idx]\n",
    "    return dict(zip(['sent_mapping', 'sent_content', 'doct_mapping', 'selcluster_idx'],output))\n",
    "\n",
    "output = df_merged.apply(\n",
    "    lambda row: reinterpret_coref_clusters(row, search_word, df_sentences), \n",
    "    axis=1, result_type='expand')\n",
    "df_merged = df_merged.join(output)\n",
    "\n",
    "df_merged.to_csv(f'outputs/coreference-partial.csv')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_0</th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>split_anchor_span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We introduce</td>\n",
       "      <td>a new language representation model called BER...</td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>(2, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unlike recent language representation models ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre - train deep bidirectional ...</td>\n",
       "      <td>[Unlike, recent, language, representation, mod...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a result , the pre - trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine - tuned with just one additi...</td>\n",
       "      <td>[As, a, result, ,, the, pre, -, trained, BERT,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>It</td>\n",
       "      <td>obtains new state - of - the - art results on ...</td>\n",
       "      <td>[It, obtains, new, state, -, of, -, the, -, ar...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "      <td>[BERT:, Pre-training, of, Deep, Bidirectional,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Training, is, computationally, expensive,, of...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining ( Devlin et al . , 2019 ) that car...</td>\n",
       "      <td>[We, present, a, replication, study, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained , and can match...</td>\n",
       "      <td>[We, find, that, BERT, was, significantly, und...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, best, model, achieves, state-of-the-art,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, highlight, the, importance, o...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, our, models, and, code.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "      <td>[RoBERTa:, A, Robustly, Optimized, BERT, Pretr...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[As, Transfer, Learning, from, large-scale, pr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "      <td>[In, this, work,, we, propose, a, method, to, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>a BERT model</td>\n",
       "      <td>by 40 % , while retaining 97 % of its language...</td>\n",
       "      <td>[While, most, prior, work, investigated, the, ...</td>\n",
       "      <td>(37, 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, leverage, the, inductive, biases, learned...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, smaller,, faster, and, lighter, model, i...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "      <td>[DistilBERT,, a, distilled, version, of, BERT:...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-trained, text, encoders, have, rapidly, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>We focus on</td>\n",
       "      <td>one such model , BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "      <td>[We, focus, on, one, such, model, ,, BERT, ,, ...</td>\n",
       "      <td>(3, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>We find that</td>\n",
       "      <td>the model</td>\n",
       "      <td>represents the steps of the traditional NLP pi...</td>\n",
       "      <td>[We, find, that, the, model, represents, the, ...</td>\n",
       "      <td>(3, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Qualitative analysis reveals that</td>\n",
       "      <td>the model</td>\n",
       "      <td>can and often does adjust this pipeline dynami...</td>\n",
       "      <td>[Qualitative, analysis, reveals, that, the, mo...</td>\n",
       "      <td>(4, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Large pre - trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP , motivat...</td>\n",
       "      <td>[Large, pre, -, trained, neural, networks, suc...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Most, recent, analysis, has, focused, on, mod...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Complementary to these works , we propose meth...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Complementary, to, these, works, ,, we, propo...</td>\n",
       "      <td>(22, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "      <td>[BERT, 's, attention, heads, exhibit, patterns...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, show, that, certain, attention, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, example,, we, find, heads, that, attend,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Lastly , we propose an attention - based probi...</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention</td>\n",
       "      <td>[Lastly, ,, we, propose, an, attention, -, bas...</td>\n",
       "      <td>(23, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At</td>\n",
       "      <td>[What, Does, BERT, Look, At]</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>An Analysis of</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td></td>\n",
       "      <td>[An, Analysis, of, BERT, 's]</td>\n",
       "      <td>(3, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "      <td>[Recently,, neural, models, pretrained, on, a,...</td>\n",
       "      <td>(23, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>In this paper , we describe</td>\n",
       "      <td>a simple re - implementation of BERT for query...</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, ,, we, describe, a, simple, ...</td>\n",
       "      <td>(6, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td>Our system</td>\n",
       "      <td>is the state of the art on the TREC - CAR data...</td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, code, to, reproduce, our, results, is, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Passage, Re-ranking, with, BERT]</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I assess the extent to which</td>\n",
       "      <td>the recently introduced BERT model</td>\n",
       "      <td>captures English syntactic phenomena , using (...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>(6, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td></td>\n",
       "      <td>The BERT model</td>\n",
       "      <td>performs remarkably well on all cases</td>\n",
       "      <td>[The, BERT, model, performs, remarkably, well,...</td>\n",
       "      <td>(0, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "      <td>[Assessing, BERT's, Syntactic, Abilities]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "      <td>[A, new, release, of, BERT, (Devlin,, 2018), i...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "      <td>[This, paper, explores, the, broader, cross-li...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best - published methods for zero - s...</td>\n",
       "      <td>[We, compare, mBERT, with, the, best, -, publi...</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner , determine to what extent mBER...</td>\n",
       "      <td>[Additionally, ,, we, investigate, the, most, ...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Beto,, Bentz,, Becas:, The, Surprising, Cross...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, pre-trained, language, models, are,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, accelerate, inference, and, reduce, model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "      <td>a small student TinyBERT</td>\n",
       "      <td></td>\n",
       "      <td>[By, leveraging, this, new, KD, method, ,, the...</td>\n",
       "      <td>(22, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Moreover , we introduce a new two - stage lear...</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "      <td>[Moreover, ,, we, introduce, a, new, two, -, s...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general - domain and task...</td>\n",
       "      <td>[This, framework, ensures, that, TinyBERT, can...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "      <td>[TinyBERT, is, empirically, effective, and, ac...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state - of -...</td>\n",
       "      <td>[TinyBERT, is, also, significantly, better, th...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "      <td>[TinyBERT:, Distilling, BERT, for, Natural, La...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "      <td>[BERT,, a, pre-trained, Transformer, model,, h...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "      <td>[In, this, paper,, we, describe, BERTSUM,, a, ...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, codes, to, reproduce, our, results, are,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "      <td>[Fine-tune, BERT, for, Extractive, Summarization]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Question-answering, plays, an, important, rol...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Inspired, by, the, recent, success, of, machi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, the, best, of, our, knowledge,, no, exist...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work,, we, first, build, an, RRC, d...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>the popular language model BERT</td>\n",
       "      <td>to enhance the performance of fine - tuning of...</td>\n",
       "      <td>[Since, ReviewRC, has, limited, training, exam...</td>\n",
       "      <td>(29, 34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, show, the, generality, of, the, approach,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, the...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, datasets, and, code, are, available, at,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "      <td>[BERT, Post-Training, for, Review, Reading, Co...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pre-training, has, proven, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>As a state - of - the - art language model pre...</td>\n",
       "      <td>BERT ( Bidirectional Encoder Representations f...</td>\n",
       "      <td>has achieved amazing results in many language ...</td>\n",
       "      <td>[As, a, state, -, of, -, the, -, art, language...</td>\n",
       "      <td>(16, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "      <td>[In, this, paper, ,, we, conduct, exhaustive, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, the, proposed, solution, obtains, n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "      <td>[How, to, Fine-Tune, BERT, for, Text, Classifi...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Devlin et al . , 2018 ) is a Markov random f...</td>\n",
       "      <td>[We, show, that, BERT, (, Devlin, et, al, ., ,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[This, formulation, gives, way, to, a, natural...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high - quality , ...</td>\n",
       "      <td>[We, generate, from, BERT, and, find, that, it...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "      <td>[Compared, to, the, generations, of, a, tradit...</td>\n",
       "      <td>(15, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth , and It Must Speak : BERT as a Ma...</td>\n",
       "      <td>[BERT, has, a, Mouth, ,, and, It, Must, Speak,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Aspect-based, sentiment, analysis, (ABSA),, w...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, construct, an, auxiliar...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "      <td>[We, fine-tune, the, pre-trained, model, from,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect-Based, Sentiment...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering , we explore simple appl...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle</td>\n",
       "      <td>[This, required, confronting, the, challenge, ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, address, this, issue, by, applying, infer...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, TREC, microblog, and, newswi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "      <td>[BERT-based, architectures, currently, give, s...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>In the current work , we focus on the interpre...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, the, current, work, ,, we, focus, on, the...</td>\n",
       "      <td>(24, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "      <td>[Using, a, subset, of, GLUE, tasks, and, a, se...</td>\n",
       "      <td>(31, 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, findings, suggest, that, there, is, a, l...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, different, heads, consistently, use, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, manually, disabling, attentio...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Recently , an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking ( WW...</td>\n",
       "      <td>[Recently, ,, an, upgraded, version, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, technical, report,, we, adapt, whol...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>was trained on the latest Chinese Wikipedia dump</td>\n",
       "      <td>[The, model, was, trained, on, the, latest, Ch...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>Chinese BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "      <td>[We, aim, to, provide, easy, extensibility, an...</td>\n",
       "      <td>(10, 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>is verified on various NLP tasks , across sent...</td>\n",
       "      <td>[The, model, is, verified, on, various, NLP, t...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, on, these, datasets, s...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE , BERT - wwm</td>\n",
       "      <td>[Moreover, ,, we, also, examine, the, effectiv...</td>\n",
       "      <td>(14, 15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, the, pre-trained, model, (both, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Pre-Training, with, Whole, Word, Masking, for...</td>\n",
       "      <td>(7, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open - ...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>However , previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "      <td>[However, ,, previous, work, trains, BERT, by,...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "      <td>[To, tackle, this, issue,, we, propose, a, mul...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, addition,, we, find, that, splitting, art...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "      <td>[By, leveraging, a, passage, ranker, to, selec...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "      <td>[Experiments, on, four, standard, benchmarks, ...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>In particular , on the OpenSQuAD dataset , our...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models , and 5.8 % EM and 6.5 % $ F_1 $ over B...</td>\n",
       "      <td>[In, particular, ,, on, the, OpenSQuAD, datase...</td>\n",
       "      <td>(24, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "      <td>[Multi-passage, BERT:, A, Globally, Normalized...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[They, often, suffer, from, small-scale, human...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[Recently, a, new, language, representation, m...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>However , there has not been much effort on ex...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding</td>\n",
       "      <td>[However, ,, there, has, not, been, much, effo...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>In this work , we propose a joint intent class...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work, ,, we, propose, a, joint, int...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, our...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[One, of, the, major, challenges, to, multi-tu...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Existing, methods, either, prepend, history, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, conceptually, simple, yet, hi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[It, enables, seamless, integration, of, conve...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, first, explain, our, view, that, ConvQA, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, demonstrate, the, effectiveness,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, we, analyze, the, impact, of, diffe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "      <td>[We, explore, several, different, ways, to, le...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question - answering focused passage rankin...</td>\n",
       "      <td>[Experimental, results, on, MS, MARCO, demonst...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "      <td>[Experimental, results, on, TREC, show, the, g...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query - docum...</td>\n",
       "      <td>[Analyses, illustrate, how, BERT, allocates, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>We present</td>\n",
       "      <td>simple BERT - based models for relation extrac...</td>\n",
       "      <td></td>\n",
       "      <td>[We, present, simple, BERT, -, based, models, ...</td>\n",
       "      <td>(2, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, recent, years,, state-of-the-art, perform...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "      <td>[In, this, paper,, extensive, experiments, on,...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "      <td>[To, our, knowledge,, we, are, the, first, to,...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td></td>\n",
       "      <td>Our models</td>\n",
       "      <td>provide strong baselines for future</td>\n",
       "      <td>[Our, models, provide, strong, baselines, for,...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Multi-task, learning, allows, the, sharing, o...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, natural, language, processing, several, r...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, are, based, on, fine-tuning, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "      <td>[We, explore, the, multi-task, learning, setti...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, new, adaptation, modules,, PAL...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "      <td>[By, using, PALs, in, parallel, with, BERT, la...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "      <td>[BERT, and, PALs:, Projected, Attention, Layer...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-training, by, language, modeling, has, be...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, we, introduce, a, suite, of,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>As a case study , we apply these diagnostics to</td>\n",
       "      <td>the popular BERT model</td>\n",
       "      <td>, finding that it can generally distinguish go...</td>\n",
       "      <td>[As, a, case, study, ,, we, apply, these, diag...</td>\n",
       "      <td>(10, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "      <td>[What, BERT, Is, Not:, Lessons, from, a, New, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Language model pre - training , such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, it, is, unclear, why, the, pre-trai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>In this paper , we propose to visualize loss l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets</td>\n",
       "      <td>[In, this, paper, ,, we, propose, to, visualiz...</td>\n",
       "      <td>(17, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[First,, we, find, that, pre-training, reaches...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>We also demonstrate that the fine - tuning pro...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over - parameterized for downstream ...</td>\n",
       "      <td>[We, also, demonstrate, that, the, fine, -, tu...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Second , the visualization results indicate th...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "      <td>[Second, ,, the, visualization, results, indic...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Third , the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine - tuning , whic...</td>\n",
       "      <td>[Third, ,, the, lower, layers, of, BERT, are, ...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Recently, proposed, contextual, augmentation,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td></td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(0, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "      <td>[We, propose, a, novel, data, augmentation, me...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "      <td>[We, retrofit, BERT, to, conditional, BERT, by...</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, our, paper,, “conditional, masked, langua...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>task.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[task.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "      <td>[The, well, trained, conditional, BERT, can, b...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, six, various, different, tex...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "      <td>[Starting, from, a, public, multilingual, BERT...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, our, model, especially, outpe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, showcase, the, effectiveness, of, our, me...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "      <td>[Recently,, a, simple, combination, of, passag...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, present, a, data, augme...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "      <td>[We, apply, a, stage-wise, approach, to, fine,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, show, large, gains, in...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine-Tuning, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td></td>\n",
       "      <td>The BERT language model ( LM )</td>\n",
       "      <td>( Devlin et al . , 2019 ) is surprisingly good...</td>\n",
       "      <td>[The, BERT, language, model, (, LM, ), (, Devl...</td>\n",
       "      <td>(0, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Petroni, et, al.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>( 2019 ) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre - training</td>\n",
       "      <td>[(, 2019, ), take, this, as, evidence, that, B...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about ( the surface...</td>\n",
       "      <td>[We, take, issue, with, this, interpretation, ...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>More specifically , we show that</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "      <td>[More, specifically, ,, we, show, that, BERT, ...</td>\n",
       "      <td>(6, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>As a remedy , we propose E - BERT , an extensi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>that replaces entity mentions with symbolic en...</td>\n",
       "      <td>[As, a, remedy, ,, we, propose, E, -, BERT, ,,...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>E - BERT outperforms both</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and ERNIE ( Zhang et al . , 2019 ) on hard - t...</td>\n",
       "      <td>[E, -, BERT, outperforms, both, BERT, and, ERN...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and E -</td>\n",
       "      <td>[We, take, this, as, evidence, that, E, -, BER...</td>\n",
       "      <td>(22, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>However , just how contextual are the contextu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[However, ,, just, how, contextual, are, the, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Are, there, infinitely, many, context-specifi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, one,, we, find, that, the, contextualize...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, representations, of, the, same, word, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[This, suggests, that, upper, layers, of, cont...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>In all layers of ELMo ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2 , on average , less than 5 % of th...</td>\n",
       "      <td>[In, all, layers, of, ELMo, ,, BERT, ,, and, G...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Comparing the Geometry of</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "      <td>[Comparing, the, Geometry, of, BERT,, ELMo,, a...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               split_0  \\\n",
       "0                                         We introduce   \n",
       "1       Unlike recent language representation models ,   \n",
       "2                      As a result , the pre - trained   \n",
       "3                                                        \n",
       "4                                                        \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7                    We present a replication study of   \n",
       "8                                         We find that   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12                                                       \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18                                                       \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20                                         We focus on   \n",
       "21                                        We find that   \n",
       "22                   Qualitative analysis reveals that   \n",
       "23                                                       \n",
       "24         Large pre - trained neural networks such as   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works , we propose meth...   \n",
       "27                                                       \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly , we propose an attention - based probi...   \n",
       "31                                           What Does   \n",
       "32                                      An Analysis of   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34                         In this paper , we describe   \n",
       "35                                                       \n",
       "36   The code to reproduce our results is available...   \n",
       "37                             Passage Re-ranking with   \n",
       "38                        I assess the extent to which   \n",
       "39                                                       \n",
       "40                                           Assessing   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42                                    A new release of   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44                                          We compare   \n",
       "45   Additionally , we investigate the most effecti...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47                Language model pre-training, such as   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method , the plenty ...   \n",
       "51   Moreover , we introduce a new two - stage lear...   \n",
       "52                         This framework ensures that   \n",
       "53                                                       \n",
       "54                                                       \n",
       "55                                                       \n",
       "56                                                       \n",
       "57                          In this paper, we describe   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60                                           Fine-tune   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69                                                       \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state - of - the - art language model pre...   \n",
       "72   In this paper , we conduct exhaustive experime...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74                                    How to Fine-Tune   \n",
       "75                                        We show that   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77                                    We generate from   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79                                                       \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82             We fine-tune the pre-trained model from   \n",
       "83                                           Utilizing   \n",
       "84              Following recent successes in applying   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88                              Simple Applications of   \n",
       "89                                                       \n",
       "90   In the current work , we focus on the interpre...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                       Revealing the Dark Secrets of   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97                   Recently , an upgraded version of   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99                                                       \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101                                                      \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover , we also examine the effectiveness o...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105   Pre-Training with Whole Word Masking for Chinese   \n",
       "106                                                      \n",
       "107                     However , previous work trains   \n",
       "108   To tackle this issue, we propose a multi-passage   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular , on the OpenSQuAD dataset , our...   \n",
       "113                                      Multi-passage   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116      Recently a new language representation model,   \n",
       "117  However , there has not been much effort on ex...   \n",
       "118  In this work , we propose a joint intent class...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120                                                      \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129                                                      \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134                            Analyses illustrate how   \n",
       "135                     Understanding the Behaviors of   \n",
       "136                                         We present   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140                                                      \n",
       "141                                             Simple   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147                     By using PALs in parallel with   \n",
       "148                                                      \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151    As a case study , we apply these diagnostics to   \n",
       "152                                               What   \n",
       "153            Language model pre - training , such as   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper , we propose to visualize loss l...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine - tuning pro...   \n",
       "158  Second , the visualization results indicate th...   \n",
       "159                        Third , the lower layers of   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163                                                      \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165                                        We retrofit   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168                       The well trained conditional   \n",
       "169  Experiments on six various different text clas...   \n",
       "170                                        Conditional   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172                Starting from a public multilingual   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175                                Small and Practical   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178      We apply a stage-wise approach to fine tuning   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180                              Data Augmentation for   \n",
       "181                                                      \n",
       "182                                     Petroni et al.   \n",
       "183                ( 2019 ) take this as evidence that   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185                   More specifically , we show that   \n",
       "186  As a remedy , we propose E - BERT , an extensi...   \n",
       "187                          E - BERT outperforms both   \n",
       "188  We take this as evidence that E - BERT is rich...   \n",
       "189                                                      \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However , just how contextual are the contextu...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196                            In all layers of ELMo ,   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198                          Comparing the Geometry of   \n",
       "\n",
       "                                               split_1  \\\n",
       "0    a new language representation model called BER...   \n",
       "1                                                 BERT   \n",
       "2                                                 BERT   \n",
       "3                                                   It   \n",
       "4                                                BERT:   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7                                                 BERT   \n",
       "8                                                 BERT   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12                                            RoBERTa:   \n",
       "13                                                       \n",
       "14                                         DistilBERT,   \n",
       "15                                        a BERT model   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18                                         DistilBERT,   \n",
       "19                                                       \n",
       "20                               one such model , BERT   \n",
       "21                                           the model   \n",
       "22                                           the model   \n",
       "23                                                BERT   \n",
       "24                                                BERT   \n",
       "25                                                       \n",
       "26                                                BERT   \n",
       "27                                             BERT 's   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                             BERT 's   \n",
       "31                                                BERT   \n",
       "32                                             BERT 's   \n",
       "33                                                BERT   \n",
       "34   a simple re - implementation of BERT for query...   \n",
       "35                                          Our system   \n",
       "36                                                       \n",
       "37                                                BERT   \n",
       "38                  the recently introduced BERT model   \n",
       "39                                      The BERT model   \n",
       "40                                              BERT's   \n",
       "41                                                       \n",
       "42                                                BERT   \n",
       "43                                               mBERT   \n",
       "44                                               mBERT   \n",
       "45                                               mBERT   \n",
       "46                                                BERT   \n",
       "47                                               BERT,   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                            a small student TinyBERT   \n",
       "51                                            TinyBERT   \n",
       "52                                            TinyBERT   \n",
       "53                                            TinyBERT   \n",
       "54                                            TinyBERT   \n",
       "55                                           TinyBERT:   \n",
       "56                                               BERT,   \n",
       "57                                            BERTSUM,   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                                                BERT   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65                     the popular language model BERT   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69                                                BERT   \n",
       "70                                                       \n",
       "71   BERT ( Bidirectional Encoder Representations f...   \n",
       "72                                                BERT   \n",
       "73                                                       \n",
       "74                                                BERT   \n",
       "75                                                BERT   \n",
       "76                                                BERT   \n",
       "77                                                BERT   \n",
       "78                                                BERT   \n",
       "79                                                BERT   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82                                                BERT   \n",
       "83                                                BERT   \n",
       "84                                                BERT   \n",
       "85                                                BERT   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                                                BERT   \n",
       "89                                          BERT-based   \n",
       "90                                                BERT   \n",
       "91                                              BERT's   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                BERT   \n",
       "95                                                BERT   \n",
       "96                                              (BERT)   \n",
       "97                                                BERT   \n",
       "98                                                       \n",
       "99                                           The model   \n",
       "100                                       Chinese BERT   \n",
       "101                                          The model   \n",
       "102                                                      \n",
       "103                                               BERT   \n",
       "104                                                      \n",
       "105                                               BERT   \n",
       "106                                               BERT   \n",
       "107                                               BERT   \n",
       "108                                               BERT   \n",
       "109                                                      \n",
       "110                                               BERT   \n",
       "111                                               BERT   \n",
       "112                                               BERT   \n",
       "113                                              BERT:   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116                                               BERT   \n",
       "117                                               BERT   \n",
       "118                                               BERT   \n",
       "119                                                      \n",
       "120                                               BERT   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125                                               BERT   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129                                               BERT   \n",
       "130                                               BERT   \n",
       "131                                               BERT   \n",
       "132                                               BERT   \n",
       "133                                               BERT   \n",
       "134                                               BERT   \n",
       "135                                               BERT   \n",
       "136  simple BERT - based models for relation extrac...   \n",
       "137                                                      \n",
       "138                                         BERT-based   \n",
       "139                                               BERT   \n",
       "140                                         Our models   \n",
       "141                                               BERT   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145                                               BERT   \n",
       "146                                                      \n",
       "147                                               BERT   \n",
       "148                                               BERT   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151                             the popular BERT model   \n",
       "152                                               BERT   \n",
       "153                                               BERT   \n",
       "154                                                      \n",
       "155                                               BERT   \n",
       "156                                                      \n",
       "157                                               BERT   \n",
       "158                                               BERT   \n",
       "159                                               BERT   \n",
       "160                                               BERT   \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164                                               BERT   \n",
       "165                                               BERT   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168                                               BERT   \n",
       "169                                                      \n",
       "170                                               BERT   \n",
       "171                                                      \n",
       "172                                               BERT   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                                               BERT   \n",
       "176                                               BERT   \n",
       "177                                                      \n",
       "178                                               BERT   \n",
       "179                                                      \n",
       "180                                               BERT   \n",
       "181                     The BERT language model ( LM )   \n",
       "182                                                      \n",
       "183                                               BERT   \n",
       "184                                               BERT   \n",
       "185                                            BERT 's   \n",
       "186                                               BERT   \n",
       "187                                               BERT   \n",
       "188                                               BERT   \n",
       "189                                               BERT   \n",
       "190                                                      \n",
       "191                                               BERT   \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196                                               BERT   \n",
       "197                                                      \n",
       "198                                              BERT,   \n",
       "\n",
       "                                               split_2  \\\n",
       "0                                                        \n",
       "1    is designed to pre - train deep bidirectional ...   \n",
       "2    model can be fine - tuned with just one additi...   \n",
       "3    obtains new state - of - the - art results on ...   \n",
       "4    Pre-training of Deep Bidirectional Transformer...   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7    pretraining ( Devlin et al . , 2019 ) that car...   \n",
       "8    was significantly undertrained , and can match...   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12      A Robustly Optimized BERT Pretraining Approach   \n",
       "13                                                       \n",
       "14   which can then be fine-tuned with good perform...   \n",
       "15   by 40 % , while retaining 97 % of its language...   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18   a distilled version of BERT: smaller, faster, ...   \n",
       "19                                                       \n",
       "20   , and aim to quantify where linguistic informa...   \n",
       "21   represents the steps of the traditional NLP pi...   \n",
       "22   can and often does adjust this pipeline dynami...   \n",
       "23              Rediscovers the Classical NLP Pipeline   \n",
       "24   have had great recent success in NLP , motivat...   \n",
       "25                                                       \n",
       "26                                                       \n",
       "27   attention heads exhibit patterns such as atten...   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                           attention   \n",
       "31                                             Look At   \n",
       "32                                                       \n",
       "33   (Devlin et al., 2018), have achieved impressiv...   \n",
       "34                                                       \n",
       "35   is the state of the art on the TREC - CAR data...   \n",
       "36                                                       \n",
       "37                                                       \n",
       "38   captures English syntactic phenomena , using (...   \n",
       "39               performs remarkably well on all cases   \n",
       "40                                 Syntactic Abilities   \n",
       "41                                                       \n",
       "42   (Devlin, 2018) includes a model simultaneously...   \n",
       "43   (multilingual) as a zero shot language transfe...   \n",
       "44   with the best - published methods for zero - s...   \n",
       "45   in this manner , determine to what extent mBER...   \n",
       "46                                                       \n",
       "47   has significantly improved the performances of...   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                                                       \n",
       "51   , which performs transformer distillation at b...   \n",
       "52   can capture both the general - domain and task...   \n",
       "53   is empirically effective and achieves comparab...   \n",
       "54   is also significantly better than state - of -...   \n",
       "55   Distilling BERT for Natural Language Understan...   \n",
       "56   a pre-trained Transformer model, has achieved ...   \n",
       "57   a simple variant of BERT, for extractive summa...   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                        for Extractive Summarization   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65   to enhance the performance of fine - tuning of...   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69   Post-Training for Review Reading Comprehension...   \n",
       "70                                                       \n",
       "71   has achieved amazing results in many language ...   \n",
       "72   on text classification task and provide a gene...   \n",
       "73                                                       \n",
       "74                            for Text Classification?   \n",
       "75   ( Devlin et al . , 2018 ) is a Markov random f...   \n",
       "76                                                       \n",
       "77   and find that it can produce high - quality , ...   \n",
       "78   generates sentences that are more diverse but ...   \n",
       "79   has a Mouth , and It Must Speak : BERT as a Ma...   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82   and achieve new state-of-the-art results on Se...   \n",
       "83   for Aspect-Based Sentiment Analysis via Constr...   \n",
       "84   to question answering , we explore simple appl...   \n",
       "85                              was designed to handle   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                       for Ad Hoc Document Retrieval   \n",
       "89   architectures currently give state-of-the-art ...   \n",
       "90                                                       \n",
       "91                                              heads.   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                       \n",
       "95                                                       \n",
       "96   has shown marvelous improvements across variou...   \n",
       "97   has been released with Whole Word Masking ( WW...   \n",
       "98                                                       \n",
       "99    was trained on the latest Chinese Wikipedia dump   \n",
       "100  without changing any neural architecture or ev...   \n",
       "101  is verified on various NLP tasks , across sent...   \n",
       "102                                                      \n",
       "103                               , ERNIE , BERT - wwm   \n",
       "104                                                      \n",
       "105                                                      \n",
       "106  model has been successfully applied to open - ...   \n",
       "107  by viewing passages corresponding to the same ...   \n",
       "108  model to globally normalize answer scores acro...   \n",
       "109                                                      \n",
       "110                               gains additional 2%.   \n",
       "111  outperforms all state-of-the-art models on all...   \n",
       "112  models , and 5.8 % EM and 6.5 % $ F_1 $ over B...   \n",
       "113  A Globally Normalized BERT Model for Open-doma...   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116  (Bidirectional Encoder Representations from Tr...   \n",
       "117                 for natural language understanding   \n",
       "118                                                      \n",
       "119                                                      \n",
       "120   for Joint Intent Classification and Slot Filling   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125  (Bidirectional Encoder Representations from Tr...   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129  with History Answer Embedding for Conversation...   \n",
       "130                                   in ranking tasks   \n",
       "131  and fine-tune it on two ranking tasks: MS MARC...   \n",
       "132  in question - answering focused passage rankin...   \n",
       "133  pre-trained on surrounding contexts and the ne...   \n",
       "134  allocates its attentions between query - docum...   \n",
       "135                                         in Ranking   \n",
       "136                                                      \n",
       "137                                                      \n",
       "138    model can achieve state-of-the-art performance.   \n",
       "139                                    in this manner.   \n",
       "140                provide strong baselines for future   \n",
       "141  Models for Relation Extraction and Semantic Ro...   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145  model on the GLUE benchmark, and how to best a...   \n",
       "146                                                      \n",
       "147  layers, we match the performance of fine-tuned...   \n",
       "148  and PALs: Projected Attention Layers for Effic...   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151  , finding that it can generally distinguish go...   \n",
       "152  Is Not: Lessons from a New Suite of Psycholing...   \n",
       "153  , has achieved remarkable results in many NLP ...   \n",
       "154                                                      \n",
       "155                               on specific datasets   \n",
       "156                                                      \n",
       "157  is highly over - parameterized for downstream ...   \n",
       "158  tends to generalize better because of the flat...   \n",
       "159  are more invariant during fine - tuning , whic...   \n",
       "160                                                      \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  demonstrates that a deep bidirectional languag...   \n",
       "164                           contextual augmentation.   \n",
       "165  to conditional BERT by introducing a new condi...   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168  can be applied to enhance contextual augmentat...   \n",
       "169                                                      \n",
       "170                            Contextual Augmentation   \n",
       "171                                                      \n",
       "172  checkpoint, our final model is 6x smaller and ...   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                       Models for Sequence Labeling   \n",
       "176  reader was found to be very effective for ques...   \n",
       "177                                                      \n",
       "178  on multiple datasets, starting with data that ...   \n",
       "179                                                      \n",
       "180      Fine-Tuning in Open-Domain Question Answering   \n",
       "181  ( Devlin et al . , 2019 ) is surprisingly good...   \n",
       "182                                                      \n",
       "183  memorizes factual knowledge during pre - training   \n",
       "184  is partly due to reasoning about ( the surface...   \n",
       "185  precision drops dramatically when we filter ce...   \n",
       "186  that replaces entity mentions with symbolic en...   \n",
       "187  and ERNIE ( Zhang et al . , 2019 ) on hard - t...   \n",
       "188                                            and E -   \n",
       "189  is Not a Knowledge Base (Yet): Factual Knowled...   \n",
       "190                                                      \n",
       "191                                                      \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196  , and GPT-2 , on average , less than 5 % of th...   \n",
       "197                                                      \n",
       "198                         ELMo, and GPT-2 Embeddings   \n",
       "\n",
       "                                          split_tokens split_anchor_span  \n",
       "0    [We, introduce, a, new, language, representati...           (2, 18)  \n",
       "1    [Unlike, recent, language, representation, mod...            (6, 7)  \n",
       "2    [As, a, result, ,, the, pre, -, trained, BERT,...            (8, 9)  \n",
       "3    [It, obtains, new, state, -, of, -, the, -, ar...            (0, 1)  \n",
       "4    [BERT:, Pre-training, of, Deep, Bidirectional,...            (0, 1)  \n",
       "5    [Language, model, pretraining, has, led, to, s...              None  \n",
       "6    [Training, is, computationally, expensive,, of...              None  \n",
       "7    [We, present, a, replication, study, of, BERT,...            (6, 7)  \n",
       "8    [We, find, that, BERT, was, significantly, und...            (3, 4)  \n",
       "9    [Our, best, model, achieves, state-of-the-art,...              None  \n",
       "10   [These, results, highlight, the, importance, o...              None  \n",
       "11              [We, release, our, models, and, code.]              None  \n",
       "12   [RoBERTa:, A, Robustly, Optimized, BERT, Pretr...            (0, 1)  \n",
       "13   [As, Transfer, Learning, from, large-scale, pr...              None  \n",
       "14   [In, this, work,, we, propose, a, method, to, ...          (16, 17)  \n",
       "15   [While, most, prior, work, investigated, the, ...          (37, 40)  \n",
       "16   [To, leverage, the, inductive, biases, learned...              None  \n",
       "17   [Our, smaller,, faster, and, lighter, model, i...              None  \n",
       "18   [DistilBERT,, a, distilled, version, of, BERT:...            (0, 1)  \n",
       "19   [Pre-trained, text, encoders, have, rapidly, a...              None  \n",
       "20   [We, focus, on, one, such, model, ,, BERT, ,, ...            (3, 8)  \n",
       "21   [We, find, that, the, model, represents, the, ...            (3, 5)  \n",
       "22   [Qualitative, analysis, reveals, that, the, mo...            (4, 6)  \n",
       "23   [BERT, Rediscovers, the, Classical, NLP, Pipel...            (0, 1)  \n",
       "24   [Large, pre, -, trained, neural, networks, suc...            (8, 9)  \n",
       "25   [Most, recent, analysis, has, focused, on, mod...              None  \n",
       "26   [Complementary, to, these, works, ,, we, propo...          (22, 23)  \n",
       "27   [BERT, 's, attention, heads, exhibit, patterns...            (0, 2)  \n",
       "28   [We, further, show, that, certain, attention, ...              None  \n",
       "29   [For, example,, we, find, heads, that, attend,...              None  \n",
       "30   [Lastly, ,, we, propose, an, attention, -, bas...          (23, 25)  \n",
       "31                        [What, Does, BERT, Look, At]            (2, 3)  \n",
       "32                        [An, Analysis, of, BERT, 's]            (3, 5)  \n",
       "33   [Recently,, neural, models, pretrained, on, a,...          (23, 24)  \n",
       "34   [In, this, paper, ,, we, describe, a, simple, ...           (6, 21)  \n",
       "35   [Our, system, is, the, state, of, the, art, on...            (0, 2)  \n",
       "36   [The, code, to, reproduce, our, results, is, a...              None  \n",
       "37                   [Passage, Re-ranking, with, BERT]            (3, 4)  \n",
       "38   [I, assess, the, extent, to, which, the, recen...           (6, 11)  \n",
       "39   [The, BERT, model, performs, remarkably, well,...            (0, 3)  \n",
       "40           [Assessing, BERT's, Syntactic, Abilities]            (1, 2)  \n",
       "41   [Pretrained, contextual, representation, model...              None  \n",
       "42   [A, new, release, of, BERT, (Devlin,, 2018), i...            (4, 5)  \n",
       "43   [This, paper, explores, the, broader, cross-li...            (8, 9)  \n",
       "44   [We, compare, mBERT, with, the, best, -, publi...            (2, 3)  \n",
       "45   [Additionally, ,, we, investigate, the, most, ...          (10, 11)  \n",
       "46   [Beto,, Bentz,, Becas:, The, Surprising, Cross...            (8, 9)  \n",
       "47   [Language, model, pre-training,, such, as, BER...            (5, 6)  \n",
       "48   [However,, pre-trained, language, models, are,...              None  \n",
       "49   [To, accelerate, inference, and, reduce, model...              None  \n",
       "50   [By, leveraging, this, new, KD, method, ,, the...          (22, 26)  \n",
       "51   [Moreover, ,, we, introduce, a, new, two, -, s...          (12, 13)  \n",
       "52   [This, framework, ensures, that, TinyBERT, can...            (4, 5)  \n",
       "53   [TinyBERT, is, empirically, effective, and, ac...            (0, 1)  \n",
       "54   [TinyBERT, is, also, significantly, better, th...            (0, 1)  \n",
       "55   [TinyBERT:, Distilling, BERT, for, Natural, La...            (0, 1)  \n",
       "56   [BERT,, a, pre-trained, Transformer, model,, h...            (0, 1)  \n",
       "57   [In, this, paper,, we, describe, BERTSUM,, a, ...            (5, 6)  \n",
       "58   [Our, system, is, the, state, of, the, art, on...              None  \n",
       "59   [The, codes, to, reproduce, our, results, are,...              None  \n",
       "60   [Fine-tune, BERT, for, Extractive, Summarization]            (1, 2)  \n",
       "61   [Question-answering, plays, an, important, rol...              None  \n",
       "62   [Inspired, by, the, recent, success, of, machi...              None  \n",
       "63   [To, the, best, of, our, knowledge,, no, exist...              None  \n",
       "64   [In, this, work,, we, first, build, an, RRC, d...              None  \n",
       "65   [Since, ReviewRC, has, limited, training, exam...          (29, 34)  \n",
       "66   [To, show, the, generality, of, the, approach,...              None  \n",
       "67   [Experimental, results, demonstrate, that, the...              None  \n",
       "68   [The, datasets, and, code, are, available, at,...              None  \n",
       "69   [BERT, Post-Training, for, Review, Reading, Co...            (0, 1)  \n",
       "70   [Language, model, pre-training, has, proven, t...              None  \n",
       "71   [As, a, state, -, of, -, the, -, art, language...          (16, 24)  \n",
       "72   [In, this, paper, ,, we, conduct, exhaustive, ...          (16, 17)  \n",
       "73   [Finally,, the, proposed, solution, obtains, n...              None  \n",
       "74   [How, to, Fine-Tune, BERT, for, Text, Classifi...            (3, 4)  \n",
       "75   [We, show, that, BERT, (, Devlin, et, al, ., ,...            (3, 4)  \n",
       "76   [This, formulation, gives, way, to, a, natural...          (12, 13)  \n",
       "77   [We, generate, from, BERT, and, find, that, it...            (3, 4)  \n",
       "78   [Compared, to, the, generations, of, a, tradit...          (15, 16)  \n",
       "79   [BERT, has, a, Mouth, ,, and, It, Must, Speak,...            (0, 1)  \n",
       "80   [Aspect-based, sentiment, analysis, (ABSA),, w...              None  \n",
       "81   [In, this, paper,, we, construct, an, auxiliar...              None  \n",
       "82   [We, fine-tune, the, pre-trained, model, from,...            (6, 7)  \n",
       "83   [Utilizing, BERT, for, Aspect-Based, Sentiment...            (1, 2)  \n",
       "84   [Following, recent, successes, in, applying, B...            (5, 6)  \n",
       "85   [This, required, confronting, the, challenge, ...          (17, 18)  \n",
       "86   [We, address, this, issue, by, applying, infer...              None  \n",
       "87   [Experiments, on, TREC, microblog, and, newswi...              None  \n",
       "88   [Simple, Applications, of, BERT, for, Ad, Hoc,...            (3, 4)  \n",
       "89   [BERT-based, architectures, currently, give, s...            (0, 1)  \n",
       "90   [In, the, current, work, ,, we, focus, on, the...          (24, 25)  \n",
       "91   [Using, a, subset, of, GLUE, tasks, and, a, se...          (31, 32)  \n",
       "92   [Our, findings, suggest, that, there, is, a, l...              None  \n",
       "93   [While, different, heads, consistently, use, t...              None  \n",
       "94   [We, show, that, manually, disabling, attentio...          (20, 21)  \n",
       "95           [Revealing, the, Dark, Secrets, of, BERT]            (5, 6)  \n",
       "96   [Bidirectional, Encoder, Representations, from...            (5, 6)  \n",
       "97   [Recently, ,, an, upgraded, version, of, BERT,...            (6, 7)  \n",
       "98   [In, this, technical, report,, we, adapt, whol...              None  \n",
       "99   [The, model, was, trained, on, the, latest, Ch...            (0, 2)  \n",
       "100  [We, aim, to, provide, easy, extensibility, an...          (10, 12)  \n",
       "101  [The, model, is, verified, on, various, NLP, t...            (0, 2)  \n",
       "102  [Experimental, results, on, these, datasets, s...              None  \n",
       "103  [Moreover, ,, we, also, examine, the, effectiv...          (14, 15)  \n",
       "104  [We, release, the, pre-trained, model, (both, ...              None  \n",
       "105  [Pre-Training, with, Whole, Word, Masking, for...            (7, 8)  \n",
       "106  [BERT, model, has, been, successfully, applied...            (0, 1)  \n",
       "107  [However, ,, previous, work, trains, BERT, by,...            (5, 6)  \n",
       "108  [To, tackle, this, issue,, we, propose, a, mul...            (8, 9)  \n",
       "109  [In, addition,, we, find, that, splitting, art...              None  \n",
       "110  [By, leveraging, a, passage, ranker, to, selec...          (10, 11)  \n",
       "111  [Experiments, on, four, standard, benchmarks, ...           (9, 10)  \n",
       "112  [In, particular, ,, on, the, OpenSQuAD, datase...          (24, 25)  \n",
       "113  [Multi-passage, BERT:, A, Globally, Normalized...            (1, 2)  \n",
       "114  [Intent, classification, and, slot, filling, a...              None  \n",
       "115  [They, often, suffer, from, small-scale, human...              None  \n",
       "116  [Recently, a, new, language, representation, m...            (6, 7)  \n",
       "117  [However, ,, there, has, not, been, much, effo...          (10, 11)  \n",
       "118  [In, this, work, ,, we, propose, a, joint, int...          (16, 17)  \n",
       "119  [Experimental, results, demonstrate, that, our...              None  \n",
       "120  [BERT, for, Joint, Intent, Classification, and...            (0, 1)  \n",
       "121  [Conversational, search, is, an, emerging, top...              None  \n",
       "122  [One, of, the, major, challenges, to, multi-tu...              None  \n",
       "123  [Existing, methods, either, prepend, history, ...              None  \n",
       "124  [We, propose, a, conceptually, simple, yet, hi...              None  \n",
       "125  [It, enables, seamless, integration, of, conve...          (16, 17)  \n",
       "126  [We, first, explain, our, view, that, ConvQA, ...              None  \n",
       "127  [We, further, demonstrate, the, effectiveness,...              None  \n",
       "128  [Finally,, we, analyze, the, impact, of, diffe...              None  \n",
       "129  [BERT, with, History, Answer, Embedding, for, ...            (0, 1)  \n",
       "130  [This, paper, studies, the, performances, and,...            (8, 9)  \n",
       "131  [We, explore, several, different, ways, to, le...           (9, 10)  \n",
       "132  [Experimental, results, on, MS, MARCO, demonst...          (10, 11)  \n",
       "133  [Experimental, results, on, TREC, show, the, g...           (9, 10)  \n",
       "134  [Analyses, illustrate, how, BERT, allocates, i...            (3, 4)  \n",
       "135  [Understanding, the, Behaviors, of, BERT, in, ...            (4, 5)  \n",
       "136  [We, present, simple, BERT, -, based, models, ...           (2, 14)  \n",
       "137  [In, recent, years,, state-of-the-art, perform...              None  \n",
       "138  [In, this, paper,, extensive, experiments, on,...          (20, 21)  \n",
       "139  [To, our, knowledge,, we, are, the, first, to,...          (10, 11)  \n",
       "140  [Our, models, provide, strong, baselines, for,...            (0, 2)  \n",
       "141  [Simple, BERT, Models, for, Relation, Extracti...            (1, 2)  \n",
       "142  [Multi-task, learning, allows, the, sharing, o...              None  \n",
       "143  [In, natural, language, processing, several, r...              None  \n",
       "144  [These, results, are, based, on, fine-tuning, ...              None  \n",
       "145  [We, explore, the, multi-task, learning, setti...           (9, 10)  \n",
       "146  [We, introduce, new, adaptation, modules,, PAL...              None  \n",
       "147  [By, using, PALs, in, parallel, with, BERT, la...            (6, 7)  \n",
       "148  [BERT, and, PALs:, Projected, Attention, Layer...            (0, 1)  \n",
       "149  [Pre-training, by, language, modeling, has, be...              None  \n",
       "150  [In, this, paper, we, introduce, a, suite, of,...              None  \n",
       "151  [As, a, case, study, ,, we, apply, these, diag...          (10, 14)  \n",
       "152  [What, BERT, Is, Not:, Lessons, from, a, New, ...            (1, 2)  \n",
       "153  [Language, model, pre, -, training, ,, such, a...            (8, 9)  \n",
       "154  [However,, it, is, unclear, why, the, pre-trai...              None  \n",
       "155  [In, this, paper, ,, we, propose, to, visualiz...          (17, 18)  \n",
       "156  [First,, we, find, that, pre-training, reaches...              None  \n",
       "157  [We, also, demonstrate, that, the, fine, -, tu...          (16, 17)  \n",
       "158  [Second, ,, the, visualization, results, indic...          (10, 11)  \n",
       "159  [Third, ,, the, lower, layers, of, BERT, are, ...            (6, 7)  \n",
       "160  [Visualizing, and, Understanding, the, Effecti...            (6, 7)  \n",
       "161  [Data, augmentation, methods, are, often, appl...              None  \n",
       "162  [Recently, proposed, contextual, augmentation,...              None  \n",
       "163  [Bidirectional, Encoder, Representations, from...            (0, 8)  \n",
       "164  [We, propose, a, novel, data, augmentation, me...          (12, 13)  \n",
       "165  [We, retrofit, BERT, to, conditional, BERT, by...            (2, 3)  \n",
       "166  [In, our, paper,, “conditional, masked, langua...              None  \n",
       "167                                            [task.]              None  \n",
       "168  [The, well, trained, conditional, BERT, can, b...            (4, 5)  \n",
       "169  [Experiments, on, six, various, different, tex...              None  \n",
       "170      [Conditional, BERT, Contextual, Augmentation]            (1, 2)  \n",
       "171  [We, propose, a, practical, scheme, to, train,...              None  \n",
       "172  [Starting, from, a, public, multilingual, BERT...            (5, 6)  \n",
       "173  [We, show, that, our, model, especially, outpe...              None  \n",
       "174  [We, showcase, the, effectiveness, of, our, me...              None  \n",
       "175  [Small, and, Practical, BERT, Models, for, Seq...            (3, 4)  \n",
       "176  [Recently,, a, simple, combination, of, passag...          (13, 14)  \n",
       "177  [In, this, paper,, we, present, a, data, augme...              None  \n",
       "178  [We, apply, a, stage-wise, approach, to, fine,...            (8, 9)  \n",
       "179  [Experimental, results, show, large, gains, in...              None  \n",
       "180  [Data, Augmentation, for, BERT, Fine-Tuning, i...            (3, 4)  \n",
       "181  [The, BERT, language, model, (, LM, ), (, Devl...            (0, 7)  \n",
       "182                                 [Petroni, et, al.]              None  \n",
       "183  [(, 2019, ), take, this, as, evidence, that, B...            (8, 9)  \n",
       "184  [We, take, issue, with, this, interpretation, ...          (12, 13)  \n",
       "185  [More, specifically, ,, we, show, that, BERT, ...            (6, 8)  \n",
       "186  [As, a, remedy, ,, we, propose, E, -, BERT, ,,...          (13, 14)  \n",
       "187  [E, -, BERT, outperforms, both, BERT, and, ERN...            (5, 6)  \n",
       "188  [We, take, this, as, evidence, that, E, -, BER...          (22, 23)  \n",
       "189  [BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...            (0, 1)  \n",
       "190  [Replacing, static, word, embeddings, with, co...              None  \n",
       "191  [However, ,, just, how, contextual, are, the, ...          (16, 17)  \n",
       "192  [Are, there, infinitely, many, context-specifi...              None  \n",
       "193  [For, one,, we, find, that, the, contextualize...              None  \n",
       "194  [While, representations, of, the, same, word, ...              None  \n",
       "195  [This, suggests, that, upper, layers, of, cont...              None  \n",
       "196  [In, all, layers, of, ELMo, ,, BERT, ,, and, G...            (6, 7)  \n",
       "197  [How, Contextual, are, Contextualized, Word, R...              None  \n",
       "198  [Comparing, the, Geometry, of, BERT,, ELMo,, a...            (4, 5)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split based on co-references to any phrase containing BERT, using allennlp coreference resolution\n",
    "# This does NOT preserve the original sentence spacing\n",
    "# REQUIRES THAT WE ALREADY RAN THE COREFERENCE PREDICTOR - this func does NOT do all of the work!\n",
    "def split_term_coreference(row, search_word, lookup, fallback):\n",
    "    # Splits on first coref instance ONLY\n",
    "    # there's probably a cleaner way to do this...\n",
    "    lookup_row = lookup.loc[lookup['ID']==row['ID']].loc[lookup['Type']==row['Type']].to_dict(orient='records')[0]\n",
    "    if lookup_row['selcluster_idx'] == -1:\n",
    "        # if we didn't identify any clusters that match the search term, use our fallback method\n",
    "        return fallback(row, search_word)\n",
    "    split_clusters = lookup_row['clusters'][lookup_row['selcluster_idx']]\n",
    "    output = []\n",
    "    for i in range(len(split_clusters)):\n",
    "        c = split_clusters[i]\n",
    "        if lookup_row['sent_mapping'][c[0]] == lookup_row['sent_mapping'][c[0]] == row['Index']:\n",
    "            sentence_start = lookup_row['sent_content'][row['Index']][0]\n",
    "            sentence_end = lookup_row['sent_content'][row['Index']][-1]\n",
    "            pre_split = lookup_row['document'][sentence_start:c[0]]\n",
    "            anchor = lookup_row['document'][c[0]:c[1]+1]\n",
    "            post_split = lookup_row['document'][c[1]+1:sentence_end]\n",
    "            output=[' '.join(pre_split),\n",
    "                    ' '.join(anchor),\n",
    "                    ' '.join(post_split)]\n",
    "            output.append(lookup_row['document'][sentence_start:sentence_end])\n",
    "            output.append((len(pre_split), len(pre_split)+len(anchor)))\n",
    "            break\n",
    "    if output == []:\n",
    "        # if there wasn't any reference in the sentence found, use our fallback method\n",
    "        return fallback(row, search_word)\n",
    "    return dict(zip(splitting_headers,output))\n",
    "\n",
    "coreference_output = df_sentences.apply(\n",
    "    lambda row: split_term_coreference(row, search_word, df_merged, split_term_whitespace), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "coreference_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping functions\n",
    "\n",
    "Assume we have an input dataframe with column headers `['split_0','split_1','split_2', 'split_tokens', 'split_anchor_span']`\n",
    "\n",
    "`'split_1'` is the column that contains our search term / anchor point\n",
    "\n",
    "`'split_0'` and `'split_2'` are the columns that contain text before and after the search terms respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group is the text uniquely identifying a group\n",
    "grouping_headers = ['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>split_0</th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>split_anchor_span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>We introduce</td>\n",
       "      <td>a new language representation model called BER...</td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>(2, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Unlike recent language representation models, ...</td>\n",
       "      <td>Unlike recent language representation models ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre - train deep bidirectional ...</td>\n",
       "      <td>[Unlike, recent, language, representation, mod...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a result, the pre-trained BERT model can be...</td>\n",
       "      <td>As a result , the pre - trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine - tuned with just one additi...</td>\n",
       "      <td>[As, a, result, ,, the, pre, -, trained, BERT,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td>It</td>\n",
       "      <td>obtains new state - of - the - art results on ...</td>\n",
       "      <td>[It, obtains, new, state, -, of, -, the, -, ar...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "      <td>[BERT:, Pre-training, of, Deep, Bidirectional,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Training, is, computationally, expensive,, of...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We present a replication study of BERT pretrai...</td>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining ( Devlin et al . , 2019 ) that car...</td>\n",
       "      <td>[We, present, a, replication, study, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained , and can match...</td>\n",
       "      <td>[We, find, that, BERT, was, significantly, und...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, best, model, achieves, state-of-the-art,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, highlight, the, importance, o...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, our, models, and, code.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "      <td>[RoBERTa:, A, Robustly, Optimized, BERT, Pretr...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[As, Transfer, Learning, from, large-scale, pr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "      <td>[In, this, work,, we, propose, a, method, to, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>a BERT model</td>\n",
       "      <td>by 40 % , while retaining 97 % of its language...</td>\n",
       "      <td>[While, most, prior, work, investigated, the, ...</td>\n",
       "      <td>(37, 40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, leverage, the, inductive, biases, learned...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, smaller,, faster, and, lighter, model, i...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "      <td>[DistilBERT,, a, distilled, version, of, BERT:...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-trained, text, encoders, have, rapidly, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We focus on one such model, BERT, and aim to q...</td>\n",
       "      <td>We focus on</td>\n",
       "      <td>one such model , BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "      <td>[We, focus, on, one, such, model, ,, BERT, ,, ...</td>\n",
       "      <td>(3, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>the model</td>\n",
       "      <td>represents the steps of the traditional NLP pi...</td>\n",
       "      <td>[We, find, that, the, model, represents, the, ...</td>\n",
       "      <td>(3, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td>Qualitative analysis reveals that</td>\n",
       "      <td>the model</td>\n",
       "      <td>can and often does adjust this pipeline dynami...</td>\n",
       "      <td>[Qualitative, analysis, reveals, that, the, mo...</td>\n",
       "      <td>(4, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>Large pre - trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP , motivat...</td>\n",
       "      <td>[Large, pre, -, trained, neural, networks, suc...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Most, recent, analysis, has, focused, on, mod...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>Complementary to these works , we propose meth...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Complementary, to, these, works, ,, we, propo...</td>\n",
       "      <td>(22, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT's attention heads exhibit patterns such a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "      <td>[BERT, 's, attention, heads, exhibit, patterns...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, show, that, certain, attention, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, example,, we, find, heads, that, attend,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>Lastly , we propose an attention - based probi...</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention</td>\n",
       "      <td>[Lastly, ,, we, propose, an, attention, -, bas...</td>\n",
       "      <td>(23, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What Does BERT Look At?</td>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At</td>\n",
       "      <td>[What, Does, BERT, Look, At]</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>An Analysis of BERT's Attention</td>\n",
       "      <td>An Analysis of</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td></td>\n",
       "      <td>[An, Analysis, of, BERT, 's]</td>\n",
       "      <td>(3, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "      <td>[Recently,, neural, models, pretrained, on, a,...</td>\n",
       "      <td>(23, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>In this paper , we describe</td>\n",
       "      <td>a simple re - implementation of BERT for query...</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, ,, we, describe, a, simple, ...</td>\n",
       "      <td>(6, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td>Our system</td>\n",
       "      <td>is the state of the art on the TREC - CAR data...</td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, code, to, reproduce, our, results, is, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Passage, Re-ranking, with, BERT]</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>I assess the extent to which</td>\n",
       "      <td>the recently introduced BERT model</td>\n",
       "      <td>captures English syntactic phenomena , using (...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>(6, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>The BERT model performs remarkably well on all...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT model</td>\n",
       "      <td>performs remarkably well on all cases</td>\n",
       "      <td>[The, BERT, model, performs, remarkably, well,...</td>\n",
       "      <td>(0, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "      <td>[Assessing, BERT's, Syntactic, Abilities]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>A new release of BERT (Devlin, 2018) includes ...</td>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "      <td>[A, new, release, of, BERT, (Devlin,, 2018), i...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "      <td>[This, paper, explores, the, broader, cross-li...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We compare mBERT with the best-published metho...</td>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best - published methods for zero - s...</td>\n",
       "      <td>[We, compare, mBERT, with, the, best, -, publi...</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner , determine to what extent mBER...</td>\n",
       "      <td>[Additionally, ,, we, investigate, the, most, ...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Beto,, Bentz,, Becas:, The, Surprising, Cross...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, pre-trained, language, models, are,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, accelerate, inference, and, reduce, model...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "      <td>a small student TinyBERT</td>\n",
       "      <td></td>\n",
       "      <td>[By, leveraging, this, new, KD, method, ,, the...</td>\n",
       "      <td>(22, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>Moreover , we introduce a new two - stage lear...</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "      <td>[Moreover, ,, we, introduce, a, new, two, -, s...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general - domain and task...</td>\n",
       "      <td>[This, framework, ensures, that, TinyBERT, can...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "      <td>[TinyBERT, is, empirically, effective, and, ac...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>TinyBERT is also significantly better than sta...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state - of -...</td>\n",
       "      <td>[TinyBERT, is, also, significantly, better, th...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "      <td>[TinyBERT:, Distilling, BERT, for, Natural, La...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "      <td>[BERT,, a, pre-trained, Transformer, model,, h...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe BERTSUM, a simple v...</td>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "      <td>[In, this, paper,, we, describe, BERTSUM,, a, ...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, codes, to, reproduce, our, results, are,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "      <td>[Fine-tune, BERT, for, Extractive, Summarization]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Question-answering, plays, an, important, rol...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Inspired, by, the, recent, success, of, machi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, the, best, of, our, knowledge,, no, exist...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work,, we, first, build, an, RRC, d...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>the popular language model BERT</td>\n",
       "      <td>to enhance the performance of fine - tuning of...</td>\n",
       "      <td>[Since, ReviewRC, has, limited, training, exam...</td>\n",
       "      <td>(29, 34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, show, the, generality, of, the, approach,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, the...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, datasets, and, code, are, available, at,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "      <td>[BERT, Post-Training, for, Review, Reading, Co...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pre-training, has, proven, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>As a state - of - the - art language model pre...</td>\n",
       "      <td>BERT ( Bidirectional Encoder Representations f...</td>\n",
       "      <td>has achieved amazing results in many language ...</td>\n",
       "      <td>[As, a, state, -, of, -, the, -, art, language...</td>\n",
       "      <td>(16, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "      <td>[In, this, paper, ,, we, conduct, exhaustive, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, the, proposed, solution, obtains, n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "      <td>[How, to, Fine-Tune, BERT, for, Text, Classifi...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Devlin et al . , 2018 ) is a Markov random f...</td>\n",
       "      <td>[We, show, that, BERT, (, Devlin, et, al, ., ,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[This, formulation, gives, way, to, a, natural...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high - quality , ...</td>\n",
       "      <td>[We, generate, from, BERT, and, find, that, it...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "      <td>[Compared, to, the, generations, of, a, tradit...</td>\n",
       "      <td>(15, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth , and It Must Speak : BERT as a Ma...</td>\n",
       "      <td>[BERT, has, a, Mouth, ,, and, It, Must, Speak,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Aspect-based, sentiment, analysis, (ABSA),, w...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, construct, an, auxiliar...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We fine-tune the pre-trained model from BERT a...</td>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "      <td>[We, fine-tune, the, pre-trained, model, from,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect-Based, Sentiment...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering , we explore simple appl...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle</td>\n",
       "      <td>[This, required, confronting, the, challenge, ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, address, this, issue, by, applying, infer...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, TREC, microblog, and, newswi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "      <td>[BERT-based, architectures, currently, give, s...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>In the current work , we focus on the interpre...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, the, current, work, ,, we, focus, on, the...</td>\n",
       "      <td>(24, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "      <td>[Using, a, subset, of, GLUE, tasks, and, a, se...</td>\n",
       "      <td>(31, 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, findings, suggest, that, there, is, a, l...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, different, heads, consistently, use, t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, manually, disabling, attentio...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, an upgraded version of BERT has been...</td>\n",
       "      <td>Recently , an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking ( WW...</td>\n",
       "      <td>[Recently, ,, an, upgraded, version, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, technical, report,, we, adapt, whol...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>was trained on the latest Chinese Wikipedia dump</td>\n",
       "      <td>[The, model, was, trained, on, the, latest, Ch...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>Chinese BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "      <td>[We, aim, to, provide, easy, extensibility, an...</td>\n",
       "      <td>(10, 12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>is verified on various NLP tasks , across sent...</td>\n",
       "      <td>[The, model, is, verified, on, various, NLP, t...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, on, these, datasets, s...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE , BERT - wwm</td>\n",
       "      <td>[Moreover, ,, we, also, examine, the, effectiv...</td>\n",
       "      <td>(14, 15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, the, pre-trained, model, (both, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Pre-Training, with, Whole, Word, Masking, for...</td>\n",
       "      <td>(7, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open - ...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, previous work trains BERT by viewing ...</td>\n",
       "      <td>However , previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "      <td>[However, ,, previous, work, trains, BERT, by,...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To tackle this issue, we propose a multi-passa...</td>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "      <td>[To, tackle, this, issue,, we, propose, a, mul...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, addition,, we, find, that, splitting, art...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "      <td>[By, leveraging, a, passage, ranker, to, selec...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "      <td>[Experiments, on, four, standard, benchmarks, ...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>In particular , on the OpenSQuAD dataset , our...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models , and 5.8 % EM and 6.5 % $ F_1 $ over B...</td>\n",
       "      <td>[In, particular, ,, on, the, OpenSQuAD, datase...</td>\n",
       "      <td>(24, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "      <td>[Multi-passage, BERT:, A, Globally, Normalized...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[They, often, suffer, from, small-scale, human...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently a new language representation model, ...</td>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[Recently, a, new, language, representation, m...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>However , there has not been much effort on ex...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding</td>\n",
       "      <td>[However, ,, there, has, not, been, much, effo...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>In this work , we propose a joint intent class...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work, ,, we, propose, a, joint, int...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, our...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[One, of, the, major, challenges, to, multi-tu...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Existing, methods, either, prepend, history, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, conceptually, simple, yet, hi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[It, enables, seamless, integration, of, conve...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, first, explain, our, view, that, ConvQA, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, demonstrate, the, effectiveness,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, we, analyze, the, impact, of, diffe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "      <td>[We, explore, several, different, ways, to, le...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question - answering focused passage rankin...</td>\n",
       "      <td>[Experimental, results, on, MS, MARCO, demonst...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "      <td>[Experimental, results, on, TREC, show, the, g...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query - docum...</td>\n",
       "      <td>[Analyses, illustrate, how, BERT, allocates, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "      <td>We present</td>\n",
       "      <td>simple BERT - based models for relation extrac...</td>\n",
       "      <td></td>\n",
       "      <td>[We, present, simple, BERT, -, based, models, ...</td>\n",
       "      <td>(2, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, recent, years,, state-of-the-art, perform...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "      <td>[In, this, paper,, extensive, experiments, on,...</td>\n",
       "      <td>(20, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "      <td>[To, our, knowledge,, we, are, the, first, to,...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td>Our models</td>\n",
       "      <td>provide strong baselines for future</td>\n",
       "      <td>[Our, models, provide, strong, baselines, for,...</td>\n",
       "      <td>(0, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Multi-task, learning, allows, the, sharing, o...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, natural, language, processing, several, r...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, are, based, on, fine-tuning, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "      <td>[We, explore, the, multi-task, learning, setti...</td>\n",
       "      <td>(9, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, new, adaptation, modules,, PAL...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>By using PALs in parallel with BERT layers, we...</td>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "      <td>[By, using, PALs, in, parallel, with, BERT, la...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "      <td>[BERT, and, PALs:, Projected, Attention, Layer...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-training, by, language, modeling, has, be...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, we, introduce, a, suite, of,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>As a case study , we apply these diagnostics to</td>\n",
       "      <td>the popular BERT model</td>\n",
       "      <td>, finding that it can generally distinguish go...</td>\n",
       "      <td>[As, a, case, study, ,, we, apply, these, diag...</td>\n",
       "      <td>(10, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "      <td>[What, BERT, Is, Not:, Lessons, from, a, New, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre - training , such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, it, is, unclear, why, the, pre-trai...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>In this paper , we propose to visualize loss l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets</td>\n",
       "      <td>[In, this, paper, ,, we, propose, to, visualiz...</td>\n",
       "      <td>(17, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[First,, we, find, that, pre-training, reaches...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>We also demonstrate that the fine - tuning pro...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over - parameterized for downstream ...</td>\n",
       "      <td>[We, also, demonstrate, that, the, fine, -, tu...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>Second , the visualization results indicate th...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "      <td>[Second, ,, the, visualization, results, indic...</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Third, the lower layers of BERT are more invar...</td>\n",
       "      <td>Third , the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine - tuning , whic...</td>\n",
       "      <td>[Third, ,, the, lower, layers, of, BERT, are, ...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Recently, proposed, contextual, augmentation,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td></td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(0, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "      <td>[We, propose, a, novel, data, augmentation, me...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "      <td>[We, retrofit, BERT, to, conditional, BERT, by...</td>\n",
       "      <td>(2, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, our, paper,, “conditional, masked, langua...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>task.</td>\n",
       "      <td>task.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[task.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The well trained conditional BERT can be appli...</td>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "      <td>[The, well, trained, conditional, BERT, can, b...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, six, various, different, tex...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Starting from a public multilingual BERT check...</td>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "      <td>[Starting, from, a, public, multilingual, BERT...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, our, model, especially, outpe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, showcase, the, effectiveness, of, our, me...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "      <td>[Recently,, a, simple, combination, of, passag...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, present, a, data, augme...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning ...</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "      <td>[We, apply, a, stage-wise, approach, to, fine,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, show, large, gains, in...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine-Tuning, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT language model ( LM )</td>\n",
       "      <td>( Devlin et al . , 2019 ) is surprisingly good...</td>\n",
       "      <td>[The, BERT, language, model, (, LM, ), (, Devl...</td>\n",
       "      <td>(0, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Petroni, et, al.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>(2019) take this as evidence that BERT memoriz...</td>\n",
       "      <td>( 2019 ) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre - training</td>\n",
       "      <td>[(, 2019, ), take, this, as, evidence, that, B...</td>\n",
       "      <td>(8, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about ( the surface...</td>\n",
       "      <td>[We, take, issue, with, this, interpretation, ...</td>\n",
       "      <td>(12, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>More specifically, we show that BERT's precisi...</td>\n",
       "      <td>More specifically , we show that</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "      <td>[More, specifically, ,, we, show, that, BERT, ...</td>\n",
       "      <td>(6, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>As a remedy, we propose E-BERT, an extension o...</td>\n",
       "      <td>As a remedy , we propose E - BERT , an extensi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>that replaces entity mentions with symbolic en...</td>\n",
       "      <td>[As, a, remedy, ,, we, propose, E, -, BERT, ,,...</td>\n",
       "      <td>(13, 14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>E-BERT outperforms both BERT and ERNIE (Zhang ...</td>\n",
       "      <td>E - BERT outperforms both</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and ERNIE ( Zhang et al . , 2019 ) on hard - t...</td>\n",
       "      <td>[E, -, BERT, outperforms, both, BERT, and, ERN...</td>\n",
       "      <td>(5, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and E -</td>\n",
       "      <td>[We, take, this, as, evidence, that, E, -, BER...</td>\n",
       "      <td>(22, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...</td>\n",
       "      <td>(0, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>However , just how contextual are the contextu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[However, ,, just, how, contextual, are, the, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Are, there, infinitely, many, context-specifi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, one,, we, find, that, the, contextualize...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, representations, of, the, same, word, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[This, suggests, that, upper, layers, of, cont...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In all layers of ELMo, BERT, and GPT-2, on ave...</td>\n",
       "      <td>In all layers of ELMo ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2 , on average , less than 5 % of th...</td>\n",
       "      <td>[In, all, layers, of, ELMo, ,, BERT, ,, and, G...</td>\n",
       "      <td>(6, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing the Geometry of BERT, ELMo, and GPT-...</td>\n",
       "      <td>Comparing the Geometry of</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "      <td>[Comparing, the, Geometry, of, BERT,, ELMo,, a...</td>\n",
       "      <td>(4, 5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  ID      Type  Index  \\\n",
       "0    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      0   \n",
       "1    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      1   \n",
       "2    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      2   \n",
       "3    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      3   \n",
       "4    https://www.semanticscholar.org/paper/BERT%3A-...   0     Title      0   \n",
       "5    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      0   \n",
       "6    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      1   \n",
       "7    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      2   \n",
       "8    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      3   \n",
       "9    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      4   \n",
       "10   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      5   \n",
       "11   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      6   \n",
       "12   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title      0   \n",
       "13   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      0   \n",
       "14   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      1   \n",
       "15   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      2   \n",
       "16   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      3   \n",
       "17   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      4   \n",
       "18   https://www.semanticscholar.org/paper/DistilBE...   2     Title      0   \n",
       "19   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      0   \n",
       "20   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      1   \n",
       "21   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      2   \n",
       "22   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      3   \n",
       "23   https://www.semanticscholar.org/paper/BERT-Red...   3     Title      0   \n",
       "24   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      0   \n",
       "25   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      1   \n",
       "26   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      2   \n",
       "27   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      3   \n",
       "28   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      4   \n",
       "29   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      5   \n",
       "30   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      6   \n",
       "31   https://www.semanticscholar.org/paper/What-Doe...   4     Title      0   \n",
       "32   https://www.semanticscholar.org/paper/What-Doe...   4     Title      1   \n",
       "33   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      0   \n",
       "34   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      1   \n",
       "35   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      2   \n",
       "36   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      3   \n",
       "37   https://www.semanticscholar.org/paper/Passage-...   5     Title      0   \n",
       "38   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      0   \n",
       "39   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      1   \n",
       "40   https://www.semanticscholar.org/paper/Assessin...   6     Title      0   \n",
       "41   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      0   \n",
       "42   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      1   \n",
       "43   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      2   \n",
       "44   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      3   \n",
       "45   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      4   \n",
       "46   https://www.semanticscholar.org/paper/Beto%2C-...   7     Title      0   \n",
       "47   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      0   \n",
       "48   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      1   \n",
       "49   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      2   \n",
       "50   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      3   \n",
       "51   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      4   \n",
       "52   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      5   \n",
       "53   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      6   \n",
       "54   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      7   \n",
       "55   https://www.semanticscholar.org/paper/TinyBERT...   8     Title      0   \n",
       "56   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      0   \n",
       "57   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      1   \n",
       "58   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      2   \n",
       "59   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      3   \n",
       "60   https://www.semanticscholar.org/paper/Fine-tun...   9     Title      0   \n",
       "61   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      0   \n",
       "62   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      1   \n",
       "63   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      2   \n",
       "64   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      3   \n",
       "65   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      4   \n",
       "66   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      5   \n",
       "67   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      6   \n",
       "68   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      7   \n",
       "69   https://www.semanticscholar.org/paper/BERT-Pos...  10     Title      0   \n",
       "70   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      0   \n",
       "71   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      1   \n",
       "72   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      2   \n",
       "73   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      3   \n",
       "74   https://www.semanticscholar.org/paper/How-to-F...  11     Title      0   \n",
       "75   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      0   \n",
       "76   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      1   \n",
       "77   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      2   \n",
       "78   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      3   \n",
       "79   https://www.semanticscholar.org/paper/BERT-has...  12     Title      0   \n",
       "80   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      0   \n",
       "81   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      1   \n",
       "82   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      2   \n",
       "83   https://www.semanticscholar.org/paper/Utilizin...  13     Title      0   \n",
       "84   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      0   \n",
       "85   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      1   \n",
       "86   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      2   \n",
       "87   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      3   \n",
       "88   https://www.semanticscholar.org/paper/Simple-A...  14     Title      0   \n",
       "89   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      0   \n",
       "90   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      1   \n",
       "91   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      2   \n",
       "92   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      3   \n",
       "93   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      4   \n",
       "94   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      5   \n",
       "95   https://www.semanticscholar.org/paper/Revealin...  15     Title      0   \n",
       "96   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      0   \n",
       "97   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      1   \n",
       "98   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      2   \n",
       "99   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      3   \n",
       "100  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      4   \n",
       "101  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      5   \n",
       "102  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      6   \n",
       "103  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      7   \n",
       "104  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      8   \n",
       "105  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title      0   \n",
       "106  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      0   \n",
       "107  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      1   \n",
       "108  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      2   \n",
       "109  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      3   \n",
       "110  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      4   \n",
       "111  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      5   \n",
       "112  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      6   \n",
       "113  https://www.semanticscholar.org/paper/Multi-pa...  17     Title      0   \n",
       "114  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      0   \n",
       "115  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      1   \n",
       "116  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      2   \n",
       "117  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      3   \n",
       "118  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      4   \n",
       "119  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      5   \n",
       "120  https://www.semanticscholar.org/paper/BERT-for...  18     Title      0   \n",
       "121  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      0   \n",
       "122  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      1   \n",
       "123  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      2   \n",
       "124  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      3   \n",
       "125  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      4   \n",
       "126  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      5   \n",
       "127  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      6   \n",
       "128  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      7   \n",
       "129  https://www.semanticscholar.org/paper/BERT-wit...  19     Title      0   \n",
       "130  https://www.semanticscholar.org/paper/Understa...  20  Abstract      0   \n",
       "131  https://www.semanticscholar.org/paper/Understa...  20  Abstract      1   \n",
       "132  https://www.semanticscholar.org/paper/Understa...  20  Abstract      2   \n",
       "133  https://www.semanticscholar.org/paper/Understa...  20  Abstract      3   \n",
       "134  https://www.semanticscholar.org/paper/Understa...  20  Abstract      4   \n",
       "135  https://www.semanticscholar.org/paper/Understa...  20     Title      0   \n",
       "136  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      0   \n",
       "137  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      1   \n",
       "138  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      2   \n",
       "139  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      3   \n",
       "140  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      4   \n",
       "141  https://www.semanticscholar.org/paper/Simple-B...  21     Title      0   \n",
       "142  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      0   \n",
       "143  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      1   \n",
       "144  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      2   \n",
       "145  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      3   \n",
       "146  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      4   \n",
       "147  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      5   \n",
       "148  https://www.semanticscholar.org/paper/BERT-and...  22     Title      0   \n",
       "149  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      0   \n",
       "150  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      1   \n",
       "151  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      2   \n",
       "152  https://www.semanticscholar.org/paper/What-BER...  23     Title      0   \n",
       "153  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      0   \n",
       "154  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      1   \n",
       "155  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      2   \n",
       "156  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      3   \n",
       "157  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      4   \n",
       "158  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      5   \n",
       "159  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      6   \n",
       "160  https://www.semanticscholar.org/paper/Visualiz...  24     Title      0   \n",
       "161  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      0   \n",
       "162  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      1   \n",
       "163  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      2   \n",
       "164  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      3   \n",
       "165  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      4   \n",
       "166  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      5   \n",
       "167  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      6   \n",
       "168  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      7   \n",
       "169  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      8   \n",
       "170  https://www.semanticscholar.org/paper/Conditio...  25     Title      0   \n",
       "171  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      0   \n",
       "172  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      1   \n",
       "173  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      2   \n",
       "174  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      3   \n",
       "175  https://www.semanticscholar.org/paper/Small-an...  26     Title      0   \n",
       "176  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      0   \n",
       "177  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      1   \n",
       "178  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      2   \n",
       "179  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      3   \n",
       "180  https://www.semanticscholar.org/paper/Data-Aug...  27     Title      0   \n",
       "181  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      0   \n",
       "182  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      1   \n",
       "183  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      2   \n",
       "184  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      3   \n",
       "185  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      4   \n",
       "186  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      5   \n",
       "187  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      6   \n",
       "188  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      7   \n",
       "189  https://www.semanticscholar.org/paper/BERT-is-...  28     Title      0   \n",
       "190  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      0   \n",
       "191  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      1   \n",
       "192  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      2   \n",
       "193  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      3   \n",
       "194  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      4   \n",
       "195  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      5   \n",
       "196  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      6   \n",
       "197  https://www.semanticscholar.org/paper/How-Cont...  29     Title      0   \n",
       "198  https://www.semanticscholar.org/paper/How-Cont...  29     Title      1   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    We introduce a new language representation mod...   \n",
       "1    Unlike recent language representation models, ...   \n",
       "2    As a result, the pre-trained BERT model can be...   \n",
       "3    It obtains new state-of-the-art results on ele...   \n",
       "4    BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7    We present a replication study of BERT pretrai...   \n",
       "8    We find that BERT was significantly undertrain...   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12   RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18   DistilBERT, a distilled version of BERT: small...   \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20   We focus on one such model, BERT, and aim to q...   \n",
       "21   We find that the model represents the steps of...   \n",
       "22   Qualitative analysis reveals that the model ca...   \n",
       "23         BERT Rediscovers the Classical NLP Pipeline   \n",
       "24   Large pre-trained neural networks such as BERT...   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works, we propose metho...   \n",
       "27   BERT's attention heads exhibit patterns such a...   \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly, we propose an attention-based probing ...   \n",
       "31                             What Does BERT Look At?   \n",
       "32                     An Analysis of BERT's Attention   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34   In this paper, we describe a simple re-impleme...   \n",
       "35   Our system is the state of the art on the TREC...   \n",
       "36   The code to reproduce our results is available...   \n",
       "37                        Passage Re-ranking with BERT   \n",
       "38   I assess the extent to which the recently intr...   \n",
       "39   The BERT model performs remarkably well on all...   \n",
       "40                Assessing BERT's Syntactic Abilities   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42   A new release of BERT (Devlin, 2018) includes ...   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44   We compare mBERT with the best-published metho...   \n",
       "45   Additionally, we investigate the most effectiv...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47   Language model pre-training, such as BERT, has...   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method, the plenty o...   \n",
       "51   Moreover, we introduce a new two-stage learnin...   \n",
       "52   This framework ensures that TinyBERT can captu...   \n",
       "53   TinyBERT is empirically effective and achieves...   \n",
       "54   TinyBERT is also significantly better than sta...   \n",
       "55   TinyBERT: Distilling BERT for Natural Language...   \n",
       "56   BERT, a pre-trained Transformer model, has ach...   \n",
       "57   In this paper, we describe BERTSUM, a simple v...   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60         Fine-tune BERT for Extractive Summarization   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69   BERT Post-Training for Review Reading Comprehe...   \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state-of-the-art language model pre-train...   \n",
       "72   In this paper, we conduct exhaustive experimen...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74      How to Fine-Tune BERT for Text Classification?   \n",
       "75   We show that BERT (Devlin et al., 2018) is a M...   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77   We generate from BERT and find that it can pro...   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79   BERT has a Mouth, and It Must Speak: BERT as a...   \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82   We fine-tune the pre-trained model from BERT a...   \n",
       "83   Utilizing BERT for Aspect-Based Sentiment Anal...   \n",
       "84   Following recent successes in applying BERT to...   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88   Simple Applications of BERT for Ad Hoc Documen...   \n",
       "89   BERT-based architectures currently give state-...   \n",
       "90   In the current work, we focus on the interpret...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                  Revealing the Dark Secrets of BERT   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97   Recently, an upgraded version of BERT has been...   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99   The model was trained on the latest Chinese Wi...   \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101  The model is verified on various NLP tasks, ac...   \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover, we also examine the effectiveness of...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105  Pre-Training with Whole Word Masking for Chine...   \n",
       "106  BERT model has been successfully applied to op...   \n",
       "107  However, previous work trains BERT by viewing ...   \n",
       "108  To tackle this issue, we propose a multi-passa...   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular, on the OpenSQuAD dataset, our m...   \n",
       "113  Multi-passage BERT: A Globally Normalized BERT...   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116  Recently a new language representation model, ...   \n",
       "117  However, there has not been much effort on exp...   \n",
       "118  In this work, we propose a joint intent classi...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120  BERT for Joint Intent Classification and Slot ...   \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129  BERT with History Answer Embedding for Convers...   \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134  Analyses illustrate how BERT allocates its att...   \n",
       "135     Understanding the Behaviors of BERT in Ranking   \n",
       "136  We present simple BERT-based models for relati...   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140  Our models provide strong baselines for future...   \n",
       "141  Simple BERT Models for Relation Extraction and...   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147  By using PALs in parallel with BERT layers, we...   \n",
       "148  BERT and PALs: Projected Attention Layers for ...   \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151  As a case study, we apply these diagnostics to...   \n",
       "152  What BERT Is Not: Lessons from a New Suite of ...   \n",
       "153  Language model pre-training, such as BERT, has...   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper, we propose to visualize loss la...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine-tuning proce...   \n",
       "158  Second, the visualization results indicate tha...   \n",
       "159  Third, the lower layers of BERT are more invar...   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165  We retrofit BERT to conditional BERT by introd...   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168  The well trained conditional BERT can be appli...   \n",
       "169  Experiments on six various different text clas...   \n",
       "170           Conditional BERT Contextual Augmentation   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172  Starting from a public multilingual BERT check...   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175  Small and Practical BERT Models for Sequence L...   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178  We apply a stage-wise approach to fine tuning ...   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180  Data Augmentation for BERT Fine-Tuning in Open...   \n",
       "181  The BERT language model (LM) (Devlin et al., 2...   \n",
       "182                                     Petroni et al.   \n",
       "183  (2019) take this as evidence that BERT memoriz...   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185  More specifically, we show that BERT's precisi...   \n",
       "186  As a remedy, we propose E-BERT, an extension o...   \n",
       "187  E-BERT outperforms both BERT and ERNIE (Zhang ...   \n",
       "188  We take this as evidence that E-BERT is richer...   \n",
       "189  BERT is Not a Knowledge Base (Yet): Factual Kn...   \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However, just how contextual are the contextua...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196  In all layers of ELMo, BERT, and GPT-2, on ave...   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198  Comparing the Geometry of BERT, ELMo, and GPT-...   \n",
       "\n",
       "                                               split_0  \\\n",
       "0                                         We introduce   \n",
       "1       Unlike recent language representation models ,   \n",
       "2                      As a result , the pre - trained   \n",
       "3                                                        \n",
       "4                                                        \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7                    We present a replication study of   \n",
       "8                                         We find that   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12                                                       \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18                                                       \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20                                         We focus on   \n",
       "21                                        We find that   \n",
       "22                   Qualitative analysis reveals that   \n",
       "23                                                       \n",
       "24         Large pre - trained neural networks such as   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works , we propose meth...   \n",
       "27                                                       \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly , we propose an attention - based probi...   \n",
       "31                                           What Does   \n",
       "32                                      An Analysis of   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34                         In this paper , we describe   \n",
       "35                                                       \n",
       "36   The code to reproduce our results is available...   \n",
       "37                             Passage Re-ranking with   \n",
       "38                        I assess the extent to which   \n",
       "39                                                       \n",
       "40                                           Assessing   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42                                    A new release of   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44                                          We compare   \n",
       "45   Additionally , we investigate the most effecti...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47                Language model pre-training, such as   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method , the plenty ...   \n",
       "51   Moreover , we introduce a new two - stage lear...   \n",
       "52                         This framework ensures that   \n",
       "53                                                       \n",
       "54                                                       \n",
       "55                                                       \n",
       "56                                                       \n",
       "57                          In this paper, we describe   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60                                           Fine-tune   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69                                                       \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state - of - the - art language model pre...   \n",
       "72   In this paper , we conduct exhaustive experime...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74                                    How to Fine-Tune   \n",
       "75                                        We show that   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77                                    We generate from   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79                                                       \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82             We fine-tune the pre-trained model from   \n",
       "83                                           Utilizing   \n",
       "84              Following recent successes in applying   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88                              Simple Applications of   \n",
       "89                                                       \n",
       "90   In the current work , we focus on the interpre...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                       Revealing the Dark Secrets of   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97                   Recently , an upgraded version of   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99                                                       \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101                                                      \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover , we also examine the effectiveness o...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105   Pre-Training with Whole Word Masking for Chinese   \n",
       "106                                                      \n",
       "107                     However , previous work trains   \n",
       "108   To tackle this issue, we propose a multi-passage   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular , on the OpenSQuAD dataset , our...   \n",
       "113                                      Multi-passage   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116      Recently a new language representation model,   \n",
       "117  However , there has not been much effort on ex...   \n",
       "118  In this work , we propose a joint intent class...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120                                                      \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129                                                      \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134                            Analyses illustrate how   \n",
       "135                     Understanding the Behaviors of   \n",
       "136                                         We present   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140                                                      \n",
       "141                                             Simple   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147                     By using PALs in parallel with   \n",
       "148                                                      \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151    As a case study , we apply these diagnostics to   \n",
       "152                                               What   \n",
       "153            Language model pre - training , such as   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper , we propose to visualize loss l...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine - tuning pro...   \n",
       "158  Second , the visualization results indicate th...   \n",
       "159                        Third , the lower layers of   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163                                                      \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165                                        We retrofit   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168                       The well trained conditional   \n",
       "169  Experiments on six various different text clas...   \n",
       "170                                        Conditional   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172                Starting from a public multilingual   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175                                Small and Practical   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178      We apply a stage-wise approach to fine tuning   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180                              Data Augmentation for   \n",
       "181                                                      \n",
       "182                                     Petroni et al.   \n",
       "183                ( 2019 ) take this as evidence that   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185                   More specifically , we show that   \n",
       "186  As a remedy , we propose E - BERT , an extensi...   \n",
       "187                          E - BERT outperforms both   \n",
       "188  We take this as evidence that E - BERT is rich...   \n",
       "189                                                      \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However , just how contextual are the contextu...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196                            In all layers of ELMo ,   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198                          Comparing the Geometry of   \n",
       "\n",
       "                                               split_1  \\\n",
       "0    a new language representation model called BER...   \n",
       "1                                                 BERT   \n",
       "2                                                 BERT   \n",
       "3                                                   It   \n",
       "4                                                BERT:   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7                                                 BERT   \n",
       "8                                                 BERT   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12                                            RoBERTa:   \n",
       "13                                                       \n",
       "14                                         DistilBERT,   \n",
       "15                                        a BERT model   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18                                         DistilBERT,   \n",
       "19                                                       \n",
       "20                               one such model , BERT   \n",
       "21                                           the model   \n",
       "22                                           the model   \n",
       "23                                                BERT   \n",
       "24                                                BERT   \n",
       "25                                                       \n",
       "26                                                BERT   \n",
       "27                                             BERT 's   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                             BERT 's   \n",
       "31                                                BERT   \n",
       "32                                             BERT 's   \n",
       "33                                                BERT   \n",
       "34   a simple re - implementation of BERT for query...   \n",
       "35                                          Our system   \n",
       "36                                                       \n",
       "37                                                BERT   \n",
       "38                  the recently introduced BERT model   \n",
       "39                                      The BERT model   \n",
       "40                                              BERT's   \n",
       "41                                                       \n",
       "42                                                BERT   \n",
       "43                                               mBERT   \n",
       "44                                               mBERT   \n",
       "45                                               mBERT   \n",
       "46                                                BERT   \n",
       "47                                               BERT,   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                            a small student TinyBERT   \n",
       "51                                            TinyBERT   \n",
       "52                                            TinyBERT   \n",
       "53                                            TinyBERT   \n",
       "54                                            TinyBERT   \n",
       "55                                           TinyBERT:   \n",
       "56                                               BERT,   \n",
       "57                                            BERTSUM,   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                                                BERT   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65                     the popular language model BERT   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69                                                BERT   \n",
       "70                                                       \n",
       "71   BERT ( Bidirectional Encoder Representations f...   \n",
       "72                                                BERT   \n",
       "73                                                       \n",
       "74                                                BERT   \n",
       "75                                                BERT   \n",
       "76                                                BERT   \n",
       "77                                                BERT   \n",
       "78                                                BERT   \n",
       "79                                                BERT   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82                                                BERT   \n",
       "83                                                BERT   \n",
       "84                                                BERT   \n",
       "85                                                BERT   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                                                BERT   \n",
       "89                                          BERT-based   \n",
       "90                                                BERT   \n",
       "91                                              BERT's   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                BERT   \n",
       "95                                                BERT   \n",
       "96                                              (BERT)   \n",
       "97                                                BERT   \n",
       "98                                                       \n",
       "99                                           The model   \n",
       "100                                       Chinese BERT   \n",
       "101                                          The model   \n",
       "102                                                      \n",
       "103                                               BERT   \n",
       "104                                                      \n",
       "105                                               BERT   \n",
       "106                                               BERT   \n",
       "107                                               BERT   \n",
       "108                                               BERT   \n",
       "109                                                      \n",
       "110                                               BERT   \n",
       "111                                               BERT   \n",
       "112                                               BERT   \n",
       "113                                              BERT:   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116                                               BERT   \n",
       "117                                               BERT   \n",
       "118                                               BERT   \n",
       "119                                                      \n",
       "120                                               BERT   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125                                               BERT   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129                                               BERT   \n",
       "130                                               BERT   \n",
       "131                                               BERT   \n",
       "132                                               BERT   \n",
       "133                                               BERT   \n",
       "134                                               BERT   \n",
       "135                                               BERT   \n",
       "136  simple BERT - based models for relation extrac...   \n",
       "137                                                      \n",
       "138                                         BERT-based   \n",
       "139                                               BERT   \n",
       "140                                         Our models   \n",
       "141                                               BERT   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145                                               BERT   \n",
       "146                                                      \n",
       "147                                               BERT   \n",
       "148                                               BERT   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151                             the popular BERT model   \n",
       "152                                               BERT   \n",
       "153                                               BERT   \n",
       "154                                                      \n",
       "155                                               BERT   \n",
       "156                                                      \n",
       "157                                               BERT   \n",
       "158                                               BERT   \n",
       "159                                               BERT   \n",
       "160                                               BERT   \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164                                               BERT   \n",
       "165                                               BERT   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168                                               BERT   \n",
       "169                                                      \n",
       "170                                               BERT   \n",
       "171                                                      \n",
       "172                                               BERT   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                                               BERT   \n",
       "176                                               BERT   \n",
       "177                                                      \n",
       "178                                               BERT   \n",
       "179                                                      \n",
       "180                                               BERT   \n",
       "181                     The BERT language model ( LM )   \n",
       "182                                                      \n",
       "183                                               BERT   \n",
       "184                                               BERT   \n",
       "185                                            BERT 's   \n",
       "186                                               BERT   \n",
       "187                                               BERT   \n",
       "188                                               BERT   \n",
       "189                                               BERT   \n",
       "190                                                      \n",
       "191                                               BERT   \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196                                               BERT   \n",
       "197                                                      \n",
       "198                                              BERT,   \n",
       "\n",
       "                                               split_2  \\\n",
       "0                                                        \n",
       "1    is designed to pre - train deep bidirectional ...   \n",
       "2    model can be fine - tuned with just one additi...   \n",
       "3    obtains new state - of - the - art results on ...   \n",
       "4    Pre-training of Deep Bidirectional Transformer...   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7    pretraining ( Devlin et al . , 2019 ) that car...   \n",
       "8    was significantly undertrained , and can match...   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12      A Robustly Optimized BERT Pretraining Approach   \n",
       "13                                                       \n",
       "14   which can then be fine-tuned with good perform...   \n",
       "15   by 40 % , while retaining 97 % of its language...   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18   a distilled version of BERT: smaller, faster, ...   \n",
       "19                                                       \n",
       "20   , and aim to quantify where linguistic informa...   \n",
       "21   represents the steps of the traditional NLP pi...   \n",
       "22   can and often does adjust this pipeline dynami...   \n",
       "23              Rediscovers the Classical NLP Pipeline   \n",
       "24   have had great recent success in NLP , motivat...   \n",
       "25                                                       \n",
       "26                                                       \n",
       "27   attention heads exhibit patterns such as atten...   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                           attention   \n",
       "31                                             Look At   \n",
       "32                                                       \n",
       "33   (Devlin et al., 2018), have achieved impressiv...   \n",
       "34                                                       \n",
       "35   is the state of the art on the TREC - CAR data...   \n",
       "36                                                       \n",
       "37                                                       \n",
       "38   captures English syntactic phenomena , using (...   \n",
       "39               performs remarkably well on all cases   \n",
       "40                                 Syntactic Abilities   \n",
       "41                                                       \n",
       "42   (Devlin, 2018) includes a model simultaneously...   \n",
       "43   (multilingual) as a zero shot language transfe...   \n",
       "44   with the best - published methods for zero - s...   \n",
       "45   in this manner , determine to what extent mBER...   \n",
       "46                                                       \n",
       "47   has significantly improved the performances of...   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                                                       \n",
       "51   , which performs transformer distillation at b...   \n",
       "52   can capture both the general - domain and task...   \n",
       "53   is empirically effective and achieves comparab...   \n",
       "54   is also significantly better than state - of -...   \n",
       "55   Distilling BERT for Natural Language Understan...   \n",
       "56   a pre-trained Transformer model, has achieved ...   \n",
       "57   a simple variant of BERT, for extractive summa...   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                        for Extractive Summarization   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65   to enhance the performance of fine - tuning of...   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69   Post-Training for Review Reading Comprehension...   \n",
       "70                                                       \n",
       "71   has achieved amazing results in many language ...   \n",
       "72   on text classification task and provide a gene...   \n",
       "73                                                       \n",
       "74                            for Text Classification?   \n",
       "75   ( Devlin et al . , 2018 ) is a Markov random f...   \n",
       "76                                                       \n",
       "77   and find that it can produce high - quality , ...   \n",
       "78   generates sentences that are more diverse but ...   \n",
       "79   has a Mouth , and It Must Speak : BERT as a Ma...   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82   and achieve new state-of-the-art results on Se...   \n",
       "83   for Aspect-Based Sentiment Analysis via Constr...   \n",
       "84   to question answering , we explore simple appl...   \n",
       "85                              was designed to handle   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                       for Ad Hoc Document Retrieval   \n",
       "89   architectures currently give state-of-the-art ...   \n",
       "90                                                       \n",
       "91                                              heads.   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                       \n",
       "95                                                       \n",
       "96   has shown marvelous improvements across variou...   \n",
       "97   has been released with Whole Word Masking ( WW...   \n",
       "98                                                       \n",
       "99    was trained on the latest Chinese Wikipedia dump   \n",
       "100  without changing any neural architecture or ev...   \n",
       "101  is verified on various NLP tasks , across sent...   \n",
       "102                                                      \n",
       "103                               , ERNIE , BERT - wwm   \n",
       "104                                                      \n",
       "105                                                      \n",
       "106  model has been successfully applied to open - ...   \n",
       "107  by viewing passages corresponding to the same ...   \n",
       "108  model to globally normalize answer scores acro...   \n",
       "109                                                      \n",
       "110                               gains additional 2%.   \n",
       "111  outperforms all state-of-the-art models on all...   \n",
       "112  models , and 5.8 % EM and 6.5 % $ F_1 $ over B...   \n",
       "113  A Globally Normalized BERT Model for Open-doma...   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116  (Bidirectional Encoder Representations from Tr...   \n",
       "117                 for natural language understanding   \n",
       "118                                                      \n",
       "119                                                      \n",
       "120   for Joint Intent Classification and Slot Filling   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125  (Bidirectional Encoder Representations from Tr...   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129  with History Answer Embedding for Conversation...   \n",
       "130                                   in ranking tasks   \n",
       "131  and fine-tune it on two ranking tasks: MS MARC...   \n",
       "132  in question - answering focused passage rankin...   \n",
       "133  pre-trained on surrounding contexts and the ne...   \n",
       "134  allocates its attentions between query - docum...   \n",
       "135                                         in Ranking   \n",
       "136                                                      \n",
       "137                                                      \n",
       "138    model can achieve state-of-the-art performance.   \n",
       "139                                    in this manner.   \n",
       "140                provide strong baselines for future   \n",
       "141  Models for Relation Extraction and Semantic Ro...   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145  model on the GLUE benchmark, and how to best a...   \n",
       "146                                                      \n",
       "147  layers, we match the performance of fine-tuned...   \n",
       "148  and PALs: Projected Attention Layers for Effic...   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151  , finding that it can generally distinguish go...   \n",
       "152  Is Not: Lessons from a New Suite of Psycholing...   \n",
       "153  , has achieved remarkable results in many NLP ...   \n",
       "154                                                      \n",
       "155                               on specific datasets   \n",
       "156                                                      \n",
       "157  is highly over - parameterized for downstream ...   \n",
       "158  tends to generalize better because of the flat...   \n",
       "159  are more invariant during fine - tuning , whic...   \n",
       "160                                                      \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  demonstrates that a deep bidirectional languag...   \n",
       "164                           contextual augmentation.   \n",
       "165  to conditional BERT by introducing a new condi...   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168  can be applied to enhance contextual augmentat...   \n",
       "169                                                      \n",
       "170                            Contextual Augmentation   \n",
       "171                                                      \n",
       "172  checkpoint, our final model is 6x smaller and ...   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                       Models for Sequence Labeling   \n",
       "176  reader was found to be very effective for ques...   \n",
       "177                                                      \n",
       "178  on multiple datasets, starting with data that ...   \n",
       "179                                                      \n",
       "180      Fine-Tuning in Open-Domain Question Answering   \n",
       "181  ( Devlin et al . , 2019 ) is surprisingly good...   \n",
       "182                                                      \n",
       "183  memorizes factual knowledge during pre - training   \n",
       "184  is partly due to reasoning about ( the surface...   \n",
       "185  precision drops dramatically when we filter ce...   \n",
       "186  that replaces entity mentions with symbolic en...   \n",
       "187  and ERNIE ( Zhang et al . , 2019 ) on hard - t...   \n",
       "188                                            and E -   \n",
       "189  is Not a Knowledge Base (Yet): Factual Knowled...   \n",
       "190                                                      \n",
       "191                                                      \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196  , and GPT-2 , on average , less than 5 % of th...   \n",
       "197                                                      \n",
       "198                         ELMo, and GPT-2 Embeddings   \n",
       "\n",
       "                                          split_tokens split_anchor_span  \n",
       "0    [We, introduce, a, new, language, representati...           (2, 18)  \n",
       "1    [Unlike, recent, language, representation, mod...            (6, 7)  \n",
       "2    [As, a, result, ,, the, pre, -, trained, BERT,...            (8, 9)  \n",
       "3    [It, obtains, new, state, -, of, -, the, -, ar...            (0, 1)  \n",
       "4    [BERT:, Pre-training, of, Deep, Bidirectional,...            (0, 1)  \n",
       "5    [Language, model, pretraining, has, led, to, s...              None  \n",
       "6    [Training, is, computationally, expensive,, of...              None  \n",
       "7    [We, present, a, replication, study, of, BERT,...            (6, 7)  \n",
       "8    [We, find, that, BERT, was, significantly, und...            (3, 4)  \n",
       "9    [Our, best, model, achieves, state-of-the-art,...              None  \n",
       "10   [These, results, highlight, the, importance, o...              None  \n",
       "11              [We, release, our, models, and, code.]              None  \n",
       "12   [RoBERTa:, A, Robustly, Optimized, BERT, Pretr...            (0, 1)  \n",
       "13   [As, Transfer, Learning, from, large-scale, pr...              None  \n",
       "14   [In, this, work,, we, propose, a, method, to, ...          (16, 17)  \n",
       "15   [While, most, prior, work, investigated, the, ...          (37, 40)  \n",
       "16   [To, leverage, the, inductive, biases, learned...              None  \n",
       "17   [Our, smaller,, faster, and, lighter, model, i...              None  \n",
       "18   [DistilBERT,, a, distilled, version, of, BERT:...            (0, 1)  \n",
       "19   [Pre-trained, text, encoders, have, rapidly, a...              None  \n",
       "20   [We, focus, on, one, such, model, ,, BERT, ,, ...            (3, 8)  \n",
       "21   [We, find, that, the, model, represents, the, ...            (3, 5)  \n",
       "22   [Qualitative, analysis, reveals, that, the, mo...            (4, 6)  \n",
       "23   [BERT, Rediscovers, the, Classical, NLP, Pipel...            (0, 1)  \n",
       "24   [Large, pre, -, trained, neural, networks, suc...            (8, 9)  \n",
       "25   [Most, recent, analysis, has, focused, on, mod...              None  \n",
       "26   [Complementary, to, these, works, ,, we, propo...          (22, 23)  \n",
       "27   [BERT, 's, attention, heads, exhibit, patterns...            (0, 2)  \n",
       "28   [We, further, show, that, certain, attention, ...              None  \n",
       "29   [For, example,, we, find, heads, that, attend,...              None  \n",
       "30   [Lastly, ,, we, propose, an, attention, -, bas...          (23, 25)  \n",
       "31                        [What, Does, BERT, Look, At]            (2, 3)  \n",
       "32                        [An, Analysis, of, BERT, 's]            (3, 5)  \n",
       "33   [Recently,, neural, models, pretrained, on, a,...          (23, 24)  \n",
       "34   [In, this, paper, ,, we, describe, a, simple, ...           (6, 21)  \n",
       "35   [Our, system, is, the, state, of, the, art, on...            (0, 2)  \n",
       "36   [The, code, to, reproduce, our, results, is, a...              None  \n",
       "37                   [Passage, Re-ranking, with, BERT]            (3, 4)  \n",
       "38   [I, assess, the, extent, to, which, the, recen...           (6, 11)  \n",
       "39   [The, BERT, model, performs, remarkably, well,...            (0, 3)  \n",
       "40           [Assessing, BERT's, Syntactic, Abilities]            (1, 2)  \n",
       "41   [Pretrained, contextual, representation, model...              None  \n",
       "42   [A, new, release, of, BERT, (Devlin,, 2018), i...            (4, 5)  \n",
       "43   [This, paper, explores, the, broader, cross-li...            (8, 9)  \n",
       "44   [We, compare, mBERT, with, the, best, -, publi...            (2, 3)  \n",
       "45   [Additionally, ,, we, investigate, the, most, ...          (10, 11)  \n",
       "46   [Beto,, Bentz,, Becas:, The, Surprising, Cross...            (8, 9)  \n",
       "47   [Language, model, pre-training,, such, as, BER...            (5, 6)  \n",
       "48   [However,, pre-trained, language, models, are,...              None  \n",
       "49   [To, accelerate, inference, and, reduce, model...              None  \n",
       "50   [By, leveraging, this, new, KD, method, ,, the...          (22, 26)  \n",
       "51   [Moreover, ,, we, introduce, a, new, two, -, s...          (12, 13)  \n",
       "52   [This, framework, ensures, that, TinyBERT, can...            (4, 5)  \n",
       "53   [TinyBERT, is, empirically, effective, and, ac...            (0, 1)  \n",
       "54   [TinyBERT, is, also, significantly, better, th...            (0, 1)  \n",
       "55   [TinyBERT:, Distilling, BERT, for, Natural, La...            (0, 1)  \n",
       "56   [BERT,, a, pre-trained, Transformer, model,, h...            (0, 1)  \n",
       "57   [In, this, paper,, we, describe, BERTSUM,, a, ...            (5, 6)  \n",
       "58   [Our, system, is, the, state, of, the, art, on...              None  \n",
       "59   [The, codes, to, reproduce, our, results, are,...              None  \n",
       "60   [Fine-tune, BERT, for, Extractive, Summarization]            (1, 2)  \n",
       "61   [Question-answering, plays, an, important, rol...              None  \n",
       "62   [Inspired, by, the, recent, success, of, machi...              None  \n",
       "63   [To, the, best, of, our, knowledge,, no, exist...              None  \n",
       "64   [In, this, work,, we, first, build, an, RRC, d...              None  \n",
       "65   [Since, ReviewRC, has, limited, training, exam...          (29, 34)  \n",
       "66   [To, show, the, generality, of, the, approach,...              None  \n",
       "67   [Experimental, results, demonstrate, that, the...              None  \n",
       "68   [The, datasets, and, code, are, available, at,...              None  \n",
       "69   [BERT, Post-Training, for, Review, Reading, Co...            (0, 1)  \n",
       "70   [Language, model, pre-training, has, proven, t...              None  \n",
       "71   [As, a, state, -, of, -, the, -, art, language...          (16, 24)  \n",
       "72   [In, this, paper, ,, we, conduct, exhaustive, ...          (16, 17)  \n",
       "73   [Finally,, the, proposed, solution, obtains, n...              None  \n",
       "74   [How, to, Fine-Tune, BERT, for, Text, Classifi...            (3, 4)  \n",
       "75   [We, show, that, BERT, (, Devlin, et, al, ., ,...            (3, 4)  \n",
       "76   [This, formulation, gives, way, to, a, natural...          (12, 13)  \n",
       "77   [We, generate, from, BERT, and, find, that, it...            (3, 4)  \n",
       "78   [Compared, to, the, generations, of, a, tradit...          (15, 16)  \n",
       "79   [BERT, has, a, Mouth, ,, and, It, Must, Speak,...            (0, 1)  \n",
       "80   [Aspect-based, sentiment, analysis, (ABSA),, w...              None  \n",
       "81   [In, this, paper,, we, construct, an, auxiliar...              None  \n",
       "82   [We, fine-tune, the, pre-trained, model, from,...            (6, 7)  \n",
       "83   [Utilizing, BERT, for, Aspect-Based, Sentiment...            (1, 2)  \n",
       "84   [Following, recent, successes, in, applying, B...            (5, 6)  \n",
       "85   [This, required, confronting, the, challenge, ...          (17, 18)  \n",
       "86   [We, address, this, issue, by, applying, infer...              None  \n",
       "87   [Experiments, on, TREC, microblog, and, newswi...              None  \n",
       "88   [Simple, Applications, of, BERT, for, Ad, Hoc,...            (3, 4)  \n",
       "89   [BERT-based, architectures, currently, give, s...            (0, 1)  \n",
       "90   [In, the, current, work, ,, we, focus, on, the...          (24, 25)  \n",
       "91   [Using, a, subset, of, GLUE, tasks, and, a, se...          (31, 32)  \n",
       "92   [Our, findings, suggest, that, there, is, a, l...              None  \n",
       "93   [While, different, heads, consistently, use, t...              None  \n",
       "94   [We, show, that, manually, disabling, attentio...          (20, 21)  \n",
       "95           [Revealing, the, Dark, Secrets, of, BERT]            (5, 6)  \n",
       "96   [Bidirectional, Encoder, Representations, from...            (5, 6)  \n",
       "97   [Recently, ,, an, upgraded, version, of, BERT,...            (6, 7)  \n",
       "98   [In, this, technical, report,, we, adapt, whol...              None  \n",
       "99   [The, model, was, trained, on, the, latest, Ch...            (0, 2)  \n",
       "100  [We, aim, to, provide, easy, extensibility, an...          (10, 12)  \n",
       "101  [The, model, is, verified, on, various, NLP, t...            (0, 2)  \n",
       "102  [Experimental, results, on, these, datasets, s...              None  \n",
       "103  [Moreover, ,, we, also, examine, the, effectiv...          (14, 15)  \n",
       "104  [We, release, the, pre-trained, model, (both, ...              None  \n",
       "105  [Pre-Training, with, Whole, Word, Masking, for...            (7, 8)  \n",
       "106  [BERT, model, has, been, successfully, applied...            (0, 1)  \n",
       "107  [However, ,, previous, work, trains, BERT, by,...            (5, 6)  \n",
       "108  [To, tackle, this, issue,, we, propose, a, mul...            (8, 9)  \n",
       "109  [In, addition,, we, find, that, splitting, art...              None  \n",
       "110  [By, leveraging, a, passage, ranker, to, selec...          (10, 11)  \n",
       "111  [Experiments, on, four, standard, benchmarks, ...           (9, 10)  \n",
       "112  [In, particular, ,, on, the, OpenSQuAD, datase...          (24, 25)  \n",
       "113  [Multi-passage, BERT:, A, Globally, Normalized...            (1, 2)  \n",
       "114  [Intent, classification, and, slot, filling, a...              None  \n",
       "115  [They, often, suffer, from, small-scale, human...              None  \n",
       "116  [Recently, a, new, language, representation, m...            (6, 7)  \n",
       "117  [However, ,, there, has, not, been, much, effo...          (10, 11)  \n",
       "118  [In, this, work, ,, we, propose, a, joint, int...          (16, 17)  \n",
       "119  [Experimental, results, demonstrate, that, our...              None  \n",
       "120  [BERT, for, Joint, Intent, Classification, and...            (0, 1)  \n",
       "121  [Conversational, search, is, an, emerging, top...              None  \n",
       "122  [One, of, the, major, challenges, to, multi-tu...              None  \n",
       "123  [Existing, methods, either, prepend, history, ...              None  \n",
       "124  [We, propose, a, conceptually, simple, yet, hi...              None  \n",
       "125  [It, enables, seamless, integration, of, conve...          (16, 17)  \n",
       "126  [We, first, explain, our, view, that, ConvQA, ...              None  \n",
       "127  [We, further, demonstrate, the, effectiveness,...              None  \n",
       "128  [Finally,, we, analyze, the, impact, of, diffe...              None  \n",
       "129  [BERT, with, History, Answer, Embedding, for, ...            (0, 1)  \n",
       "130  [This, paper, studies, the, performances, and,...            (8, 9)  \n",
       "131  [We, explore, several, different, ways, to, le...           (9, 10)  \n",
       "132  [Experimental, results, on, MS, MARCO, demonst...          (10, 11)  \n",
       "133  [Experimental, results, on, TREC, show, the, g...           (9, 10)  \n",
       "134  [Analyses, illustrate, how, BERT, allocates, i...            (3, 4)  \n",
       "135  [Understanding, the, Behaviors, of, BERT, in, ...            (4, 5)  \n",
       "136  [We, present, simple, BERT, -, based, models, ...           (2, 14)  \n",
       "137  [In, recent, years,, state-of-the-art, perform...              None  \n",
       "138  [In, this, paper,, extensive, experiments, on,...          (20, 21)  \n",
       "139  [To, our, knowledge,, we, are, the, first, to,...          (10, 11)  \n",
       "140  [Our, models, provide, strong, baselines, for,...            (0, 2)  \n",
       "141  [Simple, BERT, Models, for, Relation, Extracti...            (1, 2)  \n",
       "142  [Multi-task, learning, allows, the, sharing, o...              None  \n",
       "143  [In, natural, language, processing, several, r...              None  \n",
       "144  [These, results, are, based, on, fine-tuning, ...              None  \n",
       "145  [We, explore, the, multi-task, learning, setti...           (9, 10)  \n",
       "146  [We, introduce, new, adaptation, modules,, PAL...              None  \n",
       "147  [By, using, PALs, in, parallel, with, BERT, la...            (6, 7)  \n",
       "148  [BERT, and, PALs:, Projected, Attention, Layer...            (0, 1)  \n",
       "149  [Pre-training, by, language, modeling, has, be...              None  \n",
       "150  [In, this, paper, we, introduce, a, suite, of,...              None  \n",
       "151  [As, a, case, study, ,, we, apply, these, diag...          (10, 14)  \n",
       "152  [What, BERT, Is, Not:, Lessons, from, a, New, ...            (1, 2)  \n",
       "153  [Language, model, pre, -, training, ,, such, a...            (8, 9)  \n",
       "154  [However,, it, is, unclear, why, the, pre-trai...              None  \n",
       "155  [In, this, paper, ,, we, propose, to, visualiz...          (17, 18)  \n",
       "156  [First,, we, find, that, pre-training, reaches...              None  \n",
       "157  [We, also, demonstrate, that, the, fine, -, tu...          (16, 17)  \n",
       "158  [Second, ,, the, visualization, results, indic...          (10, 11)  \n",
       "159  [Third, ,, the, lower, layers, of, BERT, are, ...            (6, 7)  \n",
       "160  [Visualizing, and, Understanding, the, Effecti...            (6, 7)  \n",
       "161  [Data, augmentation, methods, are, often, appl...              None  \n",
       "162  [Recently, proposed, contextual, augmentation,...              None  \n",
       "163  [Bidirectional, Encoder, Representations, from...            (0, 8)  \n",
       "164  [We, propose, a, novel, data, augmentation, me...          (12, 13)  \n",
       "165  [We, retrofit, BERT, to, conditional, BERT, by...            (2, 3)  \n",
       "166  [In, our, paper,, “conditional, masked, langua...              None  \n",
       "167                                            [task.]              None  \n",
       "168  [The, well, trained, conditional, BERT, can, b...            (4, 5)  \n",
       "169  [Experiments, on, six, various, different, tex...              None  \n",
       "170      [Conditional, BERT, Contextual, Augmentation]            (1, 2)  \n",
       "171  [We, propose, a, practical, scheme, to, train,...              None  \n",
       "172  [Starting, from, a, public, multilingual, BERT...            (5, 6)  \n",
       "173  [We, show, that, our, model, especially, outpe...              None  \n",
       "174  [We, showcase, the, effectiveness, of, our, me...              None  \n",
       "175  [Small, and, Practical, BERT, Models, for, Seq...            (3, 4)  \n",
       "176  [Recently,, a, simple, combination, of, passag...          (13, 14)  \n",
       "177  [In, this, paper,, we, present, a, data, augme...              None  \n",
       "178  [We, apply, a, stage-wise, approach, to, fine,...            (8, 9)  \n",
       "179  [Experimental, results, show, large, gains, in...              None  \n",
       "180  [Data, Augmentation, for, BERT, Fine-Tuning, i...            (3, 4)  \n",
       "181  [The, BERT, language, model, (, LM, ), (, Devl...            (0, 7)  \n",
       "182                                 [Petroni, et, al.]              None  \n",
       "183  [(, 2019, ), take, this, as, evidence, that, B...            (8, 9)  \n",
       "184  [We, take, issue, with, this, interpretation, ...          (12, 13)  \n",
       "185  [More, specifically, ,, we, show, that, BERT, ...            (6, 8)  \n",
       "186  [As, a, remedy, ,, we, propose, E, -, BERT, ,,...          (13, 14)  \n",
       "187  [E, -, BERT, outperforms, both, BERT, and, ERN...            (5, 6)  \n",
       "188  [We, take, this, as, evidence, that, E, -, BER...          (22, 23)  \n",
       "189  [BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...            (0, 1)  \n",
       "190  [Replacing, static, word, embeddings, with, co...              None  \n",
       "191  [However, ,, just, how, contextual, are, the, ...          (16, 17)  \n",
       "192  [Are, there, infinitely, many, context-specifi...              None  \n",
       "193  [For, one,, we, find, that, the, contextualize...              None  \n",
       "194  [While, representations, of, the, same, word, ...              None  \n",
       "195  [This, suggests, that, upper, layers, of, cont...              None  \n",
       "196  [In, all, layers, of, ELMo, ,, BERT, ,, and, G...            (6, 7)  \n",
       "197  [How, Contextual, are, Contextualized, Word, R...              None  \n",
       "198  [Comparing, the, Geometry, of, BERT,, ELMo,, a...            (4, 5)  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input = df_sentences.join(coreference_output)\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>split_0</th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>split_anchor_span</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>We introduce</td>\n",
       "      <td>a new language representation model called BER...</td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>(2, 18)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Unlike recent language representation models, ...</td>\n",
       "      <td>Unlike recent language representation models ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre - train deep bidirectional ...</td>\n",
       "      <td>[Unlike, recent, language, representation, mod...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a result, the pre-trained BERT model can be...</td>\n",
       "      <td>As a result , the pre - trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine - tuned with just one additi...</td>\n",
       "      <td>[As, a, result, ,, the, pre, -, trained, BERT,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td>It</td>\n",
       "      <td>obtains new state - of - the - art results on ...</td>\n",
       "      <td>[It, obtains, new, state, -, of, -, the, -, ar...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>obtains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "      <td>[BERT:, Pre-training, of, Deep, Bidirectional,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>Pre-training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Training, is, computationally, expensive,, of...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We present a replication study of BERT pretrai...</td>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining ( Devlin et al . , 2019 ) that car...</td>\n",
       "      <td>[We, present, a, replication, study, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>pretraining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained , and can match...</td>\n",
       "      <td>[We, find, that, BERT, was, significantly, und...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, best, model, achieves, state-of-the-art,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, highlight, the, importance, o...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, our, models, and, code.]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "      <td>[RoBERTa:, A, Robustly, Optimized, BERT, Pretr...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[As, Transfer, Learning, from, large-scale, pr...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "      <td>[In, this, work,, we, propose, a, method, to, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>a BERT model</td>\n",
       "      <td>by 40 % , while retaining 97 % of its language...</td>\n",
       "      <td>[While, most, prior, work, investigated, the, ...</td>\n",
       "      <td>(37, 40)</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, leverage, the, inductive, biases, learned...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, smaller,, faster, and, lighter, model, i...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "      <td>[DistilBERT,, a, distilled, version, of, BERT:...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-trained, text, encoders, have, rapidly, a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We focus on one such model, BERT, and aim to q...</td>\n",
       "      <td>We focus on</td>\n",
       "      <td>one such model , BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "      <td>[We, focus, on, one, such, model, ,, BERT, ,, ...</td>\n",
       "      <td>(3, 8)</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>the model</td>\n",
       "      <td>represents the steps of the traditional NLP pi...</td>\n",
       "      <td>[We, find, that, the, model, represents, the, ...</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>represents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td>Qualitative analysis reveals that</td>\n",
       "      <td>the model</td>\n",
       "      <td>can and often does adjust this pipeline dynami...</td>\n",
       "      <td>[Qualitative, analysis, reveals, that, the, mo...</td>\n",
       "      <td>(4, 6)</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>Rediscovers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>Large pre - trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP , motivat...</td>\n",
       "      <td>[Large, pre, -, trained, neural, networks, suc...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Most, recent, analysis, has, focused, on, mod...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>Complementary to these works , we propose meth...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Complementary, to, these, works, ,, we, propo...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT's attention heads exhibit patterns such a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "      <td>[BERT, 's, attention, heads, exhibit, patterns...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, show, that, certain, attention, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, example,, we, find, heads, that, attend,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>Lastly , we propose an attention - based probi...</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention</td>\n",
       "      <td>[Lastly, ,, we, propose, an, attention, -, bas...</td>\n",
       "      <td>(23, 25)</td>\n",
       "      <td>attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What Does BERT Look At?</td>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At</td>\n",
       "      <td>[What, Does, BERT, Look, At]</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>Look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>An Analysis of BERT's Attention</td>\n",
       "      <td>An Analysis of</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td></td>\n",
       "      <td>[An, Analysis, of, BERT, 's]</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "      <td>[Recently,, neural, models, pretrained, on, a,...</td>\n",
       "      <td>(23, 24)</td>\n",
       "      <td>(Devlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>In this paper , we describe</td>\n",
       "      <td>a simple re - implementation of BERT for query...</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, ,, we, describe, a, simple, ...</td>\n",
       "      <td>(6, 21)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td>Our system</td>\n",
       "      <td>is the state of the art on the TREC - CAR data...</td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, code, to, reproduce, our, results, is, a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Passage, Re-ranking, with, BERT]</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>I assess the extent to which</td>\n",
       "      <td>the recently introduced BERT model</td>\n",
       "      <td>captures English syntactic phenomena , using (...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>(6, 11)</td>\n",
       "      <td>captures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>The BERT model performs remarkably well on all...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT model</td>\n",
       "      <td>performs remarkably well on all cases</td>\n",
       "      <td>[The, BERT, model, performs, remarkably, well,...</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>performs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "      <td>[Assessing, BERT's, Syntactic, Abilities]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Syntactic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>A new release of BERT (Devlin, 2018) includes ...</td>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "      <td>[A, new, release, of, BERT, (Devlin,, 2018), i...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>(Devlin,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "      <td>[This, paper, explores, the, broader, cross-li...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>(multilingual)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We compare mBERT with the best-published metho...</td>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best - published methods for zero - s...</td>\n",
       "      <td>[We, compare, mBERT, with, the, best, -, publi...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner , determine to what extent mBER...</td>\n",
       "      <td>[Additionally, ,, we, investigate, the, most, ...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Beto,, Bentz,, Becas:, The, Surprising, Cross...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, pre-trained, language, models, are,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, accelerate, inference, and, reduce, model...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "      <td>a small student TinyBERT</td>\n",
       "      <td></td>\n",
       "      <td>[By, leveraging, this, new, KD, method, ,, the...</td>\n",
       "      <td>(22, 26)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>Moreover , we introduce a new two - stage lear...</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "      <td>[Moreover, ,, we, introduce, a, new, two, -, s...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general - domain and task...</td>\n",
       "      <td>[This, framework, ensures, that, TinyBERT, can...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "      <td>[TinyBERT, is, empirically, effective, and, ac...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>TinyBERT is also significantly better than sta...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state - of -...</td>\n",
       "      <td>[TinyBERT, is, also, significantly, better, th...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "      <td>[TinyBERT:, Distilling, BERT, for, Natural, La...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>Distilling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "      <td>[BERT,, a, pre-trained, Transformer, model,, h...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe BERTSUM, a simple v...</td>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "      <td>[In, this, paper,, we, describe, BERTSUM,, a, ...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, codes, to, reproduce, our, results, are,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "      <td>[Fine-tune, BERT, for, Extractive, Summarization]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Question-answering, plays, an, important, rol...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Inspired, by, the, recent, success, of, machi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, the, best, of, our, knowledge,, no, exist...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work,, we, first, build, an, RRC, d...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>the popular language model BERT</td>\n",
       "      <td>to enhance the performance of fine - tuning of...</td>\n",
       "      <td>[Since, ReviewRC, has, limited, training, exam...</td>\n",
       "      <td>(29, 34)</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, show, the, generality, of, the, approach,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, the...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, datasets, and, code, are, available, at,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "      <td>[BERT, Post-Training, for, Review, Reading, Co...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>Post-Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pre-training, has, proven, t...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>As a state - of - the - art language model pre...</td>\n",
       "      <td>BERT ( Bidirectional Encoder Representations f...</td>\n",
       "      <td>has achieved amazing results in many language ...</td>\n",
       "      <td>[As, a, state, -, of, -, the, -, art, language...</td>\n",
       "      <td>(16, 24)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "      <td>[In, this, paper, ,, we, conduct, exhaustive, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, the, proposed, solution, obtains, n...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "      <td>[How, to, Fine-Tune, BERT, for, Text, Classifi...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Devlin et al . , 2018 ) is a Markov random f...</td>\n",
       "      <td>[We, show, that, BERT, (, Devlin, et, al, ., ,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[This, formulation, gives, way, to, a, natural...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high - quality , ...</td>\n",
       "      <td>[We, generate, from, BERT, and, find, that, it...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "      <td>[Compared, to, the, generations, of, a, tradit...</td>\n",
       "      <td>(15, 16)</td>\n",
       "      <td>generates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth , and It Must Speak : BERT as a Ma...</td>\n",
       "      <td>[BERT, has, a, Mouth, ,, and, It, Must, Speak,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Aspect-based, sentiment, analysis, (ABSA),, w...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, construct, an, auxiliar...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We fine-tune the pre-trained model from BERT a...</td>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "      <td>[We, fine-tune, the, pre-trained, model, from,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect-Based, Sentiment...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering , we explore simple appl...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle</td>\n",
       "      <td>[This, required, confronting, the, challenge, ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, address, this, issue, by, applying, infer...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, TREC, microblog, and, newswi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "      <td>[BERT-based, architectures, currently, give, s...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>architectures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>In the current work , we focus on the interpre...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, the, current, work, ,, we, focus, on, the...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "      <td>[Using, a, subset, of, GLUE, tasks, and, a, se...</td>\n",
       "      <td>(31, 32)</td>\n",
       "      <td>heads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, findings, suggest, that, there, is, a, l...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, different, heads, consistently, use, t...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, manually, disabling, attentio...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, an upgraded version of BERT has been...</td>\n",
       "      <td>Recently , an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking ( WW...</td>\n",
       "      <td>[Recently, ,, an, upgraded, version, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, technical, report,, we, adapt, whol...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>was trained on the latest Chinese Wikipedia dump</td>\n",
       "      <td>[The, model, was, trained, on, the, latest, Ch...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>Chinese BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "      <td>[We, aim, to, provide, easy, extensibility, an...</td>\n",
       "      <td>(10, 12)</td>\n",
       "      <td>without</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>is verified on various NLP tasks , across sent...</td>\n",
       "      <td>[The, model, is, verified, on, various, NLP, t...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, on, these, datasets, s...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE , BERT - wwm</td>\n",
       "      <td>[Moreover, ,, we, also, examine, the, effectiv...</td>\n",
       "      <td>(14, 15)</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, the, pre-trained, model, (both, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Pre-Training, with, Whole, Word, Masking, for...</td>\n",
       "      <td>(7, 8)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open - ...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, previous work trains BERT by viewing ...</td>\n",
       "      <td>However , previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "      <td>[However, ,, previous, work, trains, BERT, by,...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To tackle this issue, we propose a multi-passa...</td>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "      <td>[To, tackle, this, issue,, we, propose, a, mul...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, addition,, we, find, that, splitting, art...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "      <td>[By, leveraging, a, passage, ranker, to, selec...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>gains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "      <td>[Experiments, on, four, standard, benchmarks, ...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>outperforms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>In particular , on the OpenSQuAD dataset , our...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models , and 5.8 % EM and 6.5 % $ F_1 $ over B...</td>\n",
       "      <td>[In, particular, ,, on, the, OpenSQuAD, datase...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "      <td>[Multi-passage, BERT:, A, Globally, Normalized...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[They, often, suffer, from, small-scale, human...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently a new language representation model, ...</td>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[Recently, a, new, language, representation, m...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>(Bidirectional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>However , there has not been much effort on ex...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding</td>\n",
       "      <td>[However, ,, there, has, not, been, much, effo...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>In this work , we propose a joint intent class...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work, ,, we, propose, a, joint, int...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, our...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[One, of, the, major, challenges, to, multi-tu...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Existing, methods, either, prepend, history, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, conceptually, simple, yet, hi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[It, enables, seamless, integration, of, conve...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>(Bidirectional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, first, explain, our, view, that, ConvQA, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, demonstrate, the, effectiveness,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, we, analyze, the, impact, of, diffe...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "      <td>[We, explore, several, different, ways, to, le...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question - answering focused passage rankin...</td>\n",
       "      <td>[Experimental, results, on, MS, MARCO, demonst...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "      <td>[Experimental, results, on, TREC, show, the, g...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>pre-trained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query - docum...</td>\n",
       "      <td>[Analyses, illustrate, how, BERT, allocates, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>allocates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "      <td>We present</td>\n",
       "      <td>simple BERT - based models for relation extrac...</td>\n",
       "      <td></td>\n",
       "      <td>[We, present, simple, BERT, -, based, models, ...</td>\n",
       "      <td>(2, 14)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, recent, years,, state-of-the-art, perform...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "      <td>[In, this, paper,, extensive, experiments, on,...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "      <td>[To, our, knowledge,, we, are, the, first, to,...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td>Our models</td>\n",
       "      <td>provide strong baselines for future</td>\n",
       "      <td>[Our, models, provide, strong, baselines, for,...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Multi-task, learning, allows, the, sharing, o...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, natural, language, processing, several, r...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, are, based, on, fine-tuning, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "      <td>[We, explore, the, multi-task, learning, setti...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, new, adaptation, modules,, PAL...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>By using PALs in parallel with BERT layers, we...</td>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "      <td>[By, using, PALs, in, parallel, with, BERT, la...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>layers,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "      <td>[BERT, and, PALs:, Projected, Attention, Layer...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-training, by, language, modeling, has, be...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, we, introduce, a, suite, of,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>As a case study , we apply these diagnostics to</td>\n",
       "      <td>the popular BERT model</td>\n",
       "      <td>, finding that it can generally distinguish go...</td>\n",
       "      <td>[As, a, case, study, ,, we, apply, these, diag...</td>\n",
       "      <td>(10, 14)</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "      <td>[What, BERT, Is, Not:, Lessons, from, a, New, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre - training , such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, it, is, unclear, why, the, pre-trai...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>In this paper , we propose to visualize loss l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets</td>\n",
       "      <td>[In, this, paper, ,, we, propose, to, visualiz...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[First,, we, find, that, pre-training, reaches...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>We also demonstrate that the fine - tuning pro...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over - parameterized for downstream ...</td>\n",
       "      <td>[We, also, demonstrate, that, the, fine, -, tu...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>Second , the visualization results indicate th...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "      <td>[Second, ,, the, visualization, results, indic...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>tends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Third, the lower layers of BERT are more invar...</td>\n",
       "      <td>Third , the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine - tuning , whic...</td>\n",
       "      <td>[Third, ,, the, lower, layers, of, BERT, are, ...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Recently, proposed, contextual, augmentation,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td></td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>demonstrates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "      <td>[We, propose, a, novel, data, augmentation, me...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>contextual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "      <td>[We, retrofit, BERT, to, conditional, BERT, by...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, our, paper,, “conditional, masked, langua...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>task.</td>\n",
       "      <td>task.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[task.]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The well trained conditional BERT can be appli...</td>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "      <td>[The, well, trained, conditional, BERT, can, b...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, six, various, different, tex...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Contextual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Starting from a public multilingual BERT check...</td>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "      <td>[Starting, from, a, public, multilingual, BERT...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>checkpoint,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, our, model, especially, outpe...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, showcase, the, effectiveness, of, our, me...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "      <td>[Recently,, a, simple, combination, of, passag...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>reader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, present, a, data, augme...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning ...</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "      <td>[We, apply, a, stage-wise, approach, to, fine,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, show, large, gains, in...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine-Tuning, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Fine-Tuning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT language model ( LM )</td>\n",
       "      <td>( Devlin et al . , 2019 ) is surprisingly good...</td>\n",
       "      <td>[The, BERT, language, model, (, LM, ), (, Devl...</td>\n",
       "      <td>(0, 7)</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Petroni, et, al.]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>(2019) take this as evidence that BERT memoriz...</td>\n",
       "      <td>( 2019 ) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre - training</td>\n",
       "      <td>[(, 2019, ), take, this, as, evidence, that, B...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>memorizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about ( the surface...</td>\n",
       "      <td>[We, take, issue, with, this, interpretation, ...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>More specifically, we show that BERT's precisi...</td>\n",
       "      <td>More specifically , we show that</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "      <td>[More, specifically, ,, we, show, that, BERT, ...</td>\n",
       "      <td>(6, 8)</td>\n",
       "      <td>precision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>As a remedy, we propose E-BERT, an extension o...</td>\n",
       "      <td>As a remedy , we propose E - BERT , an extensi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>that replaces entity mentions with symbolic en...</td>\n",
       "      <td>[As, a, remedy, ,, we, propose, E, -, BERT, ,,...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>E-BERT outperforms both BERT and ERNIE (Zhang ...</td>\n",
       "      <td>E - BERT outperforms both</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and ERNIE ( Zhang et al . , 2019 ) on hard - t...</td>\n",
       "      <td>[E, -, BERT, outperforms, both, BERT, and, ERN...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and E -</td>\n",
       "      <td>[We, take, this, as, evidence, that, E, -, BER...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>However , just how contextual are the contextu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[However, ,, just, how, contextual, are, the, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Are, there, infinitely, many, context-specifi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, one,, we, find, that, the, contextualize...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, representations, of, the, same, word, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[This, suggests, that, upper, layers, of, cont...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In all layers of ELMo, BERT, and GPT-2, on ave...</td>\n",
       "      <td>In all layers of ELMo ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2 , on average , less than 5 % of th...</td>\n",
       "      <td>[In, all, layers, of, ELMo, ,, BERT, ,, and, G...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing the Geometry of BERT, ELMo, and GPT-...</td>\n",
       "      <td>Comparing the Geometry of</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "      <td>[Comparing, the, Geometry, of, BERT,, ELMo,, a...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>ELMo,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  ID      Type  Index  \\\n",
       "0    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      0   \n",
       "1    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      1   \n",
       "2    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      2   \n",
       "3    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      3   \n",
       "4    https://www.semanticscholar.org/paper/BERT%3A-...   0     Title      0   \n",
       "5    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      0   \n",
       "6    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      1   \n",
       "7    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      2   \n",
       "8    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      3   \n",
       "9    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      4   \n",
       "10   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      5   \n",
       "11   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      6   \n",
       "12   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title      0   \n",
       "13   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      0   \n",
       "14   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      1   \n",
       "15   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      2   \n",
       "16   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      3   \n",
       "17   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      4   \n",
       "18   https://www.semanticscholar.org/paper/DistilBE...   2     Title      0   \n",
       "19   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      0   \n",
       "20   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      1   \n",
       "21   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      2   \n",
       "22   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      3   \n",
       "23   https://www.semanticscholar.org/paper/BERT-Red...   3     Title      0   \n",
       "24   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      0   \n",
       "25   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      1   \n",
       "26   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      2   \n",
       "27   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      3   \n",
       "28   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      4   \n",
       "29   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      5   \n",
       "30   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      6   \n",
       "31   https://www.semanticscholar.org/paper/What-Doe...   4     Title      0   \n",
       "32   https://www.semanticscholar.org/paper/What-Doe...   4     Title      1   \n",
       "33   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      0   \n",
       "34   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      1   \n",
       "35   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      2   \n",
       "36   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      3   \n",
       "37   https://www.semanticscholar.org/paper/Passage-...   5     Title      0   \n",
       "38   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      0   \n",
       "39   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      1   \n",
       "40   https://www.semanticscholar.org/paper/Assessin...   6     Title      0   \n",
       "41   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      0   \n",
       "42   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      1   \n",
       "43   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      2   \n",
       "44   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      3   \n",
       "45   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      4   \n",
       "46   https://www.semanticscholar.org/paper/Beto%2C-...   7     Title      0   \n",
       "47   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      0   \n",
       "48   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      1   \n",
       "49   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      2   \n",
       "50   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      3   \n",
       "51   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      4   \n",
       "52   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      5   \n",
       "53   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      6   \n",
       "54   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      7   \n",
       "55   https://www.semanticscholar.org/paper/TinyBERT...   8     Title      0   \n",
       "56   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      0   \n",
       "57   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      1   \n",
       "58   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      2   \n",
       "59   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      3   \n",
       "60   https://www.semanticscholar.org/paper/Fine-tun...   9     Title      0   \n",
       "61   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      0   \n",
       "62   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      1   \n",
       "63   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      2   \n",
       "64   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      3   \n",
       "65   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      4   \n",
       "66   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      5   \n",
       "67   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      6   \n",
       "68   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      7   \n",
       "69   https://www.semanticscholar.org/paper/BERT-Pos...  10     Title      0   \n",
       "70   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      0   \n",
       "71   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      1   \n",
       "72   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      2   \n",
       "73   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      3   \n",
       "74   https://www.semanticscholar.org/paper/How-to-F...  11     Title      0   \n",
       "75   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      0   \n",
       "76   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      1   \n",
       "77   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      2   \n",
       "78   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      3   \n",
       "79   https://www.semanticscholar.org/paper/BERT-has...  12     Title      0   \n",
       "80   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      0   \n",
       "81   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      1   \n",
       "82   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      2   \n",
       "83   https://www.semanticscholar.org/paper/Utilizin...  13     Title      0   \n",
       "84   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      0   \n",
       "85   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      1   \n",
       "86   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      2   \n",
       "87   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      3   \n",
       "88   https://www.semanticscholar.org/paper/Simple-A...  14     Title      0   \n",
       "89   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      0   \n",
       "90   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      1   \n",
       "91   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      2   \n",
       "92   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      3   \n",
       "93   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      4   \n",
       "94   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      5   \n",
       "95   https://www.semanticscholar.org/paper/Revealin...  15     Title      0   \n",
       "96   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      0   \n",
       "97   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      1   \n",
       "98   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      2   \n",
       "99   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      3   \n",
       "100  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      4   \n",
       "101  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      5   \n",
       "102  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      6   \n",
       "103  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      7   \n",
       "104  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      8   \n",
       "105  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title      0   \n",
       "106  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      0   \n",
       "107  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      1   \n",
       "108  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      2   \n",
       "109  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      3   \n",
       "110  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      4   \n",
       "111  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      5   \n",
       "112  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      6   \n",
       "113  https://www.semanticscholar.org/paper/Multi-pa...  17     Title      0   \n",
       "114  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      0   \n",
       "115  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      1   \n",
       "116  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      2   \n",
       "117  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      3   \n",
       "118  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      4   \n",
       "119  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      5   \n",
       "120  https://www.semanticscholar.org/paper/BERT-for...  18     Title      0   \n",
       "121  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      0   \n",
       "122  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      1   \n",
       "123  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      2   \n",
       "124  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      3   \n",
       "125  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      4   \n",
       "126  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      5   \n",
       "127  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      6   \n",
       "128  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      7   \n",
       "129  https://www.semanticscholar.org/paper/BERT-wit...  19     Title      0   \n",
       "130  https://www.semanticscholar.org/paper/Understa...  20  Abstract      0   \n",
       "131  https://www.semanticscholar.org/paper/Understa...  20  Abstract      1   \n",
       "132  https://www.semanticscholar.org/paper/Understa...  20  Abstract      2   \n",
       "133  https://www.semanticscholar.org/paper/Understa...  20  Abstract      3   \n",
       "134  https://www.semanticscholar.org/paper/Understa...  20  Abstract      4   \n",
       "135  https://www.semanticscholar.org/paper/Understa...  20     Title      0   \n",
       "136  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      0   \n",
       "137  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      1   \n",
       "138  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      2   \n",
       "139  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      3   \n",
       "140  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      4   \n",
       "141  https://www.semanticscholar.org/paper/Simple-B...  21     Title      0   \n",
       "142  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      0   \n",
       "143  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      1   \n",
       "144  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      2   \n",
       "145  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      3   \n",
       "146  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      4   \n",
       "147  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      5   \n",
       "148  https://www.semanticscholar.org/paper/BERT-and...  22     Title      0   \n",
       "149  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      0   \n",
       "150  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      1   \n",
       "151  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      2   \n",
       "152  https://www.semanticscholar.org/paper/What-BER...  23     Title      0   \n",
       "153  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      0   \n",
       "154  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      1   \n",
       "155  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      2   \n",
       "156  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      3   \n",
       "157  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      4   \n",
       "158  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      5   \n",
       "159  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      6   \n",
       "160  https://www.semanticscholar.org/paper/Visualiz...  24     Title      0   \n",
       "161  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      0   \n",
       "162  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      1   \n",
       "163  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      2   \n",
       "164  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      3   \n",
       "165  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      4   \n",
       "166  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      5   \n",
       "167  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      6   \n",
       "168  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      7   \n",
       "169  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      8   \n",
       "170  https://www.semanticscholar.org/paper/Conditio...  25     Title      0   \n",
       "171  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      0   \n",
       "172  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      1   \n",
       "173  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      2   \n",
       "174  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      3   \n",
       "175  https://www.semanticscholar.org/paper/Small-an...  26     Title      0   \n",
       "176  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      0   \n",
       "177  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      1   \n",
       "178  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      2   \n",
       "179  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      3   \n",
       "180  https://www.semanticscholar.org/paper/Data-Aug...  27     Title      0   \n",
       "181  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      0   \n",
       "182  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      1   \n",
       "183  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      2   \n",
       "184  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      3   \n",
       "185  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      4   \n",
       "186  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      5   \n",
       "187  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      6   \n",
       "188  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      7   \n",
       "189  https://www.semanticscholar.org/paper/BERT-is-...  28     Title      0   \n",
       "190  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      0   \n",
       "191  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      1   \n",
       "192  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      2   \n",
       "193  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      3   \n",
       "194  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      4   \n",
       "195  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      5   \n",
       "196  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      6   \n",
       "197  https://www.semanticscholar.org/paper/How-Cont...  29     Title      0   \n",
       "198  https://www.semanticscholar.org/paper/How-Cont...  29     Title      1   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    We introduce a new language representation mod...   \n",
       "1    Unlike recent language representation models, ...   \n",
       "2    As a result, the pre-trained BERT model can be...   \n",
       "3    It obtains new state-of-the-art results on ele...   \n",
       "4    BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7    We present a replication study of BERT pretrai...   \n",
       "8    We find that BERT was significantly undertrain...   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12   RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18   DistilBERT, a distilled version of BERT: small...   \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20   We focus on one such model, BERT, and aim to q...   \n",
       "21   We find that the model represents the steps of...   \n",
       "22   Qualitative analysis reveals that the model ca...   \n",
       "23         BERT Rediscovers the Classical NLP Pipeline   \n",
       "24   Large pre-trained neural networks such as BERT...   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works, we propose metho...   \n",
       "27   BERT's attention heads exhibit patterns such a...   \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly, we propose an attention-based probing ...   \n",
       "31                             What Does BERT Look At?   \n",
       "32                     An Analysis of BERT's Attention   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34   In this paper, we describe a simple re-impleme...   \n",
       "35   Our system is the state of the art on the TREC...   \n",
       "36   The code to reproduce our results is available...   \n",
       "37                        Passage Re-ranking with BERT   \n",
       "38   I assess the extent to which the recently intr...   \n",
       "39   The BERT model performs remarkably well on all...   \n",
       "40                Assessing BERT's Syntactic Abilities   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42   A new release of BERT (Devlin, 2018) includes ...   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44   We compare mBERT with the best-published metho...   \n",
       "45   Additionally, we investigate the most effectiv...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47   Language model pre-training, such as BERT, has...   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method, the plenty o...   \n",
       "51   Moreover, we introduce a new two-stage learnin...   \n",
       "52   This framework ensures that TinyBERT can captu...   \n",
       "53   TinyBERT is empirically effective and achieves...   \n",
       "54   TinyBERT is also significantly better than sta...   \n",
       "55   TinyBERT: Distilling BERT for Natural Language...   \n",
       "56   BERT, a pre-trained Transformer model, has ach...   \n",
       "57   In this paper, we describe BERTSUM, a simple v...   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60         Fine-tune BERT for Extractive Summarization   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69   BERT Post-Training for Review Reading Comprehe...   \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state-of-the-art language model pre-train...   \n",
       "72   In this paper, we conduct exhaustive experimen...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74      How to Fine-Tune BERT for Text Classification?   \n",
       "75   We show that BERT (Devlin et al., 2018) is a M...   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77   We generate from BERT and find that it can pro...   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79   BERT has a Mouth, and It Must Speak: BERT as a...   \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82   We fine-tune the pre-trained model from BERT a...   \n",
       "83   Utilizing BERT for Aspect-Based Sentiment Anal...   \n",
       "84   Following recent successes in applying BERT to...   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88   Simple Applications of BERT for Ad Hoc Documen...   \n",
       "89   BERT-based architectures currently give state-...   \n",
       "90   In the current work, we focus on the interpret...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                  Revealing the Dark Secrets of BERT   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97   Recently, an upgraded version of BERT has been...   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99   The model was trained on the latest Chinese Wi...   \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101  The model is verified on various NLP tasks, ac...   \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover, we also examine the effectiveness of...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105  Pre-Training with Whole Word Masking for Chine...   \n",
       "106  BERT model has been successfully applied to op...   \n",
       "107  However, previous work trains BERT by viewing ...   \n",
       "108  To tackle this issue, we propose a multi-passa...   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular, on the OpenSQuAD dataset, our m...   \n",
       "113  Multi-passage BERT: A Globally Normalized BERT...   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116  Recently a new language representation model, ...   \n",
       "117  However, there has not been much effort on exp...   \n",
       "118  In this work, we propose a joint intent classi...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120  BERT for Joint Intent Classification and Slot ...   \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129  BERT with History Answer Embedding for Convers...   \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134  Analyses illustrate how BERT allocates its att...   \n",
       "135     Understanding the Behaviors of BERT in Ranking   \n",
       "136  We present simple BERT-based models for relati...   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140  Our models provide strong baselines for future...   \n",
       "141  Simple BERT Models for Relation Extraction and...   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147  By using PALs in parallel with BERT layers, we...   \n",
       "148  BERT and PALs: Projected Attention Layers for ...   \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151  As a case study, we apply these diagnostics to...   \n",
       "152  What BERT Is Not: Lessons from a New Suite of ...   \n",
       "153  Language model pre-training, such as BERT, has...   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper, we propose to visualize loss la...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine-tuning proce...   \n",
       "158  Second, the visualization results indicate tha...   \n",
       "159  Third, the lower layers of BERT are more invar...   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165  We retrofit BERT to conditional BERT by introd...   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168  The well trained conditional BERT can be appli...   \n",
       "169  Experiments on six various different text clas...   \n",
       "170           Conditional BERT Contextual Augmentation   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172  Starting from a public multilingual BERT check...   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175  Small and Practical BERT Models for Sequence L...   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178  We apply a stage-wise approach to fine tuning ...   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180  Data Augmentation for BERT Fine-Tuning in Open...   \n",
       "181  The BERT language model (LM) (Devlin et al., 2...   \n",
       "182                                     Petroni et al.   \n",
       "183  (2019) take this as evidence that BERT memoriz...   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185  More specifically, we show that BERT's precisi...   \n",
       "186  As a remedy, we propose E-BERT, an extension o...   \n",
       "187  E-BERT outperforms both BERT and ERNIE (Zhang ...   \n",
       "188  We take this as evidence that E-BERT is richer...   \n",
       "189  BERT is Not a Knowledge Base (Yet): Factual Kn...   \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However, just how contextual are the contextua...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196  In all layers of ELMo, BERT, and GPT-2, on ave...   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198  Comparing the Geometry of BERT, ELMo, and GPT-...   \n",
       "\n",
       "                                               split_0  \\\n",
       "0                                         We introduce   \n",
       "1       Unlike recent language representation models ,   \n",
       "2                      As a result , the pre - trained   \n",
       "3                                                        \n",
       "4                                                        \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7                    We present a replication study of   \n",
       "8                                         We find that   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12                                                       \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18                                                       \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20                                         We focus on   \n",
       "21                                        We find that   \n",
       "22                   Qualitative analysis reveals that   \n",
       "23                                                       \n",
       "24         Large pre - trained neural networks such as   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works , we propose meth...   \n",
       "27                                                       \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly , we propose an attention - based probi...   \n",
       "31                                           What Does   \n",
       "32                                      An Analysis of   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34                         In this paper , we describe   \n",
       "35                                                       \n",
       "36   The code to reproduce our results is available...   \n",
       "37                             Passage Re-ranking with   \n",
       "38                        I assess the extent to which   \n",
       "39                                                       \n",
       "40                                           Assessing   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42                                    A new release of   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44                                          We compare   \n",
       "45   Additionally , we investigate the most effecti...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47                Language model pre-training, such as   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method , the plenty ...   \n",
       "51   Moreover , we introduce a new two - stage lear...   \n",
       "52                         This framework ensures that   \n",
       "53                                                       \n",
       "54                                                       \n",
       "55                                                       \n",
       "56                                                       \n",
       "57                          In this paper, we describe   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60                                           Fine-tune   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69                                                       \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state - of - the - art language model pre...   \n",
       "72   In this paper , we conduct exhaustive experime...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74                                    How to Fine-Tune   \n",
       "75                                        We show that   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77                                    We generate from   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79                                                       \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82             We fine-tune the pre-trained model from   \n",
       "83                                           Utilizing   \n",
       "84              Following recent successes in applying   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88                              Simple Applications of   \n",
       "89                                                       \n",
       "90   In the current work , we focus on the interpre...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                       Revealing the Dark Secrets of   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97                   Recently , an upgraded version of   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99                                                       \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101                                                      \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover , we also examine the effectiveness o...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105   Pre-Training with Whole Word Masking for Chinese   \n",
       "106                                                      \n",
       "107                     However , previous work trains   \n",
       "108   To tackle this issue, we propose a multi-passage   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular , on the OpenSQuAD dataset , our...   \n",
       "113                                      Multi-passage   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116      Recently a new language representation model,   \n",
       "117  However , there has not been much effort on ex...   \n",
       "118  In this work , we propose a joint intent class...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120                                                      \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129                                                      \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134                            Analyses illustrate how   \n",
       "135                     Understanding the Behaviors of   \n",
       "136                                         We present   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140                                                      \n",
       "141                                             Simple   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147                     By using PALs in parallel with   \n",
       "148                                                      \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151    As a case study , we apply these diagnostics to   \n",
       "152                                               What   \n",
       "153            Language model pre - training , such as   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper , we propose to visualize loss l...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine - tuning pro...   \n",
       "158  Second , the visualization results indicate th...   \n",
       "159                        Third , the lower layers of   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163                                                      \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165                                        We retrofit   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168                       The well trained conditional   \n",
       "169  Experiments on six various different text clas...   \n",
       "170                                        Conditional   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172                Starting from a public multilingual   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175                                Small and Practical   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178      We apply a stage-wise approach to fine tuning   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180                              Data Augmentation for   \n",
       "181                                                      \n",
       "182                                     Petroni et al.   \n",
       "183                ( 2019 ) take this as evidence that   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185                   More specifically , we show that   \n",
       "186  As a remedy , we propose E - BERT , an extensi...   \n",
       "187                          E - BERT outperforms both   \n",
       "188  We take this as evidence that E - BERT is rich...   \n",
       "189                                                      \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However , just how contextual are the contextu...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196                            In all layers of ELMo ,   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198                          Comparing the Geometry of   \n",
       "\n",
       "                                               split_1  \\\n",
       "0    a new language representation model called BER...   \n",
       "1                                                 BERT   \n",
       "2                                                 BERT   \n",
       "3                                                   It   \n",
       "4                                                BERT:   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7                                                 BERT   \n",
       "8                                                 BERT   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12                                            RoBERTa:   \n",
       "13                                                       \n",
       "14                                         DistilBERT,   \n",
       "15                                        a BERT model   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18                                         DistilBERT,   \n",
       "19                                                       \n",
       "20                               one such model , BERT   \n",
       "21                                           the model   \n",
       "22                                           the model   \n",
       "23                                                BERT   \n",
       "24                                                BERT   \n",
       "25                                                       \n",
       "26                                                BERT   \n",
       "27                                             BERT 's   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                             BERT 's   \n",
       "31                                                BERT   \n",
       "32                                             BERT 's   \n",
       "33                                                BERT   \n",
       "34   a simple re - implementation of BERT for query...   \n",
       "35                                          Our system   \n",
       "36                                                       \n",
       "37                                                BERT   \n",
       "38                  the recently introduced BERT model   \n",
       "39                                      The BERT model   \n",
       "40                                              BERT's   \n",
       "41                                                       \n",
       "42                                                BERT   \n",
       "43                                               mBERT   \n",
       "44                                               mBERT   \n",
       "45                                               mBERT   \n",
       "46                                                BERT   \n",
       "47                                               BERT,   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                            a small student TinyBERT   \n",
       "51                                            TinyBERT   \n",
       "52                                            TinyBERT   \n",
       "53                                            TinyBERT   \n",
       "54                                            TinyBERT   \n",
       "55                                           TinyBERT:   \n",
       "56                                               BERT,   \n",
       "57                                            BERTSUM,   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                                                BERT   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65                     the popular language model BERT   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69                                                BERT   \n",
       "70                                                       \n",
       "71   BERT ( Bidirectional Encoder Representations f...   \n",
       "72                                                BERT   \n",
       "73                                                       \n",
       "74                                                BERT   \n",
       "75                                                BERT   \n",
       "76                                                BERT   \n",
       "77                                                BERT   \n",
       "78                                                BERT   \n",
       "79                                                BERT   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82                                                BERT   \n",
       "83                                                BERT   \n",
       "84                                                BERT   \n",
       "85                                                BERT   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                                                BERT   \n",
       "89                                          BERT-based   \n",
       "90                                                BERT   \n",
       "91                                              BERT's   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                BERT   \n",
       "95                                                BERT   \n",
       "96                                              (BERT)   \n",
       "97                                                BERT   \n",
       "98                                                       \n",
       "99                                           The model   \n",
       "100                                       Chinese BERT   \n",
       "101                                          The model   \n",
       "102                                                      \n",
       "103                                               BERT   \n",
       "104                                                      \n",
       "105                                               BERT   \n",
       "106                                               BERT   \n",
       "107                                               BERT   \n",
       "108                                               BERT   \n",
       "109                                                      \n",
       "110                                               BERT   \n",
       "111                                               BERT   \n",
       "112                                               BERT   \n",
       "113                                              BERT:   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116                                               BERT   \n",
       "117                                               BERT   \n",
       "118                                               BERT   \n",
       "119                                                      \n",
       "120                                               BERT   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125                                               BERT   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129                                               BERT   \n",
       "130                                               BERT   \n",
       "131                                               BERT   \n",
       "132                                               BERT   \n",
       "133                                               BERT   \n",
       "134                                               BERT   \n",
       "135                                               BERT   \n",
       "136  simple BERT - based models for relation extrac...   \n",
       "137                                                      \n",
       "138                                         BERT-based   \n",
       "139                                               BERT   \n",
       "140                                         Our models   \n",
       "141                                               BERT   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145                                               BERT   \n",
       "146                                                      \n",
       "147                                               BERT   \n",
       "148                                               BERT   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151                             the popular BERT model   \n",
       "152                                               BERT   \n",
       "153                                               BERT   \n",
       "154                                                      \n",
       "155                                               BERT   \n",
       "156                                                      \n",
       "157                                               BERT   \n",
       "158                                               BERT   \n",
       "159                                               BERT   \n",
       "160                                               BERT   \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164                                               BERT   \n",
       "165                                               BERT   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168                                               BERT   \n",
       "169                                                      \n",
       "170                                               BERT   \n",
       "171                                                      \n",
       "172                                               BERT   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                                               BERT   \n",
       "176                                               BERT   \n",
       "177                                                      \n",
       "178                                               BERT   \n",
       "179                                                      \n",
       "180                                               BERT   \n",
       "181                     The BERT language model ( LM )   \n",
       "182                                                      \n",
       "183                                               BERT   \n",
       "184                                               BERT   \n",
       "185                                            BERT 's   \n",
       "186                                               BERT   \n",
       "187                                               BERT   \n",
       "188                                               BERT   \n",
       "189                                               BERT   \n",
       "190                                                      \n",
       "191                                               BERT   \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196                                               BERT   \n",
       "197                                                      \n",
       "198                                              BERT,   \n",
       "\n",
       "                                               split_2  \\\n",
       "0                                                        \n",
       "1    is designed to pre - train deep bidirectional ...   \n",
       "2    model can be fine - tuned with just one additi...   \n",
       "3    obtains new state - of - the - art results on ...   \n",
       "4    Pre-training of Deep Bidirectional Transformer...   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7    pretraining ( Devlin et al . , 2019 ) that car...   \n",
       "8    was significantly undertrained , and can match...   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12      A Robustly Optimized BERT Pretraining Approach   \n",
       "13                                                       \n",
       "14   which can then be fine-tuned with good perform...   \n",
       "15   by 40 % , while retaining 97 % of its language...   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18   a distilled version of BERT: smaller, faster, ...   \n",
       "19                                                       \n",
       "20   , and aim to quantify where linguistic informa...   \n",
       "21   represents the steps of the traditional NLP pi...   \n",
       "22   can and often does adjust this pipeline dynami...   \n",
       "23              Rediscovers the Classical NLP Pipeline   \n",
       "24   have had great recent success in NLP , motivat...   \n",
       "25                                                       \n",
       "26                                                       \n",
       "27   attention heads exhibit patterns such as atten...   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                           attention   \n",
       "31                                             Look At   \n",
       "32                                                       \n",
       "33   (Devlin et al., 2018), have achieved impressiv...   \n",
       "34                                                       \n",
       "35   is the state of the art on the TREC - CAR data...   \n",
       "36                                                       \n",
       "37                                                       \n",
       "38   captures English syntactic phenomena , using (...   \n",
       "39               performs remarkably well on all cases   \n",
       "40                                 Syntactic Abilities   \n",
       "41                                                       \n",
       "42   (Devlin, 2018) includes a model simultaneously...   \n",
       "43   (multilingual) as a zero shot language transfe...   \n",
       "44   with the best - published methods for zero - s...   \n",
       "45   in this manner , determine to what extent mBER...   \n",
       "46                                                       \n",
       "47   has significantly improved the performances of...   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                                                       \n",
       "51   , which performs transformer distillation at b...   \n",
       "52   can capture both the general - domain and task...   \n",
       "53   is empirically effective and achieves comparab...   \n",
       "54   is also significantly better than state - of -...   \n",
       "55   Distilling BERT for Natural Language Understan...   \n",
       "56   a pre-trained Transformer model, has achieved ...   \n",
       "57   a simple variant of BERT, for extractive summa...   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                        for Extractive Summarization   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65   to enhance the performance of fine - tuning of...   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69   Post-Training for Review Reading Comprehension...   \n",
       "70                                                       \n",
       "71   has achieved amazing results in many language ...   \n",
       "72   on text classification task and provide a gene...   \n",
       "73                                                       \n",
       "74                            for Text Classification?   \n",
       "75   ( Devlin et al . , 2018 ) is a Markov random f...   \n",
       "76                                                       \n",
       "77   and find that it can produce high - quality , ...   \n",
       "78   generates sentences that are more diverse but ...   \n",
       "79   has a Mouth , and It Must Speak : BERT as a Ma...   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82   and achieve new state-of-the-art results on Se...   \n",
       "83   for Aspect-Based Sentiment Analysis via Constr...   \n",
       "84   to question answering , we explore simple appl...   \n",
       "85                              was designed to handle   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                       for Ad Hoc Document Retrieval   \n",
       "89   architectures currently give state-of-the-art ...   \n",
       "90                                                       \n",
       "91                                              heads.   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                       \n",
       "95                                                       \n",
       "96   has shown marvelous improvements across variou...   \n",
       "97   has been released with Whole Word Masking ( WW...   \n",
       "98                                                       \n",
       "99    was trained on the latest Chinese Wikipedia dump   \n",
       "100  without changing any neural architecture or ev...   \n",
       "101  is verified on various NLP tasks , across sent...   \n",
       "102                                                      \n",
       "103                               , ERNIE , BERT - wwm   \n",
       "104                                                      \n",
       "105                                                      \n",
       "106  model has been successfully applied to open - ...   \n",
       "107  by viewing passages corresponding to the same ...   \n",
       "108  model to globally normalize answer scores acro...   \n",
       "109                                                      \n",
       "110                               gains additional 2%.   \n",
       "111  outperforms all state-of-the-art models on all...   \n",
       "112  models , and 5.8 % EM and 6.5 % $ F_1 $ over B...   \n",
       "113  A Globally Normalized BERT Model for Open-doma...   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116  (Bidirectional Encoder Representations from Tr...   \n",
       "117                 for natural language understanding   \n",
       "118                                                      \n",
       "119                                                      \n",
       "120   for Joint Intent Classification and Slot Filling   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125  (Bidirectional Encoder Representations from Tr...   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129  with History Answer Embedding for Conversation...   \n",
       "130                                   in ranking tasks   \n",
       "131  and fine-tune it on two ranking tasks: MS MARC...   \n",
       "132  in question - answering focused passage rankin...   \n",
       "133  pre-trained on surrounding contexts and the ne...   \n",
       "134  allocates its attentions between query - docum...   \n",
       "135                                         in Ranking   \n",
       "136                                                      \n",
       "137                                                      \n",
       "138    model can achieve state-of-the-art performance.   \n",
       "139                                    in this manner.   \n",
       "140                provide strong baselines for future   \n",
       "141  Models for Relation Extraction and Semantic Ro...   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145  model on the GLUE benchmark, and how to best a...   \n",
       "146                                                      \n",
       "147  layers, we match the performance of fine-tuned...   \n",
       "148  and PALs: Projected Attention Layers for Effic...   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151  , finding that it can generally distinguish go...   \n",
       "152  Is Not: Lessons from a New Suite of Psycholing...   \n",
       "153  , has achieved remarkable results in many NLP ...   \n",
       "154                                                      \n",
       "155                               on specific datasets   \n",
       "156                                                      \n",
       "157  is highly over - parameterized for downstream ...   \n",
       "158  tends to generalize better because of the flat...   \n",
       "159  are more invariant during fine - tuning , whic...   \n",
       "160                                                      \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  demonstrates that a deep bidirectional languag...   \n",
       "164                           contextual augmentation.   \n",
       "165  to conditional BERT by introducing a new condi...   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168  can be applied to enhance contextual augmentat...   \n",
       "169                                                      \n",
       "170                            Contextual Augmentation   \n",
       "171                                                      \n",
       "172  checkpoint, our final model is 6x smaller and ...   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                       Models for Sequence Labeling   \n",
       "176  reader was found to be very effective for ques...   \n",
       "177                                                      \n",
       "178  on multiple datasets, starting with data that ...   \n",
       "179                                                      \n",
       "180      Fine-Tuning in Open-Domain Question Answering   \n",
       "181  ( Devlin et al . , 2019 ) is surprisingly good...   \n",
       "182                                                      \n",
       "183  memorizes factual knowledge during pre - training   \n",
       "184  is partly due to reasoning about ( the surface...   \n",
       "185  precision drops dramatically when we filter ce...   \n",
       "186  that replaces entity mentions with symbolic en...   \n",
       "187  and ERNIE ( Zhang et al . , 2019 ) on hard - t...   \n",
       "188                                            and E -   \n",
       "189  is Not a Knowledge Base (Yet): Factual Knowled...   \n",
       "190                                                      \n",
       "191                                                      \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196  , and GPT-2 , on average , less than 5 % of th...   \n",
       "197                                                      \n",
       "198                         ELMo, and GPT-2 Embeddings   \n",
       "\n",
       "                                          split_tokens split_anchor_span  \\\n",
       "0    [We, introduce, a, new, language, representati...           (2, 18)   \n",
       "1    [Unlike, recent, language, representation, mod...            (6, 7)   \n",
       "2    [As, a, result, ,, the, pre, -, trained, BERT,...            (8, 9)   \n",
       "3    [It, obtains, new, state, -, of, -, the, -, ar...            (0, 1)   \n",
       "4    [BERT:, Pre-training, of, Deep, Bidirectional,...            (0, 1)   \n",
       "5    [Language, model, pretraining, has, led, to, s...              None   \n",
       "6    [Training, is, computationally, expensive,, of...              None   \n",
       "7    [We, present, a, replication, study, of, BERT,...            (6, 7)   \n",
       "8    [We, find, that, BERT, was, significantly, und...            (3, 4)   \n",
       "9    [Our, best, model, achieves, state-of-the-art,...              None   \n",
       "10   [These, results, highlight, the, importance, o...              None   \n",
       "11              [We, release, our, models, and, code.]              None   \n",
       "12   [RoBERTa:, A, Robustly, Optimized, BERT, Pretr...            (0, 1)   \n",
       "13   [As, Transfer, Learning, from, large-scale, pr...              None   \n",
       "14   [In, this, work,, we, propose, a, method, to, ...          (16, 17)   \n",
       "15   [While, most, prior, work, investigated, the, ...          (37, 40)   \n",
       "16   [To, leverage, the, inductive, biases, learned...              None   \n",
       "17   [Our, smaller,, faster, and, lighter, model, i...              None   \n",
       "18   [DistilBERT,, a, distilled, version, of, BERT:...            (0, 1)   \n",
       "19   [Pre-trained, text, encoders, have, rapidly, a...              None   \n",
       "20   [We, focus, on, one, such, model, ,, BERT, ,, ...            (3, 8)   \n",
       "21   [We, find, that, the, model, represents, the, ...            (3, 5)   \n",
       "22   [Qualitative, analysis, reveals, that, the, mo...            (4, 6)   \n",
       "23   [BERT, Rediscovers, the, Classical, NLP, Pipel...            (0, 1)   \n",
       "24   [Large, pre, -, trained, neural, networks, suc...            (8, 9)   \n",
       "25   [Most, recent, analysis, has, focused, on, mod...              None   \n",
       "26   [Complementary, to, these, works, ,, we, propo...          (22, 23)   \n",
       "27   [BERT, 's, attention, heads, exhibit, patterns...            (0, 2)   \n",
       "28   [We, further, show, that, certain, attention, ...              None   \n",
       "29   [For, example,, we, find, heads, that, attend,...              None   \n",
       "30   [Lastly, ,, we, propose, an, attention, -, bas...          (23, 25)   \n",
       "31                        [What, Does, BERT, Look, At]            (2, 3)   \n",
       "32                        [An, Analysis, of, BERT, 's]            (3, 5)   \n",
       "33   [Recently,, neural, models, pretrained, on, a,...          (23, 24)   \n",
       "34   [In, this, paper, ,, we, describe, a, simple, ...           (6, 21)   \n",
       "35   [Our, system, is, the, state, of, the, art, on...            (0, 2)   \n",
       "36   [The, code, to, reproduce, our, results, is, a...              None   \n",
       "37                   [Passage, Re-ranking, with, BERT]            (3, 4)   \n",
       "38   [I, assess, the, extent, to, which, the, recen...           (6, 11)   \n",
       "39   [The, BERT, model, performs, remarkably, well,...            (0, 3)   \n",
       "40           [Assessing, BERT's, Syntactic, Abilities]            (1, 2)   \n",
       "41   [Pretrained, contextual, representation, model...              None   \n",
       "42   [A, new, release, of, BERT, (Devlin,, 2018), i...            (4, 5)   \n",
       "43   [This, paper, explores, the, broader, cross-li...            (8, 9)   \n",
       "44   [We, compare, mBERT, with, the, best, -, publi...            (2, 3)   \n",
       "45   [Additionally, ,, we, investigate, the, most, ...          (10, 11)   \n",
       "46   [Beto,, Bentz,, Becas:, The, Surprising, Cross...            (8, 9)   \n",
       "47   [Language, model, pre-training,, such, as, BER...            (5, 6)   \n",
       "48   [However,, pre-trained, language, models, are,...              None   \n",
       "49   [To, accelerate, inference, and, reduce, model...              None   \n",
       "50   [By, leveraging, this, new, KD, method, ,, the...          (22, 26)   \n",
       "51   [Moreover, ,, we, introduce, a, new, two, -, s...          (12, 13)   \n",
       "52   [This, framework, ensures, that, TinyBERT, can...            (4, 5)   \n",
       "53   [TinyBERT, is, empirically, effective, and, ac...            (0, 1)   \n",
       "54   [TinyBERT, is, also, significantly, better, th...            (0, 1)   \n",
       "55   [TinyBERT:, Distilling, BERT, for, Natural, La...            (0, 1)   \n",
       "56   [BERT,, a, pre-trained, Transformer, model,, h...            (0, 1)   \n",
       "57   [In, this, paper,, we, describe, BERTSUM,, a, ...            (5, 6)   \n",
       "58   [Our, system, is, the, state, of, the, art, on...              None   \n",
       "59   [The, codes, to, reproduce, our, results, are,...              None   \n",
       "60   [Fine-tune, BERT, for, Extractive, Summarization]            (1, 2)   \n",
       "61   [Question-answering, plays, an, important, rol...              None   \n",
       "62   [Inspired, by, the, recent, success, of, machi...              None   \n",
       "63   [To, the, best, of, our, knowledge,, no, exist...              None   \n",
       "64   [In, this, work,, we, first, build, an, RRC, d...              None   \n",
       "65   [Since, ReviewRC, has, limited, training, exam...          (29, 34)   \n",
       "66   [To, show, the, generality, of, the, approach,...              None   \n",
       "67   [Experimental, results, demonstrate, that, the...              None   \n",
       "68   [The, datasets, and, code, are, available, at,...              None   \n",
       "69   [BERT, Post-Training, for, Review, Reading, Co...            (0, 1)   \n",
       "70   [Language, model, pre-training, has, proven, t...              None   \n",
       "71   [As, a, state, -, of, -, the, -, art, language...          (16, 24)   \n",
       "72   [In, this, paper, ,, we, conduct, exhaustive, ...          (16, 17)   \n",
       "73   [Finally,, the, proposed, solution, obtains, n...              None   \n",
       "74   [How, to, Fine-Tune, BERT, for, Text, Classifi...            (3, 4)   \n",
       "75   [We, show, that, BERT, (, Devlin, et, al, ., ,...            (3, 4)   \n",
       "76   [This, formulation, gives, way, to, a, natural...          (12, 13)   \n",
       "77   [We, generate, from, BERT, and, find, that, it...            (3, 4)   \n",
       "78   [Compared, to, the, generations, of, a, tradit...          (15, 16)   \n",
       "79   [BERT, has, a, Mouth, ,, and, It, Must, Speak,...            (0, 1)   \n",
       "80   [Aspect-based, sentiment, analysis, (ABSA),, w...              None   \n",
       "81   [In, this, paper,, we, construct, an, auxiliar...              None   \n",
       "82   [We, fine-tune, the, pre-trained, model, from,...            (6, 7)   \n",
       "83   [Utilizing, BERT, for, Aspect-Based, Sentiment...            (1, 2)   \n",
       "84   [Following, recent, successes, in, applying, B...            (5, 6)   \n",
       "85   [This, required, confronting, the, challenge, ...          (17, 18)   \n",
       "86   [We, address, this, issue, by, applying, infer...              None   \n",
       "87   [Experiments, on, TREC, microblog, and, newswi...              None   \n",
       "88   [Simple, Applications, of, BERT, for, Ad, Hoc,...            (3, 4)   \n",
       "89   [BERT-based, architectures, currently, give, s...            (0, 1)   \n",
       "90   [In, the, current, work, ,, we, focus, on, the...          (24, 25)   \n",
       "91   [Using, a, subset, of, GLUE, tasks, and, a, se...          (31, 32)   \n",
       "92   [Our, findings, suggest, that, there, is, a, l...              None   \n",
       "93   [While, different, heads, consistently, use, t...              None   \n",
       "94   [We, show, that, manually, disabling, attentio...          (20, 21)   \n",
       "95           [Revealing, the, Dark, Secrets, of, BERT]            (5, 6)   \n",
       "96   [Bidirectional, Encoder, Representations, from...            (5, 6)   \n",
       "97   [Recently, ,, an, upgraded, version, of, BERT,...            (6, 7)   \n",
       "98   [In, this, technical, report,, we, adapt, whol...              None   \n",
       "99   [The, model, was, trained, on, the, latest, Ch...            (0, 2)   \n",
       "100  [We, aim, to, provide, easy, extensibility, an...          (10, 12)   \n",
       "101  [The, model, is, verified, on, various, NLP, t...            (0, 2)   \n",
       "102  [Experimental, results, on, these, datasets, s...              None   \n",
       "103  [Moreover, ,, we, also, examine, the, effectiv...          (14, 15)   \n",
       "104  [We, release, the, pre-trained, model, (both, ...              None   \n",
       "105  [Pre-Training, with, Whole, Word, Masking, for...            (7, 8)   \n",
       "106  [BERT, model, has, been, successfully, applied...            (0, 1)   \n",
       "107  [However, ,, previous, work, trains, BERT, by,...            (5, 6)   \n",
       "108  [To, tackle, this, issue,, we, propose, a, mul...            (8, 9)   \n",
       "109  [In, addition,, we, find, that, splitting, art...              None   \n",
       "110  [By, leveraging, a, passage, ranker, to, selec...          (10, 11)   \n",
       "111  [Experiments, on, four, standard, benchmarks, ...           (9, 10)   \n",
       "112  [In, particular, ,, on, the, OpenSQuAD, datase...          (24, 25)   \n",
       "113  [Multi-passage, BERT:, A, Globally, Normalized...            (1, 2)   \n",
       "114  [Intent, classification, and, slot, filling, a...              None   \n",
       "115  [They, often, suffer, from, small-scale, human...              None   \n",
       "116  [Recently, a, new, language, representation, m...            (6, 7)   \n",
       "117  [However, ,, there, has, not, been, much, effo...          (10, 11)   \n",
       "118  [In, this, work, ,, we, propose, a, joint, int...          (16, 17)   \n",
       "119  [Experimental, results, demonstrate, that, our...              None   \n",
       "120  [BERT, for, Joint, Intent, Classification, and...            (0, 1)   \n",
       "121  [Conversational, search, is, an, emerging, top...              None   \n",
       "122  [One, of, the, major, challenges, to, multi-tu...              None   \n",
       "123  [Existing, methods, either, prepend, history, ...              None   \n",
       "124  [We, propose, a, conceptually, simple, yet, hi...              None   \n",
       "125  [It, enables, seamless, integration, of, conve...          (16, 17)   \n",
       "126  [We, first, explain, our, view, that, ConvQA, ...              None   \n",
       "127  [We, further, demonstrate, the, effectiveness,...              None   \n",
       "128  [Finally,, we, analyze, the, impact, of, diffe...              None   \n",
       "129  [BERT, with, History, Answer, Embedding, for, ...            (0, 1)   \n",
       "130  [This, paper, studies, the, performances, and,...            (8, 9)   \n",
       "131  [We, explore, several, different, ways, to, le...           (9, 10)   \n",
       "132  [Experimental, results, on, MS, MARCO, demonst...          (10, 11)   \n",
       "133  [Experimental, results, on, TREC, show, the, g...           (9, 10)   \n",
       "134  [Analyses, illustrate, how, BERT, allocates, i...            (3, 4)   \n",
       "135  [Understanding, the, Behaviors, of, BERT, in, ...            (4, 5)   \n",
       "136  [We, present, simple, BERT, -, based, models, ...           (2, 14)   \n",
       "137  [In, recent, years,, state-of-the-art, perform...              None   \n",
       "138  [In, this, paper,, extensive, experiments, on,...          (20, 21)   \n",
       "139  [To, our, knowledge,, we, are, the, first, to,...          (10, 11)   \n",
       "140  [Our, models, provide, strong, baselines, for,...            (0, 2)   \n",
       "141  [Simple, BERT, Models, for, Relation, Extracti...            (1, 2)   \n",
       "142  [Multi-task, learning, allows, the, sharing, o...              None   \n",
       "143  [In, natural, language, processing, several, r...              None   \n",
       "144  [These, results, are, based, on, fine-tuning, ...              None   \n",
       "145  [We, explore, the, multi-task, learning, setti...           (9, 10)   \n",
       "146  [We, introduce, new, adaptation, modules,, PAL...              None   \n",
       "147  [By, using, PALs, in, parallel, with, BERT, la...            (6, 7)   \n",
       "148  [BERT, and, PALs:, Projected, Attention, Layer...            (0, 1)   \n",
       "149  [Pre-training, by, language, modeling, has, be...              None   \n",
       "150  [In, this, paper, we, introduce, a, suite, of,...              None   \n",
       "151  [As, a, case, study, ,, we, apply, these, diag...          (10, 14)   \n",
       "152  [What, BERT, Is, Not:, Lessons, from, a, New, ...            (1, 2)   \n",
       "153  [Language, model, pre, -, training, ,, such, a...            (8, 9)   \n",
       "154  [However,, it, is, unclear, why, the, pre-trai...              None   \n",
       "155  [In, this, paper, ,, we, propose, to, visualiz...          (17, 18)   \n",
       "156  [First,, we, find, that, pre-training, reaches...              None   \n",
       "157  [We, also, demonstrate, that, the, fine, -, tu...          (16, 17)   \n",
       "158  [Second, ,, the, visualization, results, indic...          (10, 11)   \n",
       "159  [Third, ,, the, lower, layers, of, BERT, are, ...            (6, 7)   \n",
       "160  [Visualizing, and, Understanding, the, Effecti...            (6, 7)   \n",
       "161  [Data, augmentation, methods, are, often, appl...              None   \n",
       "162  [Recently, proposed, contextual, augmentation,...              None   \n",
       "163  [Bidirectional, Encoder, Representations, from...            (0, 8)   \n",
       "164  [We, propose, a, novel, data, augmentation, me...          (12, 13)   \n",
       "165  [We, retrofit, BERT, to, conditional, BERT, by...            (2, 3)   \n",
       "166  [In, our, paper,, “conditional, masked, langua...              None   \n",
       "167                                            [task.]              None   \n",
       "168  [The, well, trained, conditional, BERT, can, b...            (4, 5)   \n",
       "169  [Experiments, on, six, various, different, tex...              None   \n",
       "170      [Conditional, BERT, Contextual, Augmentation]            (1, 2)   \n",
       "171  [We, propose, a, practical, scheme, to, train,...              None   \n",
       "172  [Starting, from, a, public, multilingual, BERT...            (5, 6)   \n",
       "173  [We, show, that, our, model, especially, outpe...              None   \n",
       "174  [We, showcase, the, effectiveness, of, our, me...              None   \n",
       "175  [Small, and, Practical, BERT, Models, for, Seq...            (3, 4)   \n",
       "176  [Recently,, a, simple, combination, of, passag...          (13, 14)   \n",
       "177  [In, this, paper,, we, present, a, data, augme...              None   \n",
       "178  [We, apply, a, stage-wise, approach, to, fine,...            (8, 9)   \n",
       "179  [Experimental, results, show, large, gains, in...              None   \n",
       "180  [Data, Augmentation, for, BERT, Fine-Tuning, i...            (3, 4)   \n",
       "181  [The, BERT, language, model, (, LM, ), (, Devl...            (0, 7)   \n",
       "182                                 [Petroni, et, al.]              None   \n",
       "183  [(, 2019, ), take, this, as, evidence, that, B...            (8, 9)   \n",
       "184  [We, take, issue, with, this, interpretation, ...          (12, 13)   \n",
       "185  [More, specifically, ,, we, show, that, BERT, ...            (6, 8)   \n",
       "186  [As, a, remedy, ,, we, propose, E, -, BERT, ,,...          (13, 14)   \n",
       "187  [E, -, BERT, outperforms, both, BERT, and, ERN...            (5, 6)   \n",
       "188  [We, take, this, as, evidence, that, E, -, BER...          (22, 23)   \n",
       "189  [BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...            (0, 1)   \n",
       "190  [Replacing, static, word, embeddings, with, co...              None   \n",
       "191  [However, ,, just, how, contextual, are, the, ...          (16, 17)   \n",
       "192  [Are, there, infinitely, many, context-specifi...              None   \n",
       "193  [For, one,, we, find, that, the, contextualize...              None   \n",
       "194  [While, representations, of, the, same, word, ...              None   \n",
       "195  [This, suggests, that, upper, layers, of, cont...              None   \n",
       "196  [In, all, layers, of, ELMo, ,, BERT, ,, and, G...            (6, 7)   \n",
       "197  [How, Contextual, are, Contextualized, Word, R...              None   \n",
       "198  [Comparing, the, Geometry, of, BERT,, ELMo,, a...            (4, 5)   \n",
       "\n",
       "              group  \n",
       "0                    \n",
       "1                is  \n",
       "2             model  \n",
       "3           obtains  \n",
       "4      Pre-training  \n",
       "5                    \n",
       "6                    \n",
       "7       pretraining  \n",
       "8               was  \n",
       "9                    \n",
       "10                   \n",
       "11                   \n",
       "12                A  \n",
       "13                   \n",
       "14            which  \n",
       "15               by  \n",
       "16                   \n",
       "17                   \n",
       "18                a  \n",
       "19                   \n",
       "20                ,  \n",
       "21       represents  \n",
       "22              can  \n",
       "23      Rediscovers  \n",
       "24             have  \n",
       "25                   \n",
       "26                   \n",
       "27        attention  \n",
       "28                   \n",
       "29                   \n",
       "30        attention  \n",
       "31             Look  \n",
       "32                   \n",
       "33          (Devlin  \n",
       "34                   \n",
       "35               is  \n",
       "36                   \n",
       "37                   \n",
       "38         captures  \n",
       "39         performs  \n",
       "40        Syntactic  \n",
       "41                   \n",
       "42         (Devlin,  \n",
       "43   (multilingual)  \n",
       "44             with  \n",
       "45               in  \n",
       "46                   \n",
       "47              has  \n",
       "48                   \n",
       "49                   \n",
       "50                   \n",
       "51                ,  \n",
       "52              can  \n",
       "53               is  \n",
       "54               is  \n",
       "55       Distilling  \n",
       "56                a  \n",
       "57                a  \n",
       "58                   \n",
       "59                   \n",
       "60              for  \n",
       "61                   \n",
       "62                   \n",
       "63                   \n",
       "64                   \n",
       "65               to  \n",
       "66                   \n",
       "67                   \n",
       "68                   \n",
       "69    Post-Training  \n",
       "70                   \n",
       "71              has  \n",
       "72               on  \n",
       "73                   \n",
       "74              for  \n",
       "75                (  \n",
       "76                   \n",
       "77              and  \n",
       "78        generates  \n",
       "79              has  \n",
       "80                   \n",
       "81                   \n",
       "82              and  \n",
       "83              for  \n",
       "84               to  \n",
       "85              was  \n",
       "86                   \n",
       "87                   \n",
       "88              for  \n",
       "89    architectures  \n",
       "90                   \n",
       "91           heads.  \n",
       "92                   \n",
       "93                   \n",
       "94                   \n",
       "95                   \n",
       "96              has  \n",
       "97              has  \n",
       "98                   \n",
       "99              was  \n",
       "100         without  \n",
       "101              is  \n",
       "102                  \n",
       "103               ,  \n",
       "104                  \n",
       "105                  \n",
       "106           model  \n",
       "107              by  \n",
       "108           model  \n",
       "109                  \n",
       "110           gains  \n",
       "111     outperforms  \n",
       "112          models  \n",
       "113               A  \n",
       "114                  \n",
       "115                  \n",
       "116  (Bidirectional  \n",
       "117             for  \n",
       "118                  \n",
       "119                  \n",
       "120             for  \n",
       "121                  \n",
       "122                  \n",
       "123                  \n",
       "124                  \n",
       "125  (Bidirectional  \n",
       "126                  \n",
       "127                  \n",
       "128                  \n",
       "129            with  \n",
       "130              in  \n",
       "131             and  \n",
       "132              in  \n",
       "133     pre-trained  \n",
       "134       allocates  \n",
       "135              in  \n",
       "136                  \n",
       "137                  \n",
       "138           model  \n",
       "139              in  \n",
       "140         provide  \n",
       "141          Models  \n",
       "142                  \n",
       "143                  \n",
       "144                  \n",
       "145           model  \n",
       "146                  \n",
       "147         layers,  \n",
       "148             and  \n",
       "149                  \n",
       "150                  \n",
       "151               ,  \n",
       "152              Is  \n",
       "153               ,  \n",
       "154                  \n",
       "155              on  \n",
       "156                  \n",
       "157              is  \n",
       "158           tends  \n",
       "159             are  \n",
       "160                  \n",
       "161                  \n",
       "162                  \n",
       "163    demonstrates  \n",
       "164      contextual  \n",
       "165              to  \n",
       "166                  \n",
       "167                  \n",
       "168             can  \n",
       "169                  \n",
       "170      Contextual  \n",
       "171                  \n",
       "172     checkpoint,  \n",
       "173                  \n",
       "174                  \n",
       "175          Models  \n",
       "176          reader  \n",
       "177                  \n",
       "178              on  \n",
       "179                  \n",
       "180     Fine-Tuning  \n",
       "181               (  \n",
       "182                  \n",
       "183       memorizes  \n",
       "184              is  \n",
       "185       precision  \n",
       "186            that  \n",
       "187             and  \n",
       "188             and  \n",
       "189              is  \n",
       "190                  \n",
       "191                  \n",
       "192                  \n",
       "193                  \n",
       "194                  \n",
       "195                  \n",
       "196               ,  \n",
       "197                  \n",
       "198           ELMo,  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group on the literal first word that comes after the anchor point\n",
    "def group_first_word(row):\n",
    "    if (row['split_anchor_span'] is not None) and \\\n",
    "            row['split_anchor_span'][1] < len(row['split_tokens']):\n",
    "        output = [row['split_tokens'][row['split_anchor_span'][1]]]\n",
    "    else:\n",
    "        output = ['']\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_first_word(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>split_0</th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>split_anchor_span</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>We introduce</td>\n",
       "      <td>a new language representation model called BER...</td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>(2, 18)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Unlike recent language representation models, ...</td>\n",
       "      <td>Unlike recent language representation models ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre - train deep bidirectional ...</td>\n",
       "      <td>[Unlike, recent, language, representation, mod...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a result, the pre-trained BERT model can be...</td>\n",
       "      <td>As a result , the pre - trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine - tuned with just one additi...</td>\n",
       "      <td>[As, a, result, ,, the, pre, -, trained, BERT,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td>It</td>\n",
       "      <td>obtains new state - of - the - art results on ...</td>\n",
       "      <td>[It, obtains, new, state, -, of, -, the, -, ar...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>obtains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "      <td>[BERT:, Pre-training, of, Deep, Bidirectional,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Training, is, computationally, expensive,, of...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We present a replication study of BERT pretrai...</td>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining ( Devlin et al . , 2019 ) that car...</td>\n",
       "      <td>[We, present, a, replication, study, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>measures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained , and can match...</td>\n",
       "      <td>[We, find, that, BERT, was, significantly, und...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, best, model, achieves, state-of-the-art,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, highlight, the, importance, o...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, our, models, and, code.]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "      <td>[RoBERTa:, A, Robustly, Optimized, BERT, Pretr...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[As, Transfer, Learning, from, large-scale, pr...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "      <td>[In, this, work,, we, propose, a, method, to, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>a BERT model</td>\n",
       "      <td>by 40 % , while retaining 97 % of its language...</td>\n",
       "      <td>[While, most, prior, work, investigated, the, ...</td>\n",
       "      <td>(37, 40)</td>\n",
       "      <td>retaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, leverage, the, inductive, biases, learned...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, smaller,, faster, and, lighter, model, i...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "      <td>[DistilBERT,, a, distilled, version, of, BERT:...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-trained, text, encoders, have, rapidly, a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We focus on one such model, BERT, and aim to q...</td>\n",
       "      <td>We focus on</td>\n",
       "      <td>one such model , BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "      <td>[We, focus, on, one, such, model, ,, BERT, ,, ...</td>\n",
       "      <td>(3, 8)</td>\n",
       "      <td>quantify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>the model</td>\n",
       "      <td>represents the steps of the traditional NLP pi...</td>\n",
       "      <td>[We, find, that, the, model, represents, the, ...</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>represents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td>Qualitative analysis reveals that</td>\n",
       "      <td>the model</td>\n",
       "      <td>can and often does adjust this pipeline dynami...</td>\n",
       "      <td>[Qualitative, analysis, reveals, that, the, mo...</td>\n",
       "      <td>(4, 6)</td>\n",
       "      <td>does</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>Large pre - trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP , motivat...</td>\n",
       "      <td>[Large, pre, -, trained, neural, networks, suc...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Most, recent, analysis, has, focused, on, mod...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>Complementary to these works , we propose meth...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Complementary, to, these, works, ,, we, propo...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT's attention heads exhibit patterns such a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "      <td>[BERT, 's, attention, heads, exhibit, patterns...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>exhibit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, show, that, certain, attention, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, example,, we, find, heads, that, attend,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>Lastly , we propose an attention - based probi...</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention</td>\n",
       "      <td>[Lastly, ,, we, propose, an, attention, -, bas...</td>\n",
       "      <td>(23, 25)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What Does BERT Look At?</td>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At</td>\n",
       "      <td>[What, Does, BERT, Look, At]</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>An Analysis of BERT's Attention</td>\n",
       "      <td>An Analysis of</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td></td>\n",
       "      <td>[An, Analysis, of, BERT, 's]</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "      <td>[Recently,, neural, models, pretrained, on, a,...</td>\n",
       "      <td>(23, 24)</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>In this paper , we describe</td>\n",
       "      <td>a simple re - implementation of BERT for query...</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, ,, we, describe, a, simple, ...</td>\n",
       "      <td>(6, 21)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td>Our system</td>\n",
       "      <td>is the state of the art on the TREC - CAR data...</td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, code, to, reproduce, our, results, is, a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Passage, Re-ranking, with, BERT]</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>I assess the extent to which</td>\n",
       "      <td>the recently introduced BERT model</td>\n",
       "      <td>captures English syntactic phenomena , using (...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>(6, 11)</td>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>The BERT model performs remarkably well on all...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT model</td>\n",
       "      <td>performs remarkably well on all cases</td>\n",
       "      <td>[The, BERT, model, performs, remarkably, well,...</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "      <td>[Assessing, BERT's, Syntactic, Abilities]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>A new release of BERT (Devlin, 2018) includes ...</td>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "      <td>[A, new, release, of, BERT, (Devlin,, 2018), i...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>includes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "      <td>[This, paper, explores, the, broader, cross-li...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>covering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We compare mBERT with the best-published metho...</td>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best - published methods for zero - s...</td>\n",
       "      <td>[We, compare, mBERT, with, the, best, -, publi...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>published</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner , determine to what extent mBER...</td>\n",
       "      <td>[Additionally, ,, we, investigate, the, most, ...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>generalizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Beto,, Bentz,, Becas:, The, Surprising, Cross...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, pre-trained, language, models, are,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, accelerate, inference, and, reduce, model...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "      <td>a small student TinyBERT</td>\n",
       "      <td></td>\n",
       "      <td>[By, leveraging, this, new, KD, method, ,, the...</td>\n",
       "      <td>(22, 26)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>Moreover , we introduce a new two - stage lear...</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "      <td>[Moreover, ,, we, introduce, a, new, two, -, s...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>performs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general - domain and task...</td>\n",
       "      <td>[This, framework, ensures, that, TinyBERT, can...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>capture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "      <td>[TinyBERT, is, empirically, effective, and, ac...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>TinyBERT is also significantly better than sta...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state - of -...</td>\n",
       "      <td>[TinyBERT, is, also, significantly, better, th...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "      <td>[TinyBERT:, Distilling, BERT, for, Natural, La...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "      <td>[BERT,, a, pre-trained, Transformer, model,, h...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe BERTSUM, a simple v...</td>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "      <td>[In, this, paper,, we, describe, BERTSUM,, a, ...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, codes, to, reproduce, our, results, are,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "      <td>[Fine-tune, BERT, for, Extractive, Summarization]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Question-answering, plays, an, important, rol...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Inspired, by, the, recent, success, of, machi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, the, best, of, our, knowledge,, no, exist...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work,, we, first, build, an, RRC, d...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>the popular language model BERT</td>\n",
       "      <td>to enhance the performance of fine - tuning of...</td>\n",
       "      <td>[Since, ReviewRC, has, limited, training, exam...</td>\n",
       "      <td>(29, 34)</td>\n",
       "      <td>enhance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, show, the, generality, of, the, approach,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, the...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, datasets, and, code, are, available, at,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "      <td>[BERT, Post-Training, for, Review, Reading, Co...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pre-training, has, proven, t...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>As a state - of - the - art language model pre...</td>\n",
       "      <td>BERT ( Bidirectional Encoder Representations f...</td>\n",
       "      <td>has achieved amazing results in many language ...</td>\n",
       "      <td>[As, a, state, -, of, -, the, -, art, language...</td>\n",
       "      <td>(16, 24)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "      <td>[In, this, paper, ,, we, conduct, exhaustive, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, the, proposed, solution, obtains, n...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "      <td>[How, to, Fine-Tune, BERT, for, Text, Classifi...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Devlin et al . , 2018 ) is a Markov random f...</td>\n",
       "      <td>[We, show, that, BERT, (, Devlin, et, al, ., ,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[This, formulation, gives, way, to, a, natural...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high - quality , ...</td>\n",
       "      <td>[We, generate, from, BERT, and, find, that, it...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "      <td>[Compared, to, the, generations, of, a, tradit...</td>\n",
       "      <td>(15, 16)</td>\n",
       "      <td>generates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth , and It Must Speak : BERT as a Ma...</td>\n",
       "      <td>[BERT, has, a, Mouth, ,, and, It, Must, Speak,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Aspect-based, sentiment, analysis, (ABSA),, w...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, construct, an, auxiliar...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We fine-tune the pre-trained model from BERT a...</td>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "      <td>[We, fine-tune, the, pre-trained, model, from,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>achieve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect-Based, Sentiment...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering , we explore simple appl...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>answering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle</td>\n",
       "      <td>[This, required, confronting, the, challenge, ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, address, this, issue, by, applying, infer...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, TREC, microblog, and, newswi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "      <td>[BERT-based, architectures, currently, give, s...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>give</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>In the current work , we focus on the interpre...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, the, current, work, ,, we, focus, on, the...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "      <td>[Using, a, subset, of, GLUE, tasks, and, a, se...</td>\n",
       "      <td>(31, 32)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, findings, suggest, that, there, is, a, l...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, different, heads, consistently, use, t...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, manually, disabling, attentio...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, an upgraded version of BERT has been...</td>\n",
       "      <td>Recently , an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking ( WW...</td>\n",
       "      <td>[Recently, ,, an, upgraded, version, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, technical, report,, we, adapt, whol...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>was trained on the latest Chinese Wikipedia dump</td>\n",
       "      <td>[The, model, was, trained, on, the, latest, Ch...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>Chinese BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "      <td>[We, aim, to, provide, easy, extensibility, an...</td>\n",
       "      <td>(10, 12)</td>\n",
       "      <td>changing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>is verified on various NLP tasks , across sent...</td>\n",
       "      <td>[The, model, is, verified, on, various, NLP, t...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, on, these, datasets, s...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE , BERT - wwm</td>\n",
       "      <td>[Moreover, ,, we, also, examine, the, effectiv...</td>\n",
       "      <td>(14, 15)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, the, pre-trained, model, (both, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Pre-Training, with, Whole, Word, Masking, for...</td>\n",
       "      <td>(7, 8)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open - ...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, previous work trains BERT by viewing ...</td>\n",
       "      <td>However , previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "      <td>[However, ,, previous, work, trains, BERT, by,...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>viewing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To tackle this issue, we propose a multi-passa...</td>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "      <td>[To, tackle, this, issue,, we, propose, a, mul...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>normalize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, addition,, we, find, that, splitting, art...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "      <td>[By, leveraging, a, passage, ranker, to, selec...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "      <td>[Experiments, on, four, standard, benchmarks, ...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>outperforms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>In particular , on the OpenSQuAD dataset , our...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models , and 5.8 % EM and 6.5 % $ F_1 $ over B...</td>\n",
       "      <td>[In, particular, ,, on, the, OpenSQuAD, datase...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td>based</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "      <td>[Multi-passage, BERT:, A, Globally, Normalized...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[They, often, suffer, from, small-scale, human...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently a new language representation model, ...</td>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[Recently, a, new, language, representation, m...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>facilitates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>However , there has not been much effort on ex...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding</td>\n",
       "      <td>[However, ,, there, has, not, been, much, effo...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>In this work , we propose a joint intent class...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work, ,, we, propose, a, joint, int...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, our...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[One, of, the, major, challenges, to, multi-tu...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Existing, methods, either, prepend, history, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, conceptually, simple, yet, hi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[It, enables, seamless, integration, of, conve...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, first, explain, our, view, that, ConvQA, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, demonstrate, the, effectiveness,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, we, analyze, the, impact, of, diffe...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "      <td>[We, explore, several, different, ways, to, le...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question - answering focused passage rankin...</td>\n",
       "      <td>[Experimental, results, on, MS, MARCO, demonst...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "      <td>[Experimental, results, on, TREC, show, the, g...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>surrounding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query - docum...</td>\n",
       "      <td>[Analyses, illustrate, how, BERT, allocates, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>allocates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>Ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "      <td>We present</td>\n",
       "      <td>simple BERT - based models for relation extrac...</td>\n",
       "      <td></td>\n",
       "      <td>[We, present, simple, BERT, -, based, models, ...</td>\n",
       "      <td>(2, 14)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, recent, years,, state-of-the-art, perform...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "      <td>[In, this, paper,, extensive, experiments, on,...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td>achieve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "      <td>[To, our, knowledge,, we, are, the, first, to,...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td>Our models</td>\n",
       "      <td>provide strong baselines for future</td>\n",
       "      <td>[Our, models, provide, strong, baselines, for,...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Labeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Multi-task, learning, allows, the, sharing, o...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, natural, language, processing, several, r...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, are, based, on, fine-tuning, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "      <td>[We, explore, the, multi-task, learning, setti...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, new, adaptation, modules,, PAL...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>By using PALs in parallel with BERT layers, we...</td>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "      <td>[By, using, PALs, in, parallel, with, BERT, la...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "      <td>[BERT, and, PALs:, Projected, Attention, Layer...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>Projected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-training, by, language, modeling, has, be...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, we, introduce, a, suite, of,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>As a case study , we apply these diagnostics to</td>\n",
       "      <td>the popular BERT model</td>\n",
       "      <td>, finding that it can generally distinguish go...</td>\n",
       "      <td>[As, a, case, study, ,, we, apply, these, diag...</td>\n",
       "      <td>(10, 14)</td>\n",
       "      <td>finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "      <td>[What, BERT, Is, Not:, Lessons, from, a, New, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre - training , such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, it, is, unclear, why, the, pre-trai...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>In this paper , we propose to visualize loss l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets</td>\n",
       "      <td>[In, this, paper, ,, we, propose, to, visualiz...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[First,, we, find, that, pre-training, reaches...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>We also demonstrate that the fine - tuning pro...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over - parameterized for downstream ...</td>\n",
       "      <td>[We, also, demonstrate, that, the, fine, -, tu...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>Second , the visualization results indicate th...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "      <td>[Second, ,, the, visualization, results, indic...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>tends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Third, the lower layers of BERT are more invar...</td>\n",
       "      <td>Third , the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine - tuning , whic...</td>\n",
       "      <td>[Third, ,, the, lower, layers, of, BERT, are, ...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Recently, proposed, contextual, augmentation,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td></td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>demonstrates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "      <td>[We, propose, a, novel, data, augmentation, me...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "      <td>[We, retrofit, BERT, to, conditional, BERT, by...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>conditional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, our, paper,, “conditional, masked, langua...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>task.</td>\n",
       "      <td>task.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[task.]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The well trained conditional BERT can be appli...</td>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "      <td>[The, well, trained, conditional, BERT, can, b...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, six, various, different, tex...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Starting from a public multilingual BERT check...</td>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "      <td>[Starting, from, a, public, multilingual, BERT...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, our, model, especially, outpe...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, showcase, the, effectiveness, of, our, me...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "      <td>[Recently,, a, simple, combination, of, passag...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, present, a, data, augme...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning ...</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "      <td>[We, apply, a, stage-wise, approach, to, fine,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>starting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, show, large, gains, in...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine-Tuning, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT language model ( LM )</td>\n",
       "      <td>( Devlin et al . , 2019 ) is surprisingly good...</td>\n",
       "      <td>[The, BERT, language, model, (, LM, ), (, Devl...</td>\n",
       "      <td>(0, 7)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Petroni, et, al.]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>(2019) take this as evidence that BERT memoriz...</td>\n",
       "      <td>( 2019 ) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre - training</td>\n",
       "      <td>[(, 2019, ), take, this, as, evidence, that, B...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>memorizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about ( the surface...</td>\n",
       "      <td>[We, take, issue, with, this, interpretation, ...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>More specifically, we show that BERT's precisi...</td>\n",
       "      <td>More specifically , we show that</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "      <td>[More, specifically, ,, we, show, that, BERT, ...</td>\n",
       "      <td>(6, 8)</td>\n",
       "      <td>filter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>As a remedy, we propose E-BERT, an extension o...</td>\n",
       "      <td>As a remedy , we propose E - BERT , an extensi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>that replaces entity mentions with symbolic en...</td>\n",
       "      <td>[As, a, remedy, ,, we, propose, E, -, BERT, ,,...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>replaces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>E-BERT outperforms both BERT and ERNIE (Zhang ...</td>\n",
       "      <td>E - BERT outperforms both</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and ERNIE ( Zhang et al . , 2019 ) on hard - t...</td>\n",
       "      <td>[E, -, BERT, outperforms, both, BERT, and, ERN...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and E -</td>\n",
       "      <td>[We, take, this, as, evidence, that, E, -, BER...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>However , just how contextual are the contextu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[However, ,, just, how, contextual, are, the, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Are, there, infinitely, many, context-specifi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, one,, we, find, that, the, contextualize...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, representations, of, the, same, word, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[This, suggests, that, upper, layers, of, cont...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In all layers of ELMo, BERT, and GPT-2, on ave...</td>\n",
       "      <td>In all layers of ELMo ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2 , on average , less than 5 % of th...</td>\n",
       "      <td>[In, all, layers, of, ELMo, ,, BERT, ,, and, G...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing the Geometry of BERT, ELMo, and GPT-...</td>\n",
       "      <td>Comparing the Geometry of</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "      <td>[Comparing, the, Geometry, of, BERT,, ELMo,, a...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  ID      Type  Index  \\\n",
       "0    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      0   \n",
       "1    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      1   \n",
       "2    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      2   \n",
       "3    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      3   \n",
       "4    https://www.semanticscholar.org/paper/BERT%3A-...   0     Title      0   \n",
       "5    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      0   \n",
       "6    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      1   \n",
       "7    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      2   \n",
       "8    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      3   \n",
       "9    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      4   \n",
       "10   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      5   \n",
       "11   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      6   \n",
       "12   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title      0   \n",
       "13   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      0   \n",
       "14   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      1   \n",
       "15   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      2   \n",
       "16   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      3   \n",
       "17   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      4   \n",
       "18   https://www.semanticscholar.org/paper/DistilBE...   2     Title      0   \n",
       "19   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      0   \n",
       "20   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      1   \n",
       "21   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      2   \n",
       "22   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      3   \n",
       "23   https://www.semanticscholar.org/paper/BERT-Red...   3     Title      0   \n",
       "24   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      0   \n",
       "25   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      1   \n",
       "26   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      2   \n",
       "27   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      3   \n",
       "28   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      4   \n",
       "29   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      5   \n",
       "30   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      6   \n",
       "31   https://www.semanticscholar.org/paper/What-Doe...   4     Title      0   \n",
       "32   https://www.semanticscholar.org/paper/What-Doe...   4     Title      1   \n",
       "33   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      0   \n",
       "34   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      1   \n",
       "35   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      2   \n",
       "36   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      3   \n",
       "37   https://www.semanticscholar.org/paper/Passage-...   5     Title      0   \n",
       "38   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      0   \n",
       "39   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      1   \n",
       "40   https://www.semanticscholar.org/paper/Assessin...   6     Title      0   \n",
       "41   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      0   \n",
       "42   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      1   \n",
       "43   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      2   \n",
       "44   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      3   \n",
       "45   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      4   \n",
       "46   https://www.semanticscholar.org/paper/Beto%2C-...   7     Title      0   \n",
       "47   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      0   \n",
       "48   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      1   \n",
       "49   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      2   \n",
       "50   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      3   \n",
       "51   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      4   \n",
       "52   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      5   \n",
       "53   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      6   \n",
       "54   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      7   \n",
       "55   https://www.semanticscholar.org/paper/TinyBERT...   8     Title      0   \n",
       "56   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      0   \n",
       "57   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      1   \n",
       "58   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      2   \n",
       "59   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      3   \n",
       "60   https://www.semanticscholar.org/paper/Fine-tun...   9     Title      0   \n",
       "61   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      0   \n",
       "62   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      1   \n",
       "63   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      2   \n",
       "64   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      3   \n",
       "65   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      4   \n",
       "66   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      5   \n",
       "67   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      6   \n",
       "68   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      7   \n",
       "69   https://www.semanticscholar.org/paper/BERT-Pos...  10     Title      0   \n",
       "70   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      0   \n",
       "71   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      1   \n",
       "72   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      2   \n",
       "73   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      3   \n",
       "74   https://www.semanticscholar.org/paper/How-to-F...  11     Title      0   \n",
       "75   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      0   \n",
       "76   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      1   \n",
       "77   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      2   \n",
       "78   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      3   \n",
       "79   https://www.semanticscholar.org/paper/BERT-has...  12     Title      0   \n",
       "80   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      0   \n",
       "81   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      1   \n",
       "82   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      2   \n",
       "83   https://www.semanticscholar.org/paper/Utilizin...  13     Title      0   \n",
       "84   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      0   \n",
       "85   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      1   \n",
       "86   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      2   \n",
       "87   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      3   \n",
       "88   https://www.semanticscholar.org/paper/Simple-A...  14     Title      0   \n",
       "89   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      0   \n",
       "90   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      1   \n",
       "91   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      2   \n",
       "92   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      3   \n",
       "93   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      4   \n",
       "94   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      5   \n",
       "95   https://www.semanticscholar.org/paper/Revealin...  15     Title      0   \n",
       "96   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      0   \n",
       "97   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      1   \n",
       "98   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      2   \n",
       "99   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      3   \n",
       "100  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      4   \n",
       "101  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      5   \n",
       "102  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      6   \n",
       "103  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      7   \n",
       "104  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      8   \n",
       "105  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title      0   \n",
       "106  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      0   \n",
       "107  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      1   \n",
       "108  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      2   \n",
       "109  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      3   \n",
       "110  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      4   \n",
       "111  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      5   \n",
       "112  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      6   \n",
       "113  https://www.semanticscholar.org/paper/Multi-pa...  17     Title      0   \n",
       "114  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      0   \n",
       "115  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      1   \n",
       "116  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      2   \n",
       "117  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      3   \n",
       "118  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      4   \n",
       "119  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      5   \n",
       "120  https://www.semanticscholar.org/paper/BERT-for...  18     Title      0   \n",
       "121  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      0   \n",
       "122  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      1   \n",
       "123  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      2   \n",
       "124  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      3   \n",
       "125  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      4   \n",
       "126  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      5   \n",
       "127  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      6   \n",
       "128  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      7   \n",
       "129  https://www.semanticscholar.org/paper/BERT-wit...  19     Title      0   \n",
       "130  https://www.semanticscholar.org/paper/Understa...  20  Abstract      0   \n",
       "131  https://www.semanticscholar.org/paper/Understa...  20  Abstract      1   \n",
       "132  https://www.semanticscholar.org/paper/Understa...  20  Abstract      2   \n",
       "133  https://www.semanticscholar.org/paper/Understa...  20  Abstract      3   \n",
       "134  https://www.semanticscholar.org/paper/Understa...  20  Abstract      4   \n",
       "135  https://www.semanticscholar.org/paper/Understa...  20     Title      0   \n",
       "136  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      0   \n",
       "137  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      1   \n",
       "138  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      2   \n",
       "139  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      3   \n",
       "140  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      4   \n",
       "141  https://www.semanticscholar.org/paper/Simple-B...  21     Title      0   \n",
       "142  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      0   \n",
       "143  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      1   \n",
       "144  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      2   \n",
       "145  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      3   \n",
       "146  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      4   \n",
       "147  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      5   \n",
       "148  https://www.semanticscholar.org/paper/BERT-and...  22     Title      0   \n",
       "149  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      0   \n",
       "150  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      1   \n",
       "151  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      2   \n",
       "152  https://www.semanticscholar.org/paper/What-BER...  23     Title      0   \n",
       "153  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      0   \n",
       "154  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      1   \n",
       "155  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      2   \n",
       "156  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      3   \n",
       "157  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      4   \n",
       "158  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      5   \n",
       "159  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      6   \n",
       "160  https://www.semanticscholar.org/paper/Visualiz...  24     Title      0   \n",
       "161  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      0   \n",
       "162  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      1   \n",
       "163  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      2   \n",
       "164  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      3   \n",
       "165  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      4   \n",
       "166  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      5   \n",
       "167  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      6   \n",
       "168  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      7   \n",
       "169  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      8   \n",
       "170  https://www.semanticscholar.org/paper/Conditio...  25     Title      0   \n",
       "171  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      0   \n",
       "172  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      1   \n",
       "173  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      2   \n",
       "174  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      3   \n",
       "175  https://www.semanticscholar.org/paper/Small-an...  26     Title      0   \n",
       "176  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      0   \n",
       "177  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      1   \n",
       "178  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      2   \n",
       "179  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      3   \n",
       "180  https://www.semanticscholar.org/paper/Data-Aug...  27     Title      0   \n",
       "181  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      0   \n",
       "182  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      1   \n",
       "183  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      2   \n",
       "184  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      3   \n",
       "185  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      4   \n",
       "186  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      5   \n",
       "187  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      6   \n",
       "188  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      7   \n",
       "189  https://www.semanticscholar.org/paper/BERT-is-...  28     Title      0   \n",
       "190  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      0   \n",
       "191  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      1   \n",
       "192  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      2   \n",
       "193  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      3   \n",
       "194  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      4   \n",
       "195  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      5   \n",
       "196  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      6   \n",
       "197  https://www.semanticscholar.org/paper/How-Cont...  29     Title      0   \n",
       "198  https://www.semanticscholar.org/paper/How-Cont...  29     Title      1   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    We introduce a new language representation mod...   \n",
       "1    Unlike recent language representation models, ...   \n",
       "2    As a result, the pre-trained BERT model can be...   \n",
       "3    It obtains new state-of-the-art results on ele...   \n",
       "4    BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7    We present a replication study of BERT pretrai...   \n",
       "8    We find that BERT was significantly undertrain...   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12   RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18   DistilBERT, a distilled version of BERT: small...   \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20   We focus on one such model, BERT, and aim to q...   \n",
       "21   We find that the model represents the steps of...   \n",
       "22   Qualitative analysis reveals that the model ca...   \n",
       "23         BERT Rediscovers the Classical NLP Pipeline   \n",
       "24   Large pre-trained neural networks such as BERT...   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works, we propose metho...   \n",
       "27   BERT's attention heads exhibit patterns such a...   \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly, we propose an attention-based probing ...   \n",
       "31                             What Does BERT Look At?   \n",
       "32                     An Analysis of BERT's Attention   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34   In this paper, we describe a simple re-impleme...   \n",
       "35   Our system is the state of the art on the TREC...   \n",
       "36   The code to reproduce our results is available...   \n",
       "37                        Passage Re-ranking with BERT   \n",
       "38   I assess the extent to which the recently intr...   \n",
       "39   The BERT model performs remarkably well on all...   \n",
       "40                Assessing BERT's Syntactic Abilities   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42   A new release of BERT (Devlin, 2018) includes ...   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44   We compare mBERT with the best-published metho...   \n",
       "45   Additionally, we investigate the most effectiv...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47   Language model pre-training, such as BERT, has...   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method, the plenty o...   \n",
       "51   Moreover, we introduce a new two-stage learnin...   \n",
       "52   This framework ensures that TinyBERT can captu...   \n",
       "53   TinyBERT is empirically effective and achieves...   \n",
       "54   TinyBERT is also significantly better than sta...   \n",
       "55   TinyBERT: Distilling BERT for Natural Language...   \n",
       "56   BERT, a pre-trained Transformer model, has ach...   \n",
       "57   In this paper, we describe BERTSUM, a simple v...   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60         Fine-tune BERT for Extractive Summarization   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69   BERT Post-Training for Review Reading Comprehe...   \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state-of-the-art language model pre-train...   \n",
       "72   In this paper, we conduct exhaustive experimen...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74      How to Fine-Tune BERT for Text Classification?   \n",
       "75   We show that BERT (Devlin et al., 2018) is a M...   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77   We generate from BERT and find that it can pro...   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79   BERT has a Mouth, and It Must Speak: BERT as a...   \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82   We fine-tune the pre-trained model from BERT a...   \n",
       "83   Utilizing BERT for Aspect-Based Sentiment Anal...   \n",
       "84   Following recent successes in applying BERT to...   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88   Simple Applications of BERT for Ad Hoc Documen...   \n",
       "89   BERT-based architectures currently give state-...   \n",
       "90   In the current work, we focus on the interpret...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                  Revealing the Dark Secrets of BERT   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97   Recently, an upgraded version of BERT has been...   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99   The model was trained on the latest Chinese Wi...   \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101  The model is verified on various NLP tasks, ac...   \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover, we also examine the effectiveness of...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105  Pre-Training with Whole Word Masking for Chine...   \n",
       "106  BERT model has been successfully applied to op...   \n",
       "107  However, previous work trains BERT by viewing ...   \n",
       "108  To tackle this issue, we propose a multi-passa...   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular, on the OpenSQuAD dataset, our m...   \n",
       "113  Multi-passage BERT: A Globally Normalized BERT...   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116  Recently a new language representation model, ...   \n",
       "117  However, there has not been much effort on exp...   \n",
       "118  In this work, we propose a joint intent classi...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120  BERT for Joint Intent Classification and Slot ...   \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129  BERT with History Answer Embedding for Convers...   \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134  Analyses illustrate how BERT allocates its att...   \n",
       "135     Understanding the Behaviors of BERT in Ranking   \n",
       "136  We present simple BERT-based models for relati...   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140  Our models provide strong baselines for future...   \n",
       "141  Simple BERT Models for Relation Extraction and...   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147  By using PALs in parallel with BERT layers, we...   \n",
       "148  BERT and PALs: Projected Attention Layers for ...   \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151  As a case study, we apply these diagnostics to...   \n",
       "152  What BERT Is Not: Lessons from a New Suite of ...   \n",
       "153  Language model pre-training, such as BERT, has...   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper, we propose to visualize loss la...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine-tuning proce...   \n",
       "158  Second, the visualization results indicate tha...   \n",
       "159  Third, the lower layers of BERT are more invar...   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165  We retrofit BERT to conditional BERT by introd...   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168  The well trained conditional BERT can be appli...   \n",
       "169  Experiments on six various different text clas...   \n",
       "170           Conditional BERT Contextual Augmentation   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172  Starting from a public multilingual BERT check...   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175  Small and Practical BERT Models for Sequence L...   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178  We apply a stage-wise approach to fine tuning ...   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180  Data Augmentation for BERT Fine-Tuning in Open...   \n",
       "181  The BERT language model (LM) (Devlin et al., 2...   \n",
       "182                                     Petroni et al.   \n",
       "183  (2019) take this as evidence that BERT memoriz...   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185  More specifically, we show that BERT's precisi...   \n",
       "186  As a remedy, we propose E-BERT, an extension o...   \n",
       "187  E-BERT outperforms both BERT and ERNIE (Zhang ...   \n",
       "188  We take this as evidence that E-BERT is richer...   \n",
       "189  BERT is Not a Knowledge Base (Yet): Factual Kn...   \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However, just how contextual are the contextua...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196  In all layers of ELMo, BERT, and GPT-2, on ave...   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198  Comparing the Geometry of BERT, ELMo, and GPT-...   \n",
       "\n",
       "                                               split_0  \\\n",
       "0                                         We introduce   \n",
       "1       Unlike recent language representation models ,   \n",
       "2                      As a result , the pre - trained   \n",
       "3                                                        \n",
       "4                                                        \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7                    We present a replication study of   \n",
       "8                                         We find that   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12                                                       \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18                                                       \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20                                         We focus on   \n",
       "21                                        We find that   \n",
       "22                   Qualitative analysis reveals that   \n",
       "23                                                       \n",
       "24         Large pre - trained neural networks such as   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works , we propose meth...   \n",
       "27                                                       \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly , we propose an attention - based probi...   \n",
       "31                                           What Does   \n",
       "32                                      An Analysis of   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34                         In this paper , we describe   \n",
       "35                                                       \n",
       "36   The code to reproduce our results is available...   \n",
       "37                             Passage Re-ranking with   \n",
       "38                        I assess the extent to which   \n",
       "39                                                       \n",
       "40                                           Assessing   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42                                    A new release of   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44                                          We compare   \n",
       "45   Additionally , we investigate the most effecti...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47                Language model pre-training, such as   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method , the plenty ...   \n",
       "51   Moreover , we introduce a new two - stage lear...   \n",
       "52                         This framework ensures that   \n",
       "53                                                       \n",
       "54                                                       \n",
       "55                                                       \n",
       "56                                                       \n",
       "57                          In this paper, we describe   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60                                           Fine-tune   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69                                                       \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state - of - the - art language model pre...   \n",
       "72   In this paper , we conduct exhaustive experime...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74                                    How to Fine-Tune   \n",
       "75                                        We show that   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77                                    We generate from   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79                                                       \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82             We fine-tune the pre-trained model from   \n",
       "83                                           Utilizing   \n",
       "84              Following recent successes in applying   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88                              Simple Applications of   \n",
       "89                                                       \n",
       "90   In the current work , we focus on the interpre...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                       Revealing the Dark Secrets of   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97                   Recently , an upgraded version of   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99                                                       \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101                                                      \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover , we also examine the effectiveness o...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105   Pre-Training with Whole Word Masking for Chinese   \n",
       "106                                                      \n",
       "107                     However , previous work trains   \n",
       "108   To tackle this issue, we propose a multi-passage   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular , on the OpenSQuAD dataset , our...   \n",
       "113                                      Multi-passage   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116      Recently a new language representation model,   \n",
       "117  However , there has not been much effort on ex...   \n",
       "118  In this work , we propose a joint intent class...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120                                                      \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129                                                      \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134                            Analyses illustrate how   \n",
       "135                     Understanding the Behaviors of   \n",
       "136                                         We present   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140                                                      \n",
       "141                                             Simple   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147                     By using PALs in parallel with   \n",
       "148                                                      \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151    As a case study , we apply these diagnostics to   \n",
       "152                                               What   \n",
       "153            Language model pre - training , such as   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper , we propose to visualize loss l...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine - tuning pro...   \n",
       "158  Second , the visualization results indicate th...   \n",
       "159                        Third , the lower layers of   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163                                                      \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165                                        We retrofit   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168                       The well trained conditional   \n",
       "169  Experiments on six various different text clas...   \n",
       "170                                        Conditional   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172                Starting from a public multilingual   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175                                Small and Practical   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178      We apply a stage-wise approach to fine tuning   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180                              Data Augmentation for   \n",
       "181                                                      \n",
       "182                                     Petroni et al.   \n",
       "183                ( 2019 ) take this as evidence that   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185                   More specifically , we show that   \n",
       "186  As a remedy , we propose E - BERT , an extensi...   \n",
       "187                          E - BERT outperforms both   \n",
       "188  We take this as evidence that E - BERT is rich...   \n",
       "189                                                      \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However , just how contextual are the contextu...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196                            In all layers of ELMo ,   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198                          Comparing the Geometry of   \n",
       "\n",
       "                                               split_1  \\\n",
       "0    a new language representation model called BER...   \n",
       "1                                                 BERT   \n",
       "2                                                 BERT   \n",
       "3                                                   It   \n",
       "4                                                BERT:   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7                                                 BERT   \n",
       "8                                                 BERT   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12                                            RoBERTa:   \n",
       "13                                                       \n",
       "14                                         DistilBERT,   \n",
       "15                                        a BERT model   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18                                         DistilBERT,   \n",
       "19                                                       \n",
       "20                               one such model , BERT   \n",
       "21                                           the model   \n",
       "22                                           the model   \n",
       "23                                                BERT   \n",
       "24                                                BERT   \n",
       "25                                                       \n",
       "26                                                BERT   \n",
       "27                                             BERT 's   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                             BERT 's   \n",
       "31                                                BERT   \n",
       "32                                             BERT 's   \n",
       "33                                                BERT   \n",
       "34   a simple re - implementation of BERT for query...   \n",
       "35                                          Our system   \n",
       "36                                                       \n",
       "37                                                BERT   \n",
       "38                  the recently introduced BERT model   \n",
       "39                                      The BERT model   \n",
       "40                                              BERT's   \n",
       "41                                                       \n",
       "42                                                BERT   \n",
       "43                                               mBERT   \n",
       "44                                               mBERT   \n",
       "45                                               mBERT   \n",
       "46                                                BERT   \n",
       "47                                               BERT,   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                            a small student TinyBERT   \n",
       "51                                            TinyBERT   \n",
       "52                                            TinyBERT   \n",
       "53                                            TinyBERT   \n",
       "54                                            TinyBERT   \n",
       "55                                           TinyBERT:   \n",
       "56                                               BERT,   \n",
       "57                                            BERTSUM,   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                                                BERT   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65                     the popular language model BERT   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69                                                BERT   \n",
       "70                                                       \n",
       "71   BERT ( Bidirectional Encoder Representations f...   \n",
       "72                                                BERT   \n",
       "73                                                       \n",
       "74                                                BERT   \n",
       "75                                                BERT   \n",
       "76                                                BERT   \n",
       "77                                                BERT   \n",
       "78                                                BERT   \n",
       "79                                                BERT   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82                                                BERT   \n",
       "83                                                BERT   \n",
       "84                                                BERT   \n",
       "85                                                BERT   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                                                BERT   \n",
       "89                                          BERT-based   \n",
       "90                                                BERT   \n",
       "91                                              BERT's   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                BERT   \n",
       "95                                                BERT   \n",
       "96                                              (BERT)   \n",
       "97                                                BERT   \n",
       "98                                                       \n",
       "99                                           The model   \n",
       "100                                       Chinese BERT   \n",
       "101                                          The model   \n",
       "102                                                      \n",
       "103                                               BERT   \n",
       "104                                                      \n",
       "105                                               BERT   \n",
       "106                                               BERT   \n",
       "107                                               BERT   \n",
       "108                                               BERT   \n",
       "109                                                      \n",
       "110                                               BERT   \n",
       "111                                               BERT   \n",
       "112                                               BERT   \n",
       "113                                              BERT:   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116                                               BERT   \n",
       "117                                               BERT   \n",
       "118                                               BERT   \n",
       "119                                                      \n",
       "120                                               BERT   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125                                               BERT   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129                                               BERT   \n",
       "130                                               BERT   \n",
       "131                                               BERT   \n",
       "132                                               BERT   \n",
       "133                                               BERT   \n",
       "134                                               BERT   \n",
       "135                                               BERT   \n",
       "136  simple BERT - based models for relation extrac...   \n",
       "137                                                      \n",
       "138                                         BERT-based   \n",
       "139                                               BERT   \n",
       "140                                         Our models   \n",
       "141                                               BERT   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145                                               BERT   \n",
       "146                                                      \n",
       "147                                               BERT   \n",
       "148                                               BERT   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151                             the popular BERT model   \n",
       "152                                               BERT   \n",
       "153                                               BERT   \n",
       "154                                                      \n",
       "155                                               BERT   \n",
       "156                                                      \n",
       "157                                               BERT   \n",
       "158                                               BERT   \n",
       "159                                               BERT   \n",
       "160                                               BERT   \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164                                               BERT   \n",
       "165                                               BERT   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168                                               BERT   \n",
       "169                                                      \n",
       "170                                               BERT   \n",
       "171                                                      \n",
       "172                                               BERT   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                                               BERT   \n",
       "176                                               BERT   \n",
       "177                                                      \n",
       "178                                               BERT   \n",
       "179                                                      \n",
       "180                                               BERT   \n",
       "181                     The BERT language model ( LM )   \n",
       "182                                                      \n",
       "183                                               BERT   \n",
       "184                                               BERT   \n",
       "185                                            BERT 's   \n",
       "186                                               BERT   \n",
       "187                                               BERT   \n",
       "188                                               BERT   \n",
       "189                                               BERT   \n",
       "190                                                      \n",
       "191                                               BERT   \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196                                               BERT   \n",
       "197                                                      \n",
       "198                                              BERT,   \n",
       "\n",
       "                                               split_2  \\\n",
       "0                                                        \n",
       "1    is designed to pre - train deep bidirectional ...   \n",
       "2    model can be fine - tuned with just one additi...   \n",
       "3    obtains new state - of - the - art results on ...   \n",
       "4    Pre-training of Deep Bidirectional Transformer...   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7    pretraining ( Devlin et al . , 2019 ) that car...   \n",
       "8    was significantly undertrained , and can match...   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12      A Robustly Optimized BERT Pretraining Approach   \n",
       "13                                                       \n",
       "14   which can then be fine-tuned with good perform...   \n",
       "15   by 40 % , while retaining 97 % of its language...   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18   a distilled version of BERT: smaller, faster, ...   \n",
       "19                                                       \n",
       "20   , and aim to quantify where linguistic informa...   \n",
       "21   represents the steps of the traditional NLP pi...   \n",
       "22   can and often does adjust this pipeline dynami...   \n",
       "23              Rediscovers the Classical NLP Pipeline   \n",
       "24   have had great recent success in NLP , motivat...   \n",
       "25                                                       \n",
       "26                                                       \n",
       "27   attention heads exhibit patterns such as atten...   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                           attention   \n",
       "31                                             Look At   \n",
       "32                                                       \n",
       "33   (Devlin et al., 2018), have achieved impressiv...   \n",
       "34                                                       \n",
       "35   is the state of the art on the TREC - CAR data...   \n",
       "36                                                       \n",
       "37                                                       \n",
       "38   captures English syntactic phenomena , using (...   \n",
       "39               performs remarkably well on all cases   \n",
       "40                                 Syntactic Abilities   \n",
       "41                                                       \n",
       "42   (Devlin, 2018) includes a model simultaneously...   \n",
       "43   (multilingual) as a zero shot language transfe...   \n",
       "44   with the best - published methods for zero - s...   \n",
       "45   in this manner , determine to what extent mBER...   \n",
       "46                                                       \n",
       "47   has significantly improved the performances of...   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                                                       \n",
       "51   , which performs transformer distillation at b...   \n",
       "52   can capture both the general - domain and task...   \n",
       "53   is empirically effective and achieves comparab...   \n",
       "54   is also significantly better than state - of -...   \n",
       "55   Distilling BERT for Natural Language Understan...   \n",
       "56   a pre-trained Transformer model, has achieved ...   \n",
       "57   a simple variant of BERT, for extractive summa...   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                        for Extractive Summarization   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65   to enhance the performance of fine - tuning of...   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69   Post-Training for Review Reading Comprehension...   \n",
       "70                                                       \n",
       "71   has achieved amazing results in many language ...   \n",
       "72   on text classification task and provide a gene...   \n",
       "73                                                       \n",
       "74                            for Text Classification?   \n",
       "75   ( Devlin et al . , 2018 ) is a Markov random f...   \n",
       "76                                                       \n",
       "77   and find that it can produce high - quality , ...   \n",
       "78   generates sentences that are more diverse but ...   \n",
       "79   has a Mouth , and It Must Speak : BERT as a Ma...   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82   and achieve new state-of-the-art results on Se...   \n",
       "83   for Aspect-Based Sentiment Analysis via Constr...   \n",
       "84   to question answering , we explore simple appl...   \n",
       "85                              was designed to handle   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                       for Ad Hoc Document Retrieval   \n",
       "89   architectures currently give state-of-the-art ...   \n",
       "90                                                       \n",
       "91                                              heads.   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                       \n",
       "95                                                       \n",
       "96   has shown marvelous improvements across variou...   \n",
       "97   has been released with Whole Word Masking ( WW...   \n",
       "98                                                       \n",
       "99    was trained on the latest Chinese Wikipedia dump   \n",
       "100  without changing any neural architecture or ev...   \n",
       "101  is verified on various NLP tasks , across sent...   \n",
       "102                                                      \n",
       "103                               , ERNIE , BERT - wwm   \n",
       "104                                                      \n",
       "105                                                      \n",
       "106  model has been successfully applied to open - ...   \n",
       "107  by viewing passages corresponding to the same ...   \n",
       "108  model to globally normalize answer scores acro...   \n",
       "109                                                      \n",
       "110                               gains additional 2%.   \n",
       "111  outperforms all state-of-the-art models on all...   \n",
       "112  models , and 5.8 % EM and 6.5 % $ F_1 $ over B...   \n",
       "113  A Globally Normalized BERT Model for Open-doma...   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116  (Bidirectional Encoder Representations from Tr...   \n",
       "117                 for natural language understanding   \n",
       "118                                                      \n",
       "119                                                      \n",
       "120   for Joint Intent Classification and Slot Filling   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125  (Bidirectional Encoder Representations from Tr...   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129  with History Answer Embedding for Conversation...   \n",
       "130                                   in ranking tasks   \n",
       "131  and fine-tune it on two ranking tasks: MS MARC...   \n",
       "132  in question - answering focused passage rankin...   \n",
       "133  pre-trained on surrounding contexts and the ne...   \n",
       "134  allocates its attentions between query - docum...   \n",
       "135                                         in Ranking   \n",
       "136                                                      \n",
       "137                                                      \n",
       "138    model can achieve state-of-the-art performance.   \n",
       "139                                    in this manner.   \n",
       "140                provide strong baselines for future   \n",
       "141  Models for Relation Extraction and Semantic Ro...   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145  model on the GLUE benchmark, and how to best a...   \n",
       "146                                                      \n",
       "147  layers, we match the performance of fine-tuned...   \n",
       "148  and PALs: Projected Attention Layers for Effic...   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151  , finding that it can generally distinguish go...   \n",
       "152  Is Not: Lessons from a New Suite of Psycholing...   \n",
       "153  , has achieved remarkable results in many NLP ...   \n",
       "154                                                      \n",
       "155                               on specific datasets   \n",
       "156                                                      \n",
       "157  is highly over - parameterized for downstream ...   \n",
       "158  tends to generalize better because of the flat...   \n",
       "159  are more invariant during fine - tuning , whic...   \n",
       "160                                                      \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  demonstrates that a deep bidirectional languag...   \n",
       "164                           contextual augmentation.   \n",
       "165  to conditional BERT by introducing a new condi...   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168  can be applied to enhance contextual augmentat...   \n",
       "169                                                      \n",
       "170                            Contextual Augmentation   \n",
       "171                                                      \n",
       "172  checkpoint, our final model is 6x smaller and ...   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                       Models for Sequence Labeling   \n",
       "176  reader was found to be very effective for ques...   \n",
       "177                                                      \n",
       "178  on multiple datasets, starting with data that ...   \n",
       "179                                                      \n",
       "180      Fine-Tuning in Open-Domain Question Answering   \n",
       "181  ( Devlin et al . , 2019 ) is surprisingly good...   \n",
       "182                                                      \n",
       "183  memorizes factual knowledge during pre - training   \n",
       "184  is partly due to reasoning about ( the surface...   \n",
       "185  precision drops dramatically when we filter ce...   \n",
       "186  that replaces entity mentions with symbolic en...   \n",
       "187  and ERNIE ( Zhang et al . , 2019 ) on hard - t...   \n",
       "188                                            and E -   \n",
       "189  is Not a Knowledge Base (Yet): Factual Knowled...   \n",
       "190                                                      \n",
       "191                                                      \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196  , and GPT-2 , on average , less than 5 % of th...   \n",
       "197                                                      \n",
       "198                         ELMo, and GPT-2 Embeddings   \n",
       "\n",
       "                                          split_tokens split_anchor_span  \\\n",
       "0    [We, introduce, a, new, language, representati...           (2, 18)   \n",
       "1    [Unlike, recent, language, representation, mod...            (6, 7)   \n",
       "2    [As, a, result, ,, the, pre, -, trained, BERT,...            (8, 9)   \n",
       "3    [It, obtains, new, state, -, of, -, the, -, ar...            (0, 1)   \n",
       "4    [BERT:, Pre-training, of, Deep, Bidirectional,...            (0, 1)   \n",
       "5    [Language, model, pretraining, has, led, to, s...              None   \n",
       "6    [Training, is, computationally, expensive,, of...              None   \n",
       "7    [We, present, a, replication, study, of, BERT,...            (6, 7)   \n",
       "8    [We, find, that, BERT, was, significantly, und...            (3, 4)   \n",
       "9    [Our, best, model, achieves, state-of-the-art,...              None   \n",
       "10   [These, results, highlight, the, importance, o...              None   \n",
       "11              [We, release, our, models, and, code.]              None   \n",
       "12   [RoBERTa:, A, Robustly, Optimized, BERT, Pretr...            (0, 1)   \n",
       "13   [As, Transfer, Learning, from, large-scale, pr...              None   \n",
       "14   [In, this, work,, we, propose, a, method, to, ...          (16, 17)   \n",
       "15   [While, most, prior, work, investigated, the, ...          (37, 40)   \n",
       "16   [To, leverage, the, inductive, biases, learned...              None   \n",
       "17   [Our, smaller,, faster, and, lighter, model, i...              None   \n",
       "18   [DistilBERT,, a, distilled, version, of, BERT:...            (0, 1)   \n",
       "19   [Pre-trained, text, encoders, have, rapidly, a...              None   \n",
       "20   [We, focus, on, one, such, model, ,, BERT, ,, ...            (3, 8)   \n",
       "21   [We, find, that, the, model, represents, the, ...            (3, 5)   \n",
       "22   [Qualitative, analysis, reveals, that, the, mo...            (4, 6)   \n",
       "23   [BERT, Rediscovers, the, Classical, NLP, Pipel...            (0, 1)   \n",
       "24   [Large, pre, -, trained, neural, networks, suc...            (8, 9)   \n",
       "25   [Most, recent, analysis, has, focused, on, mod...              None   \n",
       "26   [Complementary, to, these, works, ,, we, propo...          (22, 23)   \n",
       "27   [BERT, 's, attention, heads, exhibit, patterns...            (0, 2)   \n",
       "28   [We, further, show, that, certain, attention, ...              None   \n",
       "29   [For, example,, we, find, heads, that, attend,...              None   \n",
       "30   [Lastly, ,, we, propose, an, attention, -, bas...          (23, 25)   \n",
       "31                        [What, Does, BERT, Look, At]            (2, 3)   \n",
       "32                        [An, Analysis, of, BERT, 's]            (3, 5)   \n",
       "33   [Recently,, neural, models, pretrained, on, a,...          (23, 24)   \n",
       "34   [In, this, paper, ,, we, describe, a, simple, ...           (6, 21)   \n",
       "35   [Our, system, is, the, state, of, the, art, on...            (0, 2)   \n",
       "36   [The, code, to, reproduce, our, results, is, a...              None   \n",
       "37                   [Passage, Re-ranking, with, BERT]            (3, 4)   \n",
       "38   [I, assess, the, extent, to, which, the, recen...           (6, 11)   \n",
       "39   [The, BERT, model, performs, remarkably, well,...            (0, 3)   \n",
       "40           [Assessing, BERT's, Syntactic, Abilities]            (1, 2)   \n",
       "41   [Pretrained, contextual, representation, model...              None   \n",
       "42   [A, new, release, of, BERT, (Devlin,, 2018), i...            (4, 5)   \n",
       "43   [This, paper, explores, the, broader, cross-li...            (8, 9)   \n",
       "44   [We, compare, mBERT, with, the, best, -, publi...            (2, 3)   \n",
       "45   [Additionally, ,, we, investigate, the, most, ...          (10, 11)   \n",
       "46   [Beto,, Bentz,, Becas:, The, Surprising, Cross...            (8, 9)   \n",
       "47   [Language, model, pre-training,, such, as, BER...            (5, 6)   \n",
       "48   [However,, pre-trained, language, models, are,...              None   \n",
       "49   [To, accelerate, inference, and, reduce, model...              None   \n",
       "50   [By, leveraging, this, new, KD, method, ,, the...          (22, 26)   \n",
       "51   [Moreover, ,, we, introduce, a, new, two, -, s...          (12, 13)   \n",
       "52   [This, framework, ensures, that, TinyBERT, can...            (4, 5)   \n",
       "53   [TinyBERT, is, empirically, effective, and, ac...            (0, 1)   \n",
       "54   [TinyBERT, is, also, significantly, better, th...            (0, 1)   \n",
       "55   [TinyBERT:, Distilling, BERT, for, Natural, La...            (0, 1)   \n",
       "56   [BERT,, a, pre-trained, Transformer, model,, h...            (0, 1)   \n",
       "57   [In, this, paper,, we, describe, BERTSUM,, a, ...            (5, 6)   \n",
       "58   [Our, system, is, the, state, of, the, art, on...              None   \n",
       "59   [The, codes, to, reproduce, our, results, are,...              None   \n",
       "60   [Fine-tune, BERT, for, Extractive, Summarization]            (1, 2)   \n",
       "61   [Question-answering, plays, an, important, rol...              None   \n",
       "62   [Inspired, by, the, recent, success, of, machi...              None   \n",
       "63   [To, the, best, of, our, knowledge,, no, exist...              None   \n",
       "64   [In, this, work,, we, first, build, an, RRC, d...              None   \n",
       "65   [Since, ReviewRC, has, limited, training, exam...          (29, 34)   \n",
       "66   [To, show, the, generality, of, the, approach,...              None   \n",
       "67   [Experimental, results, demonstrate, that, the...              None   \n",
       "68   [The, datasets, and, code, are, available, at,...              None   \n",
       "69   [BERT, Post-Training, for, Review, Reading, Co...            (0, 1)   \n",
       "70   [Language, model, pre-training, has, proven, t...              None   \n",
       "71   [As, a, state, -, of, -, the, -, art, language...          (16, 24)   \n",
       "72   [In, this, paper, ,, we, conduct, exhaustive, ...          (16, 17)   \n",
       "73   [Finally,, the, proposed, solution, obtains, n...              None   \n",
       "74   [How, to, Fine-Tune, BERT, for, Text, Classifi...            (3, 4)   \n",
       "75   [We, show, that, BERT, (, Devlin, et, al, ., ,...            (3, 4)   \n",
       "76   [This, formulation, gives, way, to, a, natural...          (12, 13)   \n",
       "77   [We, generate, from, BERT, and, find, that, it...            (3, 4)   \n",
       "78   [Compared, to, the, generations, of, a, tradit...          (15, 16)   \n",
       "79   [BERT, has, a, Mouth, ,, and, It, Must, Speak,...            (0, 1)   \n",
       "80   [Aspect-based, sentiment, analysis, (ABSA),, w...              None   \n",
       "81   [In, this, paper,, we, construct, an, auxiliar...              None   \n",
       "82   [We, fine-tune, the, pre-trained, model, from,...            (6, 7)   \n",
       "83   [Utilizing, BERT, for, Aspect-Based, Sentiment...            (1, 2)   \n",
       "84   [Following, recent, successes, in, applying, B...            (5, 6)   \n",
       "85   [This, required, confronting, the, challenge, ...          (17, 18)   \n",
       "86   [We, address, this, issue, by, applying, infer...              None   \n",
       "87   [Experiments, on, TREC, microblog, and, newswi...              None   \n",
       "88   [Simple, Applications, of, BERT, for, Ad, Hoc,...            (3, 4)   \n",
       "89   [BERT-based, architectures, currently, give, s...            (0, 1)   \n",
       "90   [In, the, current, work, ,, we, focus, on, the...          (24, 25)   \n",
       "91   [Using, a, subset, of, GLUE, tasks, and, a, se...          (31, 32)   \n",
       "92   [Our, findings, suggest, that, there, is, a, l...              None   \n",
       "93   [While, different, heads, consistently, use, t...              None   \n",
       "94   [We, show, that, manually, disabling, attentio...          (20, 21)   \n",
       "95           [Revealing, the, Dark, Secrets, of, BERT]            (5, 6)   \n",
       "96   [Bidirectional, Encoder, Representations, from...            (5, 6)   \n",
       "97   [Recently, ,, an, upgraded, version, of, BERT,...            (6, 7)   \n",
       "98   [In, this, technical, report,, we, adapt, whol...              None   \n",
       "99   [The, model, was, trained, on, the, latest, Ch...            (0, 2)   \n",
       "100  [We, aim, to, provide, easy, extensibility, an...          (10, 12)   \n",
       "101  [The, model, is, verified, on, various, NLP, t...            (0, 2)   \n",
       "102  [Experimental, results, on, these, datasets, s...              None   \n",
       "103  [Moreover, ,, we, also, examine, the, effectiv...          (14, 15)   \n",
       "104  [We, release, the, pre-trained, model, (both, ...              None   \n",
       "105  [Pre-Training, with, Whole, Word, Masking, for...            (7, 8)   \n",
       "106  [BERT, model, has, been, successfully, applied...            (0, 1)   \n",
       "107  [However, ,, previous, work, trains, BERT, by,...            (5, 6)   \n",
       "108  [To, tackle, this, issue,, we, propose, a, mul...            (8, 9)   \n",
       "109  [In, addition,, we, find, that, splitting, art...              None   \n",
       "110  [By, leveraging, a, passage, ranker, to, selec...          (10, 11)   \n",
       "111  [Experiments, on, four, standard, benchmarks, ...           (9, 10)   \n",
       "112  [In, particular, ,, on, the, OpenSQuAD, datase...          (24, 25)   \n",
       "113  [Multi-passage, BERT:, A, Globally, Normalized...            (1, 2)   \n",
       "114  [Intent, classification, and, slot, filling, a...              None   \n",
       "115  [They, often, suffer, from, small-scale, human...              None   \n",
       "116  [Recently, a, new, language, representation, m...            (6, 7)   \n",
       "117  [However, ,, there, has, not, been, much, effo...          (10, 11)   \n",
       "118  [In, this, work, ,, we, propose, a, joint, int...          (16, 17)   \n",
       "119  [Experimental, results, demonstrate, that, our...              None   \n",
       "120  [BERT, for, Joint, Intent, Classification, and...            (0, 1)   \n",
       "121  [Conversational, search, is, an, emerging, top...              None   \n",
       "122  [One, of, the, major, challenges, to, multi-tu...              None   \n",
       "123  [Existing, methods, either, prepend, history, ...              None   \n",
       "124  [We, propose, a, conceptually, simple, yet, hi...              None   \n",
       "125  [It, enables, seamless, integration, of, conve...          (16, 17)   \n",
       "126  [We, first, explain, our, view, that, ConvQA, ...              None   \n",
       "127  [We, further, demonstrate, the, effectiveness,...              None   \n",
       "128  [Finally,, we, analyze, the, impact, of, diffe...              None   \n",
       "129  [BERT, with, History, Answer, Embedding, for, ...            (0, 1)   \n",
       "130  [This, paper, studies, the, performances, and,...            (8, 9)   \n",
       "131  [We, explore, several, different, ways, to, le...           (9, 10)   \n",
       "132  [Experimental, results, on, MS, MARCO, demonst...          (10, 11)   \n",
       "133  [Experimental, results, on, TREC, show, the, g...           (9, 10)   \n",
       "134  [Analyses, illustrate, how, BERT, allocates, i...            (3, 4)   \n",
       "135  [Understanding, the, Behaviors, of, BERT, in, ...            (4, 5)   \n",
       "136  [We, present, simple, BERT, -, based, models, ...           (2, 14)   \n",
       "137  [In, recent, years,, state-of-the-art, perform...              None   \n",
       "138  [In, this, paper,, extensive, experiments, on,...          (20, 21)   \n",
       "139  [To, our, knowledge,, we, are, the, first, to,...          (10, 11)   \n",
       "140  [Our, models, provide, strong, baselines, for,...            (0, 2)   \n",
       "141  [Simple, BERT, Models, for, Relation, Extracti...            (1, 2)   \n",
       "142  [Multi-task, learning, allows, the, sharing, o...              None   \n",
       "143  [In, natural, language, processing, several, r...              None   \n",
       "144  [These, results, are, based, on, fine-tuning, ...              None   \n",
       "145  [We, explore, the, multi-task, learning, setti...           (9, 10)   \n",
       "146  [We, introduce, new, adaptation, modules,, PAL...              None   \n",
       "147  [By, using, PALs, in, parallel, with, BERT, la...            (6, 7)   \n",
       "148  [BERT, and, PALs:, Projected, Attention, Layer...            (0, 1)   \n",
       "149  [Pre-training, by, language, modeling, has, be...              None   \n",
       "150  [In, this, paper, we, introduce, a, suite, of,...              None   \n",
       "151  [As, a, case, study, ,, we, apply, these, diag...          (10, 14)   \n",
       "152  [What, BERT, Is, Not:, Lessons, from, a, New, ...            (1, 2)   \n",
       "153  [Language, model, pre, -, training, ,, such, a...            (8, 9)   \n",
       "154  [However,, it, is, unclear, why, the, pre-trai...              None   \n",
       "155  [In, this, paper, ,, we, propose, to, visualiz...          (17, 18)   \n",
       "156  [First,, we, find, that, pre-training, reaches...              None   \n",
       "157  [We, also, demonstrate, that, the, fine, -, tu...          (16, 17)   \n",
       "158  [Second, ,, the, visualization, results, indic...          (10, 11)   \n",
       "159  [Third, ,, the, lower, layers, of, BERT, are, ...            (6, 7)   \n",
       "160  [Visualizing, and, Understanding, the, Effecti...            (6, 7)   \n",
       "161  [Data, augmentation, methods, are, often, appl...              None   \n",
       "162  [Recently, proposed, contextual, augmentation,...              None   \n",
       "163  [Bidirectional, Encoder, Representations, from...            (0, 8)   \n",
       "164  [We, propose, a, novel, data, augmentation, me...          (12, 13)   \n",
       "165  [We, retrofit, BERT, to, conditional, BERT, by...            (2, 3)   \n",
       "166  [In, our, paper,, “conditional, masked, langua...              None   \n",
       "167                                            [task.]              None   \n",
       "168  [The, well, trained, conditional, BERT, can, b...            (4, 5)   \n",
       "169  [Experiments, on, six, various, different, tex...              None   \n",
       "170      [Conditional, BERT, Contextual, Augmentation]            (1, 2)   \n",
       "171  [We, propose, a, practical, scheme, to, train,...              None   \n",
       "172  [Starting, from, a, public, multilingual, BERT...            (5, 6)   \n",
       "173  [We, show, that, our, model, especially, outpe...              None   \n",
       "174  [We, showcase, the, effectiveness, of, our, me...              None   \n",
       "175  [Small, and, Practical, BERT, Models, for, Seq...            (3, 4)   \n",
       "176  [Recently,, a, simple, combination, of, passag...          (13, 14)   \n",
       "177  [In, this, paper,, we, present, a, data, augme...              None   \n",
       "178  [We, apply, a, stage-wise, approach, to, fine,...            (8, 9)   \n",
       "179  [Experimental, results, show, large, gains, in...              None   \n",
       "180  [Data, Augmentation, for, BERT, Fine-Tuning, i...            (3, 4)   \n",
       "181  [The, BERT, language, model, (, LM, ), (, Devl...            (0, 7)   \n",
       "182                                 [Petroni, et, al.]              None   \n",
       "183  [(, 2019, ), take, this, as, evidence, that, B...            (8, 9)   \n",
       "184  [We, take, issue, with, this, interpretation, ...          (12, 13)   \n",
       "185  [More, specifically, ,, we, show, that, BERT, ...            (6, 8)   \n",
       "186  [As, a, remedy, ,, we, propose, E, -, BERT, ,,...          (13, 14)   \n",
       "187  [E, -, BERT, outperforms, both, BERT, and, ERN...            (5, 6)   \n",
       "188  [We, take, this, as, evidence, that, E, -, BER...          (22, 23)   \n",
       "189  [BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...            (0, 1)   \n",
       "190  [Replacing, static, word, embeddings, with, co...              None   \n",
       "191  [However, ,, just, how, contextual, are, the, ...          (16, 17)   \n",
       "192  [Are, there, infinitely, many, context-specifi...              None   \n",
       "193  [For, one,, we, find, that, the, contextualize...              None   \n",
       "194  [While, representations, of, the, same, word, ...              None   \n",
       "195  [This, suggests, that, upper, layers, of, cont...              None   \n",
       "196  [In, all, layers, of, ELMo, ,, BERT, ,, and, G...            (6, 7)   \n",
       "197  [How, Contextual, are, Contextualized, Word, R...              None   \n",
       "198  [Comparing, the, Geometry, of, BERT,, ELMo,, a...            (4, 5)   \n",
       "\n",
       "            group  \n",
       "0                  \n",
       "1              is  \n",
       "2              be  \n",
       "3         obtains  \n",
       "4                  \n",
       "5                  \n",
       "6                  \n",
       "7        measures  \n",
       "8             was  \n",
       "9                  \n",
       "10                 \n",
       "11                 \n",
       "12                 \n",
       "13                 \n",
       "14             be  \n",
       "15      retaining  \n",
       "16                 \n",
       "17                 \n",
       "18                 \n",
       "19                 \n",
       "20       quantify  \n",
       "21     represents  \n",
       "22           does  \n",
       "23                 \n",
       "24           have  \n",
       "25                 \n",
       "26                 \n",
       "27        exhibit  \n",
       "28                 \n",
       "29                 \n",
       "30                 \n",
       "31                 \n",
       "32                 \n",
       "33           have  \n",
       "34                 \n",
       "35             is  \n",
       "36                 \n",
       "37                 \n",
       "38          using  \n",
       "39                 \n",
       "40                 \n",
       "41                 \n",
       "42       includes  \n",
       "43       covering  \n",
       "44      published  \n",
       "45    generalizes  \n",
       "46                 \n",
       "47            has  \n",
       "48                 \n",
       "49                 \n",
       "50                 \n",
       "51       performs  \n",
       "52        capture  \n",
       "53             is  \n",
       "54             is  \n",
       "55                 \n",
       "56            has  \n",
       "57                 \n",
       "58                 \n",
       "59                 \n",
       "60                 \n",
       "61                 \n",
       "62                 \n",
       "63                 \n",
       "64                 \n",
       "65        enhance  \n",
       "66                 \n",
       "67                 \n",
       "68                 \n",
       "69                 \n",
       "70                 \n",
       "71            has  \n",
       "72        provide  \n",
       "73                 \n",
       "74                 \n",
       "75             is  \n",
       "76                 \n",
       "77           find  \n",
       "78      generates  \n",
       "79            has  \n",
       "80                 \n",
       "81                 \n",
       "82        achieve  \n",
       "83                 \n",
       "84      answering  \n",
       "85            was  \n",
       "86                 \n",
       "87                 \n",
       "88                 \n",
       "89           give  \n",
       "90                 \n",
       "91                 \n",
       "92                 \n",
       "93                 \n",
       "94                 \n",
       "95                 \n",
       "96            has  \n",
       "97            has  \n",
       "98                 \n",
       "99            was  \n",
       "100      changing  \n",
       "101            is  \n",
       "102                \n",
       "103                \n",
       "104                \n",
       "105                \n",
       "106           has  \n",
       "107       viewing  \n",
       "108     normalize  \n",
       "109                \n",
       "110                \n",
       "111   outperforms  \n",
       "112         based  \n",
       "113                \n",
       "114                \n",
       "115                \n",
       "116   facilitates  \n",
       "117                \n",
       "118                \n",
       "119                \n",
       "120                \n",
       "121                \n",
       "122                \n",
       "123                \n",
       "124                \n",
       "125                \n",
       "126                \n",
       "127                \n",
       "128                \n",
       "129                \n",
       "130       ranking  \n",
       "131       ranking  \n",
       "132       ranking  \n",
       "133   surrounding  \n",
       "134     allocates  \n",
       "135       Ranking  \n",
       "136                \n",
       "137                \n",
       "138       achieve  \n",
       "139                \n",
       "140       provide  \n",
       "141      Labeling  \n",
       "142                \n",
       "143                \n",
       "144                \n",
       "145          best  \n",
       "146                \n",
       "147         match  \n",
       "148     Projected  \n",
       "149                \n",
       "150                \n",
       "151       finding  \n",
       "152            Is  \n",
       "153           has  \n",
       "154                \n",
       "155                \n",
       "156                \n",
       "157            is  \n",
       "158         tends  \n",
       "159           are  \n",
       "160                \n",
       "161                \n",
       "162                \n",
       "163  demonstrates  \n",
       "164                \n",
       "165   conditional  \n",
       "166                \n",
       "167                \n",
       "168            be  \n",
       "169                \n",
       "170                \n",
       "171                \n",
       "172            is  \n",
       "173                \n",
       "174                \n",
       "175                \n",
       "176           was  \n",
       "177                \n",
       "178      starting  \n",
       "179                \n",
       "180                \n",
       "181            is  \n",
       "182                \n",
       "183     memorizes  \n",
       "184            is  \n",
       "185        filter  \n",
       "186      replaces  \n",
       "187                \n",
       "188                \n",
       "189            is  \n",
       "190                \n",
       "191                \n",
       "192                \n",
       "193                \n",
       "194                \n",
       "195                \n",
       "196            be  \n",
       "197                \n",
       "198                "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group on the first verb that comes after the anchor point, using NLTK part-of-speech tagging\n",
    "def group_first_verb(row):\n",
    "    tokens = [nltk.word_tokenize(row['split_0']), \n",
    "              nltk.word_tokenize(row['split_1']),\n",
    "              nltk.word_tokenize(row['split_2'])]\n",
    "    tokens_pos = nltk.pos_tag([item for sublist in tokens for item in sublist])\n",
    "    verb = ''\n",
    "    for i in range(len(tokens[0])+len(tokens[1]), len(tokens_pos)):\n",
    "        if tokens_pos[i][1].startswith('V'):\n",
    "            verb = tokens_pos[i][0]\n",
    "            break\n",
    "    output = [verb]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_first_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>split_0</th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>split_anchor_span</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>We introduce</td>\n",
       "      <td>a new language representation model called BER...</td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>(2, 18)</td>\n",
       "      <td>stands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Unlike recent language representation models, ...</td>\n",
       "      <td>Unlike recent language representation models ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre - train deep bidirectional ...</td>\n",
       "      <td>[Unlike, recent, language, representation, mod...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>designed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a result, the pre-trained BERT model can be...</td>\n",
       "      <td>As a result , the pre - trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine - tuned with just one additi...</td>\n",
       "      <td>[As, a, result, ,, the, pre, -, trained, BERT,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>conceptually</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td>It</td>\n",
       "      <td>obtains new state - of - the - art results on ...</td>\n",
       "      <td>[It, obtains, new, state, -, of, -, the, -, ar...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>absolute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "      <td>[BERT:, Pre-training, of, Deep, Bidirectional,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>None</td>\n",
       "      <td>led</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Training, is, computationally, expensive,, of...</td>\n",
       "      <td>None</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We present a replication study of BERT pretrai...</td>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining ( Devlin et al . , 2019 ) that car...</td>\n",
       "      <td>[We, present, a, replication, study, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>measures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained , and can match...</td>\n",
       "      <td>[We, find, that, BERT, was, significantly, und...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>We</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, best, model, achieves, state-of-the-art,...</td>\n",
       "      <td>None</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, highlight, the, importance, o...</td>\n",
       "      <td>None</td>\n",
       "      <td>These</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, our, models, and, code.]</td>\n",
       "      <td>None</td>\n",
       "      <td>release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "      <td>[RoBERTa:, A, Robustly, Optimized, BERT, Pretr...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[As, Transfer, Learning, from, large-scale, pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>prevalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "      <td>[In, this, work,, we, propose, a, method, to, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>a BERT model</td>\n",
       "      <td>by 40 % , while retaining 97 % of its language...</td>\n",
       "      <td>[While, most, prior, work, investigated, the, ...</td>\n",
       "      <td>(37, 40)</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, leverage, the, inductive, biases, learned...</td>\n",
       "      <td>None</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, smaller,, faster, and, lighter, model, i...</td>\n",
       "      <td>None</td>\n",
       "      <td>cheaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "      <td>[DistilBERT,, a, distilled, version, of, BERT:...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>DistilBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-trained, text, encoders, have, rapidly, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>advanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We focus on one such model, BERT, and aim to q...</td>\n",
       "      <td>We focus on</td>\n",
       "      <td>one such model , BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "      <td>[We, focus, on, one, such, model, ,, BERT, ,, ...</td>\n",
       "      <td>(3, 8)</td>\n",
       "      <td>captured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>the model</td>\n",
       "      <td>represents the steps of the traditional NLP pi...</td>\n",
       "      <td>[We, find, that, the, model, represents, the, ...</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>roles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td>Qualitative analysis reveals that</td>\n",
       "      <td>the model</td>\n",
       "      <td>can and often does adjust this pipeline dynami...</td>\n",
       "      <td>[Qualitative, analysis, reveals, that, the, mo...</td>\n",
       "      <td>(4, 6)</td>\n",
       "      <td>analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>Rediscovers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>Large pre - trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP , motivat...</td>\n",
       "      <td>[Large, pre, -, trained, neural, networks, suc...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Most, recent, analysis, has, focused, on, mod...</td>\n",
       "      <td>None</td>\n",
       "      <td>outputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>Complementary to these works , we propose meth...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Complementary, to, these, works, ,, we, propo...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT's attention heads exhibit patterns such a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "      <td>[BERT, 's, attention, heads, exhibit, patterns...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, show, that, certain, attention, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, example,, we, find, heads, that, attend,...</td>\n",
       "      <td>None</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>Lastly , we propose an attention - based probi...</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention</td>\n",
       "      <td>[Lastly, ,, we, propose, an, attention, -, bas...</td>\n",
       "      <td>(23, 25)</td>\n",
       "      <td>Lastly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What Does BERT Look At?</td>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At</td>\n",
       "      <td>[What, Does, BERT, Look, At]</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>An Analysis of BERT's Attention</td>\n",
       "      <td>An Analysis of</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td></td>\n",
       "      <td>[An, Analysis, of, BERT, 's]</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "      <td>[Recently,, neural, models, pretrained, on, a,...</td>\n",
       "      <td>(23, 24)</td>\n",
       "      <td>Recently</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>In this paper , we describe</td>\n",
       "      <td>a simple re - implementation of BERT for query...</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, ,, we, describe, a, simple, ...</td>\n",
       "      <td>(6, 21)</td>\n",
       "      <td>describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td>Our system</td>\n",
       "      <td>is the state of the art on the TREC - CAR data...</td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, code, to, reproduce, our, results, is, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Passage, Re-ranking, with, BERT]</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>I assess the extent to which</td>\n",
       "      <td>the recently introduced BERT model</td>\n",
       "      <td>captures English syntactic phenomena , using (...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>(6, 11)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>The BERT model performs remarkably well on all...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT model</td>\n",
       "      <td>performs remarkably well on all cases</td>\n",
       "      <td>[The, BERT, model, performs, remarkably, well,...</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>performs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "      <td>[Assessing, BERT's, Syntactic, Abilities]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>None</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>A new release of BERT (Devlin, 2018) includes ...</td>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "      <td>[A, new, release, of, BERT, (Devlin,, 2018), i...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>includes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "      <td>[This, paper, explores, the, broader, cross-li...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>NER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We compare mBERT with the best-published metho...</td>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best - published methods for zero - s...</td>\n",
       "      <td>[We, compare, mBERT, with, the, best, -, publi...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>compare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner , determine to what extent mBER...</td>\n",
       "      <td>[Additionally, ,, we, investigate, the, most, ...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>investigate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Beto,, Bentz,, Becas:, The, Surprising, Cross...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>Beto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>improved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, pre-trained, language, models, are,...</td>\n",
       "      <td>None</td>\n",
       "      <td>usually</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, accelerate, inference, and, reduce, model...</td>\n",
       "      <td>None</td>\n",
       "      <td>accelerate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "      <td>a small student TinyBERT</td>\n",
       "      <td></td>\n",
       "      <td>[By, leveraging, this, new, KD, method, ,, the...</td>\n",
       "      <td>(22, 26)</td>\n",
       "      <td>transferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>Moreover , we introduce a new two - stage lear...</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "      <td>[Moreover, ,, we, introduce, a, new, two, -, s...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general - domain and task...</td>\n",
       "      <td>[This, framework, ensures, that, TinyBERT, can...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>ensures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "      <td>[TinyBERT, is, empirically, effective, and, ac...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>empirically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>TinyBERT is also significantly better than sta...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state - of -...</td>\n",
       "      <td>[TinyBERT, is, also, significantly, better, th...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "      <td>[TinyBERT:, Distilling, BERT, for, Natural, La...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>TinyBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "      <td>[BERT,, a, pre-trained, Transformer, model,, h...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>ground</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe BERTSUM, a simple v...</td>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "      <td>[In, this, paper,, we, describe, BERTSUM,, a, ...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, codes, to, reproduce, our, results, are,...</td>\n",
       "      <td>None</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "      <td>[Fine-tune, BERT, for, Extractive, Summarization]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Question-answering, plays, an, important, rol...</td>\n",
       "      <td>None</td>\n",
       "      <td>Question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Inspired, by, the, recent, success, of, machi...</td>\n",
       "      <td>None</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, the, best, of, our, knowledge,, no, exist...</td>\n",
       "      <td>None</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work,, we, first, build, an, RRC, d...</td>\n",
       "      <td>None</td>\n",
       "      <td>build</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>the popular language model BERT</td>\n",
       "      <td>to enhance the performance of fine - tuning of...</td>\n",
       "      <td>[Since, ReviewRC, has, limited, training, exam...</td>\n",
       "      <td>(29, 34)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, show, the, generality, of, the, approach,...</td>\n",
       "      <td>None</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, the...</td>\n",
       "      <td>None</td>\n",
       "      <td>results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, datasets, and, code, are, available, at,...</td>\n",
       "      <td>None</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "      <td>[BERT, Post-Training, for, Review, Reading, Co...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pre-training, has, proven, t...</td>\n",
       "      <td>None</td>\n",
       "      <td>proven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>As a state - of - the - art language model pre...</td>\n",
       "      <td>BERT ( Bidirectional Encoder Representations f...</td>\n",
       "      <td>has achieved amazing results in many language ...</td>\n",
       "      <td>[As, a, state, -, of, -, the, -, art, language...</td>\n",
       "      <td>(16, 24)</td>\n",
       "      <td>achieved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "      <td>[In, this, paper, ,, we, conduct, exhaustive, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>conduct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, the, proposed, solution, obtains, n...</td>\n",
       "      <td>None</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "      <td>[How, to, Fine-Tune, BERT, for, Text, Classifi...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>How</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Devlin et al . , 2018 ) is a Markov random f...</td>\n",
       "      <td>[We, show, that, BERT, (, Devlin, et, al, ., ,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[This, formulation, gives, way, to, a, natural...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>gives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high - quality , ...</td>\n",
       "      <td>[We, generate, from, BERT, and, find, that, it...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>generate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "      <td>[Compared, to, the, generations, of, a, tradit...</td>\n",
       "      <td>(15, 16)</td>\n",
       "      <td>diverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth , and It Must Speak : BERT as a Ma...</td>\n",
       "      <td>[BERT, has, a, Mouth, ,, and, It, Must, Speak,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Aspect-based, sentiment, analysis, (ABSA),, w...</td>\n",
       "      <td>None</td>\n",
       "      <td>analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, construct, an, auxiliar...</td>\n",
       "      <td>None</td>\n",
       "      <td>construct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We fine-tune the pre-trained model from BERT a...</td>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "      <td>[We, fine-tune, the, pre-trained, model, from,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>tune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect-Based, Sentiment...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering , we explore simple appl...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle</td>\n",
       "      <td>[This, required, confronting, the, challenge, ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>required</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, address, this, issue, by, applying, infer...</td>\n",
       "      <td>None</td>\n",
       "      <td>scores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, TREC, microblog, and, newswi...</td>\n",
       "      <td>None</td>\n",
       "      <td>effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "      <td>[BERT-based, architectures, currently, give, s...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>currently</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>In the current work , we focus on the interpre...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, the, current, work, ,, we, focus, on, the...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td>focus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "      <td>[Using, a, subset, of, GLUE, tasks, and, a, se...</td>\n",
       "      <td>(31, 32)</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, findings, suggest, that, there, is, a, l...</td>\n",
       "      <td>None</td>\n",
       "      <td>findings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, different, heads, consistently, use, t...</td>\n",
       "      <td>None</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, manually, disabling, attentio...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>Revealing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>Encoder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, an upgraded version of BERT has been...</td>\n",
       "      <td>Recently , an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking ( WW...</td>\n",
       "      <td>[Recently, ,, an, upgraded, version, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>released</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, technical, report,, we, adapt, whol...</td>\n",
       "      <td>None</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>was trained on the latest Chinese Wikipedia dump</td>\n",
       "      <td>[The, model, was, trained, on, the, latest, Ch...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>trained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>Chinese BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "      <td>[We, aim, to, provide, easy, extensibility, an...</td>\n",
       "      <td>(10, 12)</td>\n",
       "      <td>aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>is verified on various NLP tasks , across sent...</td>\n",
       "      <td>[The, model, is, verified, on, various, NLP, t...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>comprehension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, on, these, datasets, s...</td>\n",
       "      <td>None</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE , BERT - wwm</td>\n",
       "      <td>[Moreover, ,, we, also, examine, the, effectiv...</td>\n",
       "      <td>(14, 15)</td>\n",
       "      <td>examine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, the, pre-trained, model, (both, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Pre-Training, with, Whole, Word, Masking, for...</td>\n",
       "      <td>(7, 8)</td>\n",
       "      <td>Pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open - ...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, previous work trains BERT by viewing ...</td>\n",
       "      <td>However , previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "      <td>[However, ,, previous, work, trains, BERT, by,...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>cause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To tackle this issue, we propose a multi-passa...</td>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "      <td>[To, tackle, this, issue,, we, propose, a, mul...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, addition,, we, find, that, splitting, art...</td>\n",
       "      <td>None</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "      <td>[By, leveraging, a, passage, ranker, to, selec...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>By</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "      <td>[Experiments, on, four, standard, benchmarks, ...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>showed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>In particular , on the OpenSQuAD dataset , our...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models , and 5.8 % EM and 6.5 % $ F_1 $ over B...</td>\n",
       "      <td>[In, particular, ,, on, the, OpenSQuAD, datase...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "      <td>[Multi-passage, BERT:, A, Globally, Normalized...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>passage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>None</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[They, often, suffer, from, small-scale, human...</td>\n",
       "      <td>None</td>\n",
       "      <td>suffer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently a new language representation model, ...</td>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[Recently, a, new, language, representation, m...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>However , there has not been much effort on ex...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding</td>\n",
       "      <td>[However, ,, there, has, not, been, much, effo...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>In this work , we propose a joint intent class...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work, ,, we, propose, a, joint, int...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, our...</td>\n",
       "      <td>None</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>None</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[One, of, the, major, challenges, to, multi-tu...</td>\n",
       "      <td>None</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Existing, methods, either, prepend, history, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>turns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, conceptually, simple, yet, hi...</td>\n",
       "      <td>None</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[It, enables, seamless, integration, of, conve...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, first, explain, our, view, that, ConvQA, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>explain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, demonstrate, the, effectiveness,...</td>\n",
       "      <td>None</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, we, analyze, the, impact, of, diffe...</td>\n",
       "      <td>None</td>\n",
       "      <td>analyze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "      <td>[We, explore, several, different, ways, to, le...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>ranking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question - answering focused passage rankin...</td>\n",
       "      <td>[Experimental, results, on, MS, MARCO, demonst...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "      <td>[Experimental, results, on, TREC, show, the, g...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query - docum...</td>\n",
       "      <td>[Analyses, illustrate, how, BERT, allocates, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>illustrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>Understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "      <td>We present</td>\n",
       "      <td>simple BERT - based models for relation extrac...</td>\n",
       "      <td></td>\n",
       "      <td>[We, present, simple, BERT, -, based, models, ...</td>\n",
       "      <td>(2, 14)</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, recent, years,, state-of-the-art, perform...</td>\n",
       "      <td>None</td>\n",
       "      <td>achieved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "      <td>[In, this, paper,, extensive, experiments, on,...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "      <td>[To, our, knowledge,, we, are, the, first, to,...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td>Our models</td>\n",
       "      <td>provide strong baselines for future</td>\n",
       "      <td>[Our, models, provide, strong, baselines, for,...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Multi-task, learning, allows, the, sharing, o...</td>\n",
       "      <td>None</td>\n",
       "      <td>allows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, natural, language, processing, several, r...</td>\n",
       "      <td>None</td>\n",
       "      <td>successfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, are, based, on, fine-tuning, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "      <td>[We, explore, the, multi-task, learning, setti...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, new, adaptation, modules,, PAL...</td>\n",
       "      <td>None</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>By using PALs in parallel with BERT layers, we...</td>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "      <td>[By, using, PALs, in, parallel, with, BERT, la...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "      <td>[BERT, and, PALs:, Projected, Attention, Layer...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-training, by, language, modeling, has, be...</td>\n",
       "      <td>None</td>\n",
       "      <td>approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, we, introduce, a, suite, of,...</td>\n",
       "      <td>None</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>As a case study , we apply these diagnostics to</td>\n",
       "      <td>the popular BERT model</td>\n",
       "      <td>, finding that it can generally distinguish go...</td>\n",
       "      <td>[As, a, case, study, ,, we, apply, these, diag...</td>\n",
       "      <td>(10, 14)</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "      <td>[What, BERT, Is, Not:, Lessons, from, a, New, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre - training , such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>achieved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, it, is, unclear, why, the, pre-trai...</td>\n",
       "      <td>None</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>In this paper , we propose to visualize loss l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets</td>\n",
       "      <td>[In, this, paper, ,, we, propose, to, visualiz...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[First,, we, find, that, pre-training, reaches...</td>\n",
       "      <td>None</td>\n",
       "      <td>First</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>We also demonstrate that the fine - tuning pro...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over - parameterized for downstream ...</td>\n",
       "      <td>[We, also, demonstrate, that, the, fine, -, tu...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>demonstrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>Second , the visualization results indicate th...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "      <td>[Second, ,, the, visualization, results, indic...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>indicate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Third, the lower layers of BERT are more invar...</td>\n",
       "      <td>Third , the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine - tuning , whic...</td>\n",
       "      <td>[Third, ,, the, lower, layers, of, BERT, are, ...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>invariant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>Visualizing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>None</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Recently, proposed, contextual, augmentation,...</td>\n",
       "      <td>None</td>\n",
       "      <td>Recently</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td></td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>Representations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "      <td>[We, propose, a, novel, data, augmentation, me...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "      <td>[We, retrofit, BERT, to, conditional, BERT, by...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, our, paper,, “conditional, masked, langua...</td>\n",
       "      <td>None</td>\n",
       "      <td>label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>task.</td>\n",
       "      <td>task.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[task.]</td>\n",
       "      <td>None</td>\n",
       "      <td>task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The well trained conditional BERT can be appli...</td>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "      <td>[The, well, trained, conditional, BERT, can, b...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>applied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, six, various, different, tex...</td>\n",
       "      <td>None</td>\n",
       "      <td>tasks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>None</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Starting from a public multilingual BERT check...</td>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "      <td>[Starting, from, a, public, multilingual, BERT...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>6x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, our, model, especially, outpe...</td>\n",
       "      <td>None</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, showcase, the, effectiveness, of, our, me...</td>\n",
       "      <td>None</td>\n",
       "      <td>showcase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "      <td>[Recently,, a, simple, combination, of, passag...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, present, a, data, augme...</td>\n",
       "      <td>None</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning ...</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "      <td>[We, apply, a, stage-wise, approach, to, fine,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>approach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, show, large, gains, in...</td>\n",
       "      <td>None</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine-Tuning, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT language model ( LM )</td>\n",
       "      <td>( Devlin et al . , 2019 ) is surprisingly good...</td>\n",
       "      <td>[The, BERT, language, model, (, LM, ), (, Devl...</td>\n",
       "      <td>(0, 7)</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Petroni, et, al.]</td>\n",
       "      <td>None</td>\n",
       "      <td>Petroni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>(2019) take this as evidence that BERT memoriz...</td>\n",
       "      <td>( 2019 ) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre - training</td>\n",
       "      <td>[(, 2019, ), take, this, as, evidence, that, B...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about ( the surface...</td>\n",
       "      <td>[We, take, issue, with, this, interpretation, ...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>More specifically, we show that BERT's precisi...</td>\n",
       "      <td>More specifically , we show that</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "      <td>[More, specifically, ,, we, show, that, BERT, ...</td>\n",
       "      <td>(6, 8)</td>\n",
       "      <td>specifically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>As a remedy, we propose E-BERT, an extension o...</td>\n",
       "      <td>As a remedy , we propose E - BERT , an extensi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>that replaces entity mentions with symbolic en...</td>\n",
       "      <td>[As, a, remedy, ,, we, propose, E, -, BERT, ,,...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>propose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>E-BERT outperforms both BERT and ERNIE (Zhang ...</td>\n",
       "      <td>E - BERT outperforms both</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and ERNIE ( Zhang et al . , 2019 ) on hard - t...</td>\n",
       "      <td>[E, -, BERT, outperforms, both, BERT, and, ERN...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>BERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and E -</td>\n",
       "      <td>[We, take, this, as, evidence, that, E, -, BER...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>None</td>\n",
       "      <td>yielded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>However , just how contextual are the contextu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[However, ,, just, how, contextual, are, the, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>representations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Are, there, infinitely, many, context-specifi...</td>\n",
       "      <td>None</td>\n",
       "      <td>words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, one,, we, find, that, the, contextualize...</td>\n",
       "      <td>None</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, representations, of, the, same, word, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[This, suggests, that, upper, layers, of, cont...</td>\n",
       "      <td>None</td>\n",
       "      <td>suggests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In all layers of ELMo, BERT, and GPT-2, on ave...</td>\n",
       "      <td>In all layers of ELMo ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2 , on average , less than 5 % of th...</td>\n",
       "      <td>[In, all, layers, of, ELMo, ,, BERT, ,, and, G...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>None</td>\n",
       "      <td>Contextualized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing the Geometry of BERT, ELMo, and GPT-...</td>\n",
       "      <td>Comparing the Geometry of</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "      <td>[Comparing, the, Geometry, of, BERT,, ELMo,, a...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>Comparing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  ID      Type  Index  \\\n",
       "0    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      0   \n",
       "1    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      1   \n",
       "2    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      2   \n",
       "3    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      3   \n",
       "4    https://www.semanticscholar.org/paper/BERT%3A-...   0     Title      0   \n",
       "5    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      0   \n",
       "6    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      1   \n",
       "7    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      2   \n",
       "8    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      3   \n",
       "9    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      4   \n",
       "10   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      5   \n",
       "11   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      6   \n",
       "12   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title      0   \n",
       "13   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      0   \n",
       "14   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      1   \n",
       "15   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      2   \n",
       "16   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      3   \n",
       "17   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      4   \n",
       "18   https://www.semanticscholar.org/paper/DistilBE...   2     Title      0   \n",
       "19   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      0   \n",
       "20   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      1   \n",
       "21   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      2   \n",
       "22   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      3   \n",
       "23   https://www.semanticscholar.org/paper/BERT-Red...   3     Title      0   \n",
       "24   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      0   \n",
       "25   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      1   \n",
       "26   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      2   \n",
       "27   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      3   \n",
       "28   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      4   \n",
       "29   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      5   \n",
       "30   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      6   \n",
       "31   https://www.semanticscholar.org/paper/What-Doe...   4     Title      0   \n",
       "32   https://www.semanticscholar.org/paper/What-Doe...   4     Title      1   \n",
       "33   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      0   \n",
       "34   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      1   \n",
       "35   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      2   \n",
       "36   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      3   \n",
       "37   https://www.semanticscholar.org/paper/Passage-...   5     Title      0   \n",
       "38   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      0   \n",
       "39   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      1   \n",
       "40   https://www.semanticscholar.org/paper/Assessin...   6     Title      0   \n",
       "41   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      0   \n",
       "42   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      1   \n",
       "43   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      2   \n",
       "44   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      3   \n",
       "45   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      4   \n",
       "46   https://www.semanticscholar.org/paper/Beto%2C-...   7     Title      0   \n",
       "47   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      0   \n",
       "48   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      1   \n",
       "49   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      2   \n",
       "50   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      3   \n",
       "51   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      4   \n",
       "52   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      5   \n",
       "53   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      6   \n",
       "54   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      7   \n",
       "55   https://www.semanticscholar.org/paper/TinyBERT...   8     Title      0   \n",
       "56   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      0   \n",
       "57   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      1   \n",
       "58   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      2   \n",
       "59   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      3   \n",
       "60   https://www.semanticscholar.org/paper/Fine-tun...   9     Title      0   \n",
       "61   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      0   \n",
       "62   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      1   \n",
       "63   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      2   \n",
       "64   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      3   \n",
       "65   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      4   \n",
       "66   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      5   \n",
       "67   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      6   \n",
       "68   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      7   \n",
       "69   https://www.semanticscholar.org/paper/BERT-Pos...  10     Title      0   \n",
       "70   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      0   \n",
       "71   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      1   \n",
       "72   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      2   \n",
       "73   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      3   \n",
       "74   https://www.semanticscholar.org/paper/How-to-F...  11     Title      0   \n",
       "75   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      0   \n",
       "76   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      1   \n",
       "77   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      2   \n",
       "78   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      3   \n",
       "79   https://www.semanticscholar.org/paper/BERT-has...  12     Title      0   \n",
       "80   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      0   \n",
       "81   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      1   \n",
       "82   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      2   \n",
       "83   https://www.semanticscholar.org/paper/Utilizin...  13     Title      0   \n",
       "84   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      0   \n",
       "85   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      1   \n",
       "86   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      2   \n",
       "87   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      3   \n",
       "88   https://www.semanticscholar.org/paper/Simple-A...  14     Title      0   \n",
       "89   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      0   \n",
       "90   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      1   \n",
       "91   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      2   \n",
       "92   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      3   \n",
       "93   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      4   \n",
       "94   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      5   \n",
       "95   https://www.semanticscholar.org/paper/Revealin...  15     Title      0   \n",
       "96   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      0   \n",
       "97   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      1   \n",
       "98   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      2   \n",
       "99   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      3   \n",
       "100  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      4   \n",
       "101  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      5   \n",
       "102  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      6   \n",
       "103  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      7   \n",
       "104  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      8   \n",
       "105  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title      0   \n",
       "106  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      0   \n",
       "107  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      1   \n",
       "108  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      2   \n",
       "109  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      3   \n",
       "110  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      4   \n",
       "111  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      5   \n",
       "112  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      6   \n",
       "113  https://www.semanticscholar.org/paper/Multi-pa...  17     Title      0   \n",
       "114  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      0   \n",
       "115  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      1   \n",
       "116  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      2   \n",
       "117  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      3   \n",
       "118  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      4   \n",
       "119  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      5   \n",
       "120  https://www.semanticscholar.org/paper/BERT-for...  18     Title      0   \n",
       "121  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      0   \n",
       "122  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      1   \n",
       "123  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      2   \n",
       "124  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      3   \n",
       "125  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      4   \n",
       "126  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      5   \n",
       "127  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      6   \n",
       "128  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      7   \n",
       "129  https://www.semanticscholar.org/paper/BERT-wit...  19     Title      0   \n",
       "130  https://www.semanticscholar.org/paper/Understa...  20  Abstract      0   \n",
       "131  https://www.semanticscholar.org/paper/Understa...  20  Abstract      1   \n",
       "132  https://www.semanticscholar.org/paper/Understa...  20  Abstract      2   \n",
       "133  https://www.semanticscholar.org/paper/Understa...  20  Abstract      3   \n",
       "134  https://www.semanticscholar.org/paper/Understa...  20  Abstract      4   \n",
       "135  https://www.semanticscholar.org/paper/Understa...  20     Title      0   \n",
       "136  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      0   \n",
       "137  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      1   \n",
       "138  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      2   \n",
       "139  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      3   \n",
       "140  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      4   \n",
       "141  https://www.semanticscholar.org/paper/Simple-B...  21     Title      0   \n",
       "142  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      0   \n",
       "143  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      1   \n",
       "144  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      2   \n",
       "145  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      3   \n",
       "146  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      4   \n",
       "147  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      5   \n",
       "148  https://www.semanticscholar.org/paper/BERT-and...  22     Title      0   \n",
       "149  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      0   \n",
       "150  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      1   \n",
       "151  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      2   \n",
       "152  https://www.semanticscholar.org/paper/What-BER...  23     Title      0   \n",
       "153  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      0   \n",
       "154  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      1   \n",
       "155  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      2   \n",
       "156  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      3   \n",
       "157  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      4   \n",
       "158  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      5   \n",
       "159  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      6   \n",
       "160  https://www.semanticscholar.org/paper/Visualiz...  24     Title      0   \n",
       "161  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      0   \n",
       "162  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      1   \n",
       "163  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      2   \n",
       "164  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      3   \n",
       "165  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      4   \n",
       "166  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      5   \n",
       "167  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      6   \n",
       "168  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      7   \n",
       "169  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      8   \n",
       "170  https://www.semanticscholar.org/paper/Conditio...  25     Title      0   \n",
       "171  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      0   \n",
       "172  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      1   \n",
       "173  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      2   \n",
       "174  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      3   \n",
       "175  https://www.semanticscholar.org/paper/Small-an...  26     Title      0   \n",
       "176  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      0   \n",
       "177  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      1   \n",
       "178  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      2   \n",
       "179  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      3   \n",
       "180  https://www.semanticscholar.org/paper/Data-Aug...  27     Title      0   \n",
       "181  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      0   \n",
       "182  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      1   \n",
       "183  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      2   \n",
       "184  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      3   \n",
       "185  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      4   \n",
       "186  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      5   \n",
       "187  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      6   \n",
       "188  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      7   \n",
       "189  https://www.semanticscholar.org/paper/BERT-is-...  28     Title      0   \n",
       "190  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      0   \n",
       "191  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      1   \n",
       "192  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      2   \n",
       "193  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      3   \n",
       "194  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      4   \n",
       "195  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      5   \n",
       "196  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      6   \n",
       "197  https://www.semanticscholar.org/paper/How-Cont...  29     Title      0   \n",
       "198  https://www.semanticscholar.org/paper/How-Cont...  29     Title      1   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    We introduce a new language representation mod...   \n",
       "1    Unlike recent language representation models, ...   \n",
       "2    As a result, the pre-trained BERT model can be...   \n",
       "3    It obtains new state-of-the-art results on ele...   \n",
       "4    BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7    We present a replication study of BERT pretrai...   \n",
       "8    We find that BERT was significantly undertrain...   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12   RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18   DistilBERT, a distilled version of BERT: small...   \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20   We focus on one such model, BERT, and aim to q...   \n",
       "21   We find that the model represents the steps of...   \n",
       "22   Qualitative analysis reveals that the model ca...   \n",
       "23         BERT Rediscovers the Classical NLP Pipeline   \n",
       "24   Large pre-trained neural networks such as BERT...   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works, we propose metho...   \n",
       "27   BERT's attention heads exhibit patterns such a...   \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly, we propose an attention-based probing ...   \n",
       "31                             What Does BERT Look At?   \n",
       "32                     An Analysis of BERT's Attention   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34   In this paper, we describe a simple re-impleme...   \n",
       "35   Our system is the state of the art on the TREC...   \n",
       "36   The code to reproduce our results is available...   \n",
       "37                        Passage Re-ranking with BERT   \n",
       "38   I assess the extent to which the recently intr...   \n",
       "39   The BERT model performs remarkably well on all...   \n",
       "40                Assessing BERT's Syntactic Abilities   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42   A new release of BERT (Devlin, 2018) includes ...   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44   We compare mBERT with the best-published metho...   \n",
       "45   Additionally, we investigate the most effectiv...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47   Language model pre-training, such as BERT, has...   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method, the plenty o...   \n",
       "51   Moreover, we introduce a new two-stage learnin...   \n",
       "52   This framework ensures that TinyBERT can captu...   \n",
       "53   TinyBERT is empirically effective and achieves...   \n",
       "54   TinyBERT is also significantly better than sta...   \n",
       "55   TinyBERT: Distilling BERT for Natural Language...   \n",
       "56   BERT, a pre-trained Transformer model, has ach...   \n",
       "57   In this paper, we describe BERTSUM, a simple v...   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60         Fine-tune BERT for Extractive Summarization   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69   BERT Post-Training for Review Reading Comprehe...   \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state-of-the-art language model pre-train...   \n",
       "72   In this paper, we conduct exhaustive experimen...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74      How to Fine-Tune BERT for Text Classification?   \n",
       "75   We show that BERT (Devlin et al., 2018) is a M...   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77   We generate from BERT and find that it can pro...   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79   BERT has a Mouth, and It Must Speak: BERT as a...   \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82   We fine-tune the pre-trained model from BERT a...   \n",
       "83   Utilizing BERT for Aspect-Based Sentiment Anal...   \n",
       "84   Following recent successes in applying BERT to...   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88   Simple Applications of BERT for Ad Hoc Documen...   \n",
       "89   BERT-based architectures currently give state-...   \n",
       "90   In the current work, we focus on the interpret...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                  Revealing the Dark Secrets of BERT   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97   Recently, an upgraded version of BERT has been...   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99   The model was trained on the latest Chinese Wi...   \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101  The model is verified on various NLP tasks, ac...   \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover, we also examine the effectiveness of...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105  Pre-Training with Whole Word Masking for Chine...   \n",
       "106  BERT model has been successfully applied to op...   \n",
       "107  However, previous work trains BERT by viewing ...   \n",
       "108  To tackle this issue, we propose a multi-passa...   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular, on the OpenSQuAD dataset, our m...   \n",
       "113  Multi-passage BERT: A Globally Normalized BERT...   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116  Recently a new language representation model, ...   \n",
       "117  However, there has not been much effort on exp...   \n",
       "118  In this work, we propose a joint intent classi...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120  BERT for Joint Intent Classification and Slot ...   \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129  BERT with History Answer Embedding for Convers...   \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134  Analyses illustrate how BERT allocates its att...   \n",
       "135     Understanding the Behaviors of BERT in Ranking   \n",
       "136  We present simple BERT-based models for relati...   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140  Our models provide strong baselines for future...   \n",
       "141  Simple BERT Models for Relation Extraction and...   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147  By using PALs in parallel with BERT layers, we...   \n",
       "148  BERT and PALs: Projected Attention Layers for ...   \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151  As a case study, we apply these diagnostics to...   \n",
       "152  What BERT Is Not: Lessons from a New Suite of ...   \n",
       "153  Language model pre-training, such as BERT, has...   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper, we propose to visualize loss la...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine-tuning proce...   \n",
       "158  Second, the visualization results indicate tha...   \n",
       "159  Third, the lower layers of BERT are more invar...   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165  We retrofit BERT to conditional BERT by introd...   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168  The well trained conditional BERT can be appli...   \n",
       "169  Experiments on six various different text clas...   \n",
       "170           Conditional BERT Contextual Augmentation   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172  Starting from a public multilingual BERT check...   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175  Small and Practical BERT Models for Sequence L...   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178  We apply a stage-wise approach to fine tuning ...   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180  Data Augmentation for BERT Fine-Tuning in Open...   \n",
       "181  The BERT language model (LM) (Devlin et al., 2...   \n",
       "182                                     Petroni et al.   \n",
       "183  (2019) take this as evidence that BERT memoriz...   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185  More specifically, we show that BERT's precisi...   \n",
       "186  As a remedy, we propose E-BERT, an extension o...   \n",
       "187  E-BERT outperforms both BERT and ERNIE (Zhang ...   \n",
       "188  We take this as evidence that E-BERT is richer...   \n",
       "189  BERT is Not a Knowledge Base (Yet): Factual Kn...   \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However, just how contextual are the contextua...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196  In all layers of ELMo, BERT, and GPT-2, on ave...   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198  Comparing the Geometry of BERT, ELMo, and GPT-...   \n",
       "\n",
       "                                               split_0  \\\n",
       "0                                         We introduce   \n",
       "1       Unlike recent language representation models ,   \n",
       "2                      As a result , the pre - trained   \n",
       "3                                                        \n",
       "4                                                        \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7                    We present a replication study of   \n",
       "8                                         We find that   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12                                                       \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18                                                       \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20                                         We focus on   \n",
       "21                                        We find that   \n",
       "22                   Qualitative analysis reveals that   \n",
       "23                                                       \n",
       "24         Large pre - trained neural networks such as   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works , we propose meth...   \n",
       "27                                                       \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly , we propose an attention - based probi...   \n",
       "31                                           What Does   \n",
       "32                                      An Analysis of   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34                         In this paper , we describe   \n",
       "35                                                       \n",
       "36   The code to reproduce our results is available...   \n",
       "37                             Passage Re-ranking with   \n",
       "38                        I assess the extent to which   \n",
       "39                                                       \n",
       "40                                           Assessing   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42                                    A new release of   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44                                          We compare   \n",
       "45   Additionally , we investigate the most effecti...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47                Language model pre-training, such as   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method , the plenty ...   \n",
       "51   Moreover , we introduce a new two - stage lear...   \n",
       "52                         This framework ensures that   \n",
       "53                                                       \n",
       "54                                                       \n",
       "55                                                       \n",
       "56                                                       \n",
       "57                          In this paper, we describe   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60                                           Fine-tune   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69                                                       \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state - of - the - art language model pre...   \n",
       "72   In this paper , we conduct exhaustive experime...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74                                    How to Fine-Tune   \n",
       "75                                        We show that   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77                                    We generate from   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79                                                       \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82             We fine-tune the pre-trained model from   \n",
       "83                                           Utilizing   \n",
       "84              Following recent successes in applying   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88                              Simple Applications of   \n",
       "89                                                       \n",
       "90   In the current work , we focus on the interpre...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                       Revealing the Dark Secrets of   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97                   Recently , an upgraded version of   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99                                                       \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101                                                      \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover , we also examine the effectiveness o...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105   Pre-Training with Whole Word Masking for Chinese   \n",
       "106                                                      \n",
       "107                     However , previous work trains   \n",
       "108   To tackle this issue, we propose a multi-passage   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular , on the OpenSQuAD dataset , our...   \n",
       "113                                      Multi-passage   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116      Recently a new language representation model,   \n",
       "117  However , there has not been much effort on ex...   \n",
       "118  In this work , we propose a joint intent class...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120                                                      \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129                                                      \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134                            Analyses illustrate how   \n",
       "135                     Understanding the Behaviors of   \n",
       "136                                         We present   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140                                                      \n",
       "141                                             Simple   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147                     By using PALs in parallel with   \n",
       "148                                                      \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151    As a case study , we apply these diagnostics to   \n",
       "152                                               What   \n",
       "153            Language model pre - training , such as   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper , we propose to visualize loss l...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine - tuning pro...   \n",
       "158  Second , the visualization results indicate th...   \n",
       "159                        Third , the lower layers of   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163                                                      \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165                                        We retrofit   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168                       The well trained conditional   \n",
       "169  Experiments on six various different text clas...   \n",
       "170                                        Conditional   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172                Starting from a public multilingual   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175                                Small and Practical   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178      We apply a stage-wise approach to fine tuning   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180                              Data Augmentation for   \n",
       "181                                                      \n",
       "182                                     Petroni et al.   \n",
       "183                ( 2019 ) take this as evidence that   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185                   More specifically , we show that   \n",
       "186  As a remedy , we propose E - BERT , an extensi...   \n",
       "187                          E - BERT outperforms both   \n",
       "188  We take this as evidence that E - BERT is rich...   \n",
       "189                                                      \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However , just how contextual are the contextu...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196                            In all layers of ELMo ,   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198                          Comparing the Geometry of   \n",
       "\n",
       "                                               split_1  \\\n",
       "0    a new language representation model called BER...   \n",
       "1                                                 BERT   \n",
       "2                                                 BERT   \n",
       "3                                                   It   \n",
       "4                                                BERT:   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7                                                 BERT   \n",
       "8                                                 BERT   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12                                            RoBERTa:   \n",
       "13                                                       \n",
       "14                                         DistilBERT,   \n",
       "15                                        a BERT model   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18                                         DistilBERT,   \n",
       "19                                                       \n",
       "20                               one such model , BERT   \n",
       "21                                           the model   \n",
       "22                                           the model   \n",
       "23                                                BERT   \n",
       "24                                                BERT   \n",
       "25                                                       \n",
       "26                                                BERT   \n",
       "27                                             BERT 's   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                             BERT 's   \n",
       "31                                                BERT   \n",
       "32                                             BERT 's   \n",
       "33                                                BERT   \n",
       "34   a simple re - implementation of BERT for query...   \n",
       "35                                          Our system   \n",
       "36                                                       \n",
       "37                                                BERT   \n",
       "38                  the recently introduced BERT model   \n",
       "39                                      The BERT model   \n",
       "40                                              BERT's   \n",
       "41                                                       \n",
       "42                                                BERT   \n",
       "43                                               mBERT   \n",
       "44                                               mBERT   \n",
       "45                                               mBERT   \n",
       "46                                                BERT   \n",
       "47                                               BERT,   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                            a small student TinyBERT   \n",
       "51                                            TinyBERT   \n",
       "52                                            TinyBERT   \n",
       "53                                            TinyBERT   \n",
       "54                                            TinyBERT   \n",
       "55                                           TinyBERT:   \n",
       "56                                               BERT,   \n",
       "57                                            BERTSUM,   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                                                BERT   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65                     the popular language model BERT   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69                                                BERT   \n",
       "70                                                       \n",
       "71   BERT ( Bidirectional Encoder Representations f...   \n",
       "72                                                BERT   \n",
       "73                                                       \n",
       "74                                                BERT   \n",
       "75                                                BERT   \n",
       "76                                                BERT   \n",
       "77                                                BERT   \n",
       "78                                                BERT   \n",
       "79                                                BERT   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82                                                BERT   \n",
       "83                                                BERT   \n",
       "84                                                BERT   \n",
       "85                                                BERT   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                                                BERT   \n",
       "89                                          BERT-based   \n",
       "90                                                BERT   \n",
       "91                                              BERT's   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                BERT   \n",
       "95                                                BERT   \n",
       "96                                              (BERT)   \n",
       "97                                                BERT   \n",
       "98                                                       \n",
       "99                                           The model   \n",
       "100                                       Chinese BERT   \n",
       "101                                          The model   \n",
       "102                                                      \n",
       "103                                               BERT   \n",
       "104                                                      \n",
       "105                                               BERT   \n",
       "106                                               BERT   \n",
       "107                                               BERT   \n",
       "108                                               BERT   \n",
       "109                                                      \n",
       "110                                               BERT   \n",
       "111                                               BERT   \n",
       "112                                               BERT   \n",
       "113                                              BERT:   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116                                               BERT   \n",
       "117                                               BERT   \n",
       "118                                               BERT   \n",
       "119                                                      \n",
       "120                                               BERT   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125                                               BERT   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129                                               BERT   \n",
       "130                                               BERT   \n",
       "131                                               BERT   \n",
       "132                                               BERT   \n",
       "133                                               BERT   \n",
       "134                                               BERT   \n",
       "135                                               BERT   \n",
       "136  simple BERT - based models for relation extrac...   \n",
       "137                                                      \n",
       "138                                         BERT-based   \n",
       "139                                               BERT   \n",
       "140                                         Our models   \n",
       "141                                               BERT   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145                                               BERT   \n",
       "146                                                      \n",
       "147                                               BERT   \n",
       "148                                               BERT   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151                             the popular BERT model   \n",
       "152                                               BERT   \n",
       "153                                               BERT   \n",
       "154                                                      \n",
       "155                                               BERT   \n",
       "156                                                      \n",
       "157                                               BERT   \n",
       "158                                               BERT   \n",
       "159                                               BERT   \n",
       "160                                               BERT   \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164                                               BERT   \n",
       "165                                               BERT   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168                                               BERT   \n",
       "169                                                      \n",
       "170                                               BERT   \n",
       "171                                                      \n",
       "172                                               BERT   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                                               BERT   \n",
       "176                                               BERT   \n",
       "177                                                      \n",
       "178                                               BERT   \n",
       "179                                                      \n",
       "180                                               BERT   \n",
       "181                     The BERT language model ( LM )   \n",
       "182                                                      \n",
       "183                                               BERT   \n",
       "184                                               BERT   \n",
       "185                                            BERT 's   \n",
       "186                                               BERT   \n",
       "187                                               BERT   \n",
       "188                                               BERT   \n",
       "189                                               BERT   \n",
       "190                                                      \n",
       "191                                               BERT   \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196                                               BERT   \n",
       "197                                                      \n",
       "198                                              BERT,   \n",
       "\n",
       "                                               split_2  \\\n",
       "0                                                        \n",
       "1    is designed to pre - train deep bidirectional ...   \n",
       "2    model can be fine - tuned with just one additi...   \n",
       "3    obtains new state - of - the - art results on ...   \n",
       "4    Pre-training of Deep Bidirectional Transformer...   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7    pretraining ( Devlin et al . , 2019 ) that car...   \n",
       "8    was significantly undertrained , and can match...   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12      A Robustly Optimized BERT Pretraining Approach   \n",
       "13                                                       \n",
       "14   which can then be fine-tuned with good perform...   \n",
       "15   by 40 % , while retaining 97 % of its language...   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18   a distilled version of BERT: smaller, faster, ...   \n",
       "19                                                       \n",
       "20   , and aim to quantify where linguistic informa...   \n",
       "21   represents the steps of the traditional NLP pi...   \n",
       "22   can and often does adjust this pipeline dynami...   \n",
       "23              Rediscovers the Classical NLP Pipeline   \n",
       "24   have had great recent success in NLP , motivat...   \n",
       "25                                                       \n",
       "26                                                       \n",
       "27   attention heads exhibit patterns such as atten...   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                           attention   \n",
       "31                                             Look At   \n",
       "32                                                       \n",
       "33   (Devlin et al., 2018), have achieved impressiv...   \n",
       "34                                                       \n",
       "35   is the state of the art on the TREC - CAR data...   \n",
       "36                                                       \n",
       "37                                                       \n",
       "38   captures English syntactic phenomena , using (...   \n",
       "39               performs remarkably well on all cases   \n",
       "40                                 Syntactic Abilities   \n",
       "41                                                       \n",
       "42   (Devlin, 2018) includes a model simultaneously...   \n",
       "43   (multilingual) as a zero shot language transfe...   \n",
       "44   with the best - published methods for zero - s...   \n",
       "45   in this manner , determine to what extent mBER...   \n",
       "46                                                       \n",
       "47   has significantly improved the performances of...   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                                                       \n",
       "51   , which performs transformer distillation at b...   \n",
       "52   can capture both the general - domain and task...   \n",
       "53   is empirically effective and achieves comparab...   \n",
       "54   is also significantly better than state - of -...   \n",
       "55   Distilling BERT for Natural Language Understan...   \n",
       "56   a pre-trained Transformer model, has achieved ...   \n",
       "57   a simple variant of BERT, for extractive summa...   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                        for Extractive Summarization   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65   to enhance the performance of fine - tuning of...   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69   Post-Training for Review Reading Comprehension...   \n",
       "70                                                       \n",
       "71   has achieved amazing results in many language ...   \n",
       "72   on text classification task and provide a gene...   \n",
       "73                                                       \n",
       "74                            for Text Classification?   \n",
       "75   ( Devlin et al . , 2018 ) is a Markov random f...   \n",
       "76                                                       \n",
       "77   and find that it can produce high - quality , ...   \n",
       "78   generates sentences that are more diverse but ...   \n",
       "79   has a Mouth , and It Must Speak : BERT as a Ma...   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82   and achieve new state-of-the-art results on Se...   \n",
       "83   for Aspect-Based Sentiment Analysis via Constr...   \n",
       "84   to question answering , we explore simple appl...   \n",
       "85                              was designed to handle   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                       for Ad Hoc Document Retrieval   \n",
       "89   architectures currently give state-of-the-art ...   \n",
       "90                                                       \n",
       "91                                              heads.   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                       \n",
       "95                                                       \n",
       "96   has shown marvelous improvements across variou...   \n",
       "97   has been released with Whole Word Masking ( WW...   \n",
       "98                                                       \n",
       "99    was trained on the latest Chinese Wikipedia dump   \n",
       "100  without changing any neural architecture or ev...   \n",
       "101  is verified on various NLP tasks , across sent...   \n",
       "102                                                      \n",
       "103                               , ERNIE , BERT - wwm   \n",
       "104                                                      \n",
       "105                                                      \n",
       "106  model has been successfully applied to open - ...   \n",
       "107  by viewing passages corresponding to the same ...   \n",
       "108  model to globally normalize answer scores acro...   \n",
       "109                                                      \n",
       "110                               gains additional 2%.   \n",
       "111  outperforms all state-of-the-art models on all...   \n",
       "112  models , and 5.8 % EM and 6.5 % $ F_1 $ over B...   \n",
       "113  A Globally Normalized BERT Model for Open-doma...   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116  (Bidirectional Encoder Representations from Tr...   \n",
       "117                 for natural language understanding   \n",
       "118                                                      \n",
       "119                                                      \n",
       "120   for Joint Intent Classification and Slot Filling   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125  (Bidirectional Encoder Representations from Tr...   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129  with History Answer Embedding for Conversation...   \n",
       "130                                   in ranking tasks   \n",
       "131  and fine-tune it on two ranking tasks: MS MARC...   \n",
       "132  in question - answering focused passage rankin...   \n",
       "133  pre-trained on surrounding contexts and the ne...   \n",
       "134  allocates its attentions between query - docum...   \n",
       "135                                         in Ranking   \n",
       "136                                                      \n",
       "137                                                      \n",
       "138    model can achieve state-of-the-art performance.   \n",
       "139                                    in this manner.   \n",
       "140                provide strong baselines for future   \n",
       "141  Models for Relation Extraction and Semantic Ro...   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145  model on the GLUE benchmark, and how to best a...   \n",
       "146                                                      \n",
       "147  layers, we match the performance of fine-tuned...   \n",
       "148  and PALs: Projected Attention Layers for Effic...   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151  , finding that it can generally distinguish go...   \n",
       "152  Is Not: Lessons from a New Suite of Psycholing...   \n",
       "153  , has achieved remarkable results in many NLP ...   \n",
       "154                                                      \n",
       "155                               on specific datasets   \n",
       "156                                                      \n",
       "157  is highly over - parameterized for downstream ...   \n",
       "158  tends to generalize better because of the flat...   \n",
       "159  are more invariant during fine - tuning , whic...   \n",
       "160                                                      \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  demonstrates that a deep bidirectional languag...   \n",
       "164                           contextual augmentation.   \n",
       "165  to conditional BERT by introducing a new condi...   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168  can be applied to enhance contextual augmentat...   \n",
       "169                                                      \n",
       "170                            Contextual Augmentation   \n",
       "171                                                      \n",
       "172  checkpoint, our final model is 6x smaller and ...   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                       Models for Sequence Labeling   \n",
       "176  reader was found to be very effective for ques...   \n",
       "177                                                      \n",
       "178  on multiple datasets, starting with data that ...   \n",
       "179                                                      \n",
       "180      Fine-Tuning in Open-Domain Question Answering   \n",
       "181  ( Devlin et al . , 2019 ) is surprisingly good...   \n",
       "182                                                      \n",
       "183  memorizes factual knowledge during pre - training   \n",
       "184  is partly due to reasoning about ( the surface...   \n",
       "185  precision drops dramatically when we filter ce...   \n",
       "186  that replaces entity mentions with symbolic en...   \n",
       "187  and ERNIE ( Zhang et al . , 2019 ) on hard - t...   \n",
       "188                                            and E -   \n",
       "189  is Not a Knowledge Base (Yet): Factual Knowled...   \n",
       "190                                                      \n",
       "191                                                      \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196  , and GPT-2 , on average , less than 5 % of th...   \n",
       "197                                                      \n",
       "198                         ELMo, and GPT-2 Embeddings   \n",
       "\n",
       "                                          split_tokens split_anchor_span  \\\n",
       "0    [We, introduce, a, new, language, representati...           (2, 18)   \n",
       "1    [Unlike, recent, language, representation, mod...            (6, 7)   \n",
       "2    [As, a, result, ,, the, pre, -, trained, BERT,...            (8, 9)   \n",
       "3    [It, obtains, new, state, -, of, -, the, -, ar...            (0, 1)   \n",
       "4    [BERT:, Pre-training, of, Deep, Bidirectional,...            (0, 1)   \n",
       "5    [Language, model, pretraining, has, led, to, s...              None   \n",
       "6    [Training, is, computationally, expensive,, of...              None   \n",
       "7    [We, present, a, replication, study, of, BERT,...            (6, 7)   \n",
       "8    [We, find, that, BERT, was, significantly, und...            (3, 4)   \n",
       "9    [Our, best, model, achieves, state-of-the-art,...              None   \n",
       "10   [These, results, highlight, the, importance, o...              None   \n",
       "11              [We, release, our, models, and, code.]              None   \n",
       "12   [RoBERTa:, A, Robustly, Optimized, BERT, Pretr...            (0, 1)   \n",
       "13   [As, Transfer, Learning, from, large-scale, pr...              None   \n",
       "14   [In, this, work,, we, propose, a, method, to, ...          (16, 17)   \n",
       "15   [While, most, prior, work, investigated, the, ...          (37, 40)   \n",
       "16   [To, leverage, the, inductive, biases, learned...              None   \n",
       "17   [Our, smaller,, faster, and, lighter, model, i...              None   \n",
       "18   [DistilBERT,, a, distilled, version, of, BERT:...            (0, 1)   \n",
       "19   [Pre-trained, text, encoders, have, rapidly, a...              None   \n",
       "20   [We, focus, on, one, such, model, ,, BERT, ,, ...            (3, 8)   \n",
       "21   [We, find, that, the, model, represents, the, ...            (3, 5)   \n",
       "22   [Qualitative, analysis, reveals, that, the, mo...            (4, 6)   \n",
       "23   [BERT, Rediscovers, the, Classical, NLP, Pipel...            (0, 1)   \n",
       "24   [Large, pre, -, trained, neural, networks, suc...            (8, 9)   \n",
       "25   [Most, recent, analysis, has, focused, on, mod...              None   \n",
       "26   [Complementary, to, these, works, ,, we, propo...          (22, 23)   \n",
       "27   [BERT, 's, attention, heads, exhibit, patterns...            (0, 2)   \n",
       "28   [We, further, show, that, certain, attention, ...              None   \n",
       "29   [For, example,, we, find, heads, that, attend,...              None   \n",
       "30   [Lastly, ,, we, propose, an, attention, -, bas...          (23, 25)   \n",
       "31                        [What, Does, BERT, Look, At]            (2, 3)   \n",
       "32                        [An, Analysis, of, BERT, 's]            (3, 5)   \n",
       "33   [Recently,, neural, models, pretrained, on, a,...          (23, 24)   \n",
       "34   [In, this, paper, ,, we, describe, a, simple, ...           (6, 21)   \n",
       "35   [Our, system, is, the, state, of, the, art, on...            (0, 2)   \n",
       "36   [The, code, to, reproduce, our, results, is, a...              None   \n",
       "37                   [Passage, Re-ranking, with, BERT]            (3, 4)   \n",
       "38   [I, assess, the, extent, to, which, the, recen...           (6, 11)   \n",
       "39   [The, BERT, model, performs, remarkably, well,...            (0, 3)   \n",
       "40           [Assessing, BERT's, Syntactic, Abilities]            (1, 2)   \n",
       "41   [Pretrained, contextual, representation, model...              None   \n",
       "42   [A, new, release, of, BERT, (Devlin,, 2018), i...            (4, 5)   \n",
       "43   [This, paper, explores, the, broader, cross-li...            (8, 9)   \n",
       "44   [We, compare, mBERT, with, the, best, -, publi...            (2, 3)   \n",
       "45   [Additionally, ,, we, investigate, the, most, ...          (10, 11)   \n",
       "46   [Beto,, Bentz,, Becas:, The, Surprising, Cross...            (8, 9)   \n",
       "47   [Language, model, pre-training,, such, as, BER...            (5, 6)   \n",
       "48   [However,, pre-trained, language, models, are,...              None   \n",
       "49   [To, accelerate, inference, and, reduce, model...              None   \n",
       "50   [By, leveraging, this, new, KD, method, ,, the...          (22, 26)   \n",
       "51   [Moreover, ,, we, introduce, a, new, two, -, s...          (12, 13)   \n",
       "52   [This, framework, ensures, that, TinyBERT, can...            (4, 5)   \n",
       "53   [TinyBERT, is, empirically, effective, and, ac...            (0, 1)   \n",
       "54   [TinyBERT, is, also, significantly, better, th...            (0, 1)   \n",
       "55   [TinyBERT:, Distilling, BERT, for, Natural, La...            (0, 1)   \n",
       "56   [BERT,, a, pre-trained, Transformer, model,, h...            (0, 1)   \n",
       "57   [In, this, paper,, we, describe, BERTSUM,, a, ...            (5, 6)   \n",
       "58   [Our, system, is, the, state, of, the, art, on...              None   \n",
       "59   [The, codes, to, reproduce, our, results, are,...              None   \n",
       "60   [Fine-tune, BERT, for, Extractive, Summarization]            (1, 2)   \n",
       "61   [Question-answering, plays, an, important, rol...              None   \n",
       "62   [Inspired, by, the, recent, success, of, machi...              None   \n",
       "63   [To, the, best, of, our, knowledge,, no, exist...              None   \n",
       "64   [In, this, work,, we, first, build, an, RRC, d...              None   \n",
       "65   [Since, ReviewRC, has, limited, training, exam...          (29, 34)   \n",
       "66   [To, show, the, generality, of, the, approach,...              None   \n",
       "67   [Experimental, results, demonstrate, that, the...              None   \n",
       "68   [The, datasets, and, code, are, available, at,...              None   \n",
       "69   [BERT, Post-Training, for, Review, Reading, Co...            (0, 1)   \n",
       "70   [Language, model, pre-training, has, proven, t...              None   \n",
       "71   [As, a, state, -, of, -, the, -, art, language...          (16, 24)   \n",
       "72   [In, this, paper, ,, we, conduct, exhaustive, ...          (16, 17)   \n",
       "73   [Finally,, the, proposed, solution, obtains, n...              None   \n",
       "74   [How, to, Fine-Tune, BERT, for, Text, Classifi...            (3, 4)   \n",
       "75   [We, show, that, BERT, (, Devlin, et, al, ., ,...            (3, 4)   \n",
       "76   [This, formulation, gives, way, to, a, natural...          (12, 13)   \n",
       "77   [We, generate, from, BERT, and, find, that, it...            (3, 4)   \n",
       "78   [Compared, to, the, generations, of, a, tradit...          (15, 16)   \n",
       "79   [BERT, has, a, Mouth, ,, and, It, Must, Speak,...            (0, 1)   \n",
       "80   [Aspect-based, sentiment, analysis, (ABSA),, w...              None   \n",
       "81   [In, this, paper,, we, construct, an, auxiliar...              None   \n",
       "82   [We, fine-tune, the, pre-trained, model, from,...            (6, 7)   \n",
       "83   [Utilizing, BERT, for, Aspect-Based, Sentiment...            (1, 2)   \n",
       "84   [Following, recent, successes, in, applying, B...            (5, 6)   \n",
       "85   [This, required, confronting, the, challenge, ...          (17, 18)   \n",
       "86   [We, address, this, issue, by, applying, infer...              None   \n",
       "87   [Experiments, on, TREC, microblog, and, newswi...              None   \n",
       "88   [Simple, Applications, of, BERT, for, Ad, Hoc,...            (3, 4)   \n",
       "89   [BERT-based, architectures, currently, give, s...            (0, 1)   \n",
       "90   [In, the, current, work, ,, we, focus, on, the...          (24, 25)   \n",
       "91   [Using, a, subset, of, GLUE, tasks, and, a, se...          (31, 32)   \n",
       "92   [Our, findings, suggest, that, there, is, a, l...              None   \n",
       "93   [While, different, heads, consistently, use, t...              None   \n",
       "94   [We, show, that, manually, disabling, attentio...          (20, 21)   \n",
       "95           [Revealing, the, Dark, Secrets, of, BERT]            (5, 6)   \n",
       "96   [Bidirectional, Encoder, Representations, from...            (5, 6)   \n",
       "97   [Recently, ,, an, upgraded, version, of, BERT,...            (6, 7)   \n",
       "98   [In, this, technical, report,, we, adapt, whol...              None   \n",
       "99   [The, model, was, trained, on, the, latest, Ch...            (0, 2)   \n",
       "100  [We, aim, to, provide, easy, extensibility, an...          (10, 12)   \n",
       "101  [The, model, is, verified, on, various, NLP, t...            (0, 2)   \n",
       "102  [Experimental, results, on, these, datasets, s...              None   \n",
       "103  [Moreover, ,, we, also, examine, the, effectiv...          (14, 15)   \n",
       "104  [We, release, the, pre-trained, model, (both, ...              None   \n",
       "105  [Pre-Training, with, Whole, Word, Masking, for...            (7, 8)   \n",
       "106  [BERT, model, has, been, successfully, applied...            (0, 1)   \n",
       "107  [However, ,, previous, work, trains, BERT, by,...            (5, 6)   \n",
       "108  [To, tackle, this, issue,, we, propose, a, mul...            (8, 9)   \n",
       "109  [In, addition,, we, find, that, splitting, art...              None   \n",
       "110  [By, leveraging, a, passage, ranker, to, selec...          (10, 11)   \n",
       "111  [Experiments, on, four, standard, benchmarks, ...           (9, 10)   \n",
       "112  [In, particular, ,, on, the, OpenSQuAD, datase...          (24, 25)   \n",
       "113  [Multi-passage, BERT:, A, Globally, Normalized...            (1, 2)   \n",
       "114  [Intent, classification, and, slot, filling, a...              None   \n",
       "115  [They, often, suffer, from, small-scale, human...              None   \n",
       "116  [Recently, a, new, language, representation, m...            (6, 7)   \n",
       "117  [However, ,, there, has, not, been, much, effo...          (10, 11)   \n",
       "118  [In, this, work, ,, we, propose, a, joint, int...          (16, 17)   \n",
       "119  [Experimental, results, demonstrate, that, our...              None   \n",
       "120  [BERT, for, Joint, Intent, Classification, and...            (0, 1)   \n",
       "121  [Conversational, search, is, an, emerging, top...              None   \n",
       "122  [One, of, the, major, challenges, to, multi-tu...              None   \n",
       "123  [Existing, methods, either, prepend, history, ...              None   \n",
       "124  [We, propose, a, conceptually, simple, yet, hi...              None   \n",
       "125  [It, enables, seamless, integration, of, conve...          (16, 17)   \n",
       "126  [We, first, explain, our, view, that, ConvQA, ...              None   \n",
       "127  [We, further, demonstrate, the, effectiveness,...              None   \n",
       "128  [Finally,, we, analyze, the, impact, of, diffe...              None   \n",
       "129  [BERT, with, History, Answer, Embedding, for, ...            (0, 1)   \n",
       "130  [This, paper, studies, the, performances, and,...            (8, 9)   \n",
       "131  [We, explore, several, different, ways, to, le...           (9, 10)   \n",
       "132  [Experimental, results, on, MS, MARCO, demonst...          (10, 11)   \n",
       "133  [Experimental, results, on, TREC, show, the, g...           (9, 10)   \n",
       "134  [Analyses, illustrate, how, BERT, allocates, i...            (3, 4)   \n",
       "135  [Understanding, the, Behaviors, of, BERT, in, ...            (4, 5)   \n",
       "136  [We, present, simple, BERT, -, based, models, ...           (2, 14)   \n",
       "137  [In, recent, years,, state-of-the-art, perform...              None   \n",
       "138  [In, this, paper,, extensive, experiments, on,...          (20, 21)   \n",
       "139  [To, our, knowledge,, we, are, the, first, to,...          (10, 11)   \n",
       "140  [Our, models, provide, strong, baselines, for,...            (0, 2)   \n",
       "141  [Simple, BERT, Models, for, Relation, Extracti...            (1, 2)   \n",
       "142  [Multi-task, learning, allows, the, sharing, o...              None   \n",
       "143  [In, natural, language, processing, several, r...              None   \n",
       "144  [These, results, are, based, on, fine-tuning, ...              None   \n",
       "145  [We, explore, the, multi-task, learning, setti...           (9, 10)   \n",
       "146  [We, introduce, new, adaptation, modules,, PAL...              None   \n",
       "147  [By, using, PALs, in, parallel, with, BERT, la...            (6, 7)   \n",
       "148  [BERT, and, PALs:, Projected, Attention, Layer...            (0, 1)   \n",
       "149  [Pre-training, by, language, modeling, has, be...              None   \n",
       "150  [In, this, paper, we, introduce, a, suite, of,...              None   \n",
       "151  [As, a, case, study, ,, we, apply, these, diag...          (10, 14)   \n",
       "152  [What, BERT, Is, Not:, Lessons, from, a, New, ...            (1, 2)   \n",
       "153  [Language, model, pre, -, training, ,, such, a...            (8, 9)   \n",
       "154  [However,, it, is, unclear, why, the, pre-trai...              None   \n",
       "155  [In, this, paper, ,, we, propose, to, visualiz...          (17, 18)   \n",
       "156  [First,, we, find, that, pre-training, reaches...              None   \n",
       "157  [We, also, demonstrate, that, the, fine, -, tu...          (16, 17)   \n",
       "158  [Second, ,, the, visualization, results, indic...          (10, 11)   \n",
       "159  [Third, ,, the, lower, layers, of, BERT, are, ...            (6, 7)   \n",
       "160  [Visualizing, and, Understanding, the, Effecti...            (6, 7)   \n",
       "161  [Data, augmentation, methods, are, often, appl...              None   \n",
       "162  [Recently, proposed, contextual, augmentation,...              None   \n",
       "163  [Bidirectional, Encoder, Representations, from...            (0, 8)   \n",
       "164  [We, propose, a, novel, data, augmentation, me...          (12, 13)   \n",
       "165  [We, retrofit, BERT, to, conditional, BERT, by...            (2, 3)   \n",
       "166  [In, our, paper,, “conditional, masked, langua...              None   \n",
       "167                                            [task.]              None   \n",
       "168  [The, well, trained, conditional, BERT, can, b...            (4, 5)   \n",
       "169  [Experiments, on, six, various, different, tex...              None   \n",
       "170      [Conditional, BERT, Contextual, Augmentation]            (1, 2)   \n",
       "171  [We, propose, a, practical, scheme, to, train,...              None   \n",
       "172  [Starting, from, a, public, multilingual, BERT...            (5, 6)   \n",
       "173  [We, show, that, our, model, especially, outpe...              None   \n",
       "174  [We, showcase, the, effectiveness, of, our, me...              None   \n",
       "175  [Small, and, Practical, BERT, Models, for, Seq...            (3, 4)   \n",
       "176  [Recently,, a, simple, combination, of, passag...          (13, 14)   \n",
       "177  [In, this, paper,, we, present, a, data, augme...              None   \n",
       "178  [We, apply, a, stage-wise, approach, to, fine,...            (8, 9)   \n",
       "179  [Experimental, results, show, large, gains, in...              None   \n",
       "180  [Data, Augmentation, for, BERT, Fine-Tuning, i...            (3, 4)   \n",
       "181  [The, BERT, language, model, (, LM, ), (, Devl...            (0, 7)   \n",
       "182                                 [Petroni, et, al.]              None   \n",
       "183  [(, 2019, ), take, this, as, evidence, that, B...            (8, 9)   \n",
       "184  [We, take, issue, with, this, interpretation, ...          (12, 13)   \n",
       "185  [More, specifically, ,, we, show, that, BERT, ...            (6, 8)   \n",
       "186  [As, a, remedy, ,, we, propose, E, -, BERT, ,,...          (13, 14)   \n",
       "187  [E, -, BERT, outperforms, both, BERT, and, ERN...            (5, 6)   \n",
       "188  [We, take, this, as, evidence, that, E, -, BER...          (22, 23)   \n",
       "189  [BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...            (0, 1)   \n",
       "190  [Replacing, static, word, embeddings, with, co...              None   \n",
       "191  [However, ,, just, how, contextual, are, the, ...          (16, 17)   \n",
       "192  [Are, there, infinitely, many, context-specifi...              None   \n",
       "193  [For, one,, we, find, that, the, contextualize...              None   \n",
       "194  [While, representations, of, the, same, word, ...              None   \n",
       "195  [This, suggests, that, upper, layers, of, cont...              None   \n",
       "196  [In, all, layers, of, ELMo, ,, BERT, ,, and, G...            (6, 7)   \n",
       "197  [How, Contextual, are, Contextualized, Word, R...              None   \n",
       "198  [Comparing, the, Geometry, of, BERT,, ELMo,, a...            (4, 5)   \n",
       "\n",
       "               group  \n",
       "0             stands  \n",
       "1           designed  \n",
       "2       conceptually  \n",
       "3           absolute  \n",
       "4               BERT  \n",
       "5                led  \n",
       "6                 is  \n",
       "7           measures  \n",
       "8                 We  \n",
       "9                 of  \n",
       "10             These  \n",
       "11           release  \n",
       "12           RoBERTa  \n",
       "13         prevalent  \n",
       "14           propose  \n",
       "15              show  \n",
       "16         introduce  \n",
       "17           cheaper  \n",
       "18        DistilBERT  \n",
       "19          advanced  \n",
       "20          captured  \n",
       "21             roles  \n",
       "22          analysis  \n",
       "23       Rediscovers  \n",
       "24           success  \n",
       "25           outputs  \n",
       "26           propose  \n",
       "27              BERT  \n",
       "28              show  \n",
       "29              find  \n",
       "30            Lastly  \n",
       "31              BERT  \n",
       "32          Analysis  \n",
       "33          Recently  \n",
       "34          describe  \n",
       "35              task  \n",
       "36         available  \n",
       "37                Re  \n",
       "38                 )  \n",
       "39          performs  \n",
       "40              BERT  \n",
       "41            models  \n",
       "42          includes  \n",
       "43               NER  \n",
       "44           compare  \n",
       "45       investigate  \n",
       "46              Beto  \n",
       "47          improved  \n",
       "48           usually  \n",
       "49        accelerate  \n",
       "50       transferred  \n",
       "51         introduce  \n",
       "52           ensures  \n",
       "53       empirically  \n",
       "54                is  \n",
       "55          TinyBERT  \n",
       "56            ground  \n",
       "57          describe  \n",
       "58             state  \n",
       "59         available  \n",
       "60              BERT  \n",
       "61          Question  \n",
       "62                 )  \n",
       "63              done  \n",
       "64             build  \n",
       "65               has  \n",
       "66           applied  \n",
       "67           results  \n",
       "68         available  \n",
       "69              Post  \n",
       "70            proven  \n",
       "71          achieved  \n",
       "72           conduct  \n",
       "73             state  \n",
       "74               How  \n",
       "75              show  \n",
       "76             gives  \n",
       "77          generate  \n",
       "78           diverse  \n",
       "79               has  \n",
       "80          analysis  \n",
       "81         construct  \n",
       "82              tune  \n",
       "83              BERT  \n",
       "84           explore  \n",
       "85          required  \n",
       "86            scores  \n",
       "87         effective  \n",
       "88            Simple  \n",
       "89         currently  \n",
       "90             focus  \n",
       "91           propose  \n",
       "92          findings  \n",
       "93              have  \n",
       "94              show  \n",
       "95         Revealing  \n",
       "96           Encoder  \n",
       "97          released  \n",
       "98              word  \n",
       "99           trained  \n",
       "100              aim  \n",
       "101    comprehension  \n",
       "102             show  \n",
       "103          examine  \n",
       "104          release  \n",
       "105              Pre  \n",
       "106     successfully  \n",
       "107            cause  \n",
       "108          propose  \n",
       "109             find  \n",
       "110               By  \n",
       "111           showed  \n",
       "112              and  \n",
       "113          passage  \n",
       "114              are  \n",
       "115           suffer  \n",
       "116            state  \n",
       "117           effort  \n",
       "118          propose  \n",
       "119      demonstrate  \n",
       "120             BERT  \n",
       "121               is  \n",
       "122               is  \n",
       "123            turns  \n",
       "124          propose  \n",
       "125          history  \n",
       "126          explain  \n",
       "127      demonstrate  \n",
       "128          analyze  \n",
       "129             BERT  \n",
       "130          studies  \n",
       "131          ranking  \n",
       "132      demonstrate  \n",
       "133          results  \n",
       "134       illustrate  \n",
       "135    Understanding  \n",
       "136          present  \n",
       "137         achieved  \n",
       "138             show  \n",
       "139            first  \n",
       "140          provide  \n",
       "141           Models  \n",
       "142           allows  \n",
       "143     successfully  \n",
       "144              are  \n",
       "145              add  \n",
       "146        introduce  \n",
       "147            match  \n",
       "148             BERT  \n",
       "149         approach  \n",
       "150        introduce  \n",
       "151            clear  \n",
       "152               Is  \n",
       "153         achieved  \n",
       "154          unclear  \n",
       "155          propose  \n",
       "156            First  \n",
       "157      demonstrate  \n",
       "158         indicate  \n",
       "159        invariant  \n",
       "160      Visualizing  \n",
       "161              are  \n",
       "162         Recently  \n",
       "163  Representations  \n",
       "164          propose  \n",
       "165             BERT  \n",
       "166            label  \n",
       "167             task  \n",
       "168          applied  \n",
       "169            tasks  \n",
       "170             BERT  \n",
       "171          propose  \n",
       "172               6x  \n",
       "173             show  \n",
       "174         showcase  \n",
       "175            Small  \n",
       "176            found  \n",
       "177          present  \n",
       "178         approach  \n",
       "179             show  \n",
       "180     Augmentation  \n",
       "181             good  \n",
       "182          Petroni  \n",
       "183                )  \n",
       "184            issue  \n",
       "185     specifically  \n",
       "186          propose  \n",
       "187             BERT  \n",
       "188             take  \n",
       "189               is  \n",
       "190          yielded  \n",
       "191  representations  \n",
       "192            words  \n",
       "193             find  \n",
       "194             have  \n",
       "195         suggests  \n",
       "196        explained  \n",
       "197   Contextualized  \n",
       "198        Comparing  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group on the main verb in the sentence, using allennlp dependency parsing (based on demo code)\n",
    "def group_main_verb(row):\n",
    "    p = dependency_predictor.predict(\n",
    "        sentence=' '.join([row['split_0'], row['split_1'], row['split_2']]).strip()\n",
    "    )\n",
    "    output = [p['hierplane_tree']['root']['word']]\n",
    "    return dict(zip(grouping_headers, output))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_main_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "# Return the match bounds of the sequence of elements of given sizes starting at list1[i1] and list2[i2] \n",
    "# that match\n",
    "# If no given size is returned, returns max matching sequence length\n",
    "# (ratio of element matches must be 1:some or some:1 between l1 and l2)\n",
    "# Returns [(l1 bounds), (l2 bounds)] or None if they do not match\n",
    "def list_elements_match(list1, list2, i1, i2, size1=None, size2=None):\n",
    "    matchlen = 0\n",
    "    if size1 is not None and size2 is not None:\n",
    "        # check for exact text match\n",
    "        matchlen = len(''.join(list1[i1:i1+size1]))\n",
    "        if ''.join(list1[i1:i1+size1]) != ''.join(list2[i2:i2+size2]):\n",
    "            return None\n",
    "    elif size1 is not None:\n",
    "        # and size2 is none\n",
    "        matchlen = len(''.join(list1[i1:i1+size1]))\n",
    "        if ''.join(list1[i1:i1+size1]) != ''.join(list2[i2:])[:matchlen]:\n",
    "            return None\n",
    "    elif size2 is not None:\n",
    "        # and size1 is none\n",
    "        matchlen = len(''.join(list2[i2:i2+size2]))\n",
    "        if ''.join(list2[i2:i2+size2]) != ''.join(list1[i1:])[:matchlen]:\n",
    "            return None\n",
    "    else:\n",
    "        # both are none; just calculate the match length\n",
    "        matchlen = 0\n",
    "        while l1concat[matching] == l2concat[matching]:\n",
    "            matchlen += 1\n",
    "    matchphrase = ''.join(list1[i1:])[:matchlen]\n",
    "    # get the exact bounds for list1\n",
    "    bound1 = 0\n",
    "    for i in range(len(list1)-i1):\n",
    "        if ''.join(list1[i1:i1+i]) == matchphrase:\n",
    "            bound1 = i\n",
    "            break\n",
    "    # get the exact bounds for list2\n",
    "    bound2 = 0\n",
    "    for i in range(len(list2)-i2):\n",
    "        if ''.join(list2[i2:i2+i]) == matchphrase:\n",
    "            bound2 = i\n",
    "            break\n",
    "    return [(i1, i1+bound1), (i2, i2+bound2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>split_0</th>\n",
       "      <th>split_1</th>\n",
       "      <th>split_2</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>split_anchor_span</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>We introduce</td>\n",
       "      <td>a new language representation model called BER...</td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, a, new, language, representati...</td>\n",
       "      <td>(2, 18)</td>\n",
       "      <td>[DET, NOUN, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Unlike recent language representation models, ...</td>\n",
       "      <td>Unlike recent language representation models ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is designed to pre - train deep bidirectional ...</td>\n",
       "      <td>[Unlike, recent, language, representation, mod...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a result, the pre-trained BERT model can be...</td>\n",
       "      <td>As a result , the pre - trained</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model can be fine - tuned with just one additi...</td>\n",
       "      <td>[As, a, result, ,, the, pre, -, trained, BERT,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>[NOUN, ADV, PUNCT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>It obtains new state-of-the-art results on ele...</td>\n",
       "      <td></td>\n",
       "      <td>It</td>\n",
       "      <td>obtains new state - of - the - art results on ...</td>\n",
       "      <td>[It, obtains, new, state, -, of, -, the, -, ar...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[VERB, NOUN, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT%3A-...</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Trans...</td>\n",
       "      <td></td>\n",
       "      <td>BERT:</td>\n",
       "      <td>Pre-training of Deep Bidirectional Transformer...</td>\n",
       "      <td>[BERT:, Pre-training, of, Deep, Bidirectional,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td>Language model pretraining has led to signific...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pretraining, has, led, to, s...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td>Training is computationally expensive, often d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Training, is, computationally, expensive,, of...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We present a replication study of BERT pretrai...</td>\n",
       "      <td>We present a replication study of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pretraining ( Devlin et al . , 2019 ) that car...</td>\n",
       "      <td>[We, present, a, replication, study, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>[PROPN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We find that BERT was significantly undertrain...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was significantly undertrained , and can match...</td>\n",
       "      <td>[We, find, that, BERT, was, significantly, und...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>[ADV, VERB, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td>Our best model achieves state-of-the-art resul...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, best, model, achieves, state-of-the-art,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td>These results highlight the importance of prev...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, highlight, the, importance, o...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td>We release our models and code.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, our, models, and, code.]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.semanticscholar.org/paper/RoBERTa%...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining...</td>\n",
       "      <td></td>\n",
       "      <td>RoBERTa:</td>\n",
       "      <td>A Robustly Optimized BERT Pretraining Approach</td>\n",
       "      <td>[RoBERTa:, A, Robustly, Optimized, BERT, Pretr...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[As, Transfer, Learning, from, large-scale, pr...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>In this work, we propose a method to pre-train...</td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>which can then be fine-tuned with good perform...</td>\n",
       "      <td>[In, this, work,, we, propose, a, method, to, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>While most prior work investigated the use of ...</td>\n",
       "      <td>a BERT model</td>\n",
       "      <td>by 40 % , while retaining 97 % of its language...</td>\n",
       "      <td>[While, most, prior, work, investigated, the, ...</td>\n",
       "      <td>(37, 40)</td>\n",
       "      <td>[VERB, ADJ, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td>To leverage the inductive biases learned by la...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, leverage, the, inductive, biases, learned...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td>Our smaller, faster and lighter model is cheap...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, smaller,, faster, and, lighter, model, i...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.semanticscholar.org/paper/DistilBE...</td>\n",
       "      <td>2</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td></td>\n",
       "      <td>DistilBERT,</td>\n",
       "      <td>a distilled version of BERT: smaller, faster, ...</td>\n",
       "      <td>[DistilBERT,, a, distilled, version, of, BERT:...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[NUM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td>Pre-trained text encoders have rapidly advance...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-trained, text, encoders, have, rapidly, a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We focus on one such model, BERT, and aim to q...</td>\n",
       "      <td>We focus on</td>\n",
       "      <td>one such model , BERT</td>\n",
       "      <td>, and aim to quantify where linguistic informa...</td>\n",
       "      <td>[We, focus, on, one, such, model, ,, BERT, ,, ...</td>\n",
       "      <td>(3, 8)</td>\n",
       "      <td>[NOUN, ADP, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We find that the model represents the steps of...</td>\n",
       "      <td>We find that</td>\n",
       "      <td>the model</td>\n",
       "      <td>represents the steps of the traditional NLP pi...</td>\n",
       "      <td>[We, find, that, the, model, represents, the, ...</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>[VERB, VERB, ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Qualitative analysis reveals that the model ca...</td>\n",
       "      <td>Qualitative analysis reveals that</td>\n",
       "      <td>the model</td>\n",
       "      <td>can and often does adjust this pipeline dynami...</td>\n",
       "      <td>[Qualitative, analysis, reveals, that, the, mo...</td>\n",
       "      <td>(4, 6)</td>\n",
       "      <td>[NOUN, VERB, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Red...</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Rediscovers the Classical NLP Pipeline</td>\n",
       "      <td>[BERT, Rediscovers, the, Classical, NLP, Pipel...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Large pre-trained neural networks such as BERT...</td>\n",
       "      <td>Large pre - trained neural networks such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>have had great recent success in NLP , motivat...</td>\n",
       "      <td>[Large, pre, -, trained, neural, networks, suc...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>[VERB, ADJ, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td>Most recent analysis has focused on model outp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Most, recent, analysis, has, focused, on, mod...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Complementary to these works, we propose metho...</td>\n",
       "      <td>Complementary to these works , we propose meth...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Complementary, to, these, works, ,, we, propo...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>[PROPN, ADP, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>BERT's attention heads exhibit patterns such a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention heads exhibit patterns such as atten...</td>\n",
       "      <td>[BERT, 's, attention, heads, exhibit, patterns...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td>We further show that certain attention heads c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, show, that, certain, attention, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td>For example, we find heads that attend to the ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, example,, we, find, heads, that, attend,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Lastly, we propose an attention-based probing ...</td>\n",
       "      <td>Lastly , we propose an attention - based probi...</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>attention</td>\n",
       "      <td>[Lastly, ,, we, propose, an, attention, -, bas...</td>\n",
       "      <td>(23, 25)</td>\n",
       "      <td>[ADP, VERB, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What Does BERT Look At?</td>\n",
       "      <td>What Does</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Look At</td>\n",
       "      <td>[What, Does, BERT, Look, At]</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-Doe...</td>\n",
       "      <td>4</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>An Analysis of BERT's Attention</td>\n",
       "      <td>An Analysis of</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td></td>\n",
       "      <td>[An, Analysis, of, BERT, 's]</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>[PROPN, ADP, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>Recently, neural models pretrained on a langua...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin et al., 2018), have achieved impressiv...</td>\n",
       "      <td>[Recently,, neural, models, pretrained, on, a,...</td>\n",
       "      <td>(23, 24)</td>\n",
       "      <td>[PROPN, PROPN, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe a simple re-impleme...</td>\n",
       "      <td>In this paper , we describe</td>\n",
       "      <td>a simple re - implementation of BERT for query...</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, ,, we, describe, a, simple, ...</td>\n",
       "      <td>(6, 21)</td>\n",
       "      <td>[DET, NOUN, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the TREC...</td>\n",
       "      <td></td>\n",
       "      <td>Our system</td>\n",
       "      <td>is the state of the art on the TREC - CAR data...</td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>[AUX, NOUN, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td>The code to reproduce our results is available...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, code, to, reproduce, our, results, is, a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Passage-...</td>\n",
       "      <td>5</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Passage Re-ranking with BERT</td>\n",
       "      <td>Passage Re-ranking with</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Passage, Re-ranking, with, BERT]</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>[PROPN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>I assess the extent to which the recently intr...</td>\n",
       "      <td>I assess the extent to which</td>\n",
       "      <td>the recently introduced BERT model</td>\n",
       "      <td>captures English syntactic phenomena , using (...</td>\n",
       "      <td>[I, assess, the, extent, to, which, the, recen...</td>\n",
       "      <td>(6, 11)</td>\n",
       "      <td>[NOUN, ADJ, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>The BERT model performs remarkably well on all...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT model</td>\n",
       "      <td>performs remarkably well on all cases</td>\n",
       "      <td>[The, BERT, model, performs, remarkably, well,...</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Assessin...</td>\n",
       "      <td>6</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Assessing BERT's Syntactic Abilities</td>\n",
       "      <td>Assessing</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>Syntactic Abilities</td>\n",
       "      <td>[Assessing, BERT's, Syntactic, Abilities]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td>Pretrained contextual representation models (P...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pretrained, contextual, representation, model...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>A new release of BERT (Devlin, 2018) includes ...</td>\n",
       "      <td>A new release of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Devlin, 2018) includes a model simultaneously...</td>\n",
       "      <td>[A, new, release, of, BERT, (Devlin,, 2018), i...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>[PROPN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>This paper explores the broader cross-lingual ...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>(multilingual) as a zero shot language transfe...</td>\n",
       "      <td>[This, paper, explores, the, broader, cross-li...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>[ADJ, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We compare mBERT with the best-published metho...</td>\n",
       "      <td>We compare</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>with the best - published methods for zero - s...</td>\n",
       "      <td>[We, compare, mBERT, with, the, best, -, publi...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>[NOUN, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we investigate the most effectiv...</td>\n",
       "      <td>Additionally , we investigate the most effecti...</td>\n",
       "      <td>mBERT</td>\n",
       "      <td>in this manner , determine to what extent mBER...</td>\n",
       "      <td>[Additionally, ,, we, investigate, the, most, ...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>[NOUN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Beto%2C-...</td>\n",
       "      <td>7</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>Beto, Bentz, Becas: The Surprising Cross-Lingu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Beto,, Bentz,, Becas:, The, Surprising, Cross...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>[PROPN, ADP, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre-training, such as</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>has significantly improved the performances of...</td>\n",
       "      <td>[Language, model, pre-training,, such, as, BER...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td>However, pre-trained language models are usual...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, pre-trained, language, models, are,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td>To accelerate inference and reduce model size ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, accelerate, inference, and, reduce, model...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>By leveraging this new KD method, the plenty o...</td>\n",
       "      <td>By leveraging this new KD method , the plenty ...</td>\n",
       "      <td>a small student TinyBERT</td>\n",
       "      <td></td>\n",
       "      <td>[By, leveraging, this, new, KD, method, ,, the...</td>\n",
       "      <td>(22, 26)</td>\n",
       "      <td>[DET, NOUN, ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Moreover, we introduce a new two-stage learnin...</td>\n",
       "      <td>Moreover , we introduce a new two - stage lear...</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>, which performs transformer distillation at b...</td>\n",
       "      <td>[Moreover, ,, we, introduce, a, new, two, -, s...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This framework ensures that TinyBERT can captu...</td>\n",
       "      <td>This framework ensures that</td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>can capture both the general - domain and task...</td>\n",
       "      <td>[This, framework, ensures, that, TinyBERT, can...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>[VERB, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>TinyBERT is empirically effective and achieves...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is empirically effective and achieves comparab...</td>\n",
       "      <td>[TinyBERT, is, empirically, effective, and, ac...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[AUX, ADJ, ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>TinyBERT is also significantly better than sta...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT</td>\n",
       "      <td>is also significantly better than state - of -...</td>\n",
       "      <td>[TinyBERT, is, also, significantly, better, th...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[AUX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>https://www.semanticscholar.org/paper/TinyBERT...</td>\n",
       "      <td>8</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>TinyBERT: Distilling BERT for Natural Language...</td>\n",
       "      <td></td>\n",
       "      <td>TinyBERT:</td>\n",
       "      <td>Distilling BERT for Natural Language Understan...</td>\n",
       "      <td>[TinyBERT:, Distilling, BERT, for, Natural, La...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[CCONJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT, a pre-trained Transformer model, has ach...</td>\n",
       "      <td></td>\n",
       "      <td>BERT,</td>\n",
       "      <td>a pre-trained Transformer model, has achieved ...</td>\n",
       "      <td>[BERT,, a, pre-trained, Transformer, model,, h...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[PROPN, VERB, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe BERTSUM, a simple v...</td>\n",
       "      <td>In this paper, we describe</td>\n",
       "      <td>BERTSUM,</td>\n",
       "      <td>a simple variant of BERT, for extractive summa...</td>\n",
       "      <td>[In, this, paper,, we, describe, BERTSUM,, a, ...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>[PROPN, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td>Our system is the state of the art on the CNN/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, system, is, the, state, of, the, art, on...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td>The codes to reproduce our results are availab...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, codes, to, reproduce, our, results, are,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Fine-tun...</td>\n",
       "      <td>9</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Fine-tune BERT for Extractive Summarization</td>\n",
       "      <td>Fine-tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Extractive Summarization</td>\n",
       "      <td>[Fine-tune, BERT, for, Extractive, Summarization]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td>Question-answering plays an important role in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Question-answering, plays, an, important, rol...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td>Inspired by the recent success of machine read...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Inspired, by, the, recent, success, of, machi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td>To the best of our knowledge, no existing work...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, the, best, of, our, knowledge,, no, exist...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td>In this work, we first build an RRC dataset ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work,, we, first, build, an, RRC, d...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>Since ReviewRC has limited training examples f...</td>\n",
       "      <td>the popular language model BERT</td>\n",
       "      <td>to enhance the performance of fine - tuning of...</td>\n",
       "      <td>[Since, ReviewRC, has, limited, training, exam...</td>\n",
       "      <td>(29, 34)</td>\n",
       "      <td>[VERB, ADV, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td>To show the generality of the approach, the pr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[To, show, the, generality, of, the, approach,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td>Experimental results demonstrate that the prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, the...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td>The datasets and code are available at this ht...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The, datasets, and, code, are, available, at,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-Pos...</td>\n",
       "      <td>10</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT Post-Training for Review Reading Comprehe...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>Post-Training for Review Reading Comprehension...</td>\n",
       "      <td>[BERT, Post-Training, for, Review, Reading, Co...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td>Language model pre-training has proven to be u...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Language, model, pre-training, has, proven, t...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>As a state-of-the-art language model pre-train...</td>\n",
       "      <td>As a state - of - the - art language model pre...</td>\n",
       "      <td>BERT ( Bidirectional Encoder Representations f...</td>\n",
       "      <td>has achieved amazing results in many language ...</td>\n",
       "      <td>[As, a, state, -, of, -, the, -, art, language...</td>\n",
       "      <td>(16, 24)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we conduct exhaustive experimen...</td>\n",
       "      <td>In this paper , we conduct exhaustive experime...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on text classification task and provide a gene...</td>\n",
       "      <td>[In, this, paper, ,, we, conduct, exhaustive, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>[PROPN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td>Finally, the proposed solution obtains new sta...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, the, proposed, solution, obtains, n...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-to-F...</td>\n",
       "      <td>11</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How to Fine-Tune BERT for Text Classification?</td>\n",
       "      <td>How to Fine-Tune</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Text Classification?</td>\n",
       "      <td>[How, to, Fine-Tune, BERT, for, Text, Classifi...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>[NOUN, ADJ, ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We show that BERT (Devlin et al., 2018) is a M...</td>\n",
       "      <td>We show that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>( Devlin et al . , 2018 ) is a Markov random f...</td>\n",
       "      <td>[We, show, that, BERT, (, Devlin, et, al, ., ,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>[PROPN, SCONJ, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>This formulation gives way to a natural proced...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[This, formulation, gives, way, to, a, natural...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>[PROPN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We generate from BERT and find that it can pro...</td>\n",
       "      <td>We generate from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and find that it can produce high - quality , ...</td>\n",
       "      <td>[We, generate, from, BERT, and, find, that, it...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>Compared to the generations of a traditional l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>generates sentences that are more diverse but ...</td>\n",
       "      <td>[Compared, to, the, generations, of, a, tradit...</td>\n",
       "      <td>(15, 16)</td>\n",
       "      <td>[NOUN, PUNCT, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-has...</td>\n",
       "      <td>12</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT has a Mouth, and It Must Speak: BERT as a...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>has a Mouth , and It Must Speak : BERT as a Ma...</td>\n",
       "      <td>[BERT, has, a, Mouth, ,, and, It, Must, Speak,...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[AUX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td>Aspect-based sentiment analysis (ABSA), which ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Aspect-based, sentiment, analysis, (ABSA),, w...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td>In this paper, we construct an auxiliary sente...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, construct, an, auxiliar...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We fine-tune the pre-trained model from BERT a...</td>\n",
       "      <td>We fine-tune the pre-trained model from</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and achieve new state-of-the-art results on Se...</td>\n",
       "      <td>[We, fine-tune, the, pre-trained, model, from,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>[NOUN, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Utilizin...</td>\n",
       "      <td>13</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Utilizing BERT for Aspect-Based Sentiment Anal...</td>\n",
       "      <td>Utilizing</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Aspect-Based Sentiment Analysis via Constr...</td>\n",
       "      <td>[Utilizing, BERT, for, Aspect-Based, Sentiment...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Following recent successes in applying BERT to...</td>\n",
       "      <td>Following recent successes in applying</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to question answering , we explore simple appl...</td>\n",
       "      <td>[Following, recent, successes, in, applying, B...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>[VERB, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>This required confronting the challenge posed ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>was designed to handle</td>\n",
       "      <td>[This, required, confronting, the, challenge, ...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>[ADP, NOUN, SCONJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td>We address this issue by applying inference on...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, address, this, issue, by, applying, infer...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td>Experiments on TREC microblog and newswire tes...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, TREC, microblog, and, newswi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-A...</td>\n",
       "      <td>14</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple Applications of BERT for Ad Hoc Documen...</td>\n",
       "      <td>Simple Applications of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Ad Hoc Document Retrieval</td>\n",
       "      <td>[Simple, Applications, of, BERT, for, Ad, Hoc,...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>[PROPN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT-based architectures currently give state-...</td>\n",
       "      <td></td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>architectures currently give state-of-the-art ...</td>\n",
       "      <td>[BERT-based, architectures, currently, give, s...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[PROPN, ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In the current work, we focus on the interpret...</td>\n",
       "      <td>In the current work , we focus on the interpre...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, the, current, work, ,, we, focus, on, the...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td>[PROPN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>Using a subset of GLUE tasks and a set of hand...</td>\n",
       "      <td>BERT's</td>\n",
       "      <td>heads.</td>\n",
       "      <td>[Using, a, subset, of, GLUE, tasks, and, a, se...</td>\n",
       "      <td>(31, 32)</td>\n",
       "      <td>[ADJ, ADP, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td>Our findings suggest that there is a limited s...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Our, findings, suggest, that, there, is, a, l...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td>While different heads consistently use the sam...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, different, heads, consistently, use, t...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>We show that manually disabling attention in c...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, manually, disabling, attentio...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td>[NOUN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Revealin...</td>\n",
       "      <td>15</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Revealing the Dark Secrets of BERT</td>\n",
       "      <td>Revealing the Dark Secrets of</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Revealing, the, Dark, Secrets, of, BERT]</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>[PROPN, ADP, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>(BERT)</td>\n",
       "      <td>has shown marvelous improvements across variou...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>[NOUN, ADP, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently, an upgraded version of BERT has been...</td>\n",
       "      <td>Recently , an upgraded version of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>has been released with Whole Word Masking ( WW...</td>\n",
       "      <td>[Recently, ,, an, upgraded, version, of, BERT,...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td>In this technical report, we adapt whole word ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, technical, report,, we, adapt, whol...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>The model was trained on the latest Chinese Wi...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>was trained on the latest Chinese Wikipedia dump</td>\n",
       "      <td>[The, model, was, trained, on, the, latest, Ch...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>We aim to provide easy extensibility and bette...</td>\n",
       "      <td>Chinese BERT</td>\n",
       "      <td>without changing any neural architecture or ev...</td>\n",
       "      <td>[We, aim, to, provide, easy, extensibility, an...</td>\n",
       "      <td>(10, 12)</td>\n",
       "      <td>[ADP, VERB, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>The model is verified on various NLP tasks, ac...</td>\n",
       "      <td></td>\n",
       "      <td>The model</td>\n",
       "      <td>is verified on various NLP tasks , across sent...</td>\n",
       "      <td>[The, model, is, verified, on, various, NLP, t...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>[VERB, NOUN, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td>Experimental results on these datasets show th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, on, these, datasets, s...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, we also examine the effectiveness of...</td>\n",
       "      <td>Moreover , we also examine the effectiveness o...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, ERNIE , BERT - wwm</td>\n",
       "      <td>[Moreover, ,, we, also, examine, the, effectiv...</td>\n",
       "      <td>(14, 15)</td>\n",
       "      <td>[PROPN, PUNCT, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td>We release the pre-trained model (both TensorF...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, release, the, pre-trained, model, (both, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Pre-Trai...</td>\n",
       "      <td>16</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chine...</td>\n",
       "      <td>Pre-Training with Whole Word Masking for Chinese</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Pre-Training, with, Whole, Word, Masking, for...</td>\n",
       "      <td>(7, 8)</td>\n",
       "      <td>[NOUN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT model has been successfully applied to op...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>model has been successfully applied to open - ...</td>\n",
       "      <td>[BERT, model, has, been, successfully, applied...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[NOUN, ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, previous work trains BERT by viewing ...</td>\n",
       "      <td>However , previous work trains</td>\n",
       "      <td>BERT</td>\n",
       "      <td>by viewing passages corresponding to the same ...</td>\n",
       "      <td>[However, ,, previous, work, trains, BERT, by,...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>[VERB, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>To tackle this issue, we propose a multi-passa...</td>\n",
       "      <td>To tackle this issue, we propose a multi-passage</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model to globally normalize answer scores acro...</td>\n",
       "      <td>[To, tackle, this, issue,, we, propose, a, mul...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>[NOUN, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td>In addition, we find that splitting articles i...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, addition,, we, find, that, splitting, art...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>By leveraging a passage ranker to select high-...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>gains additional 2%.</td>\n",
       "      <td>[By, leveraging, a, passage, ranker, to, selec...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>[VERB, ADJ, ADJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>Experiments on four standard benchmarks showed...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>outperforms all state-of-the-art models on all...</td>\n",
       "      <td>[Experiments, on, four, standard, benchmarks, ...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>[PROPN, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In particular, on the OpenSQuAD dataset, our m...</td>\n",
       "      <td>In particular , on the OpenSQuAD dataset , our...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>models , and 5.8 % EM and 6.5 % $ F_1 $ over B...</td>\n",
       "      <td>[In, particular, ,, on, the, OpenSQuAD, datase...</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td>[NOUN, DET, SYM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Multi-pa...</td>\n",
       "      <td>17</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-passage BERT: A Globally Normalized BERT...</td>\n",
       "      <td>Multi-passage</td>\n",
       "      <td>BERT:</td>\n",
       "      <td>A Globally Normalized BERT Model for Open-doma...</td>\n",
       "      <td>[Multi-passage, BERT:, A, Globally, Normalized...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[ADJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td>Intent classification and slot filling are two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Intent, classification, and, slot, filling, a...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td>They often suffer from small-scale human-label...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[They, often, suffer, from, small-scale, human...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Recently a new language representation model, ...</td>\n",
       "      <td>Recently a new language representation model,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[Recently, a, new, language, representation, m...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>[PUNCT, NOUN, ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>However, there has not been much effort on exp...</td>\n",
       "      <td>However , there has not been much effort on ex...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>for natural language understanding</td>\n",
       "      <td>[However, ,, there, has, not, been, much, effo...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>[VERB, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we propose a joint intent classi...</td>\n",
       "      <td>In this work , we propose a joint intent class...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[In, this, work, ,, we, propose, a, joint, int...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>[PROPN, ADP, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td>Experimental results demonstrate that our prop...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, demonstrate, that, our...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-for...</td>\n",
       "      <td>18</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT for Joint Intent Classification and Slot ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>for Joint Intent Classification and Slot Filling</td>\n",
       "      <td>[BERT, for, Joint, Intent, Classification, and...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td>Conversational search is an emerging topic in ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Conversational, search, is, an, emerging, top...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td>One of the major challenges to multi-turn conv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[One, of, the, major, challenges, to, multi-tu...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td>Existing methods either prepend history turns ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Existing, methods, either, prepend, history, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td>We propose a conceptually simple yet highly ef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, conceptually, simple, yet, hi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>It enables seamless integration of conversatio...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>(Bidirectional Encoder Representations from Tr...</td>\n",
       "      <td>[It, enables, seamless, integration, of, conve...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>[ADP, VERB, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td>We first explain our view that ConvQA is a sim...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, first, explain, our, view, that, ConvQA, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td>We further demonstrate the effectiveness of ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, further, demonstrate, the, effectiveness,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td>Finally, we analyze the impact of different nu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Finally,, we, analyze, the, impact, of, diffe...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-wit...</td>\n",
       "      <td>19</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT with History Answer Embedding for Convers...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>with History Answer Embedding for Conversation...</td>\n",
       "      <td>[BERT, with, History, Answer, Embedding, for, ...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>This paper studies the performances and behavi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in ranking tasks</td>\n",
       "      <td>[This, paper, studies, the, performances, and,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>[PROPN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>We explore several different ways to leverage ...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and fine-tune it on two ranking tasks: MS MARC...</td>\n",
       "      <td>[We, explore, several, different, ways, to, le...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>[NOUN, VERB, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>Experimental results on MS MARCO demonstrate t...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in question - answering focused passage rankin...</td>\n",
       "      <td>[Experimental, results, on, MS, MARCO, demonst...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>[PROPN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>Experimental results on TREC show the gaps bet...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>pre-trained on surrounding contexts and the ne...</td>\n",
       "      <td>[Experimental, results, on, TREC, show, the, g...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>[VERB, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyses illustrate how BERT allocates its att...</td>\n",
       "      <td>Analyses illustrate how</td>\n",
       "      <td>BERT</td>\n",
       "      <td>allocates its attentions between query - docum...</td>\n",
       "      <td>[Analyses, illustrate, how, BERT, allocates, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>[VERB, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Understa...</td>\n",
       "      <td>20</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Understanding the Behaviors of BERT in Ranking</td>\n",
       "      <td>Understanding the Behaviors of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in Ranking</td>\n",
       "      <td>[Understanding, the, Behaviors, of, BERT, in, ...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We present simple BERT-based models for relati...</td>\n",
       "      <td>We present</td>\n",
       "      <td>simple BERT - based models for relation extrac...</td>\n",
       "      <td></td>\n",
       "      <td>[We, present, simple, BERT, -, based, models, ...</td>\n",
       "      <td>(2, 14)</td>\n",
       "      <td>[ADJ, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td>In recent years, state-of-the-art performance ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, recent, years,, state-of-the-art, perform...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>In this paper, extensive experiments on datase...</td>\n",
       "      <td>BERT-based</td>\n",
       "      <td>model can achieve state-of-the-art performance.</td>\n",
       "      <td>[In, this, paper,, extensive, experiments, on,...</td>\n",
       "      <td>(20, 21)</td>\n",
       "      <td>[NOUN, ADP, PUNCT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>To our knowledge, we are the first to successf...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>in this manner.</td>\n",
       "      <td>[To, our, knowledge,, we, are, the, first, to,...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>[VERB, ADJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>Our models provide strong baselines for future...</td>\n",
       "      <td></td>\n",
       "      <td>Our models</td>\n",
       "      <td>provide strong baselines for future</td>\n",
       "      <td>[Our, models, provide, strong, baselines, for,...</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Simple-B...</td>\n",
       "      <td>21</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Simple BERT Models for Relation Extraction and...</td>\n",
       "      <td>Simple</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Relation Extraction and Semantic Ro...</td>\n",
       "      <td>[Simple, BERT, Models, for, Relation, Extracti...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td>Multi-task learning allows the sharing of usef...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Multi-task, learning, allows, the, sharing, o...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td>In natural language processing several recent ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, natural, language, processing, several, r...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td>These results are based on fine-tuning on each...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[These, results, are, based, on, fine-tuning, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>We explore the multi-task learning setting for...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>model on the GLUE benchmark, and how to best a...</td>\n",
       "      <td>[We, explore, the, multi-task, learning, setti...</td>\n",
       "      <td>(9, 10)</td>\n",
       "      <td>[NOUN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td>We introduce new adaptation modules, PALs or `...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, introduce, new, adaptation, modules,, PAL...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>By using PALs in parallel with BERT layers, we...</td>\n",
       "      <td>By using PALs in parallel with</td>\n",
       "      <td>BERT</td>\n",
       "      <td>layers, we match the performance of fine-tuned...</td>\n",
       "      <td>[By, using, PALs, in, parallel, with, BERT, la...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>[NOUN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-and...</td>\n",
       "      <td>22</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT and PALs: Projected Attention Layers for ...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>and PALs: Projected Attention Layers for Effic...</td>\n",
       "      <td>[BERT, and, PALs:, Projected, Attention, Layer...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td>Pre-training by language modeling has become a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Pre-training, by, language, modeling, has, be...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td>In this paper we introduce a suite of diagnost...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper, we, introduce, a, suite, of,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>As a case study, we apply these diagnostics to...</td>\n",
       "      <td>As a case study , we apply these diagnostics to</td>\n",
       "      <td>the popular BERT model</td>\n",
       "      <td>, finding that it can generally distinguish go...</td>\n",
       "      <td>[As, a, case, study, ,, we, apply, these, diag...</td>\n",
       "      <td>(10, 14)</td>\n",
       "      <td>[VERB, VERB, ADJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>https://www.semanticscholar.org/paper/What-BER...</td>\n",
       "      <td>23</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>What BERT Is Not: Lessons from a New Suite of ...</td>\n",
       "      <td>What</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Is Not: Lessons from a New Suite of Psycholing...</td>\n",
       "      <td>[What, BERT, Is, Not:, Lessons, from, a, New, ...</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[AUX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Language model pre-training, such as BERT, has...</td>\n",
       "      <td>Language model pre - training , such as</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, has achieved remarkable results in many NLP ...</td>\n",
       "      <td>[Language, model, pre, -, training, ,, such, a...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>[PROPN, SCONJ, ADJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td>However, it is unclear why the pre-training-th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[However,, it, is, unclear, why, the, pre-trai...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>In this paper, we propose to visualize loss la...</td>\n",
       "      <td>In this paper , we propose to visualize loss l...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on specific datasets</td>\n",
       "      <td>[In, this, paper, ,, we, propose, to, visualiz...</td>\n",
       "      <td>(17, 18)</td>\n",
       "      <td>[NOUN, VERB, ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td>First, we find that pre-training reaches a goo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[First,, we, find, that, pre-training, reaches...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We also demonstrate that the fine-tuning proce...</td>\n",
       "      <td>We also demonstrate that the fine - tuning pro...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is highly over - parameterized for downstream ...</td>\n",
       "      <td>[We, also, demonstrate, that, the, fine, -, tu...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>[ADV, ADJ, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>Second, the visualization results indicate tha...</td>\n",
       "      <td>Second , the visualization results indicate th...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>tends to generalize better because of the flat...</td>\n",
       "      <td>[Second, ,, the, visualization, results, indic...</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>[VERB, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>Third, the lower layers of BERT are more invar...</td>\n",
       "      <td>Third , the lower layers of</td>\n",
       "      <td>BERT</td>\n",
       "      <td>are more invariant during fine - tuning , whic...</td>\n",
       "      <td>[Third, ,, the, lower, layers, of, BERT, are, ...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>[ADJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Visualiz...</td>\n",
       "      <td>24</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>Visualizing and Understanding the Effectivenes...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[Visualizing, and, Understanding, the, Effecti...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>[PROPN, ADP, PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td>Data augmentation methods are often applied to...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Data, augmentation, methods, are, often, appl...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td>Recently proposed contextual augmentation augm...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Recently, proposed, contextual, augmentation,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td></td>\n",
       "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
       "      <td>demonstrates that a deep bidirectional languag...</td>\n",
       "      <td>[Bidirectional, Encoder, Representations, from...</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>We propose a novel data augmentation method fo...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>contextual augmentation.</td>\n",
       "      <td>[We, propose, a, novel, data, augmentation, me...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>[NOUN, ADJ, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>We retrofit BERT to conditional BERT by introd...</td>\n",
       "      <td>We retrofit</td>\n",
       "      <td>BERT</td>\n",
       "      <td>to conditional BERT by introducing a new condi...</td>\n",
       "      <td>[We, retrofit, BERT, to, conditional, BERT, by...</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td>In our paper, “conditional masked language mod...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, our, paper,, “conditional, masked, langua...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>task.</td>\n",
       "      <td>task.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[task.]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>The well trained conditional BERT can be appli...</td>\n",
       "      <td>The well trained conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>can be applied to enhance contextual augmentat...</td>\n",
       "      <td>[The, well, trained, conditional, BERT, can, b...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>[VERB, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>8</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td>Experiments on six various different text clas...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experiments, on, six, various, different, tex...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Conditio...</td>\n",
       "      <td>25</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Conditional BERT Contextual Augmentation</td>\n",
       "      <td>Conditional</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Contextual Augmentation</td>\n",
       "      <td>[Conditional, BERT, Contextual, Augmentation]</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td>We propose a practical scheme to train a singl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, propose, a, practical, scheme, to, train,...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Starting from a public multilingual BERT check...</td>\n",
       "      <td>Starting from a public multilingual</td>\n",
       "      <td>BERT</td>\n",
       "      <td>checkpoint, our final model is 6x smaller and ...</td>\n",
       "      <td>[Starting, from, a, public, multilingual, BERT...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>[NOUN, ADP, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td>We show that our model especially outperforms ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, show, that, our, model, especially, outpe...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td>We showcase the effectiveness of our method by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[We, showcase, the, effectiveness, of, our, me...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Small-an...</td>\n",
       "      <td>26</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Small and Practical BERT Models for Sequence L...</td>\n",
       "      <td>Small and Practical</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Models for Sequence Labeling</td>\n",
       "      <td>[Small, and, Practical, BERT, Models, for, Seq...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>[PROPN, ADJ, ADJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>Recently, a simple combination of passage retr...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>reader was found to be very effective for ques...</td>\n",
       "      <td>[Recently,, a, simple, combination, of, passag...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>[NOUN, CCONJ, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td>In this paper, we present a data augmentation ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[In, this, paper,, we, present, a, data, augme...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning ...</td>\n",
       "      <td>We apply a stage-wise approach to fine tuning</td>\n",
       "      <td>BERT</td>\n",
       "      <td>on multiple datasets, starting with data that ...</td>\n",
       "      <td>[We, apply, a, stage-wise, approach, to, fine,...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>[NOUN, ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td>Experimental results show large gains in effec...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Experimental, results, show, large, gains, in...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>https://www.semanticscholar.org/paper/Data-Aug...</td>\n",
       "      <td>27</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Augmentation for BERT Fine-Tuning in Open...</td>\n",
       "      <td>Data Augmentation for</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Fine-Tuning in Open-Domain Question Answering</td>\n",
       "      <td>[Data, Augmentation, for, BERT, Fine-Tuning, i...</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>[ADP, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>The BERT language model (LM) (Devlin et al., 2...</td>\n",
       "      <td></td>\n",
       "      <td>The BERT language model ( LM )</td>\n",
       "      <td>( Devlin et al . , 2019 ) is surprisingly good...</td>\n",
       "      <td>[The, BERT, language, model, (, LM, ), (, Devl...</td>\n",
       "      <td>(0, 7)</td>\n",
       "      <td>[NOUN, ADJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td>Petroni et al.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Petroni, et, al.]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>(2019) take this as evidence that BERT memoriz...</td>\n",
       "      <td>( 2019 ) take this as evidence that</td>\n",
       "      <td>BERT</td>\n",
       "      <td>memorizes factual knowledge during pre - training</td>\n",
       "      <td>[(, 2019, ), take, this, as, evidence, that, B...</td>\n",
       "      <td>(8, 9)</td>\n",
       "      <td>[PROPN, NOUN, SCONJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>We take issue with this interpretation and arg...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>is partly due to reasoning about ( the surface...</td>\n",
       "      <td>[We, take, issue, with, this, interpretation, ...</td>\n",
       "      <td>(12, 13)</td>\n",
       "      <td>[ADV, VERB, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>More specifically, we show that BERT's precisi...</td>\n",
       "      <td>More specifically , we show that</td>\n",
       "      <td>BERT 's</td>\n",
       "      <td>precision drops dramatically when we filter ce...</td>\n",
       "      <td>[More, specifically, ,, we, show, that, BERT, ...</td>\n",
       "      <td>(6, 8)</td>\n",
       "      <td>[NOUN, VERB, ADV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>As a remedy, we propose E-BERT, an extension o...</td>\n",
       "      <td>As a remedy , we propose E - BERT , an extensi...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>that replaces entity mentions with symbolic en...</td>\n",
       "      <td>[As, a, remedy, ,, we, propose, E, -, BERT, ,,...</td>\n",
       "      <td>(13, 14)</td>\n",
       "      <td>[PROPN, NOUN, VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>E-BERT outperforms both BERT and ERNIE (Zhang ...</td>\n",
       "      <td>E - BERT outperforms both</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and ERNIE ( Zhang et al . , 2019 ) on hard - t...</td>\n",
       "      <td>[E, -, BERT, outperforms, both, BERT, and, ERN...</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>[ADJ, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>7</td>\n",
       "      <td>We take this as evidence that E-BERT is richer...</td>\n",
       "      <td>We take this as evidence that E - BERT is rich...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>and E -</td>\n",
       "      <td>[We, take, this, as, evidence, that, E, -, BER...</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>[ADJ, NOUN, SCONJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>https://www.semanticscholar.org/paper/BERT-is-...</td>\n",
       "      <td>28</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>BERT is Not a Knowledge Base (Yet): Factual Kn...</td>\n",
       "      <td></td>\n",
       "      <td>BERT</td>\n",
       "      <td>is Not a Knowledge Base (Yet): Factual Knowled...</td>\n",
       "      <td>[BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>[AUX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td>Replacing static word embeddings with contextu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Replacing, static, word, embeddings, with, co...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>However, just how contextual are the contextua...</td>\n",
       "      <td>However , just how contextual are the contextu...</td>\n",
       "      <td>BERT</td>\n",
       "      <td></td>\n",
       "      <td>[However, ,, just, how, contextual, are, the, ...</td>\n",
       "      <td>(16, 17)</td>\n",
       "      <td>[NOUN, NOUN, SCONJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>2</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td>Are there infinitely many context-specific rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Are, there, infinitely, many, context-specifi...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>3</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td>For one, we find that the contextualized repre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[For, one,, we, find, that, the, contextualize...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>4</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td>While representations of the same word in diff...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[While, representations, of, the, same, word, ...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>5</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td>This suggests that upper layers of contextuali...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[This, suggests, that, upper, layers, of, cont...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>6</td>\n",
       "      <td>In all layers of ELMo, BERT, and GPT-2, on ave...</td>\n",
       "      <td>In all layers of ELMo ,</td>\n",
       "      <td>BERT</td>\n",
       "      <td>, and GPT-2 , on average , less than 5 % of th...</td>\n",
       "      <td>[In, all, layers, of, ELMo, ,, BERT, ,, and, G...</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>[ADP, NOUN, ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td>How Contextual are Contextualized Word Represe...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[How, Contextual, are, Contextualized, Word, R...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>https://www.semanticscholar.org/paper/How-Cont...</td>\n",
       "      <td>29</td>\n",
       "      <td>Title</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparing the Geometry of BERT, ELMo, and GPT-...</td>\n",
       "      <td>Comparing the Geometry of</td>\n",
       "      <td>BERT,</td>\n",
       "      <td>ELMo, and GPT-2 Embeddings</td>\n",
       "      <td>[Comparing, the, Geometry, of, BERT,, ELMo,, a...</td>\n",
       "      <td>(4, 5)</td>\n",
       "      <td>[PROPN, VERB]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  ID      Type  Index  \\\n",
       "0    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      0   \n",
       "1    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      1   \n",
       "2    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      2   \n",
       "3    https://www.semanticscholar.org/paper/BERT%3A-...   0  Abstract      3   \n",
       "4    https://www.semanticscholar.org/paper/BERT%3A-...   0     Title      0   \n",
       "5    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      0   \n",
       "6    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      1   \n",
       "7    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      2   \n",
       "8    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      3   \n",
       "9    https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      4   \n",
       "10   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      5   \n",
       "11   https://www.semanticscholar.org/paper/RoBERTa%...   1  Abstract      6   \n",
       "12   https://www.semanticscholar.org/paper/RoBERTa%...   1     Title      0   \n",
       "13   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      0   \n",
       "14   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      1   \n",
       "15   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      2   \n",
       "16   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      3   \n",
       "17   https://www.semanticscholar.org/paper/DistilBE...   2  Abstract      4   \n",
       "18   https://www.semanticscholar.org/paper/DistilBE...   2     Title      0   \n",
       "19   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      0   \n",
       "20   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      1   \n",
       "21   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      2   \n",
       "22   https://www.semanticscholar.org/paper/BERT-Red...   3  Abstract      3   \n",
       "23   https://www.semanticscholar.org/paper/BERT-Red...   3     Title      0   \n",
       "24   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      0   \n",
       "25   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      1   \n",
       "26   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      2   \n",
       "27   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      3   \n",
       "28   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      4   \n",
       "29   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      5   \n",
       "30   https://www.semanticscholar.org/paper/What-Doe...   4  Abstract      6   \n",
       "31   https://www.semanticscholar.org/paper/What-Doe...   4     Title      0   \n",
       "32   https://www.semanticscholar.org/paper/What-Doe...   4     Title      1   \n",
       "33   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      0   \n",
       "34   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      1   \n",
       "35   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      2   \n",
       "36   https://www.semanticscholar.org/paper/Passage-...   5  Abstract      3   \n",
       "37   https://www.semanticscholar.org/paper/Passage-...   5     Title      0   \n",
       "38   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      0   \n",
       "39   https://www.semanticscholar.org/paper/Assessin...   6  Abstract      1   \n",
       "40   https://www.semanticscholar.org/paper/Assessin...   6     Title      0   \n",
       "41   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      0   \n",
       "42   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      1   \n",
       "43   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      2   \n",
       "44   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      3   \n",
       "45   https://www.semanticscholar.org/paper/Beto%2C-...   7  Abstract      4   \n",
       "46   https://www.semanticscholar.org/paper/Beto%2C-...   7     Title      0   \n",
       "47   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      0   \n",
       "48   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      1   \n",
       "49   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      2   \n",
       "50   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      3   \n",
       "51   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      4   \n",
       "52   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      5   \n",
       "53   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      6   \n",
       "54   https://www.semanticscholar.org/paper/TinyBERT...   8  Abstract      7   \n",
       "55   https://www.semanticscholar.org/paper/TinyBERT...   8     Title      0   \n",
       "56   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      0   \n",
       "57   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      1   \n",
       "58   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      2   \n",
       "59   https://www.semanticscholar.org/paper/Fine-tun...   9  Abstract      3   \n",
       "60   https://www.semanticscholar.org/paper/Fine-tun...   9     Title      0   \n",
       "61   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      0   \n",
       "62   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      1   \n",
       "63   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      2   \n",
       "64   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      3   \n",
       "65   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      4   \n",
       "66   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      5   \n",
       "67   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      6   \n",
       "68   https://www.semanticscholar.org/paper/BERT-Pos...  10  Abstract      7   \n",
       "69   https://www.semanticscholar.org/paper/BERT-Pos...  10     Title      0   \n",
       "70   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      0   \n",
       "71   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      1   \n",
       "72   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      2   \n",
       "73   https://www.semanticscholar.org/paper/How-to-F...  11  Abstract      3   \n",
       "74   https://www.semanticscholar.org/paper/How-to-F...  11     Title      0   \n",
       "75   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      0   \n",
       "76   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      1   \n",
       "77   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      2   \n",
       "78   https://www.semanticscholar.org/paper/BERT-has...  12  Abstract      3   \n",
       "79   https://www.semanticscholar.org/paper/BERT-has...  12     Title      0   \n",
       "80   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      0   \n",
       "81   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      1   \n",
       "82   https://www.semanticscholar.org/paper/Utilizin...  13  Abstract      2   \n",
       "83   https://www.semanticscholar.org/paper/Utilizin...  13     Title      0   \n",
       "84   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      0   \n",
       "85   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      1   \n",
       "86   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      2   \n",
       "87   https://www.semanticscholar.org/paper/Simple-A...  14  Abstract      3   \n",
       "88   https://www.semanticscholar.org/paper/Simple-A...  14     Title      0   \n",
       "89   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      0   \n",
       "90   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      1   \n",
       "91   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      2   \n",
       "92   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      3   \n",
       "93   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      4   \n",
       "94   https://www.semanticscholar.org/paper/Revealin...  15  Abstract      5   \n",
       "95   https://www.semanticscholar.org/paper/Revealin...  15     Title      0   \n",
       "96   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      0   \n",
       "97   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      1   \n",
       "98   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      2   \n",
       "99   https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      3   \n",
       "100  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      4   \n",
       "101  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      5   \n",
       "102  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      6   \n",
       "103  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      7   \n",
       "104  https://www.semanticscholar.org/paper/Pre-Trai...  16  Abstract      8   \n",
       "105  https://www.semanticscholar.org/paper/Pre-Trai...  16     Title      0   \n",
       "106  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      0   \n",
       "107  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      1   \n",
       "108  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      2   \n",
       "109  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      3   \n",
       "110  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      4   \n",
       "111  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      5   \n",
       "112  https://www.semanticscholar.org/paper/Multi-pa...  17  Abstract      6   \n",
       "113  https://www.semanticscholar.org/paper/Multi-pa...  17     Title      0   \n",
       "114  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      0   \n",
       "115  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      1   \n",
       "116  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      2   \n",
       "117  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      3   \n",
       "118  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      4   \n",
       "119  https://www.semanticscholar.org/paper/BERT-for...  18  Abstract      5   \n",
       "120  https://www.semanticscholar.org/paper/BERT-for...  18     Title      0   \n",
       "121  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      0   \n",
       "122  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      1   \n",
       "123  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      2   \n",
       "124  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      3   \n",
       "125  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      4   \n",
       "126  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      5   \n",
       "127  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      6   \n",
       "128  https://www.semanticscholar.org/paper/BERT-wit...  19  Abstract      7   \n",
       "129  https://www.semanticscholar.org/paper/BERT-wit...  19     Title      0   \n",
       "130  https://www.semanticscholar.org/paper/Understa...  20  Abstract      0   \n",
       "131  https://www.semanticscholar.org/paper/Understa...  20  Abstract      1   \n",
       "132  https://www.semanticscholar.org/paper/Understa...  20  Abstract      2   \n",
       "133  https://www.semanticscholar.org/paper/Understa...  20  Abstract      3   \n",
       "134  https://www.semanticscholar.org/paper/Understa...  20  Abstract      4   \n",
       "135  https://www.semanticscholar.org/paper/Understa...  20     Title      0   \n",
       "136  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      0   \n",
       "137  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      1   \n",
       "138  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      2   \n",
       "139  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      3   \n",
       "140  https://www.semanticscholar.org/paper/Simple-B...  21  Abstract      4   \n",
       "141  https://www.semanticscholar.org/paper/Simple-B...  21     Title      0   \n",
       "142  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      0   \n",
       "143  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      1   \n",
       "144  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      2   \n",
       "145  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      3   \n",
       "146  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      4   \n",
       "147  https://www.semanticscholar.org/paper/BERT-and...  22  Abstract      5   \n",
       "148  https://www.semanticscholar.org/paper/BERT-and...  22     Title      0   \n",
       "149  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      0   \n",
       "150  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      1   \n",
       "151  https://www.semanticscholar.org/paper/What-BER...  23  Abstract      2   \n",
       "152  https://www.semanticscholar.org/paper/What-BER...  23     Title      0   \n",
       "153  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      0   \n",
       "154  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      1   \n",
       "155  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      2   \n",
       "156  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      3   \n",
       "157  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      4   \n",
       "158  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      5   \n",
       "159  https://www.semanticscholar.org/paper/Visualiz...  24  Abstract      6   \n",
       "160  https://www.semanticscholar.org/paper/Visualiz...  24     Title      0   \n",
       "161  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      0   \n",
       "162  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      1   \n",
       "163  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      2   \n",
       "164  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      3   \n",
       "165  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      4   \n",
       "166  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      5   \n",
       "167  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      6   \n",
       "168  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      7   \n",
       "169  https://www.semanticscholar.org/paper/Conditio...  25  Abstract      8   \n",
       "170  https://www.semanticscholar.org/paper/Conditio...  25     Title      0   \n",
       "171  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      0   \n",
       "172  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      1   \n",
       "173  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      2   \n",
       "174  https://www.semanticscholar.org/paper/Small-an...  26  Abstract      3   \n",
       "175  https://www.semanticscholar.org/paper/Small-an...  26     Title      0   \n",
       "176  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      0   \n",
       "177  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      1   \n",
       "178  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      2   \n",
       "179  https://www.semanticscholar.org/paper/Data-Aug...  27  Abstract      3   \n",
       "180  https://www.semanticscholar.org/paper/Data-Aug...  27     Title      0   \n",
       "181  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      0   \n",
       "182  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      1   \n",
       "183  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      2   \n",
       "184  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      3   \n",
       "185  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      4   \n",
       "186  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      5   \n",
       "187  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      6   \n",
       "188  https://www.semanticscholar.org/paper/BERT-is-...  28  Abstract      7   \n",
       "189  https://www.semanticscholar.org/paper/BERT-is-...  28     Title      0   \n",
       "190  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      0   \n",
       "191  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      1   \n",
       "192  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      2   \n",
       "193  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      3   \n",
       "194  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      4   \n",
       "195  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      5   \n",
       "196  https://www.semanticscholar.org/paper/How-Cont...  29  Abstract      6   \n",
       "197  https://www.semanticscholar.org/paper/How-Cont...  29     Title      0   \n",
       "198  https://www.semanticscholar.org/paper/How-Cont...  29     Title      1   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    We introduce a new language representation mod...   \n",
       "1    Unlike recent language representation models, ...   \n",
       "2    As a result, the pre-trained BERT model can be...   \n",
       "3    It obtains new state-of-the-art results on ele...   \n",
       "4    BERT: Pre-training of Deep Bidirectional Trans...   \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7    We present a replication study of BERT pretrai...   \n",
       "8    We find that BERT was significantly undertrain...   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12   RoBERTa: A Robustly Optimized BERT Pretraining...   \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18   DistilBERT, a distilled version of BERT: small...   \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20   We focus on one such model, BERT, and aim to q...   \n",
       "21   We find that the model represents the steps of...   \n",
       "22   Qualitative analysis reveals that the model ca...   \n",
       "23         BERT Rediscovers the Classical NLP Pipeline   \n",
       "24   Large pre-trained neural networks such as BERT...   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works, we propose metho...   \n",
       "27   BERT's attention heads exhibit patterns such a...   \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly, we propose an attention-based probing ...   \n",
       "31                             What Does BERT Look At?   \n",
       "32                     An Analysis of BERT's Attention   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34   In this paper, we describe a simple re-impleme...   \n",
       "35   Our system is the state of the art on the TREC...   \n",
       "36   The code to reproduce our results is available...   \n",
       "37                        Passage Re-ranking with BERT   \n",
       "38   I assess the extent to which the recently intr...   \n",
       "39   The BERT model performs remarkably well on all...   \n",
       "40                Assessing BERT's Syntactic Abilities   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42   A new release of BERT (Devlin, 2018) includes ...   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44   We compare mBERT with the best-published metho...   \n",
       "45   Additionally, we investigate the most effectiv...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47   Language model pre-training, such as BERT, has...   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method, the plenty o...   \n",
       "51   Moreover, we introduce a new two-stage learnin...   \n",
       "52   This framework ensures that TinyBERT can captu...   \n",
       "53   TinyBERT is empirically effective and achieves...   \n",
       "54   TinyBERT is also significantly better than sta...   \n",
       "55   TinyBERT: Distilling BERT for Natural Language...   \n",
       "56   BERT, a pre-trained Transformer model, has ach...   \n",
       "57   In this paper, we describe BERTSUM, a simple v...   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60         Fine-tune BERT for Extractive Summarization   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69   BERT Post-Training for Review Reading Comprehe...   \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state-of-the-art language model pre-train...   \n",
       "72   In this paper, we conduct exhaustive experimen...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74      How to Fine-Tune BERT for Text Classification?   \n",
       "75   We show that BERT (Devlin et al., 2018) is a M...   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77   We generate from BERT and find that it can pro...   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79   BERT has a Mouth, and It Must Speak: BERT as a...   \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82   We fine-tune the pre-trained model from BERT a...   \n",
       "83   Utilizing BERT for Aspect-Based Sentiment Anal...   \n",
       "84   Following recent successes in applying BERT to...   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88   Simple Applications of BERT for Ad Hoc Documen...   \n",
       "89   BERT-based architectures currently give state-...   \n",
       "90   In the current work, we focus on the interpret...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                  Revealing the Dark Secrets of BERT   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97   Recently, an upgraded version of BERT has been...   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99   The model was trained on the latest Chinese Wi...   \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101  The model is verified on various NLP tasks, ac...   \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover, we also examine the effectiveness of...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105  Pre-Training with Whole Word Masking for Chine...   \n",
       "106  BERT model has been successfully applied to op...   \n",
       "107  However, previous work trains BERT by viewing ...   \n",
       "108  To tackle this issue, we propose a multi-passa...   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular, on the OpenSQuAD dataset, our m...   \n",
       "113  Multi-passage BERT: A Globally Normalized BERT...   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116  Recently a new language representation model, ...   \n",
       "117  However, there has not been much effort on exp...   \n",
       "118  In this work, we propose a joint intent classi...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120  BERT for Joint Intent Classification and Slot ...   \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129  BERT with History Answer Embedding for Convers...   \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134  Analyses illustrate how BERT allocates its att...   \n",
       "135     Understanding the Behaviors of BERT in Ranking   \n",
       "136  We present simple BERT-based models for relati...   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140  Our models provide strong baselines for future...   \n",
       "141  Simple BERT Models for Relation Extraction and...   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147  By using PALs in parallel with BERT layers, we...   \n",
       "148  BERT and PALs: Projected Attention Layers for ...   \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151  As a case study, we apply these diagnostics to...   \n",
       "152  What BERT Is Not: Lessons from a New Suite of ...   \n",
       "153  Language model pre-training, such as BERT, has...   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper, we propose to visualize loss la...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine-tuning proce...   \n",
       "158  Second, the visualization results indicate tha...   \n",
       "159  Third, the lower layers of BERT are more invar...   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165  We retrofit BERT to conditional BERT by introd...   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168  The well trained conditional BERT can be appli...   \n",
       "169  Experiments on six various different text clas...   \n",
       "170           Conditional BERT Contextual Augmentation   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172  Starting from a public multilingual BERT check...   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175  Small and Practical BERT Models for Sequence L...   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178  We apply a stage-wise approach to fine tuning ...   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180  Data Augmentation for BERT Fine-Tuning in Open...   \n",
       "181  The BERT language model (LM) (Devlin et al., 2...   \n",
       "182                                     Petroni et al.   \n",
       "183  (2019) take this as evidence that BERT memoriz...   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185  More specifically, we show that BERT's precisi...   \n",
       "186  As a remedy, we propose E-BERT, an extension o...   \n",
       "187  E-BERT outperforms both BERT and ERNIE (Zhang ...   \n",
       "188  We take this as evidence that E-BERT is richer...   \n",
       "189  BERT is Not a Knowledge Base (Yet): Factual Kn...   \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However, just how contextual are the contextua...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196  In all layers of ELMo, BERT, and GPT-2, on ave...   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198  Comparing the Geometry of BERT, ELMo, and GPT-...   \n",
       "\n",
       "                                               split_0  \\\n",
       "0                                         We introduce   \n",
       "1       Unlike recent language representation models ,   \n",
       "2                      As a result , the pre - trained   \n",
       "3                                                        \n",
       "4                                                        \n",
       "5    Language model pretraining has led to signific...   \n",
       "6    Training is computationally expensive, often d...   \n",
       "7                    We present a replication study of   \n",
       "8                                         We find that   \n",
       "9    Our best model achieves state-of-the-art resul...   \n",
       "10   These results highlight the importance of prev...   \n",
       "11                     We release our models and code.   \n",
       "12                                                       \n",
       "13   As Transfer Learning from large-scale pre-trai...   \n",
       "14   In this work, we propose a method to pre-train...   \n",
       "15   While most prior work investigated the use of ...   \n",
       "16   To leverage the inductive biases learned by la...   \n",
       "17   Our smaller, faster and lighter model is cheap...   \n",
       "18                                                       \n",
       "19   Pre-trained text encoders have rapidly advance...   \n",
       "20                                         We focus on   \n",
       "21                                        We find that   \n",
       "22                   Qualitative analysis reveals that   \n",
       "23                                                       \n",
       "24         Large pre - trained neural networks such as   \n",
       "25   Most recent analysis has focused on model outp...   \n",
       "26   Complementary to these works , we propose meth...   \n",
       "27                                                       \n",
       "28   We further show that certain attention heads c...   \n",
       "29   For example, we find heads that attend to the ...   \n",
       "30   Lastly , we propose an attention - based probi...   \n",
       "31                                           What Does   \n",
       "32                                      An Analysis of   \n",
       "33   Recently, neural models pretrained on a langua...   \n",
       "34                         In this paper , we describe   \n",
       "35                                                       \n",
       "36   The code to reproduce our results is available...   \n",
       "37                             Passage Re-ranking with   \n",
       "38                        I assess the extent to which   \n",
       "39                                                       \n",
       "40                                           Assessing   \n",
       "41   Pretrained contextual representation models (P...   \n",
       "42                                    A new release of   \n",
       "43   This paper explores the broader cross-lingual ...   \n",
       "44                                          We compare   \n",
       "45   Additionally , we investigate the most effecti...   \n",
       "46   Beto, Bentz, Becas: The Surprising Cross-Lingu...   \n",
       "47                Language model pre-training, such as   \n",
       "48   However, pre-trained language models are usual...   \n",
       "49   To accelerate inference and reduce model size ...   \n",
       "50   By leveraging this new KD method , the plenty ...   \n",
       "51   Moreover , we introduce a new two - stage lear...   \n",
       "52                         This framework ensures that   \n",
       "53                                                       \n",
       "54                                                       \n",
       "55                                                       \n",
       "56                                                       \n",
       "57                          In this paper, we describe   \n",
       "58   Our system is the state of the art on the CNN/...   \n",
       "59   The codes to reproduce our results are availab...   \n",
       "60                                           Fine-tune   \n",
       "61   Question-answering plays an important role in ...   \n",
       "62   Inspired by the recent success of machine read...   \n",
       "63   To the best of our knowledge, no existing work...   \n",
       "64   In this work, we first build an RRC dataset ca...   \n",
       "65   Since ReviewRC has limited training examples f...   \n",
       "66   To show the generality of the approach, the pr...   \n",
       "67   Experimental results demonstrate that the prop...   \n",
       "68   The datasets and code are available at this ht...   \n",
       "69                                                       \n",
       "70   Language model pre-training has proven to be u...   \n",
       "71   As a state - of - the - art language model pre...   \n",
       "72   In this paper , we conduct exhaustive experime...   \n",
       "73   Finally, the proposed solution obtains new sta...   \n",
       "74                                    How to Fine-Tune   \n",
       "75                                        We show that   \n",
       "76   This formulation gives way to a natural proced...   \n",
       "77                                    We generate from   \n",
       "78   Compared to the generations of a traditional l...   \n",
       "79                                                       \n",
       "80   Aspect-based sentiment analysis (ABSA), which ...   \n",
       "81   In this paper, we construct an auxiliary sente...   \n",
       "82             We fine-tune the pre-trained model from   \n",
       "83                                           Utilizing   \n",
       "84              Following recent successes in applying   \n",
       "85   This required confronting the challenge posed ...   \n",
       "86   We address this issue by applying inference on...   \n",
       "87   Experiments on TREC microblog and newswire tes...   \n",
       "88                              Simple Applications of   \n",
       "89                                                       \n",
       "90   In the current work , we focus on the interpre...   \n",
       "91   Using a subset of GLUE tasks and a set of hand...   \n",
       "92   Our findings suggest that there is a limited s...   \n",
       "93   While different heads consistently use the sam...   \n",
       "94   We show that manually disabling attention in c...   \n",
       "95                       Revealing the Dark Secrets of   \n",
       "96   Bidirectional Encoder Representations from Tra...   \n",
       "97                   Recently , an upgraded version of   \n",
       "98   In this technical report, we adapt whole word ...   \n",
       "99                                                       \n",
       "100  We aim to provide easy extensibility and bette...   \n",
       "101                                                      \n",
       "102  Experimental results on these datasets show th...   \n",
       "103  Moreover , we also examine the effectiveness o...   \n",
       "104  We release the pre-trained model (both TensorF...   \n",
       "105   Pre-Training with Whole Word Masking for Chinese   \n",
       "106                                                      \n",
       "107                     However , previous work trains   \n",
       "108   To tackle this issue, we propose a multi-passage   \n",
       "109  In addition, we find that splitting articles i...   \n",
       "110  By leveraging a passage ranker to select high-...   \n",
       "111  Experiments on four standard benchmarks showed...   \n",
       "112  In particular , on the OpenSQuAD dataset , our...   \n",
       "113                                      Multi-passage   \n",
       "114  Intent classification and slot filling are two...   \n",
       "115  They often suffer from small-scale human-label...   \n",
       "116      Recently a new language representation model,   \n",
       "117  However , there has not been much effort on ex...   \n",
       "118  In this work , we propose a joint intent class...   \n",
       "119  Experimental results demonstrate that our prop...   \n",
       "120                                                      \n",
       "121  Conversational search is an emerging topic in ...   \n",
       "122  One of the major challenges to multi-turn conv...   \n",
       "123  Existing methods either prepend history turns ...   \n",
       "124  We propose a conceptually simple yet highly ef...   \n",
       "125  It enables seamless integration of conversatio...   \n",
       "126  We first explain our view that ConvQA is a sim...   \n",
       "127  We further demonstrate the effectiveness of ou...   \n",
       "128  Finally, we analyze the impact of different nu...   \n",
       "129                                                      \n",
       "130  This paper studies the performances and behavi...   \n",
       "131  We explore several different ways to leverage ...   \n",
       "132  Experimental results on MS MARCO demonstrate t...   \n",
       "133  Experimental results on TREC show the gaps bet...   \n",
       "134                            Analyses illustrate how   \n",
       "135                     Understanding the Behaviors of   \n",
       "136                                         We present   \n",
       "137  In recent years, state-of-the-art performance ...   \n",
       "138  In this paper, extensive experiments on datase...   \n",
       "139  To our knowledge, we are the first to successf...   \n",
       "140                                                      \n",
       "141                                             Simple   \n",
       "142  Multi-task learning allows the sharing of usef...   \n",
       "143  In natural language processing several recent ...   \n",
       "144  These results are based on fine-tuning on each...   \n",
       "145  We explore the multi-task learning setting for...   \n",
       "146  We introduce new adaptation modules, PALs or `...   \n",
       "147                     By using PALs in parallel with   \n",
       "148                                                      \n",
       "149  Pre-training by language modeling has become a...   \n",
       "150  In this paper we introduce a suite of diagnost...   \n",
       "151    As a case study , we apply these diagnostics to   \n",
       "152                                               What   \n",
       "153            Language model pre - training , such as   \n",
       "154  However, it is unclear why the pre-training-th...   \n",
       "155  In this paper , we propose to visualize loss l...   \n",
       "156  First, we find that pre-training reaches a goo...   \n",
       "157  We also demonstrate that the fine - tuning pro...   \n",
       "158  Second , the visualization results indicate th...   \n",
       "159                        Third , the lower layers of   \n",
       "160  Visualizing and Understanding the Effectivenes...   \n",
       "161  Data augmentation methods are often applied to...   \n",
       "162  Recently proposed contextual augmentation augm...   \n",
       "163                                                      \n",
       "164  We propose a novel data augmentation method fo...   \n",
       "165                                        We retrofit   \n",
       "166  In our paper, “conditional masked language mod...   \n",
       "167                                              task.   \n",
       "168                       The well trained conditional   \n",
       "169  Experiments on six various different text clas...   \n",
       "170                                        Conditional   \n",
       "171  We propose a practical scheme to train a singl...   \n",
       "172                Starting from a public multilingual   \n",
       "173  We show that our model especially outperforms ...   \n",
       "174  We showcase the effectiveness of our method by...   \n",
       "175                                Small and Practical   \n",
       "176  Recently, a simple combination of passage retr...   \n",
       "177  In this paper, we present a data augmentation ...   \n",
       "178      We apply a stage-wise approach to fine tuning   \n",
       "179  Experimental results show large gains in effec...   \n",
       "180                              Data Augmentation for   \n",
       "181                                                      \n",
       "182                                     Petroni et al.   \n",
       "183                ( 2019 ) take this as evidence that   \n",
       "184  We take issue with this interpretation and arg...   \n",
       "185                   More specifically , we show that   \n",
       "186  As a remedy , we propose E - BERT , an extensi...   \n",
       "187                          E - BERT outperforms both   \n",
       "188  We take this as evidence that E - BERT is rich...   \n",
       "189                                                      \n",
       "190  Replacing static word embeddings with contextu...   \n",
       "191  However , just how contextual are the contextu...   \n",
       "192  Are there infinitely many context-specific rep...   \n",
       "193  For one, we find that the contextualized repre...   \n",
       "194  While representations of the same word in diff...   \n",
       "195  This suggests that upper layers of contextuali...   \n",
       "196                            In all layers of ELMo ,   \n",
       "197  How Contextual are Contextualized Word Represe...   \n",
       "198                          Comparing the Geometry of   \n",
       "\n",
       "                                               split_1  \\\n",
       "0    a new language representation model called BER...   \n",
       "1                                                 BERT   \n",
       "2                                                 BERT   \n",
       "3                                                   It   \n",
       "4                                                BERT:   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7                                                 BERT   \n",
       "8                                                 BERT   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12                                            RoBERTa:   \n",
       "13                                                       \n",
       "14                                         DistilBERT,   \n",
       "15                                        a BERT model   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18                                         DistilBERT,   \n",
       "19                                                       \n",
       "20                               one such model , BERT   \n",
       "21                                           the model   \n",
       "22                                           the model   \n",
       "23                                                BERT   \n",
       "24                                                BERT   \n",
       "25                                                       \n",
       "26                                                BERT   \n",
       "27                                             BERT 's   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                             BERT 's   \n",
       "31                                                BERT   \n",
       "32                                             BERT 's   \n",
       "33                                                BERT   \n",
       "34   a simple re - implementation of BERT for query...   \n",
       "35                                          Our system   \n",
       "36                                                       \n",
       "37                                                BERT   \n",
       "38                  the recently introduced BERT model   \n",
       "39                                      The BERT model   \n",
       "40                                              BERT's   \n",
       "41                                                       \n",
       "42                                                BERT   \n",
       "43                                               mBERT   \n",
       "44                                               mBERT   \n",
       "45                                               mBERT   \n",
       "46                                                BERT   \n",
       "47                                               BERT,   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                            a small student TinyBERT   \n",
       "51                                            TinyBERT   \n",
       "52                                            TinyBERT   \n",
       "53                                            TinyBERT   \n",
       "54                                            TinyBERT   \n",
       "55                                           TinyBERT:   \n",
       "56                                               BERT,   \n",
       "57                                            BERTSUM,   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                                                BERT   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65                     the popular language model BERT   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69                                                BERT   \n",
       "70                                                       \n",
       "71   BERT ( Bidirectional Encoder Representations f...   \n",
       "72                                                BERT   \n",
       "73                                                       \n",
       "74                                                BERT   \n",
       "75                                                BERT   \n",
       "76                                                BERT   \n",
       "77                                                BERT   \n",
       "78                                                BERT   \n",
       "79                                                BERT   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82                                                BERT   \n",
       "83                                                BERT   \n",
       "84                                                BERT   \n",
       "85                                                BERT   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                                                BERT   \n",
       "89                                          BERT-based   \n",
       "90                                                BERT   \n",
       "91                                              BERT's   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                BERT   \n",
       "95                                                BERT   \n",
       "96                                              (BERT)   \n",
       "97                                                BERT   \n",
       "98                                                       \n",
       "99                                           The model   \n",
       "100                                       Chinese BERT   \n",
       "101                                          The model   \n",
       "102                                                      \n",
       "103                                               BERT   \n",
       "104                                                      \n",
       "105                                               BERT   \n",
       "106                                               BERT   \n",
       "107                                               BERT   \n",
       "108                                               BERT   \n",
       "109                                                      \n",
       "110                                               BERT   \n",
       "111                                               BERT   \n",
       "112                                               BERT   \n",
       "113                                              BERT:   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116                                               BERT   \n",
       "117                                               BERT   \n",
       "118                                               BERT   \n",
       "119                                                      \n",
       "120                                               BERT   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125                                               BERT   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129                                               BERT   \n",
       "130                                               BERT   \n",
       "131                                               BERT   \n",
       "132                                               BERT   \n",
       "133                                               BERT   \n",
       "134                                               BERT   \n",
       "135                                               BERT   \n",
       "136  simple BERT - based models for relation extrac...   \n",
       "137                                                      \n",
       "138                                         BERT-based   \n",
       "139                                               BERT   \n",
       "140                                         Our models   \n",
       "141                                               BERT   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145                                               BERT   \n",
       "146                                                      \n",
       "147                                               BERT   \n",
       "148                                               BERT   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151                             the popular BERT model   \n",
       "152                                               BERT   \n",
       "153                                               BERT   \n",
       "154                                                      \n",
       "155                                               BERT   \n",
       "156                                                      \n",
       "157                                               BERT   \n",
       "158                                               BERT   \n",
       "159                                               BERT   \n",
       "160                                               BERT   \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  Bidirectional Encoder Representations from Tra...   \n",
       "164                                               BERT   \n",
       "165                                               BERT   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168                                               BERT   \n",
       "169                                                      \n",
       "170                                               BERT   \n",
       "171                                                      \n",
       "172                                               BERT   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                                               BERT   \n",
       "176                                               BERT   \n",
       "177                                                      \n",
       "178                                               BERT   \n",
       "179                                                      \n",
       "180                                               BERT   \n",
       "181                     The BERT language model ( LM )   \n",
       "182                                                      \n",
       "183                                               BERT   \n",
       "184                                               BERT   \n",
       "185                                            BERT 's   \n",
       "186                                               BERT   \n",
       "187                                               BERT   \n",
       "188                                               BERT   \n",
       "189                                               BERT   \n",
       "190                                                      \n",
       "191                                               BERT   \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196                                               BERT   \n",
       "197                                                      \n",
       "198                                              BERT,   \n",
       "\n",
       "                                               split_2  \\\n",
       "0                                                        \n",
       "1    is designed to pre - train deep bidirectional ...   \n",
       "2    model can be fine - tuned with just one additi...   \n",
       "3    obtains new state - of - the - art results on ...   \n",
       "4    Pre-training of Deep Bidirectional Transformer...   \n",
       "5                                                        \n",
       "6                                                        \n",
       "7    pretraining ( Devlin et al . , 2019 ) that car...   \n",
       "8    was significantly undertrained , and can match...   \n",
       "9                                                        \n",
       "10                                                       \n",
       "11                                                       \n",
       "12      A Robustly Optimized BERT Pretraining Approach   \n",
       "13                                                       \n",
       "14   which can then be fine-tuned with good perform...   \n",
       "15   by 40 % , while retaining 97 % of its language...   \n",
       "16                                                       \n",
       "17                                                       \n",
       "18   a distilled version of BERT: smaller, faster, ...   \n",
       "19                                                       \n",
       "20   , and aim to quantify where linguistic informa...   \n",
       "21   represents the steps of the traditional NLP pi...   \n",
       "22   can and often does adjust this pipeline dynami...   \n",
       "23              Rediscovers the Classical NLP Pipeline   \n",
       "24   have had great recent success in NLP , motivat...   \n",
       "25                                                       \n",
       "26                                                       \n",
       "27   attention heads exhibit patterns such as atten...   \n",
       "28                                                       \n",
       "29                                                       \n",
       "30                                           attention   \n",
       "31                                             Look At   \n",
       "32                                                       \n",
       "33   (Devlin et al., 2018), have achieved impressiv...   \n",
       "34                                                       \n",
       "35   is the state of the art on the TREC - CAR data...   \n",
       "36                                                       \n",
       "37                                                       \n",
       "38   captures English syntactic phenomena , using (...   \n",
       "39               performs remarkably well on all cases   \n",
       "40                                 Syntactic Abilities   \n",
       "41                                                       \n",
       "42   (Devlin, 2018) includes a model simultaneously...   \n",
       "43   (multilingual) as a zero shot language transfe...   \n",
       "44   with the best - published methods for zero - s...   \n",
       "45   in this manner , determine to what extent mBER...   \n",
       "46                                                       \n",
       "47   has significantly improved the performances of...   \n",
       "48                                                       \n",
       "49                                                       \n",
       "50                                                       \n",
       "51   , which performs transformer distillation at b...   \n",
       "52   can capture both the general - domain and task...   \n",
       "53   is empirically effective and achieves comparab...   \n",
       "54   is also significantly better than state - of -...   \n",
       "55   Distilling BERT for Natural Language Understan...   \n",
       "56   a pre-trained Transformer model, has achieved ...   \n",
       "57   a simple variant of BERT, for extractive summa...   \n",
       "58                                                       \n",
       "59                                                       \n",
       "60                        for Extractive Summarization   \n",
       "61                                                       \n",
       "62                                                       \n",
       "63                                                       \n",
       "64                                                       \n",
       "65   to enhance the performance of fine - tuning of...   \n",
       "66                                                       \n",
       "67                                                       \n",
       "68                                                       \n",
       "69   Post-Training for Review Reading Comprehension...   \n",
       "70                                                       \n",
       "71   has achieved amazing results in many language ...   \n",
       "72   on text classification task and provide a gene...   \n",
       "73                                                       \n",
       "74                            for Text Classification?   \n",
       "75   ( Devlin et al . , 2018 ) is a Markov random f...   \n",
       "76                                                       \n",
       "77   and find that it can produce high - quality , ...   \n",
       "78   generates sentences that are more diverse but ...   \n",
       "79   has a Mouth , and It Must Speak : BERT as a Ma...   \n",
       "80                                                       \n",
       "81                                                       \n",
       "82   and achieve new state-of-the-art results on Se...   \n",
       "83   for Aspect-Based Sentiment Analysis via Constr...   \n",
       "84   to question answering , we explore simple appl...   \n",
       "85                              was designed to handle   \n",
       "86                                                       \n",
       "87                                                       \n",
       "88                       for Ad Hoc Document Retrieval   \n",
       "89   architectures currently give state-of-the-art ...   \n",
       "90                                                       \n",
       "91                                              heads.   \n",
       "92                                                       \n",
       "93                                                       \n",
       "94                                                       \n",
       "95                                                       \n",
       "96   has shown marvelous improvements across variou...   \n",
       "97   has been released with Whole Word Masking ( WW...   \n",
       "98                                                       \n",
       "99    was trained on the latest Chinese Wikipedia dump   \n",
       "100  without changing any neural architecture or ev...   \n",
       "101  is verified on various NLP tasks , across sent...   \n",
       "102                                                      \n",
       "103                               , ERNIE , BERT - wwm   \n",
       "104                                                      \n",
       "105                                                      \n",
       "106  model has been successfully applied to open - ...   \n",
       "107  by viewing passages corresponding to the same ...   \n",
       "108  model to globally normalize answer scores acro...   \n",
       "109                                                      \n",
       "110                               gains additional 2%.   \n",
       "111  outperforms all state-of-the-art models on all...   \n",
       "112  models , and 5.8 % EM and 6.5 % $ F_1 $ over B...   \n",
       "113  A Globally Normalized BERT Model for Open-doma...   \n",
       "114                                                      \n",
       "115                                                      \n",
       "116  (Bidirectional Encoder Representations from Tr...   \n",
       "117                 for natural language understanding   \n",
       "118                                                      \n",
       "119                                                      \n",
       "120   for Joint Intent Classification and Slot Filling   \n",
       "121                                                      \n",
       "122                                                      \n",
       "123                                                      \n",
       "124                                                      \n",
       "125  (Bidirectional Encoder Representations from Tr...   \n",
       "126                                                      \n",
       "127                                                      \n",
       "128                                                      \n",
       "129  with History Answer Embedding for Conversation...   \n",
       "130                                   in ranking tasks   \n",
       "131  and fine-tune it on two ranking tasks: MS MARC...   \n",
       "132  in question - answering focused passage rankin...   \n",
       "133  pre-trained on surrounding contexts and the ne...   \n",
       "134  allocates its attentions between query - docum...   \n",
       "135                                         in Ranking   \n",
       "136                                                      \n",
       "137                                                      \n",
       "138    model can achieve state-of-the-art performance.   \n",
       "139                                    in this manner.   \n",
       "140                provide strong baselines for future   \n",
       "141  Models for Relation Extraction and Semantic Ro...   \n",
       "142                                                      \n",
       "143                                                      \n",
       "144                                                      \n",
       "145  model on the GLUE benchmark, and how to best a...   \n",
       "146                                                      \n",
       "147  layers, we match the performance of fine-tuned...   \n",
       "148  and PALs: Projected Attention Layers for Effic...   \n",
       "149                                                      \n",
       "150                                                      \n",
       "151  , finding that it can generally distinguish go...   \n",
       "152  Is Not: Lessons from a New Suite of Psycholing...   \n",
       "153  , has achieved remarkable results in many NLP ...   \n",
       "154                                                      \n",
       "155                               on specific datasets   \n",
       "156                                                      \n",
       "157  is highly over - parameterized for downstream ...   \n",
       "158  tends to generalize better because of the flat...   \n",
       "159  are more invariant during fine - tuning , whic...   \n",
       "160                                                      \n",
       "161                                                      \n",
       "162                                                      \n",
       "163  demonstrates that a deep bidirectional languag...   \n",
       "164                           contextual augmentation.   \n",
       "165  to conditional BERT by introducing a new condi...   \n",
       "166                                                      \n",
       "167                                                      \n",
       "168  can be applied to enhance contextual augmentat...   \n",
       "169                                                      \n",
       "170                            Contextual Augmentation   \n",
       "171                                                      \n",
       "172  checkpoint, our final model is 6x smaller and ...   \n",
       "173                                                      \n",
       "174                                                      \n",
       "175                       Models for Sequence Labeling   \n",
       "176  reader was found to be very effective for ques...   \n",
       "177                                                      \n",
       "178  on multiple datasets, starting with data that ...   \n",
       "179                                                      \n",
       "180      Fine-Tuning in Open-Domain Question Answering   \n",
       "181  ( Devlin et al . , 2019 ) is surprisingly good...   \n",
       "182                                                      \n",
       "183  memorizes factual knowledge during pre - training   \n",
       "184  is partly due to reasoning about ( the surface...   \n",
       "185  precision drops dramatically when we filter ce...   \n",
       "186  that replaces entity mentions with symbolic en...   \n",
       "187  and ERNIE ( Zhang et al . , 2019 ) on hard - t...   \n",
       "188                                            and E -   \n",
       "189  is Not a Knowledge Base (Yet): Factual Knowled...   \n",
       "190                                                      \n",
       "191                                                      \n",
       "192                                                      \n",
       "193                                                      \n",
       "194                                                      \n",
       "195                                                      \n",
       "196  , and GPT-2 , on average , less than 5 % of th...   \n",
       "197                                                      \n",
       "198                         ELMo, and GPT-2 Embeddings   \n",
       "\n",
       "                                          split_tokens split_anchor_span  \\\n",
       "0    [We, introduce, a, new, language, representati...           (2, 18)   \n",
       "1    [Unlike, recent, language, representation, mod...            (6, 7)   \n",
       "2    [As, a, result, ,, the, pre, -, trained, BERT,...            (8, 9)   \n",
       "3    [It, obtains, new, state, -, of, -, the, -, ar...            (0, 1)   \n",
       "4    [BERT:, Pre-training, of, Deep, Bidirectional,...            (0, 1)   \n",
       "5    [Language, model, pretraining, has, led, to, s...              None   \n",
       "6    [Training, is, computationally, expensive,, of...              None   \n",
       "7    [We, present, a, replication, study, of, BERT,...            (6, 7)   \n",
       "8    [We, find, that, BERT, was, significantly, und...            (3, 4)   \n",
       "9    [Our, best, model, achieves, state-of-the-art,...              None   \n",
       "10   [These, results, highlight, the, importance, o...              None   \n",
       "11              [We, release, our, models, and, code.]              None   \n",
       "12   [RoBERTa:, A, Robustly, Optimized, BERT, Pretr...            (0, 1)   \n",
       "13   [As, Transfer, Learning, from, large-scale, pr...              None   \n",
       "14   [In, this, work,, we, propose, a, method, to, ...          (16, 17)   \n",
       "15   [While, most, prior, work, investigated, the, ...          (37, 40)   \n",
       "16   [To, leverage, the, inductive, biases, learned...              None   \n",
       "17   [Our, smaller,, faster, and, lighter, model, i...              None   \n",
       "18   [DistilBERT,, a, distilled, version, of, BERT:...            (0, 1)   \n",
       "19   [Pre-trained, text, encoders, have, rapidly, a...              None   \n",
       "20   [We, focus, on, one, such, model, ,, BERT, ,, ...            (3, 8)   \n",
       "21   [We, find, that, the, model, represents, the, ...            (3, 5)   \n",
       "22   [Qualitative, analysis, reveals, that, the, mo...            (4, 6)   \n",
       "23   [BERT, Rediscovers, the, Classical, NLP, Pipel...            (0, 1)   \n",
       "24   [Large, pre, -, trained, neural, networks, suc...            (8, 9)   \n",
       "25   [Most, recent, analysis, has, focused, on, mod...              None   \n",
       "26   [Complementary, to, these, works, ,, we, propo...          (22, 23)   \n",
       "27   [BERT, 's, attention, heads, exhibit, patterns...            (0, 2)   \n",
       "28   [We, further, show, that, certain, attention, ...              None   \n",
       "29   [For, example,, we, find, heads, that, attend,...              None   \n",
       "30   [Lastly, ,, we, propose, an, attention, -, bas...          (23, 25)   \n",
       "31                        [What, Does, BERT, Look, At]            (2, 3)   \n",
       "32                        [An, Analysis, of, BERT, 's]            (3, 5)   \n",
       "33   [Recently,, neural, models, pretrained, on, a,...          (23, 24)   \n",
       "34   [In, this, paper, ,, we, describe, a, simple, ...           (6, 21)   \n",
       "35   [Our, system, is, the, state, of, the, art, on...            (0, 2)   \n",
       "36   [The, code, to, reproduce, our, results, is, a...              None   \n",
       "37                   [Passage, Re-ranking, with, BERT]            (3, 4)   \n",
       "38   [I, assess, the, extent, to, which, the, recen...           (6, 11)   \n",
       "39   [The, BERT, model, performs, remarkably, well,...            (0, 3)   \n",
       "40           [Assessing, BERT's, Syntactic, Abilities]            (1, 2)   \n",
       "41   [Pretrained, contextual, representation, model...              None   \n",
       "42   [A, new, release, of, BERT, (Devlin,, 2018), i...            (4, 5)   \n",
       "43   [This, paper, explores, the, broader, cross-li...            (8, 9)   \n",
       "44   [We, compare, mBERT, with, the, best, -, publi...            (2, 3)   \n",
       "45   [Additionally, ,, we, investigate, the, most, ...          (10, 11)   \n",
       "46   [Beto,, Bentz,, Becas:, The, Surprising, Cross...            (8, 9)   \n",
       "47   [Language, model, pre-training,, such, as, BER...            (5, 6)   \n",
       "48   [However,, pre-trained, language, models, are,...              None   \n",
       "49   [To, accelerate, inference, and, reduce, model...              None   \n",
       "50   [By, leveraging, this, new, KD, method, ,, the...          (22, 26)   \n",
       "51   [Moreover, ,, we, introduce, a, new, two, -, s...          (12, 13)   \n",
       "52   [This, framework, ensures, that, TinyBERT, can...            (4, 5)   \n",
       "53   [TinyBERT, is, empirically, effective, and, ac...            (0, 1)   \n",
       "54   [TinyBERT, is, also, significantly, better, th...            (0, 1)   \n",
       "55   [TinyBERT:, Distilling, BERT, for, Natural, La...            (0, 1)   \n",
       "56   [BERT,, a, pre-trained, Transformer, model,, h...            (0, 1)   \n",
       "57   [In, this, paper,, we, describe, BERTSUM,, a, ...            (5, 6)   \n",
       "58   [Our, system, is, the, state, of, the, art, on...              None   \n",
       "59   [The, codes, to, reproduce, our, results, are,...              None   \n",
       "60   [Fine-tune, BERT, for, Extractive, Summarization]            (1, 2)   \n",
       "61   [Question-answering, plays, an, important, rol...              None   \n",
       "62   [Inspired, by, the, recent, success, of, machi...              None   \n",
       "63   [To, the, best, of, our, knowledge,, no, exist...              None   \n",
       "64   [In, this, work,, we, first, build, an, RRC, d...              None   \n",
       "65   [Since, ReviewRC, has, limited, training, exam...          (29, 34)   \n",
       "66   [To, show, the, generality, of, the, approach,...              None   \n",
       "67   [Experimental, results, demonstrate, that, the...              None   \n",
       "68   [The, datasets, and, code, are, available, at,...              None   \n",
       "69   [BERT, Post-Training, for, Review, Reading, Co...            (0, 1)   \n",
       "70   [Language, model, pre-training, has, proven, t...              None   \n",
       "71   [As, a, state, -, of, -, the, -, art, language...          (16, 24)   \n",
       "72   [In, this, paper, ,, we, conduct, exhaustive, ...          (16, 17)   \n",
       "73   [Finally,, the, proposed, solution, obtains, n...              None   \n",
       "74   [How, to, Fine-Tune, BERT, for, Text, Classifi...            (3, 4)   \n",
       "75   [We, show, that, BERT, (, Devlin, et, al, ., ,...            (3, 4)   \n",
       "76   [This, formulation, gives, way, to, a, natural...          (12, 13)   \n",
       "77   [We, generate, from, BERT, and, find, that, it...            (3, 4)   \n",
       "78   [Compared, to, the, generations, of, a, tradit...          (15, 16)   \n",
       "79   [BERT, has, a, Mouth, ,, and, It, Must, Speak,...            (0, 1)   \n",
       "80   [Aspect-based, sentiment, analysis, (ABSA),, w...              None   \n",
       "81   [In, this, paper,, we, construct, an, auxiliar...              None   \n",
       "82   [We, fine-tune, the, pre-trained, model, from,...            (6, 7)   \n",
       "83   [Utilizing, BERT, for, Aspect-Based, Sentiment...            (1, 2)   \n",
       "84   [Following, recent, successes, in, applying, B...            (5, 6)   \n",
       "85   [This, required, confronting, the, challenge, ...          (17, 18)   \n",
       "86   [We, address, this, issue, by, applying, infer...              None   \n",
       "87   [Experiments, on, TREC, microblog, and, newswi...              None   \n",
       "88   [Simple, Applications, of, BERT, for, Ad, Hoc,...            (3, 4)   \n",
       "89   [BERT-based, architectures, currently, give, s...            (0, 1)   \n",
       "90   [In, the, current, work, ,, we, focus, on, the...          (24, 25)   \n",
       "91   [Using, a, subset, of, GLUE, tasks, and, a, se...          (31, 32)   \n",
       "92   [Our, findings, suggest, that, there, is, a, l...              None   \n",
       "93   [While, different, heads, consistently, use, t...              None   \n",
       "94   [We, show, that, manually, disabling, attentio...          (20, 21)   \n",
       "95           [Revealing, the, Dark, Secrets, of, BERT]            (5, 6)   \n",
       "96   [Bidirectional, Encoder, Representations, from...            (5, 6)   \n",
       "97   [Recently, ,, an, upgraded, version, of, BERT,...            (6, 7)   \n",
       "98   [In, this, technical, report,, we, adapt, whol...              None   \n",
       "99   [The, model, was, trained, on, the, latest, Ch...            (0, 2)   \n",
       "100  [We, aim, to, provide, easy, extensibility, an...          (10, 12)   \n",
       "101  [The, model, is, verified, on, various, NLP, t...            (0, 2)   \n",
       "102  [Experimental, results, on, these, datasets, s...              None   \n",
       "103  [Moreover, ,, we, also, examine, the, effectiv...          (14, 15)   \n",
       "104  [We, release, the, pre-trained, model, (both, ...              None   \n",
       "105  [Pre-Training, with, Whole, Word, Masking, for...            (7, 8)   \n",
       "106  [BERT, model, has, been, successfully, applied...            (0, 1)   \n",
       "107  [However, ,, previous, work, trains, BERT, by,...            (5, 6)   \n",
       "108  [To, tackle, this, issue,, we, propose, a, mul...            (8, 9)   \n",
       "109  [In, addition,, we, find, that, splitting, art...              None   \n",
       "110  [By, leveraging, a, passage, ranker, to, selec...          (10, 11)   \n",
       "111  [Experiments, on, four, standard, benchmarks, ...           (9, 10)   \n",
       "112  [In, particular, ,, on, the, OpenSQuAD, datase...          (24, 25)   \n",
       "113  [Multi-passage, BERT:, A, Globally, Normalized...            (1, 2)   \n",
       "114  [Intent, classification, and, slot, filling, a...              None   \n",
       "115  [They, often, suffer, from, small-scale, human...              None   \n",
       "116  [Recently, a, new, language, representation, m...            (6, 7)   \n",
       "117  [However, ,, there, has, not, been, much, effo...          (10, 11)   \n",
       "118  [In, this, work, ,, we, propose, a, joint, int...          (16, 17)   \n",
       "119  [Experimental, results, demonstrate, that, our...              None   \n",
       "120  [BERT, for, Joint, Intent, Classification, and...            (0, 1)   \n",
       "121  [Conversational, search, is, an, emerging, top...              None   \n",
       "122  [One, of, the, major, challenges, to, multi-tu...              None   \n",
       "123  [Existing, methods, either, prepend, history, ...              None   \n",
       "124  [We, propose, a, conceptually, simple, yet, hi...              None   \n",
       "125  [It, enables, seamless, integration, of, conve...          (16, 17)   \n",
       "126  [We, first, explain, our, view, that, ConvQA, ...              None   \n",
       "127  [We, further, demonstrate, the, effectiveness,...              None   \n",
       "128  [Finally,, we, analyze, the, impact, of, diffe...              None   \n",
       "129  [BERT, with, History, Answer, Embedding, for, ...            (0, 1)   \n",
       "130  [This, paper, studies, the, performances, and,...            (8, 9)   \n",
       "131  [We, explore, several, different, ways, to, le...           (9, 10)   \n",
       "132  [Experimental, results, on, MS, MARCO, demonst...          (10, 11)   \n",
       "133  [Experimental, results, on, TREC, show, the, g...           (9, 10)   \n",
       "134  [Analyses, illustrate, how, BERT, allocates, i...            (3, 4)   \n",
       "135  [Understanding, the, Behaviors, of, BERT, in, ...            (4, 5)   \n",
       "136  [We, present, simple, BERT, -, based, models, ...           (2, 14)   \n",
       "137  [In, recent, years,, state-of-the-art, perform...              None   \n",
       "138  [In, this, paper,, extensive, experiments, on,...          (20, 21)   \n",
       "139  [To, our, knowledge,, we, are, the, first, to,...          (10, 11)   \n",
       "140  [Our, models, provide, strong, baselines, for,...            (0, 2)   \n",
       "141  [Simple, BERT, Models, for, Relation, Extracti...            (1, 2)   \n",
       "142  [Multi-task, learning, allows, the, sharing, o...              None   \n",
       "143  [In, natural, language, processing, several, r...              None   \n",
       "144  [These, results, are, based, on, fine-tuning, ...              None   \n",
       "145  [We, explore, the, multi-task, learning, setti...           (9, 10)   \n",
       "146  [We, introduce, new, adaptation, modules,, PAL...              None   \n",
       "147  [By, using, PALs, in, parallel, with, BERT, la...            (6, 7)   \n",
       "148  [BERT, and, PALs:, Projected, Attention, Layer...            (0, 1)   \n",
       "149  [Pre-training, by, language, modeling, has, be...              None   \n",
       "150  [In, this, paper, we, introduce, a, suite, of,...              None   \n",
       "151  [As, a, case, study, ,, we, apply, these, diag...          (10, 14)   \n",
       "152  [What, BERT, Is, Not:, Lessons, from, a, New, ...            (1, 2)   \n",
       "153  [Language, model, pre, -, training, ,, such, a...            (8, 9)   \n",
       "154  [However,, it, is, unclear, why, the, pre-trai...              None   \n",
       "155  [In, this, paper, ,, we, propose, to, visualiz...          (17, 18)   \n",
       "156  [First,, we, find, that, pre-training, reaches...              None   \n",
       "157  [We, also, demonstrate, that, the, fine, -, tu...          (16, 17)   \n",
       "158  [Second, ,, the, visualization, results, indic...          (10, 11)   \n",
       "159  [Third, ,, the, lower, layers, of, BERT, are, ...            (6, 7)   \n",
       "160  [Visualizing, and, Understanding, the, Effecti...            (6, 7)   \n",
       "161  [Data, augmentation, methods, are, often, appl...              None   \n",
       "162  [Recently, proposed, contextual, augmentation,...              None   \n",
       "163  [Bidirectional, Encoder, Representations, from...            (0, 8)   \n",
       "164  [We, propose, a, novel, data, augmentation, me...          (12, 13)   \n",
       "165  [We, retrofit, BERT, to, conditional, BERT, by...            (2, 3)   \n",
       "166  [In, our, paper,, “conditional, masked, langua...              None   \n",
       "167                                            [task.]              None   \n",
       "168  [The, well, trained, conditional, BERT, can, b...            (4, 5)   \n",
       "169  [Experiments, on, six, various, different, tex...              None   \n",
       "170      [Conditional, BERT, Contextual, Augmentation]            (1, 2)   \n",
       "171  [We, propose, a, practical, scheme, to, train,...              None   \n",
       "172  [Starting, from, a, public, multilingual, BERT...            (5, 6)   \n",
       "173  [We, show, that, our, model, especially, outpe...              None   \n",
       "174  [We, showcase, the, effectiveness, of, our, me...              None   \n",
       "175  [Small, and, Practical, BERT, Models, for, Seq...            (3, 4)   \n",
       "176  [Recently,, a, simple, combination, of, passag...          (13, 14)   \n",
       "177  [In, this, paper,, we, present, a, data, augme...              None   \n",
       "178  [We, apply, a, stage-wise, approach, to, fine,...            (8, 9)   \n",
       "179  [Experimental, results, show, large, gains, in...              None   \n",
       "180  [Data, Augmentation, for, BERT, Fine-Tuning, i...            (3, 4)   \n",
       "181  [The, BERT, language, model, (, LM, ), (, Devl...            (0, 7)   \n",
       "182                                 [Petroni, et, al.]              None   \n",
       "183  [(, 2019, ), take, this, as, evidence, that, B...            (8, 9)   \n",
       "184  [We, take, issue, with, this, interpretation, ...          (12, 13)   \n",
       "185  [More, specifically, ,, we, show, that, BERT, ...            (6, 8)   \n",
       "186  [As, a, remedy, ,, we, propose, E, -, BERT, ,,...          (13, 14)   \n",
       "187  [E, -, BERT, outperforms, both, BERT, and, ERN...            (5, 6)   \n",
       "188  [We, take, this, as, evidence, that, E, -, BER...          (22, 23)   \n",
       "189  [BERT, is, Not, a, Knowledge, Base, (Yet):, Fa...            (0, 1)   \n",
       "190  [Replacing, static, word, embeddings, with, co...              None   \n",
       "191  [However, ,, just, how, contextual, are, the, ...          (16, 17)   \n",
       "192  [Are, there, infinitely, many, context-specifi...              None   \n",
       "193  [For, one,, we, find, that, the, contextualize...              None   \n",
       "194  [While, representations, of, the, same, word, ...              None   \n",
       "195  [This, suggests, that, upper, layers, of, cont...              None   \n",
       "196  [In, all, layers, of, ELMo, ,, BERT, ,, and, G...            (6, 7)   \n",
       "197  [How, Contextual, are, Contextualized, Word, R...              None   \n",
       "198  [Comparing, the, Geometry, of, BERT,, ELMo,, a...            (4, 5)   \n",
       "\n",
       "                     group  \n",
       "0        [DET, NOUN, VERB]  \n",
       "1                   [VERB]  \n",
       "2       [NOUN, ADV, PUNCT]  \n",
       "3      [VERB, NOUN, PROPN]  \n",
       "4                  [PROPN]  \n",
       "5                           \n",
       "6                           \n",
       "7       [PROPN, ADP, NOUN]  \n",
       "8        [ADV, VERB, PRON]  \n",
       "9                           \n",
       "10                          \n",
       "11                          \n",
       "12                 [PROPN]  \n",
       "13                          \n",
       "14                  [VERB]  \n",
       "15       [VERB, ADJ, VERB]  \n",
       "16                          \n",
       "17                          \n",
       "18                   [NUM]  \n",
       "19                          \n",
       "20       [NOUN, ADP, VERB]  \n",
       "21       [VERB, VERB, ADV]  \n",
       "22      [NOUN, VERB, VERB]  \n",
       "23                  [NOUN]  \n",
       "24       [VERB, ADJ, NOUN]  \n",
       "25                          \n",
       "26      [PROPN, ADP, VERB]  \n",
       "27                 [PROPN]  \n",
       "28                          \n",
       "29                          \n",
       "30       [ADP, VERB, VERB]  \n",
       "31                 [PROPN]  \n",
       "32     [PROPN, ADP, PROPN]  \n",
       "33   [PROPN, PROPN, PROPN]  \n",
       "34       [DET, NOUN, VERB]  \n",
       "35       [AUX, NOUN, NOUN]  \n",
       "36                          \n",
       "37      [PROPN, ADP, NOUN]  \n",
       "38       [NOUN, ADJ, NOUN]  \n",
       "39                  [VERB]  \n",
       "40                 [PROPN]  \n",
       "41                          \n",
       "42      [PROPN, ADP, NOUN]  \n",
       "43        [ADJ, ADP, NOUN]  \n",
       "44            [NOUN, VERB]  \n",
       "45       [NOUN, ADP, NOUN]  \n",
       "46     [PROPN, ADP, PROPN]  \n",
       "47                  [VERB]  \n",
       "48                          \n",
       "49                          \n",
       "50        [DET, NOUN, ADP]  \n",
       "51                  [VERB]  \n",
       "52            [VERB, VERB]  \n",
       "53         [AUX, ADJ, ADV]  \n",
       "54                   [AUX]  \n",
       "55                 [CCONJ]  \n",
       "56     [PROPN, VERB, NOUN]  \n",
       "57           [PROPN, VERB]  \n",
       "58                          \n",
       "59                          \n",
       "60                  [NOUN]  \n",
       "61                          \n",
       "62                          \n",
       "63                          \n",
       "64                          \n",
       "65       [VERB, ADV, NOUN]  \n",
       "66                          \n",
       "67                          \n",
       "68                          \n",
       "69                 [PROPN]  \n",
       "70                          \n",
       "71                  [VERB]  \n",
       "72      [PROPN, ADP, NOUN]  \n",
       "73                          \n",
       "74        [NOUN, ADJ, ADP]  \n",
       "75    [PROPN, SCONJ, VERB]  \n",
       "76      [PROPN, ADP, NOUN]  \n",
       "77                  [VERB]  \n",
       "78     [NOUN, PUNCT, NOUN]  \n",
       "79                   [AUX]  \n",
       "80                          \n",
       "81                          \n",
       "82            [NOUN, NOUN]  \n",
       "83                  [NOUN]  \n",
       "84       [VERB, ADP, NOUN]  \n",
       "85      [ADP, NOUN, SCONJ]  \n",
       "86                          \n",
       "87                          \n",
       "88      [PROPN, ADP, NOUN]  \n",
       "89            [PROPN, ADV]  \n",
       "90      [PROPN, ADP, NOUN]  \n",
       "91        [ADJ, ADP, VERB]  \n",
       "92                          \n",
       "93                          \n",
       "94       [NOUN, ADP, NOUN]  \n",
       "95     [PROPN, ADP, PROPN]  \n",
       "96      [NOUN, ADP, PROPN]  \n",
       "97                  [VERB]  \n",
       "98                          \n",
       "99                  [VERB]  \n",
       "100      [ADP, VERB, VERB]  \n",
       "101     [VERB, NOUN, NOUN]  \n",
       "102                         \n",
       "103   [PROPN, PUNCT, NOUN]  \n",
       "104                         \n",
       "105      [NOUN, ADP, NOUN]  \n",
       "106            [NOUN, ADV]  \n",
       "107           [VERB, VERB]  \n",
       "108           [NOUN, VERB]  \n",
       "109                         \n",
       "110       [VERB, ADJ, ADJ]  \n",
       "111          [PROPN, VERB]  \n",
       "112       [NOUN, DET, SYM]  \n",
       "113                  [ADJ]  \n",
       "114                         \n",
       "115                         \n",
       "116     [PUNCT, NOUN, ADV]  \n",
       "117      [VERB, ADP, NOUN]  \n",
       "118     [PROPN, ADP, VERB]  \n",
       "119                         \n",
       "120                [PROPN]  \n",
       "121                         \n",
       "122                         \n",
       "123                         \n",
       "124                         \n",
       "125      [ADP, VERB, NOUN]  \n",
       "126                         \n",
       "127                         \n",
       "128                         \n",
       "129                 [VERB]  \n",
       "130     [PROPN, ADP, NOUN]  \n",
       "131     [NOUN, VERB, NOUN]  \n",
       "132     [PROPN, ADP, NOUN]  \n",
       "133      [VERB, ADP, NOUN]  \n",
       "134           [VERB, VERB]  \n",
       "135                 [VERB]  \n",
       "136            [ADJ, VERB]  \n",
       "137                         \n",
       "138     [NOUN, ADP, PUNCT]  \n",
       "139            [VERB, ADJ]  \n",
       "140                 [VERB]  \n",
       "141                 [NOUN]  \n",
       "142                         \n",
       "143                         \n",
       "144                         \n",
       "145      [NOUN, ADP, NOUN]  \n",
       "146                         \n",
       "147      [NOUN, ADP, NOUN]  \n",
       "148                [PROPN]  \n",
       "149                         \n",
       "150                         \n",
       "151      [VERB, VERB, ADJ]  \n",
       "152                  [AUX]  \n",
       "153    [PROPN, SCONJ, ADJ]  \n",
       "154                         \n",
       "155      [NOUN, VERB, ADP]  \n",
       "156                         \n",
       "157       [ADV, ADJ, VERB]  \n",
       "158           [VERB, VERB]  \n",
       "159                  [ADJ]  \n",
       "160    [PROPN, ADP, PROPN]  \n",
       "161                         \n",
       "162                         \n",
       "163                [PROPN]  \n",
       "164      [NOUN, ADJ, VERB]  \n",
       "165                 [NOUN]  \n",
       "166                         \n",
       "167                         \n",
       "168           [VERB, VERB]  \n",
       "169                         \n",
       "170                 [NOUN]  \n",
       "171                         \n",
       "172      [NOUN, ADP, VERB]  \n",
       "173                         \n",
       "174                         \n",
       "175      [PROPN, ADJ, ADJ]  \n",
       "176    [NOUN, CCONJ, NOUN]  \n",
       "177                         \n",
       "178      [NOUN, ADP, NOUN]  \n",
       "179                         \n",
       "180            [ADP, NOUN]  \n",
       "181            [NOUN, ADJ]  \n",
       "182                         \n",
       "183   [PROPN, NOUN, SCONJ]  \n",
       "184      [ADV, VERB, NOUN]  \n",
       "185      [NOUN, VERB, ADV]  \n",
       "186    [PROPN, NOUN, VERB]  \n",
       "187            [ADJ, NOUN]  \n",
       "188     [ADJ, NOUN, SCONJ]  \n",
       "189                  [AUX]  \n",
       "190                         \n",
       "191    [NOUN, NOUN, SCONJ]  \n",
       "192                         \n",
       "193                         \n",
       "194                         \n",
       "195                         \n",
       "196       [ADP, NOUN, ADP]  \n",
       "197                         \n",
       "198          [PROPN, VERB]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group on the POS of the anchor point, using allennlp dependency parsing (based on demo code)\n",
    "# I'm defining the \"POS of a phrase\" as the POS of the lowest node that contains the entire phrase\n",
    "def group_pos_anchor(row, context=1):\n",
    "    if row['split_anchor_span'] is None:\n",
    "        return dict(zip(grouping_headers, ['']))\n",
    "    p = dependency_predictor.predict(\n",
    "        sentence=' '.join(row['split_tokens']).strip()\n",
    "    )\n",
    "    # build out a more usable version of the dependency tree with information about tree level!\n",
    "    # tree_array[n] = (parent, level) so the root node is (0, 0)\n",
    "    tree_array = [(h, -1) for h in p['predicted_heads']]\n",
    "    need_connection = [0]\n",
    "    level = 0\n",
    "    while len(need_connection) > 0:\n",
    "        need_connection_update = []\n",
    "        for i in range(len(tree_array)):\n",
    "            if tree_array[i][0] in need_connection:\n",
    "                tree_array[i] = (tree_array[i][0], level)\n",
    "                need_connection_update.append(i+1)\n",
    "        need_connection = need_connection_update\n",
    "        level += 1\n",
    "    # Figure out what indexes our anchor fits into\n",
    "    # Assume that the anchor is contiguous text\n",
    "    # TODO: is this always true?\n",
    "    for i in range(len(p['words'])):\n",
    "        match = list_elements_match(\n",
    "            p['words'], row['split_tokens'], i, row['split_anchor_span'][0], \n",
    "            size2=row['split_anchor_span'][1]-row['split_anchor_span'][0])\n",
    "        if match is not None:\n",
    "            break\n",
    "    # Find the smallest containing dependency node\n",
    "    matching_nodes = [(i, tree_array[i][0], tree_array[i][1]) for i in range(match[0][0], match[0][1]+1)]\n",
    "    matching_nodes = list(set(matching_nodes))\n",
    "    while len(matching_nodes) > 1:\n",
    "        matching_nodes.sort(key=lambda x: x[2])\n",
    "        parent = matching_nodes.pop()[1]\n",
    "        matching_nodes.append((parent-1, tree_array[parent-1][0], tree_array[parent-1][1]))\n",
    "        matching_nodes = list(set(matching_nodes))\n",
    "    labeltiers = []\n",
    "    while len(labeltiers) < context:\n",
    "        labeltiers.append(p['pos'][matching_nodes[0][0]])\n",
    "        parent = matching_nodes[0][1]\n",
    "        if parent == 0:\n",
    "            break\n",
    "        matching_nodes[0] = (parent-1, tree_array[parent-1][0], tree_array[parent-1][1])\n",
    "    return dict(zip(grouping_headers, [labeltiers]))\n",
    "\n",
    "output = sample_input.apply(\n",
    "    lambda row: group_pos_anchor(row, context=3), \n",
    "    axis=1, result_type='expand')\n",
    "\n",
    "output = sample_input.join(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export whitespace-based split data\n",
    "output = df_sentences.join(whitespace_output).apply(\n",
    "    lambda row: group_first_word(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "sample_input.join(output).to_csv(f'outputs/whitespace_firstword.csv')\n",
    "\n",
    "output = df_sentences.join(whitespace_output).apply(\n",
    "    lambda row: group_first_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "sample_input.join(output).to_csv(f'outputs/whitespace_firstverb.csv')\n",
    "\n",
    "output = df_sentences.join(whitespace_output).apply(\n",
    "    lambda row: group_main_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "sample_input.join(output).to_csv(f'outputs/whitespace_mainverb.csv')\n",
    "\n",
    "output = df_sentences.join(whitespace_output).apply(\n",
    "    lambda row: group_pos_anchor(row, context=3), \n",
    "    axis=1, result_type='expand')\n",
    "sample_input.join(output).to_csv(f'outputs/whitespace_anchorpos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df_sentences.join(whitespace_output).apply(\n",
    "    lambda row: group_pos_anchor(row, context=3), \n",
    "    axis=1, result_type='expand')\n",
    "sample_input.join(output).to_csv(f'outputs/whitespace_anchorpos.csv')\n",
    "\n",
    "output = df_sentences.join(coreference_output).apply(\n",
    "    lambda row: group_pos_anchor(row, context=3), \n",
    "    axis=1, result_type='expand')\n",
    "sample_input.join(output).to_csv(f'outputs/coreference_anchorpos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export coreference-based split data\n",
    "output = df_sentences.join(coreference_output).apply(\n",
    "    lambda row: group_first_word(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "sample_input.join(output).to_csv(f'outputs/coreference_firstword.csv')\n",
    "\n",
    "output = df_sentences.join(coreference_output).apply(\n",
    "    lambda row: group_first_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "sample_input.join(output).to_csv(f'outputs/coreference_firstverb.csv')\n",
    "\n",
    "output = df_sentences.join(coreference_output).apply(\n",
    "    lambda row: group_main_verb(row), \n",
    "    axis=1, result_type='expand').sort_values(by=['group'])\n",
    "sample_input.join(output).to_csv(f'outputs/coreference_mainverb.csv')\n",
    "\n",
    "output = df_sentences.join(coreference_output).apply(\n",
    "    lambda row: group_pos_anchor(row, context=3), \n",
    "    axis=1, result_type='expand')\n",
    "sample_input.join(output).to_csv(f'outputs/coreference_anchorpos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "pd.read_csv(f'outputs/coreference_mainverb.csv').sort_values(by=['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
